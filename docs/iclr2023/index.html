<!doctype html>

<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    
    <link rel="icon" href="data:,"> <!-- favicon workaound -->

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="/static/style.css">

    <title> ICLR 2023 </title>

    <meta name="author" content="Tanel Pärnamaa">
    <meta property="og:type" content="website">

    
<meta name="description" content="A list of all iclr2023 papers ordered by rating.">
<meta property="og:title" content="iclr2023 papers">
<meta property="og:description" content="A list of all iclr2023 papers ordered by rating.">
<script id="mcjs">!function(c,h,i,m,p){m=c.createElement(h),p=c.getElementsByTagName(h)[0],m.async=1,m.src=i,p.parentNode.insertBefore(m,p)}(document,"script","https://chimpstatic.com/mcjs-connected/js/users/acb8de833508129c5c50e479e/6a420da2de6de01e2c5443ce7.js");</script>


</head>

<body>
    <nav class="navbar navbar-expand-lg">
        <div class="container"> <!-- center -->
            <div class="navcontainer"> <!-- maxwidth -->
                <a class="navbar-brand" href="/"> ICLR 2023 papers </a>
                <span id="about-link"><a href="/about">About</a></span>
                <!-- <p style="color: #8d949b; margin-bottom: 0px; margin-top: -5px;">Bird's-eye views of conference proceedings.</p> -->
            </div>    
        </div>
    </nav>
    
<div class="container papers">
    
        <div class="paperrow paper">
            <div class="title">
                1. <h><a href=https://openreview.net/pdf?id=88nT0j5jAn>Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Donggyun Kim, Jinwoo Kim, Seongwoong Cho, Chong Luo, Seunghoon Hong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=88nT0j5jAn>9.33 - [10, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> a universal few-shot learner for general dense prediction tasks
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/88nT0j5jAn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dense prediction tasks are a fundamental class of problems in computer vision. As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired. Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. It employs non-parametric matching on patch-level embedded tokens of images and labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm. We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks ($0.004\%$ of full supervision) and sometimes outperforms using $0.1\%$ of full supervision. Codes are available at https://github.com/GitGyun/visual_token_matching.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                2. <h><a href=https://openreview.net/pdf?id=lTt4KjHSsyl>Emergence of Maps in the Memories of Blind Navigation Agents</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S. Morcos, Dhruv Batra</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lTt4KjHSsyl>9.0 - [10, 10, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> ‘Blind’ AI navigation agents (with only egomotion sensing) can learn to navigate new environments and build map-like representations (supporting the ability to take shortcuts, follow walls, and predict free-space and collisions) of their environment.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lTt4KjHSsyl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Animal navigation research posits that organisms build and maintain internal spa- tial representations, or maps, of their environment. We ask if machines – specifically, artificial intelligence (AI) navigation agents – also build implicit (or ‘mental’) maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent’s perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train ‘blind’ agents – with sensing limited to only egomotion and no other sensing of any kind – to perform PointGoal navigation (‘go to $\Delta$x, $\Delta$y’) via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments (∼95% success); (2) they utilize memory over long horizons (remembering ∼1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent ‘forgets’ exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                3. <h><a href=https://openreview.net/pdf?id=Uuf2q9TfXGA>Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyuan Allen-Zhu, Yuanzhi Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Uuf2q9TfXGA>9.0 - [8, 10, 10, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a theory to explain why ensemble and knowledge distillation work for Deep Learning. It matches practice well, while traditional theory such as boosting, random feature mappings or NTKs, cannot explain the same phenomena for DL.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Uuf2q9TfXGA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We formally study how \emph{ensemble} of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using \emph{knowledge distillation}. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the \emph{same} architecture, trained using the \emph{same} algorithm on the \emph{same} data set, and they only differ by the random seeds used in the initialization.

We show that ensemble/knowledge distillation in \emph{deep learning} works very differently from traditional learning theory (such as boosting or NTKs). We develop a theory showing that when data has a structure we refer to as ``multi-view&#39;&#39;, then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the ``dark knowledge&#39;&#39; is hidden in the outputs of the ensemble and can be used in distillation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                4. <h><a href=https://openreview.net/pdf?id=zEn1BhaNYsC>Minimax Optimal Kernel Operator Learning via Multilevel Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jikai Jin, Yiping Lu, Jose Blanchet, Lexing Ying</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zEn1BhaNYsC>8.8 - [10, 8, 8, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zEn1BhaNYsC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning mappings between infinite-dimensional function spaces have achieved empirical success in many disciplines of machine learning, including generative modeling, functional data analysis, causal inference, and multi-agent reinforcement learning. In this paper, we study the statistical limit of learning a Hilbert-Schmidt operator between two infinite-dimensional Sobolev reproducing kernel Hilbert spaces. We establish the information-theoretic lower bound in terms of the Sobolev Hilbert-Schmidt norm and show that a regularization that learns the spectral components below the bias contour and ignores the ones above the variance contour can achieve the optimal learning rate. At the same time, the spectral components between the bias and variance contours give us flexibility in designing computationally feasible machine learning algorithms. Based on this observation, we develop a multilevel kernel operator learning algorithm that is optimal when learning linear operators between infinite-dimensional function spaces.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                5. <h><a href=https://openreview.net/pdf?id=VELL0PlWfc>Tailoring Language Generation Models under Total Variation Distance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haozhe Ji, Pei Ke, Zhipeng Hu, Rongsheng Zhang, Minlie Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VELL0PlWfc>8.67 - [10, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We analyze total variation distance (TVD) as a robust metric to outliers and devise a new training objective based on TVD to alleviate text degeneration and improve the generation quality.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VELL0PlWfc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The standard paradigm of neural language generation adopts maximum likelihood estimation (MLE) as the optimizing method. From a distributional view, MLE in fact minimizes the Kullback-Leibler divergence (KLD) between the distribution of the real data and that of the model. However, this approach forces the model to distribute non-zero (sometimes large) probability mass to all training samples regardless of their quality. Moreover, in the attempt to cover the low-probability regions in the data distribution, the model systematically overestimates the probability of corrupted text sequences, which we conjecture is one of the main reasons for text degeneration during autoregressive decoding. To remedy this problem, we leverage the total variation distance (TVD) with its robustness to outliers, and develop practical bounds to apply it to language generation. Then, we introduce the TaiLr objective that balances the tradeoff of estimating TVD. Intuitively, TaiLr downweights real data samples that have low model probabilities with tunable penalization intensity. Experimental results show that our method alleviates the overestimation of degenerated sequences without sacrificing diversity and improves generation quality on a wide range of text generation tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                6. <h><a href=https://openreview.net/pdf?id=FeWvD0L_a4>Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiajun Fan, Yuzheng Zhuang, Yuecheng Liu, Jianye HAO, Bin Wang, Jiangcheng Zhu, Hao Wang, Shu-Tao Xia</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FeWvD0L_a4>8.67 - [8, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We have constructed a general framework to control the behaviors in RL  and achieved SOTA performance in Atari 1B benchmark.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FeWvD0L_a4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity.  In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                7. <h><a href=https://openreview.net/pdf?id=r9hNv76KoT3>Rethinking the Expressive Power of GNNs via Graph Biconnectivity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bohang Zhang, Shengjie Luo, Liwei Wang, Di He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=r9hNv76KoT3>8.67 - [8, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/r9hNv76KoT3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs with respect to the Weisfeiler-Lehman (WL) test, for most of them, there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework (Bevilacqua et al., 2022), for which we give a theoretical justification of its power. We proceed to introduce a principled and more efficient approach, called the Generalized Distance Weisfeiler-Lehman (GD-WL), which is provably expressive for all biconnectivity metrics. Practically, we show GD-WL can be implemented by a Transformer-like architecture that preserves expressiveness and enjoys full parallelizability. A set of experiments on both synthetic and real datasets demonstrates that our approach can consistently outperform prior GNN architectures.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                8. <h><a href=https://openreview.net/pdf?id=-9PVqZ-IR_>Martingale Posterior Neural Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyungi Lee, Eunggu Yun, Giung Nam, Edwin Fong, Juho Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-9PVqZ-IR_>8.67 - [10, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Martingale Posterior Distribution, Neural Processes
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-9PVqZ-IR_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A Neural Process (NP) estimates a stochastic process implicitly defined with neural networks given a stream of data, rather than pre-specifying priors already known, such as Gaussian processes. An ideal NP would learn everything from data without any inductive biases, but in practice, we often restrict the class of stochastic processes for the ease of estimation. One such restriction is the use of a finite-dimensional latent variable accounting for the uncertainty in the functions drawn from NPs. Some recent works show that this can be improved with more “data-driven” source of uncertainty such as bootstrapping. In this work, we take a different approach based on the martingale posterior, a recently developed alternative to Bayesian inference. For the martingale posterior, instead of specifying prior-likelihood pairs, a predictive distribution for future data is specified. Under specific conditions on the predictive distribution, it can be shown that the uncertainty in the generated future data actually corresponds to the uncertainty of the implicitly defined Bayesian posteriors. Based on this result, instead of assuming any form of the latent variables, we equip a NP with a predictive distribution implicitly defined with neural networks and use the corresponding martingale posteriors as the source of uncertainty. The resulting model, which we name as Martingale Posterior Neural Process (MPNP), is demonstrated to outperform baselines on various tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                9. <h><a href=https://openreview.net/pdf?id=VZX2I_VVJKH>Learning multi-scale local conditional probability models of images</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zahra Kadkhodaie, Florentin Guth, Stéphane Mallat, Eero P Simoncelli</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VZX2I_VVJKH>8.67 - [8, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a spatially Markov wavelet conditional probability model for images, and demonstrate (through, denoising, super-resolution and synthesis) its effectiveness in capturing global dependencies.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VZX2I_VVJKH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks can learn powerful prior probability models for images, as evidenced by the high-quality generations obtained with recent score-based diffusion methods. But the means by which these networks capture complex global statistical structure, apparently without suffering from the curse of dimensionality, remain a mystery. To study this, we incorporate diffusion methods into a multi-scale decomposition, reducing dimensionality by assuming a stationary local Markov model for wavelet coefficients conditioned on coarser-scale coefficients. We instantiate this model using convolutional neural networks (CNNs) with local receptive fields, which enforce both the stationarity and Markov properties. Global structures are captured using a CNN with receptive fields covering the entire (but small) low-pass image. We test this model on a dataset of face images, which are highly non-stationary and contain large-scale geometric structures.
Remarkably, denoising, super-resolution, and image synthesis results all demonstrate that these structures can be captured with significantly smaller conditioning neighborhoods than required by a Markov model implemented in the pixel domain. Our results show that score estimation for large complex images can be reduced to low-dimensional Markov conditional models across scales,  alleviating the curse of dimensionality. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                10. <h><a href=https://openreview.net/pdf?id=EKpMeEV0hOo>SimPer: Simple Self-Supervised Learning of Periodic Targets</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuzhe Yang, Xin Liu, Jiang Wu, Silviu Borac, Dina Katabi, Ming-Zher Poh, Daniel McDuff</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EKpMeEV0hOo>8.67 - [10, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A simple contrastive self-supervised framework for learning periodic targets and tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EKpMeEV0hOo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    From human physiology to environmental evolution, important processes in nature often exhibit meaningful and strong periodic or quasi-periodic changes. Due to their inherent label scarcity, learning useful representations for periodic tasks with limited or no supervision is of great benefit. Yet, existing self-supervised learning (SSL) methods overlook the intrinsic periodicity in data, and fail to learn representations that capture periodic or frequency attributes. In this paper, we present SimPer, a simple contrastive SSL regime for learning periodic information in data. To exploit the periodic inductive bias, SimPer introduces customized augmentations, feature similarity measures, and a generalized contrastive loss for learning efficient and robust periodic representations. Extensive experiments on common real-world tasks in human behavior analysis, environmental sensing, and healthcare domains verify the superior performance of SimPer compared to state-of-the-art SSL methods, highlighting its intriguing properties including better data efficiency, robustness to spurious correlations, and generalization to distribution shifts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                11. <h><a href=https://openreview.net/pdf?id=CQsmMYmlP5T>Git Re-Basin: Merging Models modulo Permutation Symmetries</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samuel Ainsworth, Jonathan Hayase, Siddhartha Srinivasa</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CQsmMYmlP5T>8.67 - [8, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CQsmMYmlP5T.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The success of deep learning is due in large part to our ability to solve certain massive non-convex optimization problems with relative ease. Though non-convex optimization is NP-hard, simple algorithms -- often variants of stochastic gradient descent -- exhibit surprising effectiveness in fitting large neural networks in practice. We argue that neural network loss landscapes often contain (nearly) a single basin after accounting for all possible permutation symmetries of hidden units a la Entezari et al. 2021. We introduce three algorithms to permute the units of one model to bring them into alignment with a reference model in order to merge the two models in weight space. This transformation produces a functionally equivalent set of weights that lie in an approximately convex basin near the reference model. Experimentally, we demonstrate the single basin phenomenon across a variety of model architectures and datasets, including the first (to our knowledge) demonstration of zero-barrier linear mode connectivity between independently trained ResNet models on CIFAR-10. Additionally, we identify intriguing phenomena relating model width and training time to mode connectivity. Finally, we discuss shortcomings of the linear mode connectivity hypothesis, including a counterexample to the single basin theory.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                12. <h><a href=https://openreview.net/pdf?id=TfBHFLgv77>Hyperbolic Deep Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Edoardo Cetin, Benjamin Paul Chamberlain, Michael M. Bronstein, Jonathan J Hunt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TfBHFLgv77>8.67 - [10, 10, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use hyperbolic space to model the latent representations of deep RL algorithms, attaining great performance and generalization benefits.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TfBHFLgv77.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In deep reinforcement learning (RL), useful information about the state is inherently tied to its possible future successors. Consequently, encoding features that capture the hierarchical relationships between states into the model&#39;s latent representations is often conducive to recovering effective policies. In this work, we study a new class of deep RL algorithms that promote encoding such relationships by using hyperbolic space to model latent representations. However, we find that a naive application of existing methodology from the hyperbolic deep learning literature leads to fatal instabilities due to the non-stationarity and variance characterizing common gradient estimators in RL. Hence, we design a new general method that directly addresses such optimization challenges and enables stable end-to-end learning with deep hyperbolic representations. We empirically validate our framework by applying it to popular on-policy and off-policy RL algorithms on the Procgen and Atari 100K benchmarks, attaining near universal performance and generalization benefits. Given its natural fit, we hope this work will inspire future RL research to consider hyperbolic representations as a standard tool.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                13. <h><a href=https://openreview.net/pdf?id=SrC-nwieGJ>Relative representations enable zero-shot latent space communication</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, Emanuele Rodolà</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SrC-nwieGJ>8.67 - [8, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Relative representations can be leveraged to enable solving tasks regarding &#34;latent communication&#34;: from zero-shot model stitching to latent space comparison between diverse settings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SrC-nwieGJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change. In this work, we propose the latent similarity between each sample and a fixed set of anchors as an alternative data representation, demonstrating that it can enforce the desired invariances without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                14. <h><a href=https://openreview.net/pdf?id=l6CpxixmUg>Modeling content creator incentives on algorithm-curated platforms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiri Hron, Karl Krauth, Michael Jordan, Niki Kilbertus, Sarah Dean</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=l6CpxixmUg>8.67 - [8, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Algorithmic choices in modern recommenders may have significant and unexpected effects on content creator incentives.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/l6CpxixmUg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Content creators compete for user attention. Their reach crucially depends on algorithmic choices made by developers on online platforms. To maximize exposure, many creators adapt strategically, as evidenced by examples like the sprawling search engine optimization industry. This begets competition for the finite user attention pool. We formalize these dynamics in what we call an exposure game, a model of incentives induced by modern algorithms including factorization and (deep) two-tower architectures. We prove that seemingly innocuous algorithmic choices—e.g., non-negative vs. unconstrained factorization—significantly affect the existence and character of (Nash) equilibria in exposure games. We proffer use of creator behavior models like ours for an (ex-ante) pre-deployment audit. Such an audit can identify misalignment between desirable and incentivized content, and thus complement post-hoc measures like content filtering and moderation. To this end, we propose tools for numerically finding equilibria in exposure games, and illustrate results of an audit on the MovieLens and LastFM datasets. Among else, we find that the strategically produced content exhibits strong dependence between algorithmic exploration and content diversity, and between model expressivity and bias towards gender-based user and creator groups.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                15. <h><a href=https://openreview.net/pdf?id=YnkGMIh0gvX>A Call to Reflect on Evaluation Practices for Failure Detection in Image Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Paul F Jaeger, Carsten Tim Lüth, Lukas Klein, Till J. Bungert</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YnkGMIh0gvX>8.67 - [8, 10, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a holistic perspective on the task of failure detection including a large-scale empirical study for the first time enabling benchmarking confidence scoring functions w.r.t all relevant methods and distribution shifts. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YnkGMIh0gvX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reliable application of machine learning-based decision systems in the wild is one of the major challenges currently investigated by the field. A large portion of established approaches aims to detect erroneous predictions by means of assigning confidence scores. This confidence may be obtained by either quantifying the model&#39;s predictive uncertainty, learning explicit scoring functions, or assessing whether the input is in line with the training distribution. Curiously, while these approaches all state to address the same eventual goal of detecting failures of a classifier upon real-world application, they currently constitute largely separated research fields with individual evaluation protocols, which either exclude a substantial part of relevant methods or ignore large parts of relevant failure sources. In this work, we systematically reveal current pitfalls caused by these inconsistencies and derive requirements for a holistic and realistic evaluation of failure detection. To demonstrate the relevance of this unified perspective, we present a large-scale empirical study for the first time enabling benchmarking confidence scoring functions w.r.t all relevant methods and failure sources. The revelation of a simple softmax response baseline as the overall best performing method underlines the drastic shortcomings of current evaluation in the plethora of publicized research on confidence scoring. Code and trained models are at https://github.com/https://github.com/IML-DKFZ/fd-shifts
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                16. <h><a href=https://openreview.net/pdf?id=SNgLnzFQeiD>Revisiting the Entropy Semiring for Neural Speech Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Oscar Chang, Dongseong Hwang, Olivier Siohan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SNgLnzFQeiD>8.5 - [10, 6, 8, 10] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A numerically stable open-source implementation of the entropy semiring for CTC and RNN-T; obtained SOTA on Librispeech streaming.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SNgLnzFQeiD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In streaming settings, speech recognition models have to map sub-sequences of speech to text before the full audio stream becomes available. However, since alignment information between speech and text is rarely available during training, models need to learn it in a completely self-supervised way. In practice, the exponential number of possible alignments makes this extremely challenging, with models often learning peaky or sub-optimal alignments. Prima facie, the exponential nature of the alignment space makes it difficult to even quantify the uncertainty of a model&#39;s alignment distribution. Fortunately, it has been known for decades that the entropy of a probabilistic finite state transducer can be computed in time linear to the size of the transducer via a dynamic programming reduction based on semirings. In this work, we revisit the entropy semiring for neural speech recognition models, and show how alignment entropy can be used to supervise models through regularization or distillation. We also contribute an open-source implementation of CTC and RNN-T in the semiring framework that includes numerically stable and highly parallel variants of the entropy semiring. Empirically, we observe that the addition of alignment distillation improves the accuracy and latency of an already well-optimized teacher-student distillation model, achieving state-of-the-art performance on the Librispeech dataset in the streaming scenario.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                17. <h><a href=https://openreview.net/pdf?id=fB0hRu9GZUS>Generate rather than Retrieve: Large Language Models are Strong Context Generators</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fB0hRu9GZUS>8.5 - [8, 8, 10, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel generate-then-read pipeline for solving knowledge-intensive tasks by prompting a large language model to generate relevant contextual documents.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fB0hRu9GZUS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that first retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GenRead), which first prompts a large language model to generate contextual documents based on a given question, and then reads the generated documents to produce the final answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, in order to generate diverse documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GenRead achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, significantly outperforming the state-of-the-art retrieve-then-read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                18. <h><a href=https://openreview.net/pdf?id=C-xa_D3oTj6>DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pierre Schumacher, Daniel Haeufle, Dieter Büchler, Syn Schmitt, Georg Martius</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=C-xa_D3oTj6>8.5 - [8, 8, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A technique from the self-organization literature is used to improve performance of RL agents on overactuated systems with up to 120 muscle actuators.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/C-xa_D3oTj6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Muscle-actuated organisms are capable of learning an unparalleled diversity of dexterous movements despite their vast amount of muscles. 
Reinforcement learning (RL) on large musculoskeletal models, however, has not been able to show similar performance.  
We conjecture that ineffective exploration in large overactuated action spaces is a key problem.
This is supported by the finding that common exploration noise strategies are inadequate in synthetic examples of overactuated systems. 
We identify differential extrinsic plasticity (DEP), a method from the domain of self-organization, as being able to induce state-space covering exploration within seconds of interaction. 
By integrating DEP into RL, we achieve fast learning of reaching and locomotion in musculoskeletal systems, outperforming current approaches in all considered tasks in sample efficiency and robustness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                19. <h><a href=https://openreview.net/pdf?id=m1oqEOAozQU>Graph Neural Networks for Link Prediction with Subgraph Sketching</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca, Thomas Markovich, Nils Yannick Hammerla, Michael M. Bronstein, Max Hansmire</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=m1oqEOAozQU>8.5 - [10, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method that solves the expressivity issues that plague most MPNNs for link prediction while being as efficient to run as GCN. This is achieved by passing subgraph sketches as messages.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/m1oqEOAozQU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate the key components of SGNNs without explicit subgraph construction. ELPH is provably more expressive than Message Passing GNNs (MPNNs).  It outperforms existing SGNN models on many standard LP benchmarks while being orders of magnitude faster. However, it shares the common GNN limitation that it is only efficient when the dataset fits in GPU memory. Accordingly, we develop a highly scalable model, called BUDDY, which uses feature precomputation to circumvent this limitation without sacrificing predictive performance. Our experiments show that BUDDY also outperforms SGNNs on standard LP benchmarks while being highly scalable and faster than ELPH.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                20. <h><a href=https://openreview.net/pdf?id=awnvqZja69>Image as Set of Points</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xu Ma, Yuqian Zhou, Huan Wang, Can Qin, Bin Sun, Chang Liu, Yun Fu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=awnvqZja69>8.5 - [8, 8, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce Context Cluster, a new paradigm that considers an image as a set of point and employs clustering method for feature extraction.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/awnvqZja69.jpg loading="lazy">
                </div>
                <div class="abstract">
                    
What is an image, and how to extract latent features? 
Convolutional Networks (ConvNets) consider an image as organized pixels in a rectangular shape and extract features via convolutional operation in a local region; Vision Transformers (ViTs) treat an image as a sequence of patches and extract features via attention mechanism in a global range. In this work, we introduce a straightforward and promising paradigm for visual representation, which is called Context Clusters. Context clusters (CoCs) view an image as a set of unorganized points and extract features via a simplified clustering algorithm. In detail, each point includes the raw feature (e.g., color) and positional information (e.g., coordinates), and a simplified clustering algorithm is employed to group and extract deep features hierarchically. Our CoCs are convolution- and attention-free, only relying on clustering algorithm for spatial interaction. Owing to the simple design, we show CoCs endow gratifying interpretability via the visualization of the clustering process.  
Our CoCs aim at providing a new perspective on image and visual representation, which may enjoy broad applications in different domains and exhibit profound insights. Even though we are not targeting SOTA performance, COCs still achieve comparable or even better performance than ConvNets or ViTs on several benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                21. <h><a href=https://openreview.net/pdf?id=eQzLwwGyQrb>Can We Find Nash Equilibria at a Linear Rate in Markov Games?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhuoqing Song, Jason D. Lee, Zhuoran Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eQzLwwGyQrb>8.5 - [8, 8, 10, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A decentralized algorithm for finding Nash equilibria in two-player zero-sum discounted Markov games with global linear convergence.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eQzLwwGyQrb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study decentralized learning in two-player zero-sum discounted Markov games where the goal is to design a policy optimization algorithm for either agent satisfying two properties. First, the player does not need to know the policy of the opponent to update its policy. Second, when both players adopt the algorithm, their joint policy converges to a Nash equilibrium of the game. To this end, we construct a meta-algorithm, dubbed as $\texttt{Homotopy-PO}$, which provably finds a Nash equilibrium at a global linear rate. In particular, $\texttt{Homotopy-PO}$ interweaves two base algorithms $\texttt{Local-Fast}$ and $\texttt{Global-Slow}$ via homotopy continuation. $\texttt{Local-Fast}$ is an algorithm that enjoys local linear convergence while $\texttt{Global-Slow}$ is an algorithm that converges globally but at a slower sublinear rate. By switching between these two base algorithms, $\texttt{Global-Slow}$ essentially serves as a ``guide&#39;&#39; which identifies a benign neighborhood where $\texttt{Local-Fast}$ enjoys fast convergence. However, since the exact size of such a neighborhood is unknown, we apply a doubling trick to switch between these two base algorithms. The switching scheme is delicately designed so that the aggregated performance of the algorithm is driven by $\texttt{Local-Fast}$. Furthermore, we prove that $\texttt{Local-Fast}$ and $\texttt{Global-Slow}$ can both be instantiated by variants of optimistic gradient descent/ascent (OGDA) method, which is of independent interest.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                22. <h><a href=https://openreview.net/pdf?id=IM4xp7kGI5V>The Asymmetric Maximum Margin Bias of Quasi-Homogeneous Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Kunin, Atsushi Yamamura, Chao Ma, Surya Ganguli</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IM4xp7kGI5V>8.5 - [8, 8, 10, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We generalize implicit max-margin bias to a class of models which describes nearly all networks, identifying a competition between maximizing margin and minimizing an asymmetric parameter norm, which can degrade robustness and explain Neural Collapse
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IM4xp7kGI5V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, we explore the maximum-margin bias of quasi-homogeneous neural networks trained with gradient flow on an exponential loss and past a point of separability. We introduce the class of quasi-homogeneous models, which is expressive enough to describe nearly all neural networks with homogeneous activations, even those with biases, residual connections, and normalization layers, while structured enough to enable geometric analysis of its gradient dynamics. Using this analysis, we generalize the existing results of maximum-margin bias for homogeneous networks to this richer class of models. We find that gradient flow implicitly favors a subset of the parameters, unlike in the case of a homogeneous model where all parameters are treated equally. We demonstrate through simple examples how this strong favoritism toward minimizing an asymmetric norm can degrade the robustness of quasi-homogeneous models. On the other hand, we conjecture that this norm-minimization discards, when possible, unnecessary higher-order parameters, reducing the model to a sparser parameterization. Lastly, by applying our theorem to sufficiently expressive neural networks with normalization layers, we reveal a universal mechanism behind the empirical phenomenon of Neural Collapse.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                23. <h><a href=https://openreview.net/pdf?id=J7Uh781A05p>Learning rigid dynamics with face interaction graph networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kelsey R Allen, Yulia Rubanova, Tatiana Lopez-Guevara, William F Whitney, Alvaro Sanchez-Gonzalez, Peter Battaglia, Tobias Pfaff</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=J7Uh781A05p>8.5 - [6, 8, 10, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Face to face, multi-index collisions improve accuracy and efficiency of graph network models for rigid body dynamics
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/J7Uh781A05p.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Simulating rigid collisions among arbitrary shapes is notoriously difficult due to complex geometry and the strong non-linearity of the interactions. While graph neural network (GNN)-based models are effective at learning to simulate complex physical dynamics, such as fluids, cloth and articulated bodies, they have been less effective and efficient on rigid-body physics, except with very simple shapes. Existing methods that model collisions through the meshes&#39; nodes are often inaccurate because they struggle when collisions occur on faces far from nodes. Alternative approaches that represent the geometry densely with many particles are prohibitively expensive for complex shapes. Here we introduce the ``Face Interaction Graph Network&#39;&#39; (FIGNet) which extends beyond GNN-based methods, and computes interactions between mesh faces, rather than nodes. Compared to learned node- and particle-based methods, FIGNet is around 4x more accurate in simulating complex shape interactions, while also 8x more computationally efficient on sparse, rigid meshes. Moreover, FIGNet can learn frictional dynamics directly from real-world data, and can be more accurate than analytical solvers given modest amounts of training data. FIGNet represents a key step forward in one of the few remaining physical domains which have seen little competition from learned simulators, and offers allied fields such as robotics, graphics and mechanical design a new tool for simulation and model-based planning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                24. <h><a href=https://openreview.net/pdf?id=j6zUzrapY3L>DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=j6zUzrapY3L>8.0 - [10, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce an energy constrained diffusion model for semi-supervised representation learning, based on which a new class of nerual encoders is derived for efficiently and effectively learning inter-instance latent graphs
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/j6zUzrapY3L.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances&#39; information by their interactions. The diffusion process is constrained by descent criteria w.r.t. a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instance numbers, and an advanced version for learning complex structures. Experiments highlight the wide applicability of our model as a general-purpose encoder backbone with superior performance in various tasks, such as node classification on large graphs, semi-supervised image/text classification, and spatial-temporal dynamics prediction. The codes are available at https://github.com/qitianwu/DIFFormer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                25. <h><a href=https://openreview.net/pdf?id=7mgUec-7GMv>Mitigating Dataset Bias by Using Per-Sample Gradient</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sumyeong Ahn, Seongyoon Kim, Se-Young Yun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7mgUec-7GMv>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We solve the dataset bias problem by using the per-sample gradient. Furthermore, we provide the mathematical background of the proposed algorithm.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7mgUec-7GMv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The performance of deep neural networks is strongly influenced by the training dataset setup. In particular, when attributes with a strong correlation with the target attribute are present, the trained model can provide unintended prejudgments and show significant inference errors (i.e., the dataset bias problem). Various methods have been proposed to mitigate dataset bias, and their emphasis is on weakly correlated samples, called bias-conflicting samples. These methods are based on explicit bias labels provided by humans. However, such methods require human costs. Recently, several studies have sought to reduce human intervention by utilizing the output space values of neural networks, such as feature space, logits, loss, or accuracy. However, these output space values may be insufficient for the model to understand the bias attributes well. In this study, we propose a debiasing algorithm leveraging gradient called Per-sample Gradient-based Debiasing (PGD). PGD is comprised of three steps: (1) training a model on uniform batch sampling, (2) setting the importance of each sample in proportion to the norm of the sample gradient, and (3) training the model using importance-batch sampling, whose probability is obtained in step (2). Compared with existing baselines for various datasets, the proposed method showed state-of-the-art accuracy for the classification task. Furthermore, we describe theoretical understandings of how PGD can mitigate dataset bias. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                26. <h><a href=https://openreview.net/pdf?id=rJcLocAJpA6>On Compositional Uncertainty Quantification for Seq2seq Graph Parsing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zi Lin, Du Phan, Panupong Pasupat, Jeremiah Zhe Liu, Jingbo Shang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rJcLocAJpA6>8.0 - [10, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we aim to quantify and evaluate compositional uncertainty for seq2seq graph parsing by proposing a simple probabilistic framework and rigorous evaluation metrics.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rJcLocAJpA6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent years have witnessed the success of applying seq2seq models to graph parsing tasks, where the outputs are compositionally structured (e.g., a graph or a tree). However, these seq2seq approaches pose a challenge in quantifying the model’s compositional uncertainty on graph structures due to the gap between seq2seq output probability and structural probability on the graph. This work is the first to quantify and evaluate compositional uncertainty for seq2seq graph parsing tasks. First, we proposed a generic, probabilistically interpretable framework that allows correspondences between seq2seq output probability to structural probability on the graph. This framework serves as a powerful medium for quantifying a seq2seq model&#39;s compositional uncertainty on graph elements (i.e., nodes or edges). Second, to evaluate uncertainty quality in terms of calibration, we propose a novel metric called Compositional Expected Calibration Error (CECE) which can measure a model’s calibration behavior in predicting graph structures. By a thorough evaluation for compositional uncertainty on three different tasks across ten domains, we demonstrate that CECE is a better reflection for distributional shift compared to vanilla sequence ECE. Finally, we validate the effectiveness of compositional uncertainty considering the task of collaborative semantic parsing, where the model is allowed to send limited subgraphs for human review. The results show that the collaborative performance based on uncertain subgraph selection consistently outperforms random subgraph selection (30% average error reduction rate) and performs comparably to oracle subgraph selection (only 0.33 difference in average prediction error), indicating that compositional uncertainty is an ideal signal for model errors and can benefit various downstream tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                27. <h><a href=https://openreview.net/pdf?id=0Ij9_q567Ma>Targeted Hyperparameter Optimization with Lexicographic Preferences Over Multiple Objectives</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shaokun Zhang, Feiran Jia, Chi Wang, Qingyun Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0Ij9_q567Ma>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Hyperparameter tuning under lexicographic preference
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0Ij9_q567Ma.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Motivated by various practical applications, we propose a novel and general formulation of targeted multi-objective hyperparameter optimization. Our formulation allows a clear specification of an automatable optimization goal using lexicographic preference over multiple objectives. We then propose a randomized directed search method named LexiFlow to solve this problem. We demonstrate the strong empirical performance of the proposed algorithm in multiple hyperparameter optimization tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                28. <h><a href=https://openreview.net/pdf?id=CYK7RfcOzQ4>AudioGen: Textually Guided Audio Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, Yossi Adi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CYK7RfcOzQ4>8.0 - [8, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a text-to-audio generation model
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CYK7RfcOzQ4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, we tackle the problem of generating audio samples conditioned on descriptive text captions. We propose AudioGen, an auto-regressive generative model, operating on a learnt discrete audio representation, that generates audio samples conditioned on text inputs. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects&#39;&#39; can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high fidelity audio requires one to operate over extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. Finally, we apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. We further conduct an ablation study to gauge the effects of pre-trained text and audio components.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                29. <h><a href=https://openreview.net/pdf?id=nZ2NtpolC5->The Influence of Learning Rule on Representation Dynamics in Wide Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Blake Bordelon, Cengiz Pehlevan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nZ2NtpolC5->8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A theoretical analysis of deep networks and their representations when trained with a variety of learning rules.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nZ2NtpolC5-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologically-plausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel&#39;s evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy limit, we find that DFA and Hebb can only learn using the last layer features, while full FA can utilize earlier layers with a scale determined by the initial correlation between feedforward and feedback weight matrices. In the rich regime, DFA and FA utilize a temporally evolving and depth-dependent NTK. Counterintuitively, we find that FA networks trained in the rich regime exhibit more feature learning if initialized with smaller correlation between the forward and backward pass weights. GLNs admit a very simple formula for their lazy limit kernel and preserve conditional Gaussianity of their preactivations under gating functions. Error modulated Hebb rules show very small task-relevant alignment of their kernels and perform most task relevant learning in the last layer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                30. <h><a href=https://openreview.net/pdf?id=Nc1ZkRW8Vde>Near-optimal Coresets for Robust Clustering</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lingxiao Huang, Shaofeng H.-C. Jiang, Jianing Lou, Xuan Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Nc1ZkRW8Vde>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We obtain an \epsilon-coreset of near-optimal size for (k, z)-clustering (which includes k-median and k-means) with m outliers
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Nc1ZkRW8Vde.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider robust clustering problems in $\mathbb{R}^d$, specifically $k$-clustering problems (e.g., $k$-Median and $k$-Means) with $m$ \emph{outliers}, where the cost for a given center set $C \subset \mathbb{R}^d$ aggregates the distances from $C$ to all but the furthest $m$ data points, instead of all points as in classical clustering. We focus on the $\epsilon$-coreset for robust clustering, a small proxy of the dataset that preserves the clustering cost within $\epsilon$-relative error for all center sets. Our main result is an $\epsilon$-coreset of size $O(m + \mathrm{poly}(k \epsilon^{-1}))$ that can be constructed in near-linear time. This significantly improves previous results, which either suffers an exponential dependence on $(m + k)$ [Feldman and Schulman, SODA&#39;12], or has a weaker bi-criteria guarantee [Huang et al., FOCS&#39;18]. Furthermore, we show this dependence in $m$ is nearly-optimal, and the fact that it is isolated from other factors may be crucial for dealing with large number of outliers. We construct our coresets by adapting to the outlier setting a recent framework [Braverman et al., FOCS&#39;22] which was designed for capacity-constrained clustering, overcoming a new challenge that the participating terms in the cost, particularly the excluded $m$ outlier points, are dependent on the center set $C$. We validate our coresets on various datasets, and we observe a superior size-accuracy tradeoff compared with popular baselines including uniform sampling and sensitivity sampling. We also achieve a significant speedup of existing approximation algorithms for robust clustering using our coresets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                31. <h><a href=https://openreview.net/pdf?id=zqkfJA6R1-r>Improved Training of Physics-Informed Neural Networks Using Energy-Based Priors: a Study on Electrical Impedance Tomography</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Akarsh Pokkunuru, Pedram Rooshenas, Thilo Strauss, Anuj Abhishek, Taufiquar Khan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zqkfJA6R1-r>8.0 - [8, 6, 10] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zqkfJA6R1-r.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Physics-informed neural networks (PINNs) are attracting significant attention for solving partial differential equation (PDE) based inverse problems, including electrical impedance tomography (EIT). EIT is non-linear and especially its inverse problem is highly ill-posed. Therefore, successful training of PINN is extremely sensitive to interplay between different loss terms and hyper-parameters, including the learning rate. In this work, we propose a Bayesian approach through data-driven energy-based model (EBM) as a prior, to improve the overall accuracy and quality of tomographic reconstruction. In particular, the EBM is trained over the possible solutions of the PDEs with different boundary conditions. By imparting such prior onto physics-based training, PINN convergence is expedited by more than ten times faster to the PDE’s solution. Evaluation outcome shows that our proposed method is more robust for solving the EIT problem. Our code is available at: https://rooshenasgroup.github.io/eit_ebprior.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                32. <h><a href=https://openreview.net/pdf?id=9Z_GfhZnGH>Disentanglement with Biological Constraints: A Theory of Functional Cell Types</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>James C. R. Whittington, Will Dorrell, Surya Ganguli, Timothy Behrens</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9Z_GfhZnGH>8.0 - [8, 6, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove biological constraints of nonnegativity and energy efficiency lead to disentanged representations, and empirically demonstrate this in machine learning and neuroscience tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9Z_GfhZnGH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task structure shapes the structure of brain representation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                33. <h><a href=https://openreview.net/pdf?id=JJuP86nBl4q>LAVA: Data Valuation without Pre-Specified Learning Algorithms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hoang Anh Just, Feiyang Kang, Tianhao Wang, Yi Zeng, Myeongseob Ko, Ming Jin, Ruoxi Jia</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JJuP86nBl4q>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose LAVA: a novel model-agnostic approach to data valuation using a non-conventional, class-wise Wasserstein discrepancy.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JJuP86nBl4q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden. 

This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning algorithm. Our main results are as follows. $\textbf{(1)}$ We develop a proxy for the validation performance associated with a training set based on a non-conventional $\textit{class-wise}$ $\textit{Wasserstein distance}$ between the training and the validation set. We show that the distance characterizes the upper bound of the validation performance for any given model under certain Lipschitz conditions. $\textbf{(2)}$ We develop a novel method to value individual data based on the sensitivity analysis of the $\textit{class-wise}$ Wasserstein distance. Importantly, these values can be directly obtained $\textit{for free}$ from the output of off-the-shelf optimization solvers once the Wasserstein distance is computed. $\textbf{(3) }$We evaluate our new data valuation framework over various use cases related to detecting low-quality data
and show that, surprisingly, the learning-agnostic feature of our framework enables a $\textit{significant improvement}$ over the state-of-the-art performance while being $\textit{orders of magnitude faster.}$ 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                34. <h><a href=https://openreview.net/pdf?id=hQ9V5QN27eS>Pink Noise Is All You Need: Colored Noise Exploration in Deep Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Onno Eberhard, Jakob Hollenstein, Cristina Pinneri, Georg Martius</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hQ9V5QN27eS>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Pink noise, a temporally correlated noise type, outperforms other action noise types on standard continuous control benchmarks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hQ9V5QN27eS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In off-policy deep reinforcement learning with continuous action spaces, exploration is often implemented by injecting action noise into the action selection process. Popular algorithms based on stochastic policies, such as SAC or MPO, inject white noise by sampling actions from uncorrelated Gaussian distributions. In many tasks, however, white noise does not provide sufficient exploration, and temporally correlated noise is used instead. A common choice is Ornstein-Uhlenbeck (OU) noise, which is closely related to Brownian motion (red noise). Both red noise and white noise belong to the broad family of colored noise. In this work, we perform a comprehensive experimental evaluation on MPO and SAC to explore the effectiveness of other colors of noise as action noise. We find that pink noise, which is halfway between white and red noise, significantly outperforms white noise, OU noise, and other alternatives on a wide range of environments. Thus, we recommend it as the default choice for action noise in continuous control.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                35. <h><a href=https://openreview.net/pdf?id=zyLVMgsZ0U_>Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, Anru Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zyLVMgsZ0U_>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove that given an L2-accurate score estimate, diffusion models can sample from (essentially) any data distribution, even if it is highly non-log-concave and/or supported on a low dimensional manifold.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zyLVMgsZ0U_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We provide theoretical convergence guarantees for score-based generative models (SGMs) such as denoising diffusion probabilistic models (DDPMs), which constitute the backbone of large-scale real-world generative models such as DALL$\cdot$E 2. Our main result is that, assuming accurate score estimates, such SGMs can efficiently sample from essentially any realistic data distribution. In contrast to prior works, our results (1) hold for an $L^2$-accurate score estimate (rather than $L^\infty$-accurate); (2) do not require restrictive functional inequality conditions that preclude substantial non-log-concavity; (3) scale polynomially in all relevant problem parameters; and (4) match state-of-the-art complexity guarantees for discretization of the Langevin diffusion, provided that the score error is sufficiently small. We view this as strong theoretical justification for the empirical success of SGMs. We also examine SGMs based on the critically damped Langevin diffusion (CLD). Contrary to conventional wisdom, we provide evidence that the use of the CLD does *not* reduce the complexity of SGMs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                36. <h><a href=https://openreview.net/pdf?id=TD7AnQjNzR6>Statistical Efficiency of Score Matching: The View from Isoperimetry</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Frederic Koehler, Alexander Heckett, Andrej Risteski</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TD7AnQjNzR6>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show a tight connection between the statistical efficiency of score matching and the isoperimetric properties (e.g. log-Sobolev constant) of the distribution being estimated
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TD7AnQjNzR6.jpg loading="lazy">
                </div>
                <div class="abstract">
                      Deep generative models parametrized up to a normalizing constant (e.g. energy-based models) are difficult to train by maximizing the likelihood of the data because the likelihood and/or gradients thereof cannot be explicitly or efficiently written down. Score matching is a training method, whereby instead of fitting the likelihood $\log p(x)$ for the training data, we instead fit the score function $\nabla_x \log p(x)$ --- obviating the need to evaluate the partition function. Though this estimator is known to be consistent, its unclear whether (and when) its statistical efficiency is comparable to that of maximum likelihood --- which is known to be (asymptotically) optimal. We initiate this line of inquiry in this paper, and show a tight connection between statistical efficiency of score matching and the isoperimetric properties of the distribution being estimated --- i.e. the Poincar\&#39;e, log-Sobolev and isoperimetric constant --- quantities which govern the mixing time of Markov processes like Langevin dynamics. Roughly, we show that the score matching estimator is statistically comparable to the maximum likelihood when the  distribution has a small isoperimetric constant. Conversely, if the distribution has a large isoperimetric constant --- even for simple families of distributions like exponential families with rich enough sufficient statistics --- score matching will be substantially less efficient than maximum likelihood. We suitably formalize these results both in the finite sample regime, and in the asymptotic regime. Finally, we identify a direct parallel in the discrete setting, where we connect the statistical properties of pseudolikelihood estimation with approximate tensorization of entropy and the Glauber dynamics.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                37. <h><a href=https://openreview.net/pdf?id=w1hwFUb_81>Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianlong Chen, Zhenyu Zhang, AJAY KUMAR JAISWAL, Shiwei Liu, Zhangyang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=w1hwFUb_81>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new plug-and-paly strategy for training over-parameterized transformer models, leverages SMoEs with random routings to empower scaling transformers to better performance in the full capacity settings without collapse.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/w1hwFUb_81.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) $\textit{redundant experts}$ due to representational collapse; and (2) $\textit{poor expert scalability for inference and downstream fine-tuning}$, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on $\textit{exploring the overlooked scalability bottleneck of SMoEs}$ and leveraging it to effectively $\textbf{scale dense transformers}$. To this end, we propose a new plug-and-play training framework, $\textbf{SMoE-Dropout}$, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a $\textit{randomly initialized and fixed}$ router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a $\textbf{``self-slimmable”}$ property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments across diverse transformer architectures on a variety of tasks demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {$1.03\%$, $0.78\%$, $1.09\%$} on challenging reasoning tasks {$\texttt{ASDiv-A}$, $\texttt{MAWPS}$, $\texttt{SVAMP}$}, respectively. Codes and models are available in https://github.com/VITA-Group/Random-MoE-as-Dropout.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                38. <h><a href=https://openreview.net/pdf?id=wkg_b4-IwTZ>A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Puja Trivedi, Danai Koutra, Jayaraman J. Thiagarajan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wkg_b4-IwTZ>8.0 - [6, 8, 10, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Mitigating feature distortion is not enough to ensure that transfer learning from large-scale, pretrained models leads to better safety and generalization on downstream tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wkg_b4-IwTZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to rely upon simple features, as SB has recently been shown to underlie several problems in robust generalization. Using a synthetic dataset, we demonstrate the susceptibility of existing protocols to SB. Given the strong effectiveness of LP+FT, we then propose modified linear probes that help mitigate SB, and lead to better initializations for subsequent FT. We verify the effectiveness of the proposed LP+FT variants for decreasing SB in a controlled setting, and their ability to improve OOD generalization and safety on three adaptation datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                39. <h><a href=https://openreview.net/pdf?id=vuD2xEtxZcj>Minimum Variance Unbiased N:M Sparsity for the Neural Gradients</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Brian Chmiel, Itay Hubara, Ron Banner, Daniel Soudry</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vuD2xEtxZcj>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method to use structured N:M sparsity on all training GEMM operations
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vuD2xEtxZcj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In deep learning, fine-grained N:M sparsity reduces the data footprint and bandwidth of a General Matrix multiply (GEMM) up to x2,  and doubles throughput by skipping computation of zero values. So far, it was mainly only used to prune weights to accelerate the forward and backward phases. We examine how this method can be used also for the neural gradients (i.e. loss gradients with respect to the intermediate neural layer outputs). To this end, we first establish a tensor-level optimality criteria. Previous works aimed to minimize the mean-square-error (MSE) of each pruned block. We show that while minimization of the MSE works fine for pruning the weights and activations, it catastrophically fails for the neural gradients. Instead, we show that accurate pruning of the neural gradients requires an unbiased minimum-variance pruning mask. We design such specialized masks, and find that in most cases, 1:2 sparsity is sufficient for training, and 2:4 sparsity is usually enough when this is not the case. Further, we suggest combining several such methods together in order to potentially speed up training even more. A reference implementation is supplied in the supplementary material.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                40. <h><a href=https://openreview.net/pdf?id=XrgjF5-M3xi>Incremental Learning of Structured Memory via Closed-Loop Transcription</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shengbang Tong, Xili Dai, Ziyang Wu, Mingyang Li, Brent Yi, Yi Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XrgjF5-M3xi>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XrgjF5-M3xi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work proposes a minimal computational model for learning structured memories of multiple object classes in an incremental setting. Our approach is based on establishing a {\em closed-loop transcription} between the  classes and a corresponding set of subspaces, known as a linear discriminative representation, in a low-dimensional feature space. Our method is simpler than existing approaches for incremental learning, and more efficient in terms of model size, storage, and computation: it requires only a single, fixed-capacity autoencoding network with a feature space that is used for both discriminative and generative purposes. Network parameters are optimized simultaneously without architectural manipulations, by solving a constrained minimax game between the encoding and decoding maps over a single rate reduction-based objective. Experimental results show that our method can effectively alleviate catastrophic forgetting, achieving significantly better performance than prior work of generative replay on MNIST, CIFAR-10, and ImageNet-50, despite requiring fewer resources.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                41. <h><a href=https://openreview.net/pdf?id=sCYXJr3QJM8>Few-shot Cross-domain Image Generation via Inference-time Latent-code Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Arnab Kumar Mondal, Piyush Tiwary, Parag Singla, Prathosh AP</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sCYXJr3QJM8>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Adapt a GAN trained on a single large-scale source dataset to multiple target domains containing very few examples without re-training the pretrained source generator.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sCYXJr3QJM8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, our objective is to adapt a Deep generative model trained on a large-scale source dataset to multiple target domains with scarce data. Specifically, we focus on adapting a pre-trained Generative Adversarial Network (GAN) to a target domain without re-training the generator. Our method draws the motivation from the fact that out-of-distribution samples can be `embedded&#39; onto the latent space of a pre-trained source-GAN. We propose to train a small latent-generation network during the inference stage, each time a  batch of target samples is to be generated. These target latent codes are fed to the source-generator to obtain  novel target samples. Despite using the same small set of target samples and the source generator, multiple independent training episodes of the latent-generation network results in the diversity of the generated target samples. Our method, albeit simple, can be used to generate data from multiple target distributions using a generator trained on a single source distribution. We demonstrate the efficacy of our surprisingly simple method in generating multiple target datasets with only a single source generator and a few target samples.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                42. <h><a href=https://openreview.net/pdf?id=JroZRaRw7Eu>Token Merging: Your ViT But Faster</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, Judy Hoffman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JroZRaRw7Eu>8.0 - [8, 8, 10, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We merge tokens in a ViT at runtime using a fast custom matching algorithm. Our method, ToMe, can increase training and inference speed, lower training memory, and can be applied with and without training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JroZRaRw7Eu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce Token Merging (ToMe), a simple method to increase the throughput of existing ViT models without needing to train. ToMe gradually combines similar tokens in a transformer using a general and light-weight matching algorithm that is as fast as pruning while being more accurate. Off-the-shelf, ToMe can 2x the throughput of state-of-the-art ViT-L @ 512 and ViT-H @ 518 models on images and 2.2x the throughput of ViT-L on video with only a 0.2-0.3% accuracy drop in each case. ToMe can also easily be applied during training, improving in practice training speed up to 2x for MAE fine-tuning on video. Training with ToMe further minimizes accuracy drop, leading to 2x the throughput of ViT-B on audio for only a 0.4% mAP drop. Qualitatively, we find that ToMe merges object parts into one token, even over multiple frames of video. Overall, ToMe’s accuracy and speed are competitive with state-of-the-art on images, video, and audio.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                43. <h><a href=https://openreview.net/pdf?id=FjNys5c7VyY>DreamFusion: Text-to-3D using 2D Diffusion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ben Poole, Ajay Jain, Jonathan T. Barron, Ben Mildenhall</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FjNys5c7VyY>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> DeepDream on a pretrained 2D diffusion model enables text-to-3D synthesis
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FjNys5c7VyY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D or multiview data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                44. <h><a href=https://openreview.net/pdf?id=oGVu9spZaJJ>Asymptotic Instance-Optimal Algorithms for Interactive Decision Making</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kefan Dong, Tengyu Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oGVu9spZaJJ>8.0 - [6, 8, 10, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design the first instance-optimal algorithm for general interactive decision making problems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oGVu9spZaJJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Past research on interactive decision making problems (bandits, reinforcement learning, etc.) mostly focuses on the minimax regret that measures the algorithm&#39;s performance on the hardest instance. However, an ideal algorithm should adapt to the complexity of a particular problem instance and incur smaller regrets on easy instances than worst-case instances. In this paper, we design the first asymptotic instance-optimal algorithm for general interactive decision making problems with finite number of decisions under mild conditions. On every instance $f$, our algorithm outperforms all consistent algorithms (those achieving non-trivial regrets on all instances), and has asymptotic regret $\mathcal{C}(f) \ln n$, where $\mathcal{C}(f)$ is an exact characterization of the complexity of $f$. The key step of the algorithm involves hypothesis testing with active data collection. It computes the most economical decisions with which the algorithm collects observations to test whether an estimated instance is indeed correct; thus, the complexity $\mathcal{C}(f)$ is the minimum cost to test the instance $f$ against other instances.  Our results, instantiated on concrete problems, recover the classical gap-dependent bounds for multi-armed bandits and prior works on linear bandits, and improve upon the previous best instance-dependent upper bound for reinforcement learning. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                45. <h><a href=https://openreview.net/pdf?id=ueYYgo2pSSU>Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, Xianyuan Zhan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ueYYgo2pSSU>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that some form of Implicit Value Regularization (IVR) will result in the In-sample Learning paradigm in offline RL. We also propose a practical algorithm based on the IVR framework, which obtains new SOTA results.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ueYYgo2pSSU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recent proposed \textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based on the IVR framework, we further propose two practical algorithms, Sparse $Q$-learning (SQL) and Exponential $Q$-learning (EQL), which adopt the same value regularization used in existing works, but in a complete in-sample manner. Compared with IQL, we find that our algorithms introduce sparsity in learning the value function, making them more robust in noisy data regimes. We also verify the effectiveness of SQL and EQL on D4RL benchmark datasets and show the benefits of in-sample learning by comparing them with CQL in small data regimes. Code is available at \url{https://github.com/ryanxhr/SQL}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                46. <h><a href=https://openreview.net/pdf?id=JxpBP1JM15->Scaling Forward Gradient With Local Losses</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mengye Ren, Simon Kornblith, Renjie Liao, Geoffrey Hinton</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JxpBP1JM15->8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JxpBP1JM15-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Forward gradient learning computes a noisy directional gradient and is a biologically plausible alternative to backprop for learning deep neural networks. The standard forward gradient algorithm suffers from the curse of dimensionality in the number of parameters. In this paper, we propose to scale forward gradient by adding a large number of local greedy loss functions. We consider block-wise, patch-wise, and channel group-wise local losses, and show that activity perturbation reduces variance compared to weight perturbation. Inspired by MLPMixer, we also propose a new architecture, LocalMixer, that is more suitable for local learning. We find local learning can work well with both supervised classification and self-supervised contrastive learning. Empirically, it can match backprop on MNIST and CIFAR-10 and significantly outperform backprop-free algorithms on ImageNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                47. <h><a href=https://openreview.net/pdf?id=CPdc77SQfQ5>Win: Weight-Decay-Integrated Nesterov Acceleration for Adaptive Gradient Algorithms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pan Zhou, Xingyu Xie, Shuicheng YAN</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CPdc77SQfQ5>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We  propose a new and general Weight-decay-Integrated Nesterov acceleration for adaptive  algorithms to enhance their convergence speed, and also analyze their convergence  justify their convergence superiority. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CPdc77SQfQ5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training deep  networks on  large-scale datasets is computationally challenging.  In this work, we explore the problem of ``\textit{how to  accelerate  adaptive gradient algorithms in a general manner}&#34;, and aim to provide practical efficiency-boosting insights.   To this end, we   propose an effective and general   {Weight-decay-Integrated Nesterov acceleration} (Win) to accelerate adaptive  algorithms. Taking AdamW and Adam as examples,  we minimize a dynamical   loss per iteration which combines the vanilla training loss and a dynamic regularizer inspired by proximal point method (PPM) to improve the convexity of the problem. To introduce Nesterov-alike-acceleration into AdamW and Adam,  we respectively  use the  first- and second-order Taylor approximations of vanilla loss  to  update the variable  twice. In this way,  we arrive at  our Win acceleration  for AdamW and Adam that uses  a conservative step  and a  reckless step to update twice and then linearly combines these two updates for acceleration. Next,  we  extend  Win acceleration to LAMB and SGD. Our transparent acceleration derivation  could  provide insights for  other accelerated methods and their integration into  adaptive algorithms.  Besides, we prove the convergence of Win-accelerated adaptive  algorithms and  justify their convergence superiority over their non-accelerated counterparts by taking AdamW and Adam as examples.  Experimental results testify to the faster convergence speed and superior performance of our Win-accelerated AdamW, Adam, LAMB and SGD over their non-accelerated counterparts on vision classification tasks and  language modeling tasks with both CNN and Transformer backbones.  We hope Win  shall be a default acceleration option for  popular optimizers in deep learning community to improve the training efficiency. Code will be released at \url{https://github.com/sail-sg/win}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                48. <h><a href=https://openreview.net/pdf?id=VM8batVBWvg>Discrete Predictor-Corrector Diffusion Models for Image Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jose Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, Irfan Essa</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VM8batVBWvg>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a learned predictor-corrector sampler for discrete diffusion models and empirically demonstrate its effectiveness for image generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VM8batVBWvg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce Discrete Predictor-Corrector diffusion models (DPC), extending predictor-corrector samplers in Gaussian diffusion models to the discrete case. Predictor-corrector samplers are a class of samplers for diffusion models, which improve on ancestral samplers by correcting the sampling distribution of intermediate diffusion states using MCMC methods. In DPC, the Langevin corrector, which does not have a direct counterpart in discrete space, is replaced with a discrete MCMC transition defined by a learned corrector kernel. The corrector kernel is trained to make the correction steps achieve asymptotic convergence, in distribution, to the correct marginal of the intermediate diffusion states. Equipped with DPC, we revisit recent transformer-based  non-autoregressive generative models through the lens of discrete diffusion, and find that DPC can alleviate the compounding decoding error due to the parallel sampling of visual tokens. Our experiments show that DPC improves upon existing discrete latent space models for class-conditional image generation on ImageNet, and outperforms continuous diffusion models and GANs, according to standard metrics and user preference studies.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                49. <h><a href=https://openreview.net/pdf?id=zDiHoIWa0q1>Omnigrok: Grokking Beyond Algorithmic Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziming Liu, Eric J Michaud, Max Tegmark</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zDiHoIWa0q1>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We aim to understand grokking through the lens of neural loss landscapes, and show grokking can occur for various datasets beyond algorithmic datasets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zDiHoIWa0q1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the &#34;LU mechanism&#34; because training and test losses (against model weight norm) typically resemble &#34;L&#34; and &#34;U&#34;, respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules, although the grokking signals are sometimes less dramatic. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                50. <h><a href=https://openreview.net/pdf?id=0g0X4H8yN4I>​​What learning algorithm is in-context learning? Investigations with linear models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0g0X4H8yN4I>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove that the transformers can implement learning algorithms for linear models based e.g gradient descent, then observe they closely match the predictors of known algorithms, transitioning between different predictors as transformer depth vary.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0g0X4H8yN4I.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding context-specific parametric models in their hidden representations, and updating these implicit models as new examples appear in the context. Using linear regression as a model problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form computation of regression parameters. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners&#39; late layers encode weight vectors and moment matrices.  These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may work by rediscovering standard estimation algorithms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                51. <h><a href=https://openreview.net/pdf?id=Q-UHqMorzil>Sign and Basis Invariant Networks for Spectral Graph Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Derek Lim, Joshua David Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, Stefanie Jegelka</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Q-UHqMorzil>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop neural networks invariant to the symmetries of eigenvectors, which are theoretically expressive and empirically improve performance in geometric learning tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Q-UHqMorzil.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce SignNet and BasisNet---new neural architectures that are invariant to two key symmetries displayed by eigenvectors: (i) sign flips, since if v is an eigenvector then so is -v; and (ii) more general basis symmetries, which occur in higher dimensional eigenspaces with infinitely many choices of basis eigenvectors. We prove that under certain conditions our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the desired invariances. When used with Laplacian eigenvectors, our networks are provably more expressive than existing spectral methods on graphs; for instance, they subsume all spectral graph convolutions, certain spectral graph invariants, and previously proposed graph positional encodings as special cases. Experiments show that our networks significantly outperform existing baselines on molecular graph regression, learning expressive graph representations, and learning neural fields on triangle meshes. Our code is available at https://github.com/cptq/SignNet-BasisNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                52. <h><a href=https://openreview.net/pdf?id=nhKHA59gXz>Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alex Damian, Eshaan Nichani, Jason D. Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nhKHA59gXz>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We explain the mechanism behind the edge of stability phenomenon, where full batch gradient descent non-monotonically decreases the loss in the presence of instability.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nhKHA59gXz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\theta)$, is bounded by $2/\eta$, training is &#34;stable&#34; and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen at al. (2021) detailed two important phenomena. The first, dubbed \emph{progressive sharpening}, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\eta$. The second, dubbed \emph{edge of stability}, is that the sharpness hovers at $2/\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call \emph{self-stabilization}, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows \emph{projected} gradient descent (PGD) under the constraint $S(\theta) \le 2/\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent&#39;s implicit bias towards stability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                53. <h><a href=https://openreview.net/pdf?id=4-k7kUavAj>Offline Q-learning on Diverse Multi-Task Data Both Scales And Generalizes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, Sergey Levine</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4-k7kUavAj>8.0 - [8, 10, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4-k7kUavAj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly suboptimal dataset (51% human-level performance). Compared to return-conditioned supervised approaches, offline Q-learning scales similarly with model capacity and has better performance, especially when the dataset is suboptimal. Finally, we show that offline Q-learning with a diverse dataset is sufficient to learn powerful representations that facilitate rapid transfer to novel games and fast online learning on new variations of a training game, improving over existing state-of-the-art representation learning approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                54. <h><a href=https://openreview.net/pdf?id=nA5AZ8CEyow>Post-hoc Concept Bottleneck Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mert Yuksekgonul, Maggie Wang, James Zou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nA5AZ8CEyow>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a method to turn any neural network into a concept bottleneck model without sacrificing model performance, retaining interpretability benefits along with easy model editing.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nA5AZ8CEyow.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Concept Bottleneck Models (CBMs) map the inputs onto a set of interpretable concepts (``the bottleneck&#39;&#39;) and use the concepts to make predictions. A concept bottleneck enhances interpretability since it can be investigated to understand what concepts the model &#34;sees&#34; in an input and which of these concepts are deemed important. However, CBMs are restrictive in practice as they require dense concept annotations in the training data to learn the bottleneck. Moreover, CBMs often do not match the accuracy of an unrestricted neural network, reducing the incentive to deploy them in practice. In this work, we address these limitations of CBMs by introducing Post-hoc Concept Bottleneck models (PCBMs). We show that we can turn any neural network into a PCBM without sacrificing model performance while still retaining the interpretability benefits. When concept annotations are not available on the training data, we show that PCBM can transfer concepts from other datasets or from natural language descriptions of concepts via multimodal models. A key benefit of PCBM is that it enables users to quickly debug and update the model to reduce spurious correlations and improve generalization to new distributions. PCBM allows for global model edits, which can be more efficient than previous works on local interventions that fix a specific prediction. Through a model-editing user study, we show that editing PCBMs via concept-level feedback can provide significant performance gains without using data from the target domain or model retraining.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                55. <h><a href=https://openreview.net/pdf?id=_s1N-DnxdyT>Stochastic Multi-Person 3D Motion Forecasting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sirui Xu, Yu-Xiong Wang, Liangyan Gui</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_s1N-DnxdyT>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a new task of stochastic multi-person 3D motion forecasting, and propose a dual-level generative modeling framework to address this task.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_s1N-DnxdyT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper aims to deal with the ignored real-world complexities in prior work on human motion forecasting, emphasizing the social properties of multi-person motion, the diversity of motion and social interactions, and the complexity of articulated motion. To this end, we introduce a novel task of stochastic multi-person 3D motion forecasting. We propose a dual-level generative modeling framework that separately models independent individual motion at the local level and social interactions at the global level. Notably, this dual-level modeling mechanism can be achieved within a shared generative model, through introducing learnable latent codes that represent intents of future motion and switching the codes&#39; modes of operation at different levels. Our framework is general; we instantiate it with different generative models, including generative adversarial networks and diffusion models, and various multi-person forecasting models. Extensive experiments on CMU-Mocap, MuPoTS-3D, and SoMoF benchmarks show that our approach produces diverse and accurate multi-person predictions, significantly outperforming the state of the art. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                56. <h><a href=https://openreview.net/pdf?id=LFHFQbjxIiP>Conditional Antibody Design as 3D Equivariant Graph Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiangzhe Kong, Wenbing Huang, Yang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LFHFQbjxIiP>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LFHFQbjxIiP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. Then, MEAN resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. Finally, it outputs both the 1D sequences and 3D structure via a multi-round progressive full-shot scheme, which enjoys more efficiency and precision against previous autoregressive approaches. Our method significantly surpasses state-of-the-art models in sequence and structure modeling, antigen-binding CDR design, and binding affinity optimization. Specifically, the relative improvement to baselines is about 23\% in antigen-binding CDR design and 34\% for affinity optimization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                57. <h><a href=https://openreview.net/pdf?id=Ai8Hw3AXqks>Simplified State Space Layers for Sequence Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jimmy T.H. Smith, Andrew Warrington, Scott Linderman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Ai8Hw3AXqks>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a new state space sequence modeling layer, building on the recent S4 layer, that increases the state of the art on many long-range benchmark tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Ai8Hw3AXqks.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer.  Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM.  We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model.  The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.  S5 averages $87.4\%$ on the long range arena benchmark, and $98.5\%$ on the most difficult Path-X task.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                58. <h><a href=https://openreview.net/pdf?id=OpC-9aBBVJe>Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pierluca D&#39;Oro, Max Schwarzer, Evgenii Nikishin, Pierre-Luc Bacon, Marc G Bellemare, Aaron Courville</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OpC-9aBBVJe>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The combination of a large number of updates and resets drastically improves the sample efficiency of deep RL algorithms.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OpC-9aBBVJe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Increasing the replay ratio, the number of updates of an agent&#39;s parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                59. <h><a href=https://openreview.net/pdf?id=xSsW2Am-ukZ>Unmasking the Lottery Ticket Hypothesis: What&#39;s Encoded in a Winning Ticket&#39;s Mask?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mansheej Paul, Feng Chen, Brett W. Larsen, Jonathan Frankle, Surya Ganguli, Gintare Karolina Dziugaite</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xSsW2Am-ukZ>8.0 - [6, 10, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide an error landscape perspective on what information is encoded in a winning ticket&#39;s mask and how Iterative Magnitude Pruning finds matching subnetworks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xSsW2Am-ukZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As neural networks get larger and costlier, it is important to find sparse networks that require less compute and memory but can be trained to the same accuracy as the full network (i.e. matching). Iterative magnitude pruning (IMP) is a state of the art algorithm that can find such highly sparse matching subnetworks, known as winning tickets. IMP iterates through cycles of training, pruning a fraction of smallest magnitude weights, rewinding unpruned weights back to an early training point, and repeating. Despite its simplicity, the principles underlying when and how IMP finds winning tickets remain elusive. In particular, what useful information does an IMP mask found at the end of training convey to a rewound network near the beginning of training? How does SGD allow the network to extract this information? And why is iterative pruning needed, i.e. why can&#39;t we prune to very high sparsities in one shot? We investigate these questions through the lens of the geometry of the error landscape. First, we find that—at higher sparsities—pairs of pruned networks at successive pruning iterations are connected by a linear path with zero error barrier if and only if they are matching. This indicates that masks found at the end of training convey to the rewind point the identity of an axial subspace that intersects a desired linearly connected mode of a matching sublevel set. Second, we show SGD can exploit this information due to a strong form of robustness: it can return to this mode despite strong perturbations early in training. Third, we show how the flatness of the error landscape at the end of training limits the fraction of weights that can be pruned at each iteration of IMP. This analysis yields a new quantitative link between IMP performance and the Hessian eigenspectrum. Finally, we show that the role of retraining in IMP is to find a network with new small weights to prune. Overall, these results make progress toward demystifying the existence of winning tickets by revealing the fundamental role of error landscape geometry in the algorithms used to find them.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                60. <h><a href=https://openreview.net/pdf?id=Wc5bmZZU9cy>Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuaichen Chang, Jun Wang, Mingwen Dong, Lin Pan, Henghui Zhu, Alexander Hanbo Li, Wuwei Lan, Sheng Zhang, Jiarong Jiang, Joseph Lilien, Steve Ash, William Yang Wang, Zhiguo Wang, Vittorio Castelli, Patrick Ng, Bing Xiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Wc5bmZZU9cy>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Wc5bmZZU9cy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural text-to-SQL models have achieved remarkable performance in translating natural language questions into SQL queries. However, recent studies reveal that text-to-SQL models are vulnerable to task-specific perturbations. Previous curated robustness test sets usually focus on individual phenomena. In this paper, we propose a comprehensive robustness benchmark based on Spider, a cross-domain text-to-SQL benchmark, to diagnose the model robustness. We design 17 perturbations on databases, natural language questions, and SQL queries to measure the robustness from different angles. In order to collect more diversified natural question perturbations, we utilize large pretrained language models (PLMs) to simulate human behaviors in creating natural questions. We conduct a diagnostic study of the state-of-the-art models on the robustness set. Experimental results reveal that even the most robust model suffers from a 14.0% performance drop overall and a 50.7% performance drop on the most challenging perturbation. We also present a breakdown analysis regarding text-to-SQL model designs and provide insights for improving model robustness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                61. <h><a href=https://openreview.net/pdf?id=1NAzMofMnWl>DaxBench: Benchmarking Deformable Object Manipulation with Differentiable Physics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Siwei Chen, Yiqing Xu, Cunjun Yu, Linfeng Li, Xiao Ma, Zhongwen Xu, David Hsu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1NAzMofMnWl>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1NAzMofMnWl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deformable object manipulation (DOM) is a long-standing challenge in robotics and has attracted significant interest recently. This paper presents DaXBench, a differentiable simulation framework for DOM. While existing work often focuses on a specific type of deformable objects, DaXBench supports fluid, rope, cloth ...; it provides a general-purpose benchmark to evaluate widely different DOM methods, including planning, imitation learning, and reinforcement learning. DaXBench combines recent advances in deformable object simulation with JAX, a high-performance computational framework. All DOM tasks in DaXBench are wrapped with the OpenAI Gym API for easy integration with DOM algorithms. We hope that DaXBench provides to the research community a comprehensive, standardized benchmark and a valuable tool to support the development and evaluation of new DOM methods. The code and video are available online.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                62. <h><a href=https://openreview.net/pdf?id=F61FwJTZhb>Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anton Bakhtin, David J Wu, Adam Lerer, Jonathan Gray, Athul Paul Jacob, Gabriele Farina, Alexander H Miller, Noam Brown</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=F61FwJTZhb>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train a bot that places first in a no-press Diplomacy tournament with humans by using human-data-regularized reinforcement learning and planning 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/F61FwJTZhb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus.
In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                63. <h><a href=https://openreview.net/pdf?id=AWZgXGmsbA>Powderworld: A Platform for Understanding Generalization via Rich Task Distributions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kevin Frans, Phillip Isola</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AWZgXGmsbA>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Powderworld is an environment supporting the study of generalization by providing diverse tasks arising from the same core rules.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AWZgXGmsbA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    One of the grand challenges of reinforcement learning is the ability to generalize to new tasks. However, general agents require a set of rich, diverse tasks to train on. Designing a `foundation environment&#39; for such tasks is tricky -- the ideal environment would support a range of emergent phenomena, an expressive task space, and fast runtime. To take a step towards addressing this research bottleneck, this work presents Powderworld, a lightweight yet expressive simulation environment running directly on the GPU. Within Powderworld, two motivating task distributions are presented, one for world-modelling and one for reinforcement learning. Each contains hand-designed test tasks to examine generalization. Experiments indicate that increasing the environment&#39;s complexity improves generalization for world models, yet causes reinforcement learning agents to struggle. Powderworld aims to support the study of generalization by providing a source of diverse tasks arising from the same core rules.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                64. <h><a href=https://openreview.net/pdf?id=Iuubb9W6Jtk>A framework for benchmarking Class-out-of-distribution detection and its application to ImageNet</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ido Galil, Mohammed Dabbah, Ran El-Yaniv</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Iuubb9W6Jtk>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a framework for benchmarking the performance of image classifiers in detecting OOD. We apply it to benchmark 525 pretrained ImageNet classifiers, and analyze their performance resulting in interesting conclusions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Iuubb9W6Jtk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    When deployed for risk-sensitive tasks, deep neural networks must be able to detect instances with labels from outside the distribution for which they were trained.
In this paper we present a novel framework to benchmark the ability of image classifiers to detect class-out-of-distribution instances
(i.e., instances whose true labels do not appear in the training distribution) at various levels of detection difficulty.
We apply this technique to ImageNet, and benchmark 525 pretrained, publicly available, ImageNet-1k classifiers. 
The code for generating a benchmark for any ImageNet-1k classifier, along with the benchmarks prepared for the above-mentioned 525 models is available at https://github.com/mdabbah/COOD_benchmarking.

The usefulness of the proposed framework and its advantage over alternative existing benchmarks is demonstrated by analyzing the results obtained for these models, which reveals numerous novel observations including:
(1) knowledge distillation consistently improves class-out-of-distribution (C-OOD) detection performance; (2) a subset of ViTs performs better C-OOD detection than any other model; (3) the language–-vision CLIP model achieves good zero-shot detection performance, with its best instance outperforming 96% of all other models evaluated; (4) accuracy and in-distribution ranking are positively correlated to C-OOD detection; and 
(5) we compare various confidence functions for C-OOD detection.
Our companion paper, also published in ICLR 2023 (What Can We Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers), examines the uncertainty estimation performance (ranking, calibration, and selective prediction performance) of these classifiers in an in-distribution setting.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                65. <h><a href=https://openreview.net/pdf?id=lH1PV42cbF>Binding Language Models in Symbolic Languages</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lH1PV42cbF>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> binding language models in symbolic languages
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lH1PV42cbF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at anonymized.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                66. <h><a href=https://openreview.net/pdf?id=H3HcEJA2Um>Time Will Tell: New Outlooks and A Baseline for Temporal Multi-View 3D Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer, Kris M. Kitani, Masayoshi Tomizuka, Wei Zhan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=H3HcEJA2Um>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We leverage complementary coarse, long-term and fine-grained, short-term multi-view stereo for camera-only 3D object detection.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/H3HcEJA2Um.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While recent camera-only 3D detection methods leverage multiple timesteps, the limited history they use significantly hampers the extent to which temporal fusion can improve object perception. Observing that existing works&#39; fusion of multi-frame images are instances of temporal stereo matching, we find that performance is hindered by the interplay between 1) the low granularity of matching resolution and 2) the sub-optimal multi-view setup produced by limited history usage. Our theoretical and empirical analysis demonstrates that the optimal temporal difference between views varies significantly for different pixels and depths, making it necessary to fuse many timesteps over long-term history. Building on our investigation, we propose to generate a cost volume from a long history of image observations, compensating for the coarse but efficient matching resolution with a more optimal multi-view matching setup. Further, we augment the per-frame monocular depth predictions used for long-term, coarse matching with short-term, fine-grained matching and find that long and short term temporal fusion are highly complementary. While maintaining high efficiency, our framework sets new state-of-the-art on nuScenes, achieving first place on the test set and outperforming previous best art by 5.2% mAP and 3.7% NDS on the validation set. Code will be released here: https://github.com/Divadi/SOLOFusion.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                67. <h><a href=https://openreview.net/pdf?id=9XFSbDPmdW>Progress measures for grokking via mechanistic interpretability</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9XFSbDPmdW>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We fully reverse engineer how one-layer transformers implement modular addition, and use this knowledge to explain grokking. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9XFSbDPmdW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks often exhibit emergent behavior in which qualitatively new capabilities that arise from scaling up the number of parameters, training data, or even the number of steps. One approach to understanding emergence is to find the continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. In this work, we argue that progress measures can be found via mechanistic interpretability---that is, by reverse engineering learned models into components and measuring the progress of each component over the course of training. As a case study, we study small transformers trained on a modular arithmetic tasks with emergent grokking behavior. We fully reverse engineer the algorithm learned by these networks, which uses discrete fourier transforms and trigonometric identities to convert addition to rotation about a circle. After confirming the algorithm via ablation, we then use our understanding of the algorithm to define progress measures that precede the grokking phase transition on this task. We see our result as demonstrating both that it is possible to fully reverse engineer trained networks, and that doing so can be invaluable to understanding their training dynamics. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                68. <h><a href=https://openreview.net/pdf?id=vhFu1Acb0xb>Transformers are Sample-Efficient World Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Vincent Micheli, Eloi Alonso, François Fleuret</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vhFu1Acb0xb>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vhFu1Acb0xb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                69. <h><a href=https://openreview.net/pdf?id=IPrzNbddXV>FedExP: Speeding Up Federated Averaging via Extrapolation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Divyansh Jhunjhunwala, Shiqiang Wang, Gauri Joshi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IPrzNbddXV>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose FedExP, a method to adaptively determine the server step size in FedAvg for faster convergence.  
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IPrzNbddXV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated Averaging (FedAvg) remains the most popular algorithm for Federated Learning (FL) optimization due to its simple implementation, stateless nature, and privacy guarantees combined with secure aggregation. Recent work has sought to generalize the vanilla averaging in FedAvg to a generalized gradient descent step by treating client updates as pseudo-gradients and using a server step size. While the use of a server step size has been shown to provide performance improvement theoretically, the practical benefit of the server step size has not been seen in most existing works. In this work, we present FedExP, a method to adaptively determine the server step size in FL based on dynamically varying pseudo-gradients throughout the FL process. We begin by considering the overparameterized convex regime, where we reveal an interesting similarity between FedAvg and the Projection Onto Convex Sets (POCS) algorithm. We then show how FedExP can be motivated as a novel extension to the extrapolation mechanism that is used to speed up POCS. Our theoretical analysis later also discusses the implications of FedExP in underparameterized and non-convex settings. Experimental results show that FedExP consistently converges faster than FedAvg and competing baselines on a range of realistic FL datasets. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                70. <h><a href=https://openreview.net/pdf?id=3OR2tbtnYC->Near-optimal Policy Identification in Active Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiang Li, Viraj Mehta, Johannes Kirschner, Ian Char, Willie Neiswanger, Jeff Schneider, Andreas Krause, Ilija Bogunovic</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3OR2tbtnYC->8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel kernelized LSVI algorithm for active reinforcement learning which provably identifies a near-optimal policy uniformly over the entire state space.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3OR2tbtnYC-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many real-world reinforcement learning tasks require control of complex dynamical systems that involve both costly data acquisition processes and large state spaces. In cases where the expensive transition dynamics can be readily evaluated at specified states (e.g., via a simulator), agents can operate in what is often referred to as planning with a \emph{generative model}. We propose the AE-LSVI algorithm for best policy identification, a novel variant of the kernelized least-squares value iteration (LSVI) algorithm that combines optimism with pessimism for active exploration (AE). AE-LSVI provably identifies a near-optimal policy \emph{uniformly} over an entire state space and achieves polynomial sample complexity guarantees that are independent of the number of states. When specialized to the recently introduced offline contextual Bayesian optimization setting, our algorithm achieves improved sample complexity bounds. Experimentally, we demonstrate that AE-LSVI outperforms other RL algorithms in a variety of environments when robustness to the initial state is required. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                71. <h><a href=https://openreview.net/pdf?id=UkU05GOH7_6>Generating Diverse Cooperative Agents by Learning Incompatible Policies</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rujikorn Charakorn, Poramate Manoonpong, Nat Dilokthanakul</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UkU05GOH7_6>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that incompatible poclies are not similar. LIPO generates diverse cooperative partners by learning a population of incompatible policies.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UkU05GOH7_6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training a robust cooperative agent requires diverse partner agents. However, obtaining those agents is difficult. Previous works aim to learn diverse behaviors by changing the state-action distribution of agents. But, without information about the task&#39;s goal, the diversified agents are not guided to find other important, albeit sub-optimal, solutions: the agents might learn only variations of the same solution. In this work, we propose to learn diverse behaviors via policy compatibility. Conceptually, policy compatibility measures whether policies of interest can coordinate effectively. We theoretically show that incompatible policies are not similar. Thus, policy compatibility—which has been used exclusively as a measure of robustness—can be used as a proxy for learning diverse behaviors. Then, we incorporate the proposed objective into a population-based training scheme to allow concurrent training of multiple agents. Additionally, we use state-action information to induce local variations of each policy. Empirically, the proposed method consistently discovers more solutions than baseline methods across various multi-goal cooperative environments. Finally, in multi-recipe Overcooked, we show that our method produces populations of behaviorally diverse agents, which enables generalist agents trained with such a population to be more robust.

See our project page at https://bit.ly/marl-lipo

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                72. <h><a href=https://openreview.net/pdf?id=4t9q35BxGr>Inequality phenomenon in $l_{\infty}$-adversarial training, and its unrealized threats</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ranjie Duan, YueFeng Chen, Yao Zhu, Xiaojun Jia, Rong Zhang, Hui Xue&#39;</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4t9q35BxGr>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We find an intriguing phenomena of $l_{\infty}$ adversarial training, and this phenomena brings unrealized threats to adversarially trained model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4t9q35BxGr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The appearance of adversarial examples raises attention from both academia and industry. Along with the attack-defense arms race, adversarial training is the most effective against adversarial examples.
However, we find inequality phenomena occur during the $l_{\infty}$-adversarial training, that few features dominate the prediction made by the adversarially trained model. We systematically evaluate such inequality phenomena by extensive experiments and find such phenomena become more obvious when performing adversarial training with increasing adversarial strength (evaluated by $\epsilon$). We hypothesize such inequality phenomena make $l_{\infty}$-adversarially trained model less reliable than the standard trained model when few ``important features&#34; are influenced. To validate our hypothesis, we proposed two simple attacks that either perturb or replace important features with noise or occlusion. Experiments show that $l_{\infty}$-adversarially trained model can be easily attacked when the few important features are influenced. 
Our work shed light on the limitation of the practicality of $l_{\infty}$-adversarial training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                73. <h><a href=https://openreview.net/pdf?id=8qjSA5QACb40>Combinatorial-Probabilistic Trade-Off: P-Values of Community Properties Test in the Stochastic Block Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuting Shen, Junwei Lu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8qjSA5QACb40>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an inferential framework testing the general community combinatorial properties of the stochastic block model and prove the minimax lower bound of the general community property test.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8qjSA5QACb40.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose an inferential framework testing the general community combinatorial properties of the stochastic block model.  We aim to test the hypothesis on whether a certain community property is satisfied, e.g., whether a given set of nodes belong to the same community, and provide p-values for uncertainty quantification. Our framework is applicable to all symmetric community properties. To ease the challenges caused by the combinatorial nature of community properties, we develop a novel shadowing bootstrap method.  Utilizing the symmetry, our method can find a shadowing representative of the true assignment and the number of tested assignments in the alternative is largely reduced.  In theory, we introduce a combinatorial distance between two community classes and show a combinatorial-probabilistic trade-off phenomenon. Our test is honest as long as the product of the combinatorial distance between two communities and the probabilistic distance between two connection probabilities is sufficiently large.  Besides, we show that such trade-off also exists in the information-theoretic lower bound.  We also implement numerical experiments to show the validity of our method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                74. <h><a href=https://openreview.net/pdf?id=OJ8aSjCaMNK>Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Juhan Bae, Michael R. Zhang, Michael Ruan, Eric Wang, So Hasegawa, Jimmy Ba, Roger Baker Grosse</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OJ8aSjCaMNK>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> MR-VAEs can construct the rate-distortion curve in a single training run.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OJ8aSjCaMNK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Variational autoencoders (VAEs) are powerful tools for learning latent representations of data used in a wide range of applications. In practice, VAEs usually require multiple training rounds to choose the amount of information the latent variable should retain. This trade-off between the reconstruction error (distortion) and the KL divergence (rate) is typically parameterized by a hyperparameter $\beta$. In this paper, we introduce Multi-Rate VAE (MR-VAE), a computationally efficient framework for learning optimal parameters corresponding to various $\beta$ in a single training run. The key idea is to explicitly formulate a response function using hypernetworks that maps $\beta$ to the optimal parameters. MR-VAEs construct a compact response hypernetwork where the pre-activations are conditionally gated based on $\beta$. We justify the proposed architecture by analyzing linear VAEs and showing that it can represent response functions exactly for linear VAEs. With the learned hypernetwork, MR-VAEs can construct the rate-distortion curve without additional training and can be deployed with significantly less hyperparameter tuning. Empirically, our approach is competitive and often exceeds the performance of multiple $\beta$-VAEs training with minimal computation and memory overheads.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                75. <h><a href=https://openreview.net/pdf?id=-Aw0rrrPUF>GLM-130B: An Open Bilingual Pre-trained Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, Jie Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-Aw0rrrPUF>8.0 - [8, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-Aw0rrrPUF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and divergence. In this paper, we introduce the pre-training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B—the largest Chinese language model—across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization with almost no performance loss, making it the first among 100B-scale models and more importantly, allowing its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                76. <h><a href=https://openreview.net/pdf?id=WH1yCa0TbB>Learning Diffusion Bridges on Constrained Domains</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xingchao Liu, Lemeng Wu, Mao Ye, qiang liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WH1yCa0TbB>8.0 - [8, 8, 6, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WH1yCa0TbB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models have achieved promising results on generative learning recently. However, because diffusion processes are most naturally applied  on the unconstrained Euclidean space $\mathrm{R}^d$, key challenges arise for developing diffusion based models for learning data on constrained and structured domains. We present a simple and unified framework to achieve this that can be easily adopted to various types of domains, including product spaces of any type (be it bounded/unbounded, continuous/discrete, categorical/ordinal, or  their mix). In our model, the diffusion process is driven by a drift force that is a sum of two terms: one singular force designed by $Doob&#39;s~ h$-$transform$ that ensures all outcomes of the process to belong to the desirable domain, and one non-singular neural force field that is trained to make sure the outcome follows the data distribution statistically. Experiments show that our methods perform superbly on generating tabular data, images, semantic segments and 3D point clouds. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                77. <h><a href=https://openreview.net/pdf?id=K96AogLDT2K>Symmetric Pruning in Quantum Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xinbiao Wang, Junyu Liu, Tongliang Liu, Yong Luo, Yuxuan Du, Dacheng Tao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=K96AogLDT2K>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove how the symmetry enhances the training performance of QNNs and then devise an efficient symmetric pruning scheme to distill a symmetric ansatz from an over-parameterized and asymmetric ansatz.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/K96AogLDT2K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many fundamental properties of a quantum system are captured by its Hamiltonian and ground state. Despite the significance,  ground states preparation (GSP) is classically intractable for large-scale Hamiltonians. Quantum neural networks (QNNs), which exert the power of modern quantum machines, have emerged as a leading protocol to conquer this issue. As such, the performance enhancement of QNNs becomes the core in GSP. Empirical evidence showed that QNNs with handcraft symmetric ans\&#34;atze generally experience better trainability than those with asymmetric ans\&#34;atze, while theoretical explanations remain vague. To fill this knowledge gap, here we propose the effective quantum neural tangent kernel (EQNTK) and connect this concept with over-parameterization theory to quantify the convergence of QNNs towards the global optima. We uncover that the advance of symmetric ans\&#34;atze attributes to their large EQNTK value with low effective dimension, which requests few parameters and quantum circuit depth to reach the over-parameterization regime permitting a benign loss landscape and fast convergence. Guided by EQNTK, we further devise a symmetric pruning (SP) scheme to automatically tailor a symmetric ansatz from an over-parameterized and asymmetric one to greatly improve the performance of QNNs when the explicit symmetry information of Hamiltonian is unavailable. Extensive numerical simulations are conducted to validate the analytical results of EQNTK and the effectiveness of SP. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                78. <h><a href=https://openreview.net/pdf?id=MIMwy4kh9lf>Open-Vocabulary Object Detection upon Frozen Vision and Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, Anelia Angelova</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MIMwy4kh9lf>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel open-vocabulary detection approach by building upon frozen vision and language models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MIMwy4kh9lf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present F-VLM, a simple open-vocabulary object detection method built uponFrozenVision andLanguageModels.  F-VLM simplifies the current multi-stage training pipeline by eliminating the need for knowledge distillation or detection-tailored pretraining.  Surprisingly, we observe that a frozen VLM: 1) retains the locality-sensitive features necessary for detection, and 2) is a strong region classifier.  We finetune only the detector head and combine the detector and VLM outputs for each region at inference time.  F-VLM shows compelling scaling behavior and achieves +6.5 mask AP improvement over the previous state of theart  on  novel  categories  of  LVIS  open-vocabulary  detection  benchmark. In addition, we demonstrate very competitive results on COCO open-vocabulary detection benchmark and cross-dataset transfer detection, in addition to significant training speed-up and compute savings. Code will be released.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                79. <h><a href=https://openreview.net/pdf?id=ZTK3SefE8_Z>Symbolic Physics Learner: Discovering governing equations via Monte Carlo tree search</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fangzheng Sun, Yang Liu, Jian-Xun Wang, Hao Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZTK3SefE8_Z>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Proposed a novel Symbolic Physics Learner (SPL) machine to discover the mathematical structure of nonlinear dynamics based on limited measurement data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZTK3SefE8_Z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Nonlinear dynamics is ubiquitous in nature and commonly seen in various science and engineering disciplines. Distilling analytical expressions that govern nonlinear dynamics from limited data remains vital but challenging. To tackle this fundamental issue, we propose a novel Symbolic Physics Learner (SPL) machine to discover the mathematical structure of nonlinear dynamics. The key concept is to interpret mathematical operations and system state variables by computational rules and symbols, establish symbolic reasoning of mathematical formulas via expression trees, and employ a Monte Carlo tree search (MCTS) agent to explore optimal expression trees based on measurement data. The MCTS agent obtains an optimistic selection policy through the traversal of expression trees, featuring the one that maps to the arithmetic expression of underlying physics. Salient features of the proposed framework include search flexibility and enforcement of parsimony for discovered equations. The efficacy and superiority of the SPL machine are demonstrated by numerical examples, compared with state-of-the-art baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                80. <h><a href=https://openreview.net/pdf?id=oiwXWPDTyNk>Concept-level Debugging of Part-Prototype Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrea Bontempelli, Stefano Teso, Katya Tentori, Fausto Giunchiglia, Andrea Passerini</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oiwXWPDTyNk>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel and human-friendly concept-level debugger for part-prototype networks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oiwXWPDTyNk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Part-prototype Networks (ProtoPNets) are concept-based classifiers designed to achieve the same performance as black-box models without compromising transparency. ProtoPNets compute predictions based on similarity to class-specific part-prototypes learned to recognize parts of training examples, making it easy to faithfully determine what examples are responsible for any target prediction and why. However, like other models, they are prone to picking up confounders and shortcuts from the data, thus suffering from compromised prediction accuracy and limited generalization. We propose ProtoPDebug, an effective concept-level debugger for ProtoPNets in which a human supervisor, guided by the model’s explanations, supplies feedback in the form of what part-prototypes must be forgotten or kept, and the model is fine-tuned to align with this supervision. Our experimental evaluation shows that ProtoPDebug outperforms state-of-the-art debuggers for a fraction of the annotation cost. An online experiment with laypeople confirms the simplicity of the feedback requested to the users and the effectiveness of the collected feedback for learning confounder-free part-prototypes. ProtoPDebug is a promising tool for trustworthy interactive learning in critical applications, as suggested by a preliminary evaluation on a medical decision making task.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                81. <h><a href=https://openreview.net/pdf?id=UxqUgchwXkK>Fast Nonlinear Vector Quantile Regression</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aviv A. Rosenberg, Sanketh Vedula, Yaniv Romano, Alexander Bronstein</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UxqUgchwXkK>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We extend Vector Quantile Regression to support non-linear specification, while ensuring monotonicity and scaling to millions of samples.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UxqUgchwXkK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    $$
\newcommand{\rvar}[1]{\mathrm {#1}}
\newcommand{\rvec}[1]{\boldsymbol{\mathrm{#1}}}
$$
Quantile regression (QR) is a powerful tool for estimating one or more conditional quantiles of a target variable $\rvar{Y}$ given explanatory features $\rvec{X}$.
A limitation of QR is that it is only defined for scalar target variables, due to the formulation of its objective function, and since the notion of quantiles has no standard definition for multivariate distributions.
Recently, vector quantile regression (VQR) was proposed as an extension of QR for vector-valued target variables, thanks to a meaningful generalization of the notion of quantiles to multivariate distributions via optimal transport.
Despite its elegance, VQR is arguably not applicable in practice due to several limitations:
(i) it assumes a linear model for the quantiles of the target $\rvec{Y}$ given the features $\rvec{X}$;
(ii) its exact formulation is intractable even for modestly-sized problems in terms of target dimensions, number of regressed quantile levels, or number of features, and its relaxed dual formulation may violate the monotonicity of the estimated quantiles;
(iii) no fast or scalable solvers for VQR currently exist.

In this work we fully address these limitations, namely:
(i) We extend VQR to the non-linear case, showing substantial improvement over linear VQR;
(ii) We propose {vector monotone rearrangement}, a method which ensures the quantile functions estimated by VQR are monotone functions;
(iii) We provide fast, GPU-accelerated solvers for linear and nonlinear VQR which maintain a fixed memory footprint, and demonstrate that they scale to millions of samples and thousands of quantile levels;
(iv) We release an optimized python package of our solvers as to widespread the use of VQR in real-world applications.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                82. <h><a href=https://openreview.net/pdf?id=7JsGYvjE88d>Fast and Precise: Adjusting Planning Horizon with Adaptive Subgoal Search</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michał Zawalski, Michał Tyrolski, Konrad Czechowski, Tomasz Odrzygóźdź, Damian Stachura, Piotr Piękos, Yuhuai Wu, Łukasz Kuciński, Piotr Miłoś</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7JsGYvjE88d>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose Adaptive Subgoal Search (AdaSubS), a search algorithm that adjusts the planning horizon to match the local complexity of the solved problems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7JsGYvjE88d.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Complex reasoning problems contain states that vary in the computational cost required to determine the right action plan. To take advantage of this property, we propose Adaptive Subgoal Search (AdaSubS), a search method that adaptively adjusts the planning horizon. To this end, AdaSubS generates diverse sets of subgoals at different distances. A verification mechanism is employed to filter out unreachable subgoals swiftly, making it possible to focus on feasible further subgoals. In this way, AdaSubS benefits from the efficiency of planning with longer-term subgoals and the fine control with shorter-term ones, and thus scales well to difficult planning problems. We show that AdaSubS significantly surpasses hierarchical planning algorithms on three complex reasoning tasks: Sokoban, the Rubik’s Cube, and the inequality-proving benchmark INT. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                83. <h><a href=https://openreview.net/pdf?id=SMYdcXjJh1q>Aligning Model and Macaque Inferior Temporal Cortex Representations Improves Model-to-Human Behavioral Alignment and Adversarial Robustness</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Joel Dapello, Kohitij Kar, Martin Schrimpf, Robert Baldwin Geary, Michael Ferguson, David Daniel Cox, James J. DiCarlo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SMYdcXjJh1q>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Aligning late stage model representations with neural recordings from macaque IT broadly improves adversarial robustness and alignment on human behavior.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SMYdcXjJh1q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While some state-of-the-art artificial neural network systems in computer vision are strikingly accurate models of the corresponding primate visual processing, there are still many discrepancies between these models and the behavior of primates on object recognition tasks. Many current models suffer from extreme sensitivity to adversarial attacks and often do not align well with the image-by-image behavioral error patterns observed in humans. Previous research has provided strong evidence that primate object recognition behavior can be very accurately predicted by neural population activity in the inferior temporal (IT) cortex, a brain area in the late stages of the visual processing hierarchy. Therefore, here we directly test whether making the late stage representations of models more similar to that of macaque IT produces new models that exhibit more robust, primate-like behavior. We conducted chronic, large-scale multi-electrode recordings  across the IT cortex in six non-human primates (rhesus macaques). We then use these data to fine-tune (end-to-end) the model &#34;IT&#34; representations such that they are more aligned with the biological IT representations, while preserving accuracy on object recognition tasks. We generate a cohort of models with a range of IT similarity scores validated on held-out animals across two image sets with distinct statistics. Across a battery of optimization conditions, we observed a strong correlation between the models&#39; IT-likeness and alignment with human behavior, as well as an increase in its adversarial robustness. We further assessed the limitations of this approach and find that the improvements in behavioral alignment and adversarial robustness generalize across different image statistics, but not to object categories outside of those covered in our IT training set. Taken together, our results demonstrate that building models that are more aligned with the primate brain leads to more robust and human-like behavior, and call for larger neural data-sets to further augment these gains.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                84. <h><a href=https://openreview.net/pdf?id=o3yygm3lnzS>PV3D: A 3D Generative Model for Portrait Video Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eric Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Wenqing Zhang, Song Bai, Jiashi Feng, Mike Zheng Shou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=o3yygm3lnzS>8.0 - [6, 10, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/o3yygm3lnzS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent advances in generative adversarial networks (GANs) have demonstrated the capabilities of generating stunning photo-realistic portrait images. While some prior works have applied such image GANs to unconditional 2D portrait video generation and static 3D portrait synthesis, there are few works successfully extending GANs for generating 3D-aware portrait videos. In this work, we propose PV3D, the first generative framework that can synthesize multi-view consistent portrait videos. Specifically, our method extends the recent static 3D-aware image GAN to the video domain by generalizing the 3D implicit neural representation to model the spatio-temporal space. To introduce motion dynamics into the generation process, we develop a motion generator by stacking multiple motion layers to generate motion features via modulated convolution. To alleviate motion ambiguities caused by camera/human motions, we propose a simple yet effective camera condition strategy for PV3D, enabling both temporal and multi-view consistent video generation. Moreover, PV3D introduces two discriminators for regularizing the spatial and temporal domains to ensure the plausibility of the generated portrait videos. These elaborated designs enable PV3D to generate 3D-aware motion-plausible portrait videos with high-quality appearance and geometry, significantly outperforming prior works. As a result, PV3D is able to support downstream applications such as static portrait animation and view-consistent motion editing. Code and models are available at https://showlab.github.io/pv3d.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                85. <h><a href=https://openreview.net/pdf?id=aMXD8gqsIiC>A Higher Precision Algorithm for Computing the $1$-Wasserstein Distance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pankaj K Agarwal, Sharath Raghvendra, Pouyan Shirzadian, Rachita Sowle</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aMXD8gqsIiC>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aMXD8gqsIiC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the problem of computing the $1$-Wasserstein distance $\mathcal{W}(\mu,\nu)$ between two $d$-dimensional discrete distributions $\mu$ and $\nu$ whose support lie within the unit hypercube. There are several algorithms that estimate $\mathcal{W}(\mu,\nu)$ within an additive error of $\varepsilon$. However, when $\mathcal{W}(\mu,\nu)$ is small, the additive error $\varepsilon$ dominates, leading to noisy results. Consider any additive approximation algorithm with execution time $T(n,\varepsilon)$. We propose an algorithm that runs in $O(T(n,\varepsilon/d) \log n)$ time and boosts the accuracy of estimating $\mathcal{W}(\mu,\nu)$ from $\varepsilon$ to an expected additive error of $\min\{\varepsilon, (d\log_{\sqrt{d}/\varepsilon} n)\mathcal{W}(\mu,\nu)\}$. For the special case where every point in the support of $\mu$ and $\nu$ has a mass of $1/n$ (also called the Euclidean Bipartite Matching problem), we describe an algorithm to boost the accuracy of any additive approximation algorithm from $\varepsilon$ to an expected additive error of $\min\{\varepsilon, (d\log\log n)\mathcal{W}(\mu,\nu)\}$ in $O(T(n, \varepsilon/d)\log\log n)$ time. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                86. <h><a href=https://openreview.net/pdf?id=WAgXmT8BeRj>MARS: Meta-learning as Score Matching in the Function Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Krunoslav Lehman Pavasovic, Jonas Rothfuss, Andreas Krause</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WAgXmT8BeRj>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Meta-learning in the function space by estimating the score function of the data-generating process marginals.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WAgXmT8BeRj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Meta-learning aims to extract useful inductive biases from a set of related datasets. In Bayesian meta-learning, this is typically achieved by constructing a prior distribution over neural network parameters. However, specifying families of computationally viable prior distributions over the high-dimensional neural network parameters is difficult. As a result, existing approaches resort to meta-learning restrictive diagonal Gaussian priors, severely limiting their expressiveness and performance. To circumvent these issues, we approach meta-learning through the lens of functional Bayesian neural network inference which views the prior as a stochastic process and performs inference in the function space. Specifically, we view the meta-training tasks as samples from the data-generating process and formalize meta-learning as empirically estimating the law of this stochastic process. Our approach can seamlessly acquire and represent complex prior knowledge by meta-learning the score function of the data-generating process marginals instead of parameter space priors. In a comprehensive benchmark, we demonstrate that our method achieves state-of-the-art performance in terms of predictive accuracy and substantial improvements in the quality of uncertainty estimates.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                87. <h><a href=https://openreview.net/pdf?id=688hNNMigVX>Learning a Data-Driven Policy Network for Pre-Training Automated Feature Engineering</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Liyao Li, Haobo Wang, Liangyu Zha, Qingyi Huang, Sai Wu, Gang Chen, Junbo Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=688hNNMigVX>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a data-driven automated feature engineering framework Fetch.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/688hNNMigVX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Feature engineering is widely acknowledged to be pivotal in tabular data analysis and prediction. Automated feature engineering (AutoFE) emerged to automate this process managed by experienced data scientists and engineers conventionally. In this area, most — if not all — prior work adopted an identical framework from the neural architecture search (NAS) method. While feasible, we posit that the NAS framework very much contradicts the way how human experts cope with the data since the inherent Markov decision process (MDP) setup differs. We point out that its data-unobserved setup consequentially results in an incapability to generalize across different datasets as well as also high computational cost. This paper proposes a novel AutoFE framework Feature Set Data-Driven Search (FETCH), a pipeline mainly for feature generation and selection. Notably, FETCH is built on a brand-new data-driven MDP setup using the tabular dataset as the state fed into the policy network. Further, we posit that the crucial merit of FETCH is its transferability where the yielded policy network trained on a variety of datasets is indeed capable to enact feature engineering on unseen data, without requiring additional exploration. To the best of our knowledge, this is a pioneer attempt to build a tabular data pre-training paradigm via AutoFE. Extensive experiments show that FETCH systematically surpasses the current state-of-the-art AutoFE methods and validates the transferability of AutoFE pre-training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                88. <h><a href=https://openreview.net/pdf?id=JpbLyEI5EwW>Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional Data </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Spencer Frei, Gal Vardi, Peter Bartlett, Nathan Srebro, Wei Hu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JpbLyEI5EwW>8.0 - [8, 8, 6, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JpbLyEI5EwW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The implicit biases of gradient-based optimization algorithms are conjectured to be a major factor in the success of modern deep learning.  In this work, we investigate the implicit bias of gradient flow and gradient descent in two-layer fully-connected neural networks with leaky ReLU activations when the training data are nearly-orthogonal, a common property of high-dimensional data.  For gradient flow, we leverage recent work on the implicit bias for homogeneous neural networks to show that asymptotically, gradient flow produces a neural network with rank at most two.  Moreover, this network is an $\ell_2$-max-margin solution (in parameter space), and has a linear decision boundary that corresponds to an approximate-max-margin linear predictor.  For gradient descent, provided the random initialization variance is small enough, we show that a single step of gradient descent suffices to drastically reduce the rank of the network, and that the rank remains small throughout training.  We provide experiments which suggest that a small initialization scale is important for finding low-rank neural networks with gradient descent. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                89. <h><a href=https://openreview.net/pdf?id=OAw6V3ZAhSd>HyperDeepONet: learning operator with complex target function space using the limited resources via hypernetwork</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jae Yong Lee, SungWoong CHO, Hyung Ju Hwang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OAw6V3ZAhSd>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OAw6V3ZAhSd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Fast and accurate predictions for complex physical dynamics are a big challenge across various applications. Real-time prediction on resource-constrained hardware is even more crucial in the real-world problems. The deep operator network (DeepONet) has recently been proposed as a framework for learning nonlinear mappings between function spaces. However, the DeepONet requires many parameters and has a high computational cost when learning operators, particularly those with complex (discontinuous or non-smooth) target functions. In this study, we propose HyperDeepONet, which uses the expressive power of the hypernetwork to enable learning of a complex operator with smaller set of parameters. The DeepONet and its variant models can be thought of as a method of injecting the input function information into the target function. From this perspective, these models can be viewed as a special case of HyperDeepONet. We analyze the complexity of DeepONet and conclude that HyperDeepONet needs relatively lower complexity to obtain the desired accuracy for operator learning. HyperDeepONet was successfully applied to various operator learning problems using low computational resources compared to other benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                90. <h><a href=https://openreview.net/pdf?id=QB1dMPEXau5>Does Deep Learning Learn to Abstract? A Systematic Probing Framework</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shengnan An, Zeqi Lin, Bei Chen, Qiang Fu, Nanning Zheng, Jian-Guang Lou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QB1dMPEXau5>8.0 - [8, 8, 6, 10] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design a systematic probing framework along with a set of controlled probing tasks, providing strong evidence that PLMs have the abstraction capability. We conduct an in-depth analysis and provide insightful conclusions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QB1dMPEXau5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Abstraction is a desirable capability for deep learning models, which means to induce abstract concepts from concrete instances and flexibly apply them beyond the learning context. At the same time, there is a lack of clear understanding about both the presence and further characteristics of this capability in deep learning models. In this paper, we introduce a systematic probing framework to explore the abstraction capability of deep learning models from a transferability perspective. A set of controlled experiments are conducted based on this framework, providing strong evidence that two probed pre-trained language models (PLMs), T5 and GPT2, have the abstraction capability. We also conduct in-depth analysis, thus shedding further light: (1) the whole training phase exhibits a &#34;memorize-then-abstract&#34; two-stage process; (2) the learned abstract concepts are gathered in a few middle-layer attention heads, rather than being evenly distributed throughout the model; (3) the probed abstraction capabilities exhibit robustness against concept mutations, and are more robust to low-level/source-side mutations than high-level/target-side ones; (4) generic pre-training is critical to the emergence of abstraction capability, and PLMs exhibit better abstraction with larger model sizes and data scales.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                91. <h><a href=https://openreview.net/pdf?id=GcM7qfl5zY>AutoGT: Automated Graph Transformer Architecture Search</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zizhao Zhang, Xin Wang, Chaoyu Guan, Ziwei Zhang, Haoyang Li, Wenwu Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GcM7qfl5zY>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GcM7qfl5zY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Although Transformer architectures have been successfully applied to graph data with the advent of Graph Transformer, current design of Graph Transformer still heavily relies on human labor and expertise knowledge to decide proper neural architectures and suitable graph encoding strategies at each Transformer layer. In literature, there have been some works on automated design of Transformers focusing on non-graph data such as texts and images without considering graph encoding strategies, which fail to handle the non-euclidean graph data. In this paper, we study the problem of automated graph Transformer, for the first time. However, solving these problems poses the following challenges: i) how can we design a unified search space for graph Transformer, and ii) how to deal with the coupling relations between Transformer architectures and the graph encodings of each Transformer layer. To address these challenges, we propose Automated Graph Transformer (AutoGT), a neural architecture search framework that can automatically discover the optimal graph Transformer architectures by joint optimization of Transformer architecture and graph encoding strategies. Specifically, we first propose a unified graph Transformer formulation that can represent most of state-of-the-art graph Transformer architectures. Based upon the unified formulation, we further design the graph Transformer search space that includes both candidate architectures and various graph encodings. To handle the coupling relations, we propose a novel encoding-aware performance estimation strategy by gradually training and splitting the supernets according to the correlations between graph encodings and architectures. The proposed strategy can provide a more consistent and fine-grained performance prediction when evaluating the jointly optimized graph encodings and architectures. Extensive experiments and ablation studies show that our proposed AutoGT gains sufficient improvement over state-of-the-art hand-crafted baselines on all datasets, demonstrating its effectiveness and wide applicability.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                92. <h><a href=https://openreview.net/pdf?id=dqITIpZ5Z4b>A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zixiang Chen, Chris Junchi Li, Huizhuo Yuan, Quanquan Gu, Michael Jordan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dqITIpZ5Z4b>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a unified framework that nearly includes all model-free and model-based RL classes while maintaining sharp sample efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dqITIpZ5Z4b.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With the increasing need for handling large state and action spaces, general function approximation has become a key technique in reinforcement learning (RL). In this paper, we propose a general framework that unifies model-based and model-free RL, and an  Admissible Bellman Characterization (ABC) class that subsumes nearly all Markov decision process (MDP) models in the literature for tractable RL. We propose a novel estimation function with decomposable structural properties for optimization-based exploration and the functional Eluder dimension as a complexity measure of the ABC class. Under our framework, a new sample-efficient algorithm namely OPtimization-based ExploRation with Approximation (OPERA) is proposed, achieving regret bounds that match or improve over the best-known results for a variety of MDP models. In particular, for MDPs with low Witness rank, under a slightly stronger assumption, OPERA improves the state-of-the-art sample complexity results by a factor of $dH$. Our framework provides a generic interface to design and analyze new RL models and algorithms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                93. <h><a href=https://openreview.net/pdf?id=P4MUGRM4Acu>The Surprising Effectiveness of Equivariant Models in Domains with Latent Symmetry</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dian Wang, Jung Yeon Park, Neel Sortur, Lawson L.S. Wong, Robin Walters, Robert Platt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=P4MUGRM4Acu>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper discovers that equivariant models are surprisingly effective in domains with latent or partial symmetries. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/P4MUGRM4Acu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Extensive work has demonstrated that equivariant neural networks can significantly improve sample efficiency and generalization by enforcing an inductive bias in the network architecture. These applications typically assume that the domain symmetry is fully described by explicit transformations of the model inputs and outputs. However, many real-life applications contain only latent or partial symmetries which cannot be easily described by simple transformations of the input. In these cases, it is necessary to learn symmetry in the environment instead of imposing it mathematically on the network architecture. We discover, surprisingly, that imposing equivariance constraints that do not exactly match the domain symmetry is very helpful in learning the true symmetry in the environment. We differentiate between extrinsic and incorrect symmetry constraints and show that while imposing incorrect symmetry can impede the model&#39;s performance, imposing extrinsic symmetry can actually improve performance. We demonstrate that an equivariant model can significantly outperform non-equivariant methods on domains with latent symmetries both in supervised learning and in reinforcement learning for robotic manipulation and control problems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                94. <h><a href=https://openreview.net/pdf?id=JL7Va5Vy15J>The Lie Derivative for Measuring Learned Equivariance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nate Gruver, Marc Anton Finzi, Micah Goldblum, Andrew Gordon Wilson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JL7Va5Vy15J>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JL7Va5Vy15J.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Equivariance guarantees that a model&#39;s predictions capture key symmetries in data. When an image is translated or rotated, an equivariant model&#39;s representation of that image will translate or rotate accordingly. The success of convolutional neural networks has historically been tied to translation equivariance directly encoded in their architecture. The rising success of vision transformers, which have no explicit architectural bias towards equivariance, challenges this narrative and suggests that augmentations and training data might also play a significant role in their performance. In order to better understand the role of equivariance in recent vision models, we apply the Lie derivative, a method for measuring equivariance with strong mathematical foundations and minimal hyperparameters. Using the Lie derivative, we study the equivariance properties of hundreds of pretrained models, spanning CNNs, transformers, and Mixer architectures. The scale of our analysis allows us to separate the impact of architecture from other factors like model size or training method. Surprisingly, we find that many violations of equivariance can be linked to spatial aliasing in ubiquitous network layers, such as pointwise non-linearities, and that as models get larger and more accurate they tend to display more equivariance, regardless of architecture. For example, transformers can be more equivariant than convolutional neural networks after training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                95. <h><a href=https://openreview.net/pdf?id=K7CbYQbyYhY>Agree to Disagree: Diversity through Disagreement for Better Transferability</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matteo Pagliardini, Martin Jaggi, François Fleuret, Sai Praneeth Karimireddy</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=K7CbYQbyYhY>8.0 - [8, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/K7CbYQbyYhY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Gradient-based learning algorithms have an implicit \emph{simplicity bias} which in effect can limit the diversity of predictors being sampled by the learning procedure. This behavior can hinder the transferability of trained models by (i) favoring the learning of simpler but spurious features --- present in the training data but absent from the test data --- and (ii) by only leveraging a small subset of predictive features.  Such an effect is especially magnified when the test distribution does not exactly match the train distribution---referred to as the Out of Distribution (OOD) generalization problem. However, given only the training data, it is not always possible to apriori assess if a given feature is spurious or transferable. Instead, we advocate for learning an ensemble of models which capture a diverse set of predictive features. Towards this, we propose a new algorithm D-BAT (Diversity-By-disAgreement Training), which enforces agreement among the models on the training data, but disagreement on the OOD data. We show how D-BAT naturally emerges from the notion of generalized discrepancy, as well as demonstrate in multiple experiments how the proposed method can mitigate shortcut-learning, enhance uncertainty and OOD detection, as well as improve transferability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                96. <h><a href=https://openreview.net/pdf?id=xkev3_np08z>ExpressivE: A Spatio-Functional Embedding For Knowledge Graph Completion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aleksandar Pavlović, Emanuel Sallinger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xkev3_np08z>8.0 - [6, 10, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> ExpressivE: A fully expressive KGC model that captures a rich set of patterns with an intuitive geometric interpretation and state-of-the-art performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xkev3_np08z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge graphs are inherently incomplete. Therefore substantial research has been directed toward knowledge graph completion (KGC), i.e., predicting missing triples from the information represented in the knowledge graph (KG). KG embedding models (KGEs) have yielded promising results for KGC, yet any current KGE is incapable of: (1) fully capturing vital inference patterns (e.g., composition), (2) capturing prominent patterns jointly (e.g., hierarchy and composition), and (3) providing an intuitive interpretation of captured patterns. In this work, we propose ExpressivE, a fully expressive spatio-functional KGE that solves all these challenges simultaneously. ExpressivE embeds pairs of entities as points and relations as hyper-parallelograms in the virtual triple space $\mathbb{R}^{2d}$. This model design allows ExpressivE not only to capture a rich set of inference patterns jointly but additionally to display any supported inference pattern through the spatial relation of hyper-parallelograms, offering an intuitive and consistent geometric interpretation of ExpressivE embeddings and their captured patterns. Experimental results on standard KGC benchmarks reveal that ExpressivE is competitive with state-of-the-art KGEs and even significantly outperforms them on WN18RR.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                97. <h><a href=https://openreview.net/pdf?id=74A-FDAyiL>Subquadratic Algorithms for Kernel Matrices via Kernel Density Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ainesh Bakshi, Piotr Indyk, Praneeth Kacham, Sandeep Silwal, Samson Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=74A-FDAyiL>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We give a framework for using recently developed tools for kernel density estimation to solve downstream kernel problems in sub-quadratic time.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/74A-FDAyiL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Kernel matrices, as well as weighted graphs represented by them, are ubiquitous objects in machine learning, statistics and other related fields. The main drawback of using kernel methods (learning and inference using kernel matrices) is efficiency -- given $n$ input points, most kernel-based algorithms need to materialize the full $n \times n$ kernel matrix before performing any subsequent computation, thus incurring $\Omega(n^2)$ runtime. Breaking this quadratic barrier for various problems has therefore, been a subject of extensive research efforts. 

We break the quadratic barrier and obtain \emph{subquadratic} time  algorithms for several fundamental linear-algebraic and graph processing primitives, including approximating the top eigenvalue and eigenvector, spectral sparsification, solving linear systems, local clustering, low-rank approximation, arboricity estimation and counting weighted triangles. We build on the recently developed Kernel Density Estimation framework, which (after preprocessing in time subquadratic in $n$) can return estimates of row/column sums of the kernel matrix. In particular, we develop efficient reductions from \emph{weighted vertex} and \emph{weighted edge sampling} on kernel graphs, \emph{simulating random walks} on kernel graphs, and \emph{importance sampling} on matrices to Kernel Density Estimation and show that we can generate samples from these distributions in \emph{sublinear} (in the support of the distribution) time. Our reductions are the central ingredient in each of our applications and we believe they may be of independent interest. We empirically demonstrate the efficacy of our algorithms on low-rank approximation (LRA) and spectral sparsification, where we observe a $\textbf{9x}$ decrease in the number of kernel evaluations over baselines for LRA and a $\textbf{41x}$ reduction in the graph size for spectral sparsification.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                98. <h><a href=https://openreview.net/pdf?id=Y5SEe3dfniJ>Compressing multidimensional weather and climate data into neural networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Langwen Huang, Torsten Hoefler</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Y5SEe3dfniJ>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We compress weather and climate data into neural network weights.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Y5SEe3dfniJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Weather and climate simulations produce petabytes of high-resolution data that are later analyzed by researchers in order to understand climate change or severe weather. We propose a new method of compressing this multidimensional weather and climate data: a coordinate-based neural network is trained to overfit the data, and the resulting parameters are taken as a compact representation of the original grid-based data. While compression ratios range from 300x to more than 3,000x, our method outperforms the state-of-the-art compressor SZ3 in terms of weighted RMSE, MAE. It can faithfully preserve important large scale atmosphere structures and does not introduce significant artifacts.
When using the resulting neural network as a 790x compressed dataloader to train the WeatherBench forecasting model, its RMSE increases by less than 2%. The three orders of magnitude compression democratizes access to high-resolution climate data and enables numerous new research directions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                99. <h><a href=https://openreview.net/pdf?id=yHLvIlE9RGN>Evaluating Long-Term Memory in 3D Mazes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jurgis Pašukonis, Timothy P Lillicrap, Danijar Hafner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yHLvIlE9RGN>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a benchmark environment and dataset for evaluating the memory abilities of RL agents and their representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yHLvIlE9RGN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Intelligent agents need to remember salient information to reason in partially-observed environments. For example, agents with a first-person view should remember the positions of relevant objects even if they go out of view. Similarly, to effectively navigate through rooms agents need to remember the floor plan of how rooms are connected. However, most benchmark tasks in reinforcement learning do not test long-term memory in agents, slowing down progress in this important research direction. In this paper, we introduce the Memory Maze, a 3D domain of randomized mazes specifically designed for evaluating long-term memory in agents. Unlike existing benchmarks, Memory Maze measures long-term memory separate from confounding agent abilities and requires the agent to localize itself by integrating information over time. With Memory Maze, we propose an online reinforcement learning benchmark, a diverse offline dataset, and an offline probing evaluation. Recording a human player establishes a strong baseline and verifies the need to build up and retain memories, which is reflected in their gradually increasing rewards within each episode. We find that current algorithms benefit from training with truncated backpropagation through time and succeed on small mazes, but fall short of human performance on the large mazes, leaving room for future algorithmic designs to be evaluated on the Memory Maze.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                100. <h><a href=https://openreview.net/pdf?id=RWtGreRpovS>Simplicial Embeddings in Self-Supervised Learning and Downstream Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samuel Lavoie, Christos Tsirigotis, Max Schwarzer, Ankit Vani, Michael Noukhovitch, Kenji Kawaguchi, Aaron Courville</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RWtGreRpovS>8.0 - [8, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use softmax to embed representations in a collection of simplices in SSL models, which offers improved generalization properties for downstream classification.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RWtGreRpovS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Simplicial Embeddings (SEM) are representations learned through self-supervised learning (SSL), wherein a representation is projected into $L$ simplices of $V$ dimensions each using a \texttt{softmax} operation. This procedure conditions the representation onto a constrained space during pretraining and imparts an inductive bias for group sparsity. For downstream classification, we formally prove that the SEM representation leads to better generalization than an unnormalized representation.
Furthermore, we empirically demonstrate that SSL methods trained with SEMs have improved generalization on natural image datasets such as CIFAR-100 and ImageNet. Finally, when used in a downstream classification task, we show that SEM features exhibit emergent semantic coherence where small groups of learned features are distinctly predictive of semantically-relevant classes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                101. <h><a href=https://openreview.net/pdf?id=lJdOlWg8td>Efficient recurrent architectures through activity sparsity and sparse back-propagation through time</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anand Subramoney, Khaleelulla Khan Nazeer, Mark Schöne, Christian Mayr, David Kappel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lJdOlWg8td>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We add a activity sparsity mechanism to the GRU using a thresholding function, which makes both the forward and backward passes computationally sparse. This model achieves competitive performance on various benchmarks including language modeling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lJdOlWg8td.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and  low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events.  We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at https://github.com/KhaleelKhan/EvNN/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                102. <h><a href=https://openreview.net/pdf?id=dLAYGdKTi2>Mitigating Gradient Bias in Multi-objective Learning: A Provably Convergent Approach</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Heshan Devaka Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram Murugesan, Tianyi Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dLAYGdKTi2>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a gradient based multi-objective optimization algorithm which provably convergence to a Pareto stationary point in stochastic convex and non-convex settings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dLAYGdKTi2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many machine learning problems today have multiple objective functions. They appear either in learning with multiple criteria where learning has to make a trade-off between multiple performance metrics such as fairness, safety and accuracy; or, in multi-task learning where multiple tasks are optimized jointly, sharing inductive bias between them. This problems are often tackled by the multi-objective optimization framework. However, existing stochastic multi-objective gradient methods and its variants (e.g., MGDA, PCGrad, CAGrad, etc.) all adopt a biased noisy gradient direction, which leads to degraded empirical performance. 
To this end, we develop a stochastic multi-objective gradient correction (MoCo) method for multi-objective optimization. The unique feature of our method is that it can guarantee convergence without increasing the batch size even in the nonconvex setting. Simulations on multi-task supervised and reinforcement learning demonstrate the effectiveness of our method relative to the state-of-the-art methods. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                103. <h><a href=https://openreview.net/pdf?id=De4FYqjFueZ>Transformers Learn Shortcuts to Automata</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=De4FYqjFueZ>8.0 - [6, 10, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Shallow, non-recurrent Transformers can simulate the recurrent dynamics of finite-state automata, via counterintuitive shortcuts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/De4FYqjFueZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are these shallow and non-recurrent models finding? We investigate this question in the setting of learning automata, discrete dynamical systems naturally suited to recurrent modeling and expressing algorithmic tasks. Our theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. By representing automata using the algebraic structure of their underlying transformation semigroups, we obtain $O(\log T)$-depth simulators for all automata and $O(1)$-depth simulators for all automata whose associated groups are solvable. Empirically, we perform synthetic experiments by training Transformers to simulate a wide variety of automata, and show that shortcut solutions can be learned via standard training. We further investigate the brittleness of these solutions and propose potential mitigations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                104. <h><a href=https://openreview.net/pdf?id=WE_vluYUL-X>ReAct: Synergizing Reasoning and Acting in Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, Yuan Cao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WE_vluYUL-X>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We synergize reasoning and action taking in language models and make them more capable, versatile and interpretable.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WE_vluYUL-X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                105. <h><a href=https://openreview.net/pdf?id=7i6OZa7oij>Strong inductive biases provably prevent harmless interpolation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Aerni, Marco Milanta, Konstantin Donhauser, Fanny Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7i6OZa7oij>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that the strength of a model’s inductive bias determines whether interpolation of noisy data is harmless or harmful.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7i6OZa7oij.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Classical wisdom suggests that estimators should avoid fitting noise to achieve good generalization. In contrast, modern overparameterized models can yield small test error despite interpolating noise — a phenomenon often called &#34;benign overfitting&#34; or &#34;harmless interpolation&#34;. This paper argues that the degree to which interpolation is harmless hinges upon the strength of an estimator&#39;s inductive bias, i.e., how heavily the estimator favors solutions with a certain structure: while strong inductive biases prevent harmless interpolation, weak inductive biases can even require fitting noise to generalize well. Our main theoretical result establishes tight non-asymptotic bounds for high-dimensional kernel regression that reflect this phenomenon for convolutional kernels, where the filter size regulates the strength of the inductive bias. We further provide empirical evidence of the same behavior for deep neural networks with varying filter sizes and rotational invariance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                106. <h><a href=https://openreview.net/pdf?id=eWKfMBL5to>Contrastive Corpus Attribution for Explaining Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chris Lin, Hugh Chen, Chanwoo Kim, Su-In Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eWKfMBL5to>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel method to explain representations (from unsupervised and supervised models) in terms of input features.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eWKfMBL5to.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the widespread use of unsupervised models, very few methods are designed to explain them. Most explanation methods explain a scalar model output. However, unsupervised models output representation vectors, the elements of which are not good candidates to explain because they lack semantic meaning. To bridge this gap, recent works defined a scalar explanation output: a dot product-based similarity in the representation space to the sample being explained (i.e., an explicand). Although this enabled explanations of unsupervised models, the interpretation of this approach can still be opaque because similarity to the explicand&#39;s representation may not be meaningful to humans. To address this, we propose contrastive corpus similarity, a novel and semantically meaningful scalar explanation output based on a reference corpus and a contrasting foil set of samples. We demonstrate that contrastive corpus similarity is compatible with many post-hoc feature attribution methods to generate COntrastive COrpus Attributions (COCOA) and quantitatively verify that features important to the corpus are identified. We showcase the utility of COCOA in two ways: (i) we draw insights by explaining augmentations of the same image in a contrastive learning setting (SimCLR); and (ii) we perform zero-shot object localization by explaining the similarity of image representations to jointly learned text representations (CLIP).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                107. <h><a href=https://openreview.net/pdf?id=hhvkdRdWt1F>Dual Algorithmic Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Danilo Numeroso, Davide Bacciu, Petar Veličković</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hhvkdRdWt1F>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A neural algorithmic reasoning approach exploiting the duality principle
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hhvkdRdWt1F.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Algorithmic Reasoning is an emerging area of machine learning which seeks to infuse algorithmic computation in neural networks, typically by training neural models to approximate steps of classical algorithms. In this context, much of the current work has focused on learning reachability and shortest path graph algorithms, showing that joint learning on similar algorithms is beneficial for generalisation. However, when targeting more complex problems, such &#34;similar&#34; algorithms become more difficult to find. Here, we propose to learn algorithms by exploiting duality of the underlying algorithmic problem. Many algorithms solve optimisation problems. We demonstrate that simultaneously learning the dual definition of these optimisation problems in algorithmic learning allows for better learning and qualitatively better solutions. Specifically, we exploit the max-flow min-cut theorem to simultaneously learn these two algorithms over synthetically generated graphs, demonstrating the effectiveness of the proposed approach. We then validate the real-world utility of our dual algorithmic reasoner by deploying it on a challenging brain vessel classification task, which likely depends on the vessels’ flow properties. We demonstrate a clear performance gain when using our model within such a context, and empirically show that learning the max-flow and min-cut algorithms together is critical for achieving such a result.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                108. <h><a href=https://openreview.net/pdf?id=_QLsH8gatwx>Simplicial Hopfield networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thomas F Burns, Tomoki Fukai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_QLsH8gatwx>8.0 - [8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Without increasing the number of parameters, we improve the memory capacity of Hopfield networks by adding setwise connections embedded in a simplicial complex.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_QLsH8gatwx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Hopfield networks are artificial neural networks which store memory patterns on the states of their neurons by choosing recurrent connection weights and update rules such that the energy landscape of the network forms attractors around the memories. How many stable, sufficiently-attracting memory patterns can we store in such a network using $N$ neurons? The answer depends on the choice of weights and update rule. Inspired by setwise connectivity in biology, we extend Hopfield networks by adding setwise connections and embedding these connections in a simplicial complex. Simplicial complexes are higher dimensional analogues of graphs which naturally represent collections of pairwise and setwise relationships. We show that our simplicial Hopfield networks increase memory storage capacity. Surprisingly, even when connections are limited to a small random subset of equivalent size to an all-pairwise network, our networks still outperform their pairwise counterparts. Such scenarios include non-trivial simplicial topology. We also test analogous modern continuous Hopfield networks, offering a potentially promising avenue for improving the attention mechanism in Transformer models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                109. <h><a href=https://openreview.net/pdf?id=b7SBTEBFnC>Canary in a Coalmine: Better Membership Inference with Ensembled Adversarial Queries</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuxin Wen, Arpit Bansal, Hamid Kazemi, Eitan Borgnia, Micah Goldblum, Jonas Geiping, Tom Goldstein</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b7SBTEBFnC>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b7SBTEBFnC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As industrial applications are increasingly automated by machine learning models, enforcing personal data ownership and intellectual property rights requires tracing training data back to their rightful owners. Membership inference algorithms approach this problem by using statistical techniques to discern whether a target sample was included in a model&#39;s training set. However, existing methods only utilize the unaltered target sample or simple augmentations of the target to compute statistics. Such a sparse sampling of the model&#39;s behavior carries little information, leading to poor inference capabilities. In this work, we use adversarial tools to directly optimize for queries that are discriminative and diverse. Our improvements achieve significantly more accurate membership inference than existing methods, especially in offline scenarios and in the low false-positive regime which is critical in legal settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                110. <h><a href=https://openreview.net/pdf?id=HXz7Vcm3VgM>ImageNet-X: Understanding Model Mistakes with Factor of Variation Annotations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Badr Youbi Idrissi, Diane Bouchacourt, Randall Balestriero, Ivan Evtimov, Caner Hazirbas, Nicolas Ballas, Pascal Vincent, Michal Drozdzal, David Lopez-Paz, Mark Ibrahim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HXz7Vcm3VgM>8.0 - [8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> we annotate ImageNet images with factor labels to explain model mistakes
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HXz7Vcm3VgM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning vision systems are widely deployed across applications where reliability is critical. However, even today&#39;s best models can fail to recognize an object when its pose, lighting, or background varies. While existing benchmarks surface examples challenging for models, they do not explain why such mistakes arise. To address this need, we introduce ImageNet-X—a set of sixteen human annotations of factors such as pose, background, or lighting the entire ImageNet-1k validation set as well as a random subset of 12k training images. Equipped with ImageNet-X, we investigate 2,200 current recognition models and study the types of mistakes as a function of model’s (1) architecture, e.g. transformer vs. convolutional, (2) learning paradigm, e.g. supervised vs. self-supervised, and (3) training procedures, e.g., data augmentation. Regardless of these choices, we find models have consistent failure modes across ImageNet-X categories. We also find that while data augmentation can improve robustness to certain factors, they induce spill-over effects to other factors. For example, color-jitter augmentation improves robustness to color and brightness, but surprisingly hurts robustness to pose. Together, these insights suggest to advance the robustness of modern vision models, future research should focus on collecting additional data and understanding data augmentation schemes. Along with these insights, we release a toolkit based on ImageNet-X to spur further study into the mistakes image recognition systems make.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                111. <h><a href=https://openreview.net/pdf?id=0jxPyVWmiiF>A Convergent Single-Loop Algorithm for Relaxation of Gromov-Wasserstein in Graph Data </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiajin Li, Jianheng Tang, Lemin Kong, Huikang Liu, Jia Li, Anthony Man-Cho So, Jose Blanchet</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0jxPyVWmiiF>8.0 - [8, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the first provable single-loop algorithm for computing the Gromov-Wasserstein (GW) distance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0jxPyVWmiiF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, we present the Bregman Alternating Projected Gradient (BAPG) method, a single-loop algorithm that offers an approximate solution to the Gromov-Wasserstein (GW) distance. 
We introduce a novel relaxation technique that balances accuracy and computational efficiency, albeit with some compromises in the feasibility of the coupling map.  Our analysis is based on the observation that the GW problem satisfies the Luo-Tseng error bound condition, which relates to estimating the distance of a point to the critical point set of the GW problem based on the optimality residual.
This observation allows us to provide an approximation bound for the distance between the fixed-point set of BAPG and the critical point set of GW. Moreover, under a mild  technical assumption, we can  show that BAPG converges to its fixed point set.
The effectiveness of BAPG has been validated through comprehensive numerical experiments in graph alignment and partition tasks, where it outperforms existing methods in terms of both solution quality and wall-clock time.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                112. <h><a href=https://openreview.net/pdf?id=iIfDQVyuFD>Confidential-PROFITT: Confidential PROof of FaIr Training of Trees</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ali Shahin Shamsabadi, Sierra Calanda Wyllie, Nicholas Franzese, Natalie Dullerud, Sébastien Gambs, Nicolas Papernot, Xiao Wang, Adrian Weller</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iIfDQVyuFD>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a method to provide a confidential proof of fair training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iIfDQVyuFD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Post hoc auditing of model fairness suffers from potential drawbacks: (1) auditing may be highly sensitive to the test samples chosen; (2) the model and/or its training data may need to be shared with an auditor thereby breaking confidentiality. We address these issues by instead providing a certificate that demonstrates that the learning algorithm itself is fair, and hence, as a consequence, so too is the trained model. We introduce a method to provide a confidential proof of fairness for training, in the context of widely used decision trees, which we term Confidential-PROFITT. We propose novel fair decision tree learning algorithms along with customized zero-knowledge proof protocols to obtain a proof of fairness that can be audited by a third party. Using zero-knowledge proofs enables us to guarantee confidentiality of both the model and its training data. We show empirically that bounding the information gain of each node with respect to the sensitive attributes reduces the unfairness of the final tree. In extensive experiments on the COMPAS, Communities and Crime, Default Credit, and Adult datasets, we demonstrate that a company can use Confidential-PROFITT to certify the fairness of their decision tree to an auditor in less than 2 minutes, thus indicating the applicability of our approach. This is true for both the demographic parity and equalized odds definitions of fairness. Finally, we extend Confidential-PROFITT to apply to ensembles of trees.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                113. <h><a href=https://openreview.net/pdf?id=067CGykiZTS>Scaling Up Probabilistic Circuits by Latent Variable Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anji Liu, Honghua Zhang, Guy Van den Broeck</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=067CGykiZTS>8.0 - [8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/067CGykiZTS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Probabilistic Circuits (PCs) are a unified framework for tractable probabilistic models that support efficient computation of various probabilistic queries (e.g., marginal probabilities). One key challenge is to scale PCs to model large and high-dimensional real-world datasets: we observe that as the number of parameters in PCs increases, their performance immediately plateaus. This phenomenon suggests that the existing optimizers fail to exploit the full expressive power of large PCs. We propose to overcome such bottleneck by latent variable distillation: we leverage the less tractable but more expressive deep generative models to provide extra supervision over the latent variables of PCs. Specifically, we extract information from Transformer-based generative models to assign values to latent variables of PCs, providing guidance to PC optimizers. Experiments on both image and language modeling benchmarks (e.g., ImageNet and WikiText-2) show that latent variable distillation substantially boosts the performance of large PCs compared to their counterparts without latent variable distillation. In particular, on the image modeling benchmarks, PCs achieve competitive performance against some of the widely-used deep generative models, including variational autoencoders and flow-based models, opening up new avenues for tractable generative modeling. Our code can be found at https://github.com/UCLA-StarAI/LVD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                114. <h><a href=https://openreview.net/pdf?id=LV_MeMS38Q9>Betty: An Automatic Differentiation Library for Multilevel Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sang Keun Choe, Willie Neiswanger, Pengtao Xie, Eric Xing</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LV_MeMS38Q9>8.0 - [8, 10, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a scalable, user-friendly, and modular automatic differentiation library for multilevel optimization based on a novel interpretation of multilevel optimization as a dataflow graph.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LV_MeMS38Q9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from $\mathcal{O}(d^3)$ to $\mathcal{O}(d^2)$, (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that Betty can be used to implement an array of MLO programs, while also observing up to 11% increase in test accuracy, 14% decrease in GPU memory usage, and 20% decrease in training wall time over existing implementations on multiple benchmarks. We also showcase that Betty enables scaling MLO to models with hundreds of millions of parameters. We open-source the code at https://github.com/leopard-ai/betty.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                115. <h><a href=https://openreview.net/pdf?id=3lge0p5o-M->DiffEdit: Diffusion-based semantic image editing with mask guidance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guillaume Couairon, Jakob Verbeek, Holger Schwenk, Matthieu Cord</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3lge0p5o-M->7.75 - [10, 8, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3lge0p5o-M-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Image generation has recently seen tremendous advances, with diffusion models allowing to synthesize convincing images for a large variety of text prompts. In this article, we propose DiffEdit, a method to take advantage of text-conditioned diffusion models for the task of semantic image editing, where the goal is to edit an image based on a text query. Semantic image editing is an extension of image generation, with the additional constraint that the generated image should be as similar as possible to a given input image. 
Current editing methods based on diffusion models usually require to provide a mask, making the task much easier by treating it as a conditional inpainting task. In contrast, our main contribution is able to automatically generate a mask highlighting regions of the input image that need to be edited, by contrasting predictions of a diffusion model conditioned on different text prompts. Moreover, we rely on latent inference to preserve content in those regions of interest and show excellent synergies with mask-based diffusion. 
DiffEdit achieves state-of-the-art editing performance on ImageNet. In addition, we evaluate semantic image editing in more challenging settings, using images from the COCO dataset as well as text-based generated images.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                116. <h><a href=https://openreview.net/pdf?id=O-G91-4cMdv>Words are all you need? Language as an approximation for human similarity judgments</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Raja Marjieh, Pol Van Rijn, Ilia Sucholutsky, Theodore Sumers, Harin Lee, Thomas L. Griffiths, Nori Jacoby</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=O-G91-4cMdv>7.75 - [10, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that machine embeddings of text descriptions can predict human similarity judgments better than models trained from images, audio and video.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/O-G91-4cMdv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human similarity judgments are a powerful supervision signal for machine learning applications based on techniques such as contrastive learning, information retrieval, and model alignment, but classical methods for collecting human similarity judgments are too expensive to be used at scale. Recent methods propose using pre-trained deep neural networks (DNNs) to approximate human similarity, but pre-trained DNNs may not be available for certain domains (e.g., medical images, low-resource languages) and their performance in approximating human similarity has not been extensively tested. We conducted an evaluation of 611 pre-trained models across three domains -- images, audio, video -- and found that there is a large gap in performance between human similarity judgments and pre-trained DNNs. To address this gap, we propose a new class of similarity approximation methods based on language. To collect the language data required by these new methods, we also developed and validated a novel adaptive tag collection pipeline. We find that our proposed language-based methods are significantly cheaper, in the number of human judgments, than classical methods, but still improve performance over the DNN-based methods. Finally, we also develop `stacked&#39; methods that combine language embeddings with DNN embeddings, and find that these consistently provide the best approximations for human similarity across all three of our modalities. Based on the results of this comprehensive study, we provide a concise guide for researchers interested in collecting or approximating human similarity data. To accompany this guide, we also release all of the similarity and language data, a total of 206,339 human judgments, that we collected in our experiments, along with a detailed breakdown of all modeling results.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                117. <h><a href=https://openreview.net/pdf?id=PqvMRDCJT9t>Flow Matching for Generative Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, Matthew Le</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PqvMRDCJT9t>7.75 - [5, 8, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a new simulation-free approach for training Continuous Normalizing Flows, generalizing the probability paths induced by simple diffusion processes. We obtain state-of-the-art on ImageNet in both NLL and FID among competing methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PqvMRDCJT9t.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows (CNFs), allowing us to train CNFs at unprecedented scale. Specifically, we present the notion of Flow Matching (FM), a simulation-free approach for training CNFs based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples---which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing FM with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training CNFs with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport (OT) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training CNFs using Flow Matching on ImageNet leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical ODE solvers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                118. <h><a href=https://openreview.net/pdf?id=kDEL91Dufpa>On the duality between contrastive and non-contrastive self-supervised learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann LeCun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kDEL91Dufpa>7.75 - [10, 8, 5, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that contrastive and non-contrastive self-supervised methods can be shown to be closely related, and then study how implementation details impact performance. We validate empirically our findings and significantly improve known behaviours.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kDEL91Dufpa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg&#39;s with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and non-contrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of self-supervised learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                119. <h><a href=https://openreview.net/pdf?id=6K2RM6wVqKu>Uni-Mol: A Universal 3D Molecular Representation Learning Framework</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, Guolin Ke</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6K2RM6wVqKu>7.75 - [8, 10, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A universal 3D molecular pretraining framework that significantly enlarges the representation ability and application scope in drug design.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6K2RM6wVqKu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Molecular representation learning (MRL) has gained tremendous attention due to its critical role in learning from limited supervised data for applications like drug design. In most MRL methods, molecules are treated as 1D sequential tokens or 2D topology graphs, limiting their ability to incorporate 3D information for downstream tasks and, in particular, making it almost impossible for 3D geometry prediction/generation. In this paper, we propose a universal 3D MRL framework, called Uni-Mol, that significantly enlarges the representation ability and application scope of MRL schemes. Uni-Mol contains two pretrained models with the same SE(3) Transformer architecture: a molecular model pretrained by 209M molecular conformations; a pocket model pretrained by 3M candidate protein pocket data. Besides, Uni-Mol contains several finetuning strategies to apply the pretrained models to various downstream tasks. By properly incorporating 3D information, Uni-Mol outperforms SOTA in 14/15 molecular property prediction tasks. Moreover, Uni-Mol achieves superior performance in 3D spatial tasks, including protein-ligand binding pose prediction, molecular conformation generation, etc. The code, model, and data are made publicly available at https://github.com/dptech-corp/Uni-Mol.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                120. <h><a href=https://openreview.net/pdf?id=IowKt5rYWsK>GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenhongyi Yang, Jiarui Xu, Shalini De Mello, Elliot J. Crowley, Xiaolong Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IowKt5rYWsK>7.67 - [10, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A high-resolution vision transformer architecture based on a new efficient global information exchange mechanism for general visual recognition.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IowKt5rYWsK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present the Group Propagation Vision Transformer (GPViT): a novel non- hierarchical (i.e. non-pyramidal) transformer model designed for general visual recognition with high-resolution features. High-resolution features (or tokens) are a natural fit for tasks that involve perceiving fine-grained details such as detection and segmentation, but exchanging global information between these features is expensive in memory and computation because of the way self-attention scales. We provide a highly efficient alternative Group Propagation Block (GP Block) to exchange global information. In each GP Block, features are first grouped to- gether by a fixed number of learnable group tokens; we then perform Group Propagation where global information is exchanged between the grouped fea- tures; finally, global information in the updated grouped features is returned back to the image features through a transformer decoder. We evaluate GPViT on a variety of visual recognition tasks including image classification, semantic seg- mentation, object detection, and instance segmentation. Our method achieves significant performance gains over previous works across all tasks, especially on tasks that require high-resolution outputs, for example, our GPViT-L3 out- performs Swin Transformer-B by 2.0 mIoU on ADE20K semantic segmentation with only half as many parameters. Code and pre-trained models are available at https://github.com/ChenhongyiYang/GPViT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                121. <h><a href=https://openreview.net/pdf?id=iTtGCMDEzS_>BigVGAN: A Universal Neural Vocoder with Large-Scale Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, Sungroh Yoon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iTtGCMDEzS_>7.6 - [6, 8, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iTtGCMDEzS_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite recent progress in generative adversarial network (GAN)-based vocoders, where the model generates raw waveform conditioned on acoustic features, it is challenging to synthesize high-fidelity audio for numerous speakers across various recording environments. In this work, we present BigVGAN, a universal vocoder that generalizes well for various out-of-distribution scenarios without fine-tuning. We introduce periodic activation function and anti-aliased representation into the GAN generator, which brings the desired inductive bias for audio synthesis and significantly improves audio quality. In addition, we train our GAN vocoder at the largest scale up to 112M parameters, which is unprecedented in the literature. We identify and address the failure modes in large-scale GAN training for audio, while maintaining high-fidelity output without over-regularization. Our BigVGAN, trained only on clean speech (LibriTTS), achieves the state-of-the-art performance for various zero-shot (out-of-distribution) conditions, including unseen speakers, languages, recording environments, singing voices, music, and instrumental audio. We release our code and model at: https://github.com/NVIDIA/BigVGAN
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                122. <h><a href=https://openreview.net/pdf?id=H-T3F0dMbyj>CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao-Wen Dong, Naoya Takahashi, Yuki Mitsufuji, Julian McAuley, Taylor Berg-Kirkpatrick</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=H-T3F0dMbyj>7.6 - [6, 8, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new method the leverages the pretrained CLIP model and noise invariant training for learning text-queried sound separation with only noisy unlabeled videos
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/H-T3F0dMbyj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent years have seen progress beyond domain-specific sound separation for speech or music towards universal sound separation for arbitrary sounds. Prior work on universal sound separation has investigated separating a target sound out of an audio mixture given a text query. Such text-queried sound separation systems provide a natural and scalable interface for specifying arbitrary target sounds. However, supervised text-queried sound separation systems require costly labeled audio-text pairs for training. Moreover, the audio provided in existing datasets is often recorded in a controlled environment, causing a considerable generalization gap to noisy audio in the wild. In this work, we aim to approach text-queried universal sound separation by using only unlabeled data. We propose to leverage the visual modality as a bridge to learn the desired audio-textual correspondence. The proposed CLIPSep model first encodes the input query into a query vector using the contrastive language-image pretraining (CLIP) model, and the query vector is then used to condition an audio separation model to separate out the target sound. While the model is trained on image-audio pairs extracted from unlabeled videos, at test time we can instead query the model with text inputs in a zero-shot setting, thanks to the joint language-image embedding learned by the CLIP model. Further, videos in the wild often contain off-screen sounds and background noise that may hinder the model from learning the desired audio-textual correspondence. To address this problem, we further propose an approach called noise invariant training for training a query-based sound separation model on noisy data. Experimental results show that the proposed models successfully learn text-queried universal sound separation using only noisy unlabeled videos, even achieving competitive performance against a supervised model in some settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                123. <h><a href=https://openreview.net/pdf?id=3Pf3Wg6o-A4>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Antonia Creswell, Murray Shanahan, Irina Higgins</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3Pf3Wg6o-A4>7.6 - [8, 6, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Using language models to produce a human interpretable chain of logical reasoning to answer questions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3Pf3Wg6o-A4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 46 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                124. <h><a href=https://openreview.net/pdf?id=6qeBuZSo7Pr>Planning Goals for Exploration</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Edward S. Hu, Richard Chang, Oleh Rybkin, Dinesh Jayaraman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6qeBuZSo7Pr>7.6 - [8, 8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use world models to generate goals for exploration.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6qeBuZSo7Pr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dropped into an unknown environment, what should an agent do to quickly learn about the environment and how to accomplish diverse tasks within it? We address this question within the goal-conditioned reinforcement learning paradigm, by identifying how the agent should set its goals at training time to maximize exploration. We propose &#34;Planning Exploratory Goals&#34; (PEG), a method that sets goals for each training episode to directly optimize an intrinsic exploration reward. PEG first chooses goal commands such that the agent&#39;s goal-conditioned policy, at its current level of training, will end up in states with high exploration potential. It then launches an exploration policy starting at those promising states. To enable this direct optimization, PEG learns world models and adapts sampling-based planning algorithms to &#34;plan goal commands&#34;. In challenging simulated robotics environments including a multi-legged ant robot in a maze, and a robot arm on a cluttered tabletop, PEG exploration enables more efficient and effective training of goal-conditioned policies relative to baselines and ablations. Our ant successfully navigates a long maze, and the robot arm successfully builds a stack of three blocks upon command. Website: https://sites.google.com/view/exploratory-goals
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                125. <h><a href=https://openreview.net/pdf?id=FUORz1tG8Og>CROM: Continuous Reduced-Order Modeling of PDEs Using Implicit Neural Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Peter Yichen Chen, Jinxu Xiang, Dong Heon Cho, Yue Chang, G A Pershing, Henrique Teles Maia, Maurizio M Chiaramonte, Kevin Thomas Carlberg, Eitan Grinspun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FUORz1tG8Og>7.6 - [8, 8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We accelerate PDE solvers via rapid latent space traversal of continuous vector fields leveraging implicit neural representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FUORz1tG8Og.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The long runtime of high-fidelity partial differential equation (PDE) solvers makes them unsuitable for time-critical applications. We propose to accelerate PDE solvers using reduced-order modeling (ROM). Whereas prior ROM approaches reduce the dimensionality of discretized vector fields, our continuous reduced-order modeling (CROM) approach builds a low-dimensional embedding of the continuous vector fields themselves, not their discretization. We represent this reduced manifold using continuously differentiable neural fields, which may train on any and all available numerical solutions of the continuous system, even when they are obtained using diverse methods or discretizations. We validate our approach on an extensive range of PDEs with training data from voxel grids, meshes, and point clouds. Compared to prior discretization-dependent ROM methods, such as linear subspace proper orthogonal decomposition (POD) and nonlinear manifold neural-network-based autoencoders, CROM features higher accuracy, lower memory consumption, dynamically adaptive resolutions, and applicability to any discretization. For equal latent space dimension, CROM exhibits 79$\times$ and 49$\times$ better accuracy, and 39$\times$ and 132$\times$ smaller memory footprint, than POD and autoencoder methods, respectively. Experiments demonstrate 109$\times$ and 89$\times$ wall-clock speedups over unreduced models on CPUs and GPUs, respectively. Videos and codes are available on the project page: https://crom-pde.github.io
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                126. <h><a href=https://openreview.net/pdf?id=1_jtWjhSSkr>Exponential Generalization Bounds with Near-Optimal Rates for $L_q$-Stable Algorithms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaotong Yuan, Ping Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1_jtWjhSSkr>7.6 - [8, 8, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We presented a set of sharper and near-optimal exponential generalization bounds for $L_q$-stable learning algorithms
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1_jtWjhSSkr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The \emph{stability} of learning algorithms to changes in the training sample has been actively studied as a powerful proxy for reasoning about generalization. Recently, exponential  generalization and excess risk bounds with near-optimal rates have been obtained under the stringent and distribution-free notion of uniform stability~\citep{bousquet2020sharper,klochkov2021stability}. In the meanwhile, under the notion of $L_q$-stability, which is weaker and distribution dependent, exponential generalization bounds are also available yet so far only with sub-optimal rates. Therefore, a fundamental question we would like to address in this paper is whether it is possible to derive near-optimal exponential generalization bounds for $L_q$-stable learning algorithms. As the core contribution of the present work, we give an affirmative answer to this question by developing strict analogues of the near-optimal generalization and risk bounds of uniformly stable algorithms for $L_q$-stable algorithms. Further, we demonstrate the power of our improved $L_q$-stability and generalization theory by applying it to derive strong sparse excess risk bounds, under mild conditions, for computationally tractable sparsity estimation algorithms such as Iterative Hard Thresholding (IHT).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                127. <h><a href=https://openreview.net/pdf?id=yHY9NbQJ5BP>Sparsity-Constrained Optimal Transport</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianlin Liu, Joan Puigcerver, Mathieu Blondel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yHY9NbQJ5BP>7.6 - [6, 6, 8, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose formulations for optimal transport with cardinality constraints and apply them to sparse mixture of experts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yHY9NbQJ5BP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Regularized optimal transport (OT) is now increasingly used as a loss or as a matching layer in neural networks. Entropy-regularized OT can be computed using the Sinkhorn algorithm but it leads to fully-dense transportation plans, meaning that all sources are (fractionally) matched with all targets. To address this issue, several works have investigated quadratic regularization instead. This regularization preserves sparsity and leads to unconstrained and smooth (semi) dual objectives, that can be solved with off-the-shelf gradient methods. Unfortunately, quadratic regularization does not give direct control over the cardinality (number of nonzeros) of the transportation plan. We propose in this paper a new approach for OT with explicit cardinality constraints on the transportation plan. Our work is motivated by an application to sparse mixture of experts, where OT can be used to match input tokens such as image patches with expert models such as neural networks. Cardinality constraints ensure that at most $k$ tokens are matched with an expert, which is crucial for computational performance reasons. Despite the nonconvexity of cardinality constraints, we show that the corresponding (semi) dual problems are tractable and can be solved with first-order gradient methods. Our method can be thought as a middle ground between unregularized OT (recovered in the limit case $k=1$) and quadratically-regularized OT (recovered when $k$ is large enough). The smoothness of the objectives increases as $k$ increases, giving rise to a trade-off between convergence speed and sparsity of the optimal plan.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                128. <h><a href=https://openreview.net/pdf?id=XC_yGI-0j9>Efficient approximation of neural population structure and correlations with probabilistic circuits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Koosha Khalvati, Samantha Johnson, Stefan Mihalas, Michael A Buice</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XC_yGI-0j9>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a computationally efficient generative model for a wide range of population structures with higher order correlations and a large number of neurons. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XC_yGI-0j9.jpg loading="lazy">
                </div>
                <div class="abstract">
                     We present a computationally efficient framework to model a wide range of population structures with high order correlations and a large number of neurons. Our method is based on a special type of Bayesian network that has linear inference time and is founded upon the concept of contextual independence. Moreover, we use an efficient architecture learning method for network selection to model large neural populations even with a small amount of data. Our framework is both fast and accurate in approximating neural population structures. Furthermore, our approach enables us to reliably quantify higher order neural correlations. We test our method on simulated neural populations commonly used to generate higher order correlations, as well as on publicly available large-scale neural recordings from the Allen Brain Observatory. Our approach significantly outperforms other models both in terms of statistical measures and alignment with experimental evidence.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                129. <h><a href=https://openreview.net/pdf?id=P-73JPgRs0R>Effects of Graph Convolutions in Multi-layer Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aseem Baranwal, Kimon Fountoulakis, Aukosh Jagannath</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=P-73JPgRs0R>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Theoretical and empirical insights into the performance of graph convolutions in multi-layer networks
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/P-73JPgRs0R.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph Convolutional Networks (GCNs) are one of the most popular architectures that are used to solve classification problems accompanied by graphical information. We present a rigorous theoretical understanding of the effects of graph convolutions in multi-layer networks. We study these effects through the node classification problem of a non-linearly separable Gaussian mixture model coupled with a stochastic block model. First, we show that a single graph convolution expands the regime of the distance between the means where multi-layer networks can classify the data by a factor of at least $1/\sqrt[4]{\rm deg}$, where ${\rm deg}$ denotes the expected degree of a node. Second, we show that with a slightly stronger graph density, two graph convolutions improve this factor to at least $1/\sqrt[4]{n}$, where $n$ is the number of nodes in the graph. Finally, we provide both theoretical and empirical insights into the performance of graph convolutions placed in different combinations among the layers of a neural network, concluding that the performance is mutually similar for all combinations of the placement. We present extensive experiments on both synthetic and real-world data that illustrate our results.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                130. <h><a href=https://openreview.net/pdf?id=DeG07_TcZvT>Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DeG07_TcZvT>7.5 - [8, 8, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DeG07_TcZvT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create &#34;latent saliency maps&#34; that can help explain predictions in human terms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                131. <h><a href=https://openreview.net/pdf?id=aoDyX6vSqsd>Sampling-based inference for large linear models, with application to linearised Laplace</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Javier Antoran, Shreyas Padhy, Riccardo Barbano, Eric Nalisnick, David Janz, José Miguel Hernández-Lobato</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aoDyX6vSqsd>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We scale the linearised Laplace method for uncertainty estimation to large neural networks and datasets using an efficient method for posterior sampling
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aoDyX6vSqsd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large-scale linear models are ubiquitous throughout machine learning, with contemporary application as surrogate models for neural network uncertainty quantification; that is, the linearised Laplace method. Alas, the computational cost associated with Bayesian linear models constrains this method&#39;s application to small networks, small output spaces and small datasets. We address this limitation by introducing a scalable sample-based Bayesian inference method for conjugate Gaussian multi-output linear models, together with a matching method for hyperparameter (regularisation) selection. Furthermore, we use a classic feature normalisation method (the g-prior) to resolve a previously highlighted pathology of the linearised Laplace method. Together, these contributions allow us to perform linearised neural network inference with ResNet-18 on CIFAR100 (11M parameters, 100 output dimensions × 50k datapoints) and with a U-Net on a high-resolution tomographic reconstruction task (2M parameters, 251k output dimensions).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                132. <h><a href=https://openreview.net/pdf?id=9y0HFvaAYD6>Hidden Markov Transformer for Simultaneous Machine Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shaolei Zhang, Yang Feng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9y0HFvaAYD6>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9y0HFvaAYD6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Simultaneous machine translation (SiMT) outputs the target sequence while receiving the source sequence, and hence learning when to start translating each target token is the core challenge for SiMT task. However, it is non-trivial to learn the optimal moment among many possible moments of starting translating, as the moments of starting translating always hide inside the model and can only be supervised with the observed target sequence. In this paper, we propose a Hidden Markov Transformer (HMT), which treats the moments of starting translating as hidden events and the target sequence as the corresponding observed events, thereby organizing them as a hidden Markov model. HMT explicitly models multiple moments of starting translating as the candidate hidden events, and then selects one to generate the target token. During training, by maximizing the marginal likelihood of the target sequence over multiple moments of starting translating, HMT learns to start translating at the moments that target tokens can be generated more accurately. Experiments on multiple SiMT benchmarks show that HMT outperforms strong baselines and achieves state-of-the-art performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                133. <h><a href=https://openreview.net/pdf?id=DlpCotqdTy>Provably Auditing Ordinary Least Squares in Low Dimensions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ankur Moitra, Dhruv Rohatgi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DlpCotqdTy>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop provable and efficient algorithms for estimating stability of OLS to dropping samples in the low-dimensional regime.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DlpCotqdTy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Auditing the stability of a machine learning model to small changes in the training procedure is critical for engendering trust in practical applications. For example, a model should not be overly sensitive to removing a small fraction of its training data. However, algorithmically validating this property seems computationally challenging, even for the simplest of models: Ordinary Least Squares (OLS) linear regression. Concretely, recent work defines the stability of a regression as the minimum number of samples that need to be removed so that rerunning the analysis overturns the conclusion (Broderick et al., 2020), specifically meaning that the sign of a particular coefficient of the OLS regressor changes. But the only known approach for estimating this metric, besides the obvious exponential-time algorithm, is a greedy heuristic that may produce severe overestimates and therefore cannot certify stability. We show that stability can be efficiently certified in the low-dimensional regime: when the number of covariates is a constant but the number of samples is large, there are polynomial-time algorithms for estimating (a fractional version of) stability, with provable approximation guarantees. Applying our algorithms to the Boston Housing dataset, we exhibit regression analyses where our estimator outperforms the greedy heuristic, and can successfully certify stability even in the regime where a constant fraction of the samples are dropped.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                134. <h><a href=https://openreview.net/pdf?id=9piH3Hg8QEf>SMART: Self-supervised Multi-task pretrAining with contRol Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yanchao Sun, Shuang Ma, Ratnesh Madaan, Rogerio Bonatti, Furong Huang, Ashish Kapoor</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9piH3Hg8QEf>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a pretraining framework for sequential decision making based on a self-supervised objectives and a control transformer architecture, leading to significantly higher learning efficiency in various downstram control tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9piH3Hg8QEf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-supervised pretraining has been extensively studied in language and vision domains, where a unified model can be easily adapted to various downstream tasks by pretraining representations without explicit labels. When it comes to sequential decision-making tasks, however, it is difficult to properly design such a pretraining approach that can cope with both high-dimensional perceptual information and the complexity of sequential control over long interaction horizons. The challenge becomes combinatorially more complex if we want to pretrain representations amenable to a large variety of tasks. To tackle this problem, in this work, we formulate a general pretraining-finetuning pipeline for sequential decision making, under which we propose a generic pretraining framework \textit{Self-supervised Multi-task pretrAining with contRol Transformer (SMART)}. By systematically investigating pretraining regimes, we carefully design a Control Transformer (CT) coupled with a novel control-centric pretraining objective in a self-supervised manner. SMART encourages the representation to capture the common essential information relevant to short-term control and long-term control, which is transferrable across tasks. We show by extensive experiments in DeepMind Control Suite that SMART significantly improves the learning efficiency among seen and unseen downstream tasks and domains under different learning scenarios including Imitation Learning (IL) and Reinforcement Learning (RL). Benefiting from the proposed control-centric objective, SMART is resilient to distribution shift between pretraining and finetuning, and even works well with low-quality pretraining datasets that are randomly collected. The codebase, pretrained models and datasets are provided at https://github.com/microsoft/smart.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                135. <h><a href=https://openreview.net/pdf?id=Do9MOlwWHu0>Learning Sparse Group Models Through Boolean Relaxation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yijie Wang, Yuan Zhou, Xiaoqing Huang, Kun Huang, Jie Zhang, Jianzhu Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Do9MOlwWHu0>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Do9MOlwWHu0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce an efficient algorithmic framework for learning sparse group models formulated as the natural convex relaxation of a cardinality-constrained program with Boolean variables. We provide theoretical techniques to characterize the equivalent condition when the relaxation achieves the exact integral optimal solution, as well as a rounding algorithm to produce a feasible integral solution once the optimal relaxation solution is fractional. We demonstrate the power of our equivalent condition by applying it to two ensembles of random problem instances that are challenging and popularly used in literature and prove that our method achieves exactness with overwhelming probability and nearly optimal sample complexity. Empirically, we use synthetic datasets to demonstrate that our proposed method significantly outperforms the state-of-the-art group sparse learning models in terms of individual and group support recovery when the number of samples is small. Furthermore, we show the out-performance of our method in cancer drug response prediction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                136. <h><a href=https://openreview.net/pdf?id=AHvFDPi-FA>Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhendong Wang, Jonathan J Hunt, Mingyuan Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AHvFDPi-FA>7.5 - [8, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Diffusion models serve as expressive policies to boost offline RL performance. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AHvFDPi-FA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness of the diffusion model-based policy, and the coupling of the behavior cloning and policy improvement under the diffusion model both contribute to the outstanding performance of Diffusion-QL. We illustrate the superiority of our method compared to prior works in a simple 2D bandit example with a multimodal behavior policy. We then show that our method can achieve state-of-the-art performance on the majority of the D4RL benchmark tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                137. <h><a href=https://openreview.net/pdf?id=cMJo1FTwBTQ>DINO as a von Mises-Fisher mixture model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hariprasath Govindarajan, Per Sidén, Jacob Roll, Fredrik Lindsten</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cMJo1FTwBTQ>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Improving DINO with unnormalized prototypes based on a flexible von Mises-Fisher mixture model interpretation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cMJo1FTwBTQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-distillation methods using Siamese networks are popular for self-supervised pre-training. DINO is one such method based on a cross-entropy loss between $K$-dimensional probability vectors, obtained by applying a softmax function to the dot product between representations and learnt prototypes. Given the fact that the learned representations are $L^2$-normalized, we show that DINO and its derivatives, such as iBOT, can be interpreted as a mixture model of von Mises-Fisher components. With this interpretation, DINO assumes equal precision for all components when the prototypes are also $L^2$-normalized. Using this insight we propose DINO-vMF, that adds appropriate normalization constants when computing the cluster assignment probabilities. Unlike DINO, DINO-vMF is stable also for the larger ViT-Base model with unnormalized prototypes. We show that the added flexibility of the mixture model is beneficial in terms of better image representations. The DINO-vMF pre-trained model consistently performs better than DINO on a range of downstream tasks. We obtain similar improvements for iBOT-vMF vs iBOT and thereby show the relevance of our proposed modification also for other methods derived from DINO.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                138. <h><a href=https://openreview.net/pdf?id=vmjctNUSWI>Moving Forward by Moving Backward: Embedding Action Impact over Action Semantics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kuo-Hao Zeng, Luca Weihs, Roozbeh Mottaghi, Ali Farhadi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vmjctNUSWI>7.5 - [8, 8, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vmjctNUSWI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A common assumption when training embodied agents is that the impact of taking an action is stable; for instance, executing the ``move ahead&#39;&#39; action will always move the agent forward by a fixed distance, perhaps with some small amount of actuator-induced noise. This assumption is limiting; an agent may encounter settings that dramatically alter the impact of actions: a move ahead action on a wet floor may send the agent twice as far as it expects and using the same action with a broken wheel might transform the expected translation into a rotation. Instead of relying that the impact of an action stably reflects its pre-defined semantic meaning, we propose to model the impact of actions on-the-fly using latent embeddings. By combining these latent action embeddings with a novel, transformer-based, policy head, we design an Action Adaptive Policy (AAP). We evaluate our AAP on two challenging visual navigation tasks in the AI2-THOR and Habitat environments and show that our AAP is highly performant even when faced, at inference-time, with missing actions and, previously unseen, perturbed action spaces. Moreover, we observe significant improvement in robustness against these actions when evaluating in real-world scenarios.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                139. <h><a href=https://openreview.net/pdf?id=N92hjSf5NNh>MECTA: Memory-Economic Continual Test-Time Model Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, Michael Spranger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=N92hjSf5NNh>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/N92hjSf5NNh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve out-of-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memory-constrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for back-propagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-the-art CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 6% with constrained memory and significantly reduces the memory costs of ResNet50 on ImageNet by at least 70% with comparable accuracy. Our codes can be accessed at https://github.com/SonyAI/MECTA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                140. <h><a href=https://openreview.net/pdf?id=VD-AYtP0dve>Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lorenz Kuhn, Yarin Gal, Sebastian Farquhar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VD-AYtP0dve>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Semantic entropy is a novel uncertainty estimation method for natural language generation that captures uncertainty over meanings rather than sequences.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VD-AYtP0dve.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of &#34;semantic equivalence&#34;—different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy—an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                141. <h><a href=https://openreview.net/pdf?id=kt-dcBQcSA>A probabilistic framework for task-aligned intra- and inter-area neural manifold estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Edoardo Balzani, Jean-Paul G Noel, Pedro Herrero-Vidal, Dora E Angelaki, Cristina Savin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kt-dcBQcSA>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> New probabilistic estimator partitions multi-area neural variability into shared and private sources, aligned to meaningful task axes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kt-dcBQcSA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Latent manifolds provide a compact characterization of neural population activity and of shared co-variability across brain areas. Nonetheless, existing statistical tools for extracting neural manifolds face limitations in terms of interpretability of latents with respect to task variables, and can be hard to apply to datasets with no trial repeats. Here we propose a novel probabilistic framework that allows for interpretable partitioning of population variability within and across areas in the context of naturalistic behavior. Our approach for task aligned manifold estimation (TAME-GP) explicitly partitions variability into private and shared sources which can themselves be subdivided in task-relevant and task irrelevant components, uses a realistic Poisson noise model, and introduces temporal smoothing of latent trajectories in the form of a Gaussian Process prior. This TAME-GP graphical model allows for robust estimation of task-relevant variability in local population responses, and of shared co-variability between brain areas. We demonstrate the efficiency of our estimator on within model and biologically motivated simulated data. We also apply it to several datasets of neural population recordings during behavior. Overall, our results demonstrate the capacity of TAME-GP to capture meaningful intra- and inter-area neural variability with single trial resolution.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                142. <h><a href=https://openreview.net/pdf?id=QubsmJT_A0>Neuromechanical Autoencoders: Learning to Couple Elastic and Neural Network Nonlinearity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Deniz Oktay, Mehran Mirramezani, Eder Medina, Ryan P Adams</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QubsmJT_A0>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce Neuromechanical Autoencoders, a framework for co-design of neural network and mechanical metamaterials for performing morphological computation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QubsmJT_A0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Intelligent biological systems are characterized by their embodiment in a complex environment and the intimate interplay between their nervous systems and the nonlinear mechanical properties of their bodies. This coordination, in which the dynamics of the motor system co-evolved to reduce the computational burden on the brain, is referred to as &#34;mechanical intelligence&#34; or &#34;morphological computation&#34;. In this work, we seek to develop machine learning analogs of this process, in which we jointly learn the morphology of complex nonlinear elastic solids along with a deep neural network to control it. By using a specialized differentiable simulator of elastic mechanics coupled to conventional deep learning architectures---which we refer to as neuromechanical autoencoders---we are able to learn to perform morphological computation via gradient descent. Key to our approach is the use of mechanical metamaterials---cellular solids, in particular---as the morphological substrate. Just as deep neural networks provide flexible and massively-parametric function approximators for perceptual and control tasks, cellular solid metamaterials are promising as a rich and learnable space for approximating a variety of actuation tasks. In this work we take advantage of these complementary computational concepts to co-design materials and neural network controls to achieve nonintuitive mechanical behavior. We demonstrate in simulation how it is possible to achieve translation, rotation, and shape matching, as well as a &#34;digital MNIST&#34; task. We additionally manufacture and evaluate one of the designs to verify its real-world behavior.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                143. <h><a href=https://openreview.net/pdf?id=5VBBA91N6n>LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhihao Shi, Xize Liang, Jie Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5VBBA91N6n>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel and efficient subgraph-wise sampling method with a convergence guarantee by Local Message Compensation (LMC).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5VBBA91N6n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications.
However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods---a promising class of mini-batch training techniques---discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the {\it first} subgraph-wise sampling method with provable convergence. The key idea of LMC is to retrieve the discarded messages in backward passes based on a message passing formulation of backward passes. By efficient and effective compensations for the discarded messages in both forward and backward passes, LMC computes accurate mini-batch gradients and thus accelerates convergence. We further show that LMC converges to first-order stationary points of GNNs. Experiments on large-scale benchmark tasks demonstrate that LMC significantly outperforms state-of-the-art subgraph-wise sampling methods in terms of efficiency.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                144. <h><a href=https://openreview.net/pdf?id=lKOfilXucGB>Decompositional Generation Process for Instance-Dependent Partial Label Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Congyu Qiao, Ning Xu, Xin Geng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lKOfilXucGB>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b>  We consider instance-dependent PLL and assume that the generation process of the candidate labels could decompose into two sequential parts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lKOfilXucGB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Partial label learning (PLL) is a typical weakly supervised learning problem, where each training example is associated with a set of candidate labels among which only one is true. Most existing PLL approaches assume that the incorrect labels in each training example are randomly picked as the candidate labels and model the generation process of the candidate labels in a simple way.  However, these approaches usually do not perform as well as expected due to the fact that the generation process of the candidate labels is always instance-dependent. Therefore, it deserves to be modeled in a refined way.  In this paper, we consider instance-dependent PLL and assume that the generation process of the candidate labels could decompose into two sequential parts, where the correct label emerges first in the mind of the annotator but then the incorrect labels related to the feature are also selected with the correct label as candidate labels due to uncertainty of labeling. Motivated by this consideration, we propose a novel PLL method that performs Maximum A Posterior(MAP) based on an explicitly modeled generation process of candidate labels via decomposed probability distribution models. Extensive experiments on manually corrupted benchmark datasets and real-world datasets validate the effectiveness of the proposed method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                145. <h><a href=https://openreview.net/pdf?id=kfOtMqYJlUU>NeRF-SOS: Any-View Self-supervised Object Segmentation on Complex Scenes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, Dejia Xu, Zhangyang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kfOtMqYJlUU>7.5 - [8, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel collaborative contrastive loss for NeRF to segment objects in complex real-world scenes, without any annotation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kfOtMqYJlUU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural volumetric representations have shown the potential that Multi-layer Perceptrons (MLPs) can be optimized with multi-view calibrated images to represent scene geometry and appearance without explicit 3D supervision. Object segmentation can enrich many downstream applications based on the learned radiance field. However, introducing hand-crafted segmentation to define regions of interest in a complex real-world scene is non-trivial and expensive as it acquires per view annotation. This paper carries out the exploration of self-supervised learning for object segmentation using NeRF for complex real-world scenes. Our framework, called NeRF with Self-supervised Object Segmentation (NeRF-SOS), couples object segmentation and neural radiance field to segment objects in any view within a scene. By proposing a novel collaborative contrastive loss in both appearance and geometry levels, NeRF-SOS encourages NeRF models to distill compact geometry-aware segmentation clusters from their density fields and the self-supervised pre-trained 2D visual features. The self-supervised object segmentation framework can be applied to various NeRF models that both lead to photo-realistic rendering results and convincing segmentation maps for both indoor and outdoor scenarios. Extensive results on the LLFF, BlendedMVS, CO3Dv2, and Tank &amp; Temples datasets validate the effectiveness of NeRF-SOS. It consistently surpasses other 2D-based self-supervised baselines and predicts finer object masks than existing supervised counterparts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                146. <h><a href=https://openreview.net/pdf?id=kIPyTuEZuAK>A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qing Li, Siyuan Huang, Yining Hong, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kIPyTuEZuAK>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We take inspiration from arithmetic and present a new benchmark for studying systematic generalization of perception, syntax, and semantics.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kIPyTuEZuAK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inspired by humans&#39; exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, HINT, to examine machines&#39; capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models&#39; limitations, we undertake extensive experiments with various sequence-to-sequence models, including RNNs, Transformers, and GPT-3 (with the chain of thought prompting). The results indicate that current models struggle to extrapolate to long-range syntactic dependency and semantics. Models exhibit a considerable gap toward human-level generalization when evaluated with new concepts in a few-shot setting. Moreover, we discover that it is infeasible to solve HINT by merely scaling up the dataset and the model size; this strategy contributes little to the extrapolation of syntax and semantics. Finally, in zero-shot GPT-3 experiments, the chain of thought prompting exhibits impressive results and significantly boosts the test accuracy. We believe the HINT dataset and the experimental findings are of great interest to the learning community on systematic generalization.%
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                147. <h><a href=https://openreview.net/pdf?id=IloMJ5rqfnt>Accurate Image Restoration with Attention Retractable Transformer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang, Linghe Kong, Xin Yuan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IloMJ5rqfnt>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new SOTA image restoration method attention retractable Transformer.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IloMJ5rqfnt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, Transformer-based image restoration networks have achieved promising improvements over convolutional neural networks due to parameter-independent global interactions. To lower computational cost, existing works generally limit self-attention computation within non-overlapping windows. However, each group of tokens are always from a dense area of the image. This is considered as a dense attention strategy since the interactions of tokens are restrained in dense regions. Obviously, this strategy could result in restricted receptive fields. To address this issue, we propose \textbf{A}ttention \textbf{R}etractable \textbf{T}ransformer (ART) for image restoration, which presents both dense and sparse attention modules in the network. The sparse attention module allows tokens from sparse areas to interact and thus provides a wider receptive field. Furthermore, the alternating application of dense and sparse attention modules greatly enhances representation ability of Transformer while providing retractable attention on the input image.We conduct extensive experiments on image super-resolution, denoising, and JPEG compression artifact reduction tasks. Experimental results validate that our proposed ART outperforms state-of-the-art methods on various benchmark datasets both quantitatively and visually. We also provide code and models at~\url{https://github.com/gladzhang/ART}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                148. <h><a href=https://openreview.net/pdf?id=WOquZTLCBO1>VIPeR: Provably Efficient Algorithm for Offline RL with Neural Function Approximation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thanh Nguyen-Tang, Raman Arora</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WOquZTLCBO1>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A provably and computationally efficient algorithm for offline RL with deep neural networks 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WOquZTLCBO1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a novel algorithm for offline reinforcement learning called Value Iteration with Perturbed Rewards (VIPeR), which amalgamates the pessimism principle with random perturbations of the value function. Most current offline RL algorithms explicitly construct statistical confidence regions to obtain pessimism via lower confidence bounds (LCB), which cannot easily scale to complex problems where a neural network is used to estimate the value functions. Instead, VIPeR implicitly obtains pessimism by simply perturbing the offline data multiple times with carefully-designed i.i.d. Gaussian noises to learn an ensemble of estimated state-action {value functions} and acting greedily with respect to the minimum of the ensemble. The estimated state-action values are obtained by fitting a parametric model (e.g., neural networks) to the perturbed datasets using gradient descent. As a result, VIPeR only needs $\mathcal{O}(1)$ time complexity for action selection, while LCB-based algorithms require at least $\Omega(K^2)$, where $K$ is the total number of trajectories in the offline data. We also propose a novel data-splitting technique that helps remove a factor involving the log of the covering number in our bound. We prove that VIPeR yields a provable uncertainty quantifier with overparameterized neural networks and enjoys a bound on sub-optimality of $\tilde{\mathcal{O}}(  { \kappa H^{5/2}  \tilde{d} }/{\sqrt{K}})$, where $\tilde{d}$ is the effective dimension, $H$ is the horizon length and $\kappa$ measures the distributional shift. We corroborate the statistical and computational efficiency of VIPeR with an empirical evaluation on a wide set of synthetic and real-world datasets. To the best of our knowledge, VIPeR is the first algorithm for offline RL that is provably efficient for general Markov decision processes (MDPs) with neural network function approximation. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                149. <h><a href=https://openreview.net/pdf?id=1_OGWcP1s9w>Learning Fair Graph Representations via Automated Data Augmentations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongyi Ling, Zhimeng Jiang, Youzhi Luo, Shuiwang Ji, Na Zou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1_OGWcP1s9w>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an automated graph data augmentation method to learn fair graph representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1_OGWcP1s9w.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider fair graph representation learning via data augmentations. While this direction has been explored previously, existing methods invariably rely on certain assumptions on the properties of fair graph data in order to design fixed strategies on data augmentations. Nevertheless, the exact properties of fair graph data may vary significantly in different scenarios. Hence, heuristically designed augmentations may not always generate fair graph data in different application scenarios. In this work, we propose a method, known as Graphair, to learn fair representations based on automated graph data augmentations. Such fairness-aware augmentations are themselves learned from data. Our Graphair is designed to automatically discover fairness-aware augmentations from input graphs in order to circumvent sensitive information while preserving other useful information. Experimental results demonstrate that our Graphair consistently outperforms many baselines on multiple node classification datasets in terms of fairness-accuracy trade-off performance. In addition, results indicate that Graphair can automatically learn to generate fair graph data without prior knowledge on fairness-relevant graph properties.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                150. <h><a href=https://openreview.net/pdf?id=moIlFZfj_1b>Latent Neural ODEs with Sparse Bayesian Multiple Shooting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Valerii Iakovlev, Cagatay Yildiz, Markus Heinonen, Harri Lähdesmäki</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=moIlFZfj_1b>7.5 - [6, 6, 10, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/moIlFZfj_1b.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training dynamic models, such as neural ODEs, on long trajectories is a hard problem that requires using various tricks, such as trajectory splitting, to make model training work in practice. These methods are often heuristics with poor theoretical justifications, and require iterative manual tuning. We propose a principled multiple shooting technique for neural ODEs that splits the trajectories into manageable short segments, which are optimized in parallel, while ensuring probabilistic control on continuity over consecutive segments. We derive variational inference for our shooting-based latent neural ODE models and propose amortized encodings of irregularly sampled trajectories with a transformer-based recognition network with temporal attention and relative positional encoding. We demonstrate efficient and stable training, and state-of-the-art performance on multiple large-scale benchmark datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                151. <h><a href=https://openreview.net/pdf?id=Yn0xg-kHNW->Provably Efficient Risk-Sensitive Reinforcement Learning: Iterated CVaR and Worst Path</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yihan Du, Siwei Wang, Longbo Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Yn0xg-kHNW->7.5 - [8, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Yn0xg-kHNW-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we study a novel episodic risk-sensitive Reinforcement Learning (RL) problem, named Iterated CVaR RL, which aims to maximize the tail of the reward-to-go at each step, and focuses on tightly controlling the risk of getting into catastrophic situations at each stage. This formulation is applicable to real-world tasks that demand strong risk avoidance throughout the decision process, such as autonomous driving, clinical treatment planning and robotics. We investigate two performance metrics under Iterated CVaR RL, i.e., Regret Minimization  and Best Policy Identification. For both metrics, we design efficient algorithms ICVaR-RM and ICVaR-BPI, respectively, and provide nearly matching upper and lower bounds with respect to the number of episodes $K$. We also investigate an interesting limiting case of Iterated CVaR RL, called Worst Path RL, where the objective becomes to maximize the minimum possible cumulative reward. For Worst Path RL, we propose an efficient algorithm with constant upper and lower bounds. Finally, the techniques we develop for bounding the change of CVaR due to the value function shift and decomposing the regret via a distorted visitation distribution are novel, and can find applications in other risk-sensitive online learning problems. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                152. <h><a href=https://openreview.net/pdf?id=Cp-io_BoFaE>FluidLab: A Differentiable Environment for Benchmarking Complex Fluid Manipulation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhou Xian, Bo Zhu, Zhenjia Xu, Hsiao-Yu Tung, Antonio Torralba, Katerina Fragkiadaki, Chuang Gan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Cp-io_BoFaE>7.5 - [6, 6, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Cp-io_BoFaE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Humans manipulate various kinds of fluids in their everyday life: creating latte art, scooping floating objects from water, rolling an ice cream cone, etc. Using robots to augment or replace human labors in these daily settings remain as a challenging task due to the multifaceted complexities of fluids. Previous research in robotic fluid manipulation mostly consider fluids governed by an ideal, Newtonian model in simple task settings (e.g., pouring water into a container). However, the vast majority of real-world fluid systems manifest their complexities in terms of the fluid’s complex material behaviors (e.g., elastoplastic deformation) and multi-component interactions (e.g. coffee and frothed milk when making latte art), both of which were well beyond the scope of the current literature. To evaluate robot learning algorithms on understanding and interacting with such complex fluid systems, a comprehensive virtual platform with versatile simulation capabilities and well-established tasks is needed. In this work, we introduce FluidLab, a simulation environment with a diverse set of manipulation tasks involving complex fluid dynamics. These tasks address interactions between solid and fluid as well as among multiple fluids. At the heart of our platform is a fully differentiable physics simulator, FluidEngine, providing GPU-accelerated simulations and gradient calculations for various material types and their couplings, extending the scope of the existing differentiable simulation engines. We identify several challenges for fluid manipulation learning by evaluating a set of reinforcement learning and trajectory optimization methods on our platform. To address these challenges, we propose several domain-specific optimization schemes coupled with differentiable physics, which are empirically shown to be effective in tackling optimization problems featured by fluid system’s non-convex and non-smooth properties. Furthermore, we demonstrate reasonable sim-to-real transfer by deploying optimized trajectories in real-world settings. FluidLab is publicly available at: https://fluidlab2023.github.io.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                153. <h><a href=https://openreview.net/pdf?id=HjOo2k8lhFl>Learning Rationalizable Equilibria in Multiplayer Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuanhao Wang, Dingwen Kong, Yu Bai, Chi Jin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HjOo2k8lhFl>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop provably efficient algorithms for finding approximate CE and CCE that are also rationalizable.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HjOo2k8lhFl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A natural goal in multi-agent learning is to learn \emph{rationalizable} behavior, where players learn to avoid any Iteratively Dominated Action (IDA). However, standard no-regret based equilibria-finding algorithms could take exponential samples to find such rationalizable strategies. In this paper, we first propose a simple yet sample-efficient algorithm for finding a rationalizable action profile in multi-player general-sum games under bandit feedback, which substantially improves over the results of Wu et al. We further develop algorithms with the first efficient guarantees for learning rationalizable Coarse Correlated Equilibria (CCE) and Correlated Equilibria (CE). Our algorithms incorporate several novel techniques to guarantee the elimination of IDA and no (swap-)regret simultaneously, including a correlated exploration scheme and adaptive learning rates, which may be of independent interest. We complement our results with a sample complexity lower bound showing the sharpness of our guarantees.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                154. <h><a href=https://openreview.net/pdf?id=iaYcJKpY2B_>CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iaYcJKpY2B_>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We open-source a large language models, CodeGen, for program synthesis and propose a multi-turn program synthesis benchmark for evaluation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iaYcJKpY2B_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                155. <h><a href=https://openreview.net/pdf?id=ZTCxT2t2Ru>DocPrompting: Generating Code by Retrieving the Docs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuyan Zhou, Uri Alon, Frank F. Xu, Zhengbao Jiang, Graham Neubig</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZTCxT2t2Ru>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose to generalize the code generation models to unseen functions and usages through retrieving and reading code documentation
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZTCxT2t2Ru.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code
to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                156. <h><a href=https://openreview.net/pdf?id=JLINxPOVTh7>The Onset of Variance-Limited Behavior for Networks in the Lazy and Rich Regimes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, Cengiz Pehlevan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JLINxPOVTh7>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Empirical study of neural networks in the overparameterized regime shows how finite-width effects are brought on by initialization variance as sample size grows.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JLINxPOVTh7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    For small training set sizes $P$, the generalization error of wide neural networks is well-approximated by the error of an infinite width neural network (NN), either in the kernel or mean-field/feature-learning regime. However, after a critical sample size $P^*$, we empirically find the finite-width network generalization becomes worse than that of the infinite width network. In this work, we empirically study the transition from infinite-width behavior to this \textit{variance-limited} regime  as a function of sample size $P$ and network width $N$. We find that finite-size effects can become relevant for very small dataset sizes on the order of $P^* \sim \sqrt{N}$ for polynomial regression with ReLU networks. We discuss the source of these effects using an argument based on the variance of the NN&#39;s final neural tangent kernel (NTK). This transition can be pushed to larger $P$ by enhancing feature learning or by ensemble averaging the networks. We find that the learning curve for regression with the final NTK is an accurate approximation of the NN learning curve. Using this, we provide  a toy model which also exhibits $P^* \sim \sqrt{N}$ scaling and has $P$-dependent benefits from feature learning. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                157. <h><a href=https://openreview.net/pdf?id=zqwryBoXYnh>PLOT: Prompt Learning with Optimal Transport for Vision-Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, Kun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zqwryBoXYnh>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zqwryBoXYnh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With the increasing attention to large vision-language models such as CLIP, there has been a significant amount of effort dedicated to building efficient prompts. Unlike conventional methods of only learning one single prompt, we propose to learn multiple comprehensive prompts to describe diverse characteristics of categories such as intrinsic attributes or extrinsic contexts. However, directly matching each prompt to the same visual feature is problematic, as it pushes the prompts to converge to one point. To solve this problem, we propose to apply optimal transport to match the vision and text modalities. Specifically, we first model images and the categories with visual and textual feature sets. Then, we apply a two-stage optimization strategy to learn the prompts. In the inner loop, we optimize the optimal transport distance to align visual features and prompts by the Sinkhorn algorithm, while in the outer loop, we learn the prompts by this distance from the supervised data. Extensive experiments are conducted on the few-shot recognition task and the improvement demonstrates the superiority of our method. The code is available at https://github.com/CHENGY12/PLOT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                158. <h><a href=https://openreview.net/pdf?id=g8wBdhnstYz>Deterministic training of generative autoencoders using invertible layers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gianluigi Silvestri, Daan Roos, Luca Ambrogioni</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=g8wBdhnstYz>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/g8wBdhnstYz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, we provide a deterministic alternative to the stochastic variational training of generative autoencoders. We refer to these new generative autoencoders as AutoEncoders within Flows (AEF), since the encoder and decoder are defined as affine layers of an overall invertible architecture. This results in a deterministic encoding of the data, as opposed to the stochastic encoding of VAEs. The paper introduces two related families of AEFs. The first family relies on a partition of the ambient space and is trained by exact maximum-likelihood. The second family exploits a deterministic expansion of the ambient space and is trained by maximizing the log-probability in this extended space. This latter case leaves complete freedom in the choice of encoder, decoder and prior architectures, making it a drop-in replacement for the training of existing VAEs and VAE-style models. We show that these AEFs can have strikingly higher performance than architecturally identical VAEs in terms of log-likelihood and sample quality, especially for low dimensional latent spaces. Importantly, we show that AEF samples are substantially sharper than VAE samples. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                159. <h><a href=https://openreview.net/pdf?id=tVkrbkz42vc>PAC-NeRF: Physics Augmented Continuum Neural Radiance Fields for Geometry-Agnostic System Identification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy Jatavallabhula, Ming Lin, Chenfanfu Jiang, Chuang Gan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tVkrbkz42vc>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tVkrbkz42vc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing approaches to system identification (estimating the physical parameters of an object) from videos assume known object geometries. This precludes their applicability in a vast majority of scenes where object geometries are complex or unknown. In this work, we aim to identify parameters characterizing a physical system from a set of multi-view videos without any assumption on object geometry or topology. To this end, we propose &#34;Physics Augmented Continuum Neural Radiance Fields&#34; (PAC-NeRF), to estimate both the unknown geometry and physical parameters of highly dynamic objects from multi-view videos. We design PAC-NeRF to only ever produce physically plausible states by enforcing the neural radiance field to follow the conservation laws of continuum mechanics. For this, we design a hybrid Eulerian-Lagrangian representation of the neural radiance field, i.e., we use the Eulerian grid representation for NeRF density and color fields, while advecting the neural radiance fields via Lagrangian particles. This hybrid Eulerian-Lagrangian representation seamlessly blends efficient neural rendering with the material point method (MPM) for robust differentiable physics simulation. We validate the effectiveness of our proposed framework on geometry and physical parameter estimation over a vast range of materials, including elastic bodies, plasticine, sand, Newtonian and non-Newtonian fluids, and demonstrate significant performance gain on most tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                160. <h><a href=https://openreview.net/pdf?id=k60XE_b0Ix6>Learning Label Encodings for Deep Regression</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Deval Shah, Tor M. Aamodt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k60XE_b0Ix6>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an end-to-end automated approach to learn label encodings for deep regression.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k60XE_b0Ix6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep regression networks are widely used to tackle the problem of predicting a continuous value for a given input. Task-specialized approaches for training regression networks have shown significant improvement over generic approaches, such as direct regression. More recently, a generic approach based on regression by binary classification using binary-encoded labels has shown significant improvement over direct regression. The space of label encodings for regression is large. Lacking heretofore have been automated approaches to find a good label encoding for a given application. This paper introduces Regularized Label Encoding Learning (RLEL) for end-to-end training of an entire network and its label encoding. RLEL provides a generic approach for tackling regression. Underlying RLEL is our observation that the search space of label encodings can be constrained and efficiently explored by using a continuous search space of real-valued label encodings combined with a regularization function designed to encourage encodings with certain properties. These properties balance the probability of classification error in individual bits against error correction capability. Label encodings found by RLEL result in lower or comparable errors to manually designed label encodings. Applying RLEL results in $10.9\%$ and $12.4\%$ improvement in Mean Absolute Error (MAE) over direct regression and multiclass classification, respectively. Our evaluation demonstrates that RLEL can be combined with off-the-shelf feature extractors and is suitable across different architectures, datasets, and tasks. Code is available at \url{https://github.com/ubc-aamodt-group/RLEL_regression}. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                161. <h><a href=https://openreview.net/pdf?id=xmcYx_reUn6>BrainBERT: Self-supervised representation learning for intracranial recordings</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Christopher Wang, Vighnesh Subramaniam, Adam Uri Yaari, Gabriel Kreiman, Boris Katz, Ignacio Cases, Andrei Barbu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xmcYx_reUn6>7.5 - [8, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Modeling neural data with Transformers to create self-supervised contextual embeddings that increase decoding performance
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xmcYx_reUn6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We create a reusable Transformer, BrainBERT, for intracranial recordings bringing modern representation learning approaches to neuroscience. Much like in NLP and speech recognition, this Transformer enables classifying complex concepts, i.e., decoding neural data, with higher accuracy and with much less data by being pretrained in an unsupervised manner on a large corpus of unannotated neural recordings. Our approach generalizes to new subjects with electrodes in new positions and to unrelated tasks showing that the representations robustly disentangle the neural signal. Just like in NLP where one can study language by investigating what a language model learns, this approach opens the door to investigating the brain by what a model of the brain learns. As a first step along this path, we demonstrate a new analysis of the intrinsic dimensionality of the computations in different areas of the brain. To construct these representations, we combine a technique for producing super-resolution spectrograms of neural data with an approach designed for generating contextual representations of audio by masking. In the future, far more concepts will be decodable from neural recordings by using representation learning, potentially unlocking the brain like language models unlocked language.  
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                162. <h><a href=https://openreview.net/pdf?id=yAYHho4fATa>CFlowNets: Continuous Control with Generative Flow Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yinchuan Li, Shuang Luo, Haozhi Wang, Jianye HAO</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yAYHho4fATa>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Continuous GFlowNets
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yAYHho4fATa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generative flow networks (GFlowNets), as an emerging technique, can be used as an alternative to reinforcement learning for exploratory control tasks. GFlowNets aims to sample actions with a probability proportional to the reward, similar to sampling different candidates in an active learning fashion. However, existing GFlowNets cannot adapt to continuous control tasks because GFlowNets need to form a DAG and compute the flow matching loss by traversing the inflows and outflows of each node in the trajectory. In this paper, we propose generative continuous flow networks (CFlowNets) that can be applied to continuous control tasks. First, we present the theoretical formulation of CFlowNets. Then, a training framework for CFlowNets is proposed, including the action selection process, the flow approximation algorithm, and the continuous flow matching loss function. Afterward, we theoretically prove the error bound of the flow approximation. The error decreases rapidly as the number of flow samples increases. Finally, experimental results on continuous control tasks demonstrate the performance advantages of CFlowNets compared to many reinforcement learning methods, especially regarding exploration ability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                163. <h><a href=https://openreview.net/pdf?id=G-uNfHKrj46>Efficient Attention via Control Variates</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=G-uNfHKrj46>7.5 - [8, 6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel analysis of random feature attention based on control variates, which characterizes its gap to full softmax attention and induces a novel efficient variant that significantly improves the approximation while remaining efficient.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/G-uNfHKrj46.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                164. <h><a href=https://openreview.net/pdf?id=ZAKkiVxiAM9>Masked Unsupervised Self-training for Label-free Image Classification </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junnan Li, Silvio Savarese, Steven Hoi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZAKkiVxiAM9>7.5 - [8, 5, 8, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new label-free classification method which significantly improves upon CLIP by unsupervised adaptation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZAKkiVxiAM9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    State-of-the-art computer vision models are mostly trained with supervised learning using human-labeled images, which limits their scalability due to the expensive annotation cost. While self-supervised representation learning has achieved impressive progress, it still requires a second stage of finetuning on labeled data. On the other hand, models pre-trained with large-scale text supervision (e.g., CLIP) have enabled zero-shot transfer to downstream image classification tasks. However, the zero-shot performance of CLIP-like models are often insufficient for real-world adoption. In this paper, we aim to leverage the abundant unlabeled data from a target domain to improve the performance of a pre-trained zero-shot classifier, by unsupervised finetuning of the pre-trained model. We propose Masked Unsupervised Self-Training (MUST), a new approach which leverages two different and complimentary sources of training signals: pseudo-labels and raw images. MUST jointly optimizes three objectives to learn both class-level global feature and pixel-level local feature and enforces a regularization between the two. We demonstrate the efficacy of MUST on 8 downstream tasks across a variety of domains, where it improves upon CLIP by a large margin. MUST also outperforms supervised few-shot adaptation methods. It achieves a top-1 accuracy of 77.7% on ImageNet using ViT-B, +9.4% higher than CLIP, and +6.2% higher than 16-shot CLIP adaptation. Our code is available at https://github.com/salesforce/MUST.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                165. <h><a href=https://openreview.net/pdf?id=G2Q2Mh3avow>Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andy Zeng, Maria Attarian, brian ichter, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, Pete Florence</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=G2Q2Mh3avow>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompt engineering to capture new multimodal capabilities, without additional finetuning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/G2Q2Mh3avow.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We investigate how multimodal prompt engineering can use language as the intermediate representation to combine complementary knowledge from different pretrained (potentially multimodal) language models for a variety of tasks. This approach is both distinct from and complementary to the dominant paradigm of joint multimodal training. It also recalls a traditional systems-building view as in classical NLP pipelines, but with prompting large pretrained multimodal models. We refer to these as Socratic Models (SMs): a modular class of systems in which multiple pretrained models may be composed zero-shot via multimodal-informed prompting to capture new multimodal capabilities, without additional finetuning. We show that these systems provide competitive state-of-the-art performance for zero-shot image captioning and video-to-text retrieval, and also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes), and (iii) robot perception and planning. We hope this work provides (a) results for stronger zero-shot baseline performance with analysis also highlighting their limitations, (b) new perspectives for building multimodal systems powered by large pretrained models, and (c) practical application advantages in certain regimes limited by data scarcity, training compute, or model access.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                166. <h><a href=https://openreview.net/pdf?id=ZsvWb6mJnMv>Optimal Conservative Offline RL with General Function Approximation via Augmented Lagrangian</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, Jiantao Jiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZsvWb6mJnMv>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present practical and statistically optimal offline RL algorithms under general function approximation and single-policy concentrability.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZsvWb6mJnMv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning (RL), which aims at learning good policies from historical data, has received significant attention over the past years. Much effort has focused on improving offline RL practicality by addressing the prevalent issue of partial data coverage through various forms of conservative policy learning. While the majority of algorithms do not have finite-
sample guarantees, several provable conservative offline RL algorithms are designed and analyzed within the single-policy concentrability framework that handles partial coverage. Yet, in the nonlinear function approximation setting where confidence intervals are difficult to obtain, existing provable algorithms suffer from computational intractability, prohibitively strong assumptions, and suboptimal statistical rates. In this paper, we leverage the marginalized importance sampling (MIS) formulation of RL and present the first set of offline RL algorithms that are statistically optimal and practical under general function approximation and single-policy concentrability, bypassing the need for uncertainty quantification. We identify that the key to successfully solving the sample-based approximation of the MIS problem is ensuring that certain occupancy validity constraints are nearly satisfied. We enforce these constraints by a novel application of the augmented Lagrangian method and prove the following result: with the MIS formulation, augmented Lagrangian is enough for statistically optimal offline RL. In stark contrast to prior algorithms that induce additional conservatism through methods such as behavior regularization, our approach provably eliminates this need and reinterprets regularizers as &#34;enforcers of occupancy validity&#34; than &#34;promoters of conservatism.&#34;
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                167. <h><a href=https://openreview.net/pdf?id=q0nmYciuuZN>Learning on Large-scale Text-attributed Graphs via Variational Inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, Jian Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=q0nmYciuuZN>7.5 - [8, 8, 8, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a GLEM framework to effectively fuse GNN and LM with scalability, SOTA results are achieved on OGB datasets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/q0nmYciuuZN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper studies learning on text-attributed graphs (TAGs), where each node is associated with a text description. An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs). However, the problem becomes very challenging when graphs are large due to the high computational complexity brought by training large language models and  GNNs together. In this paper, we propose an efficient and effective solution to learning on large text-attributed graphs by fusing graph structure and language learning with a variational Expectation-Maximization (EM) framework, called GLEM. Instead of simultaneously training large language models and GNNs on big graphs, GLEM proposes to alternatively update the two modules in the E-step and M-step. Such a procedure allows training the two modules separately while simultaneously allowing the two modules to interact and mutually enhance each other. Extensive experiments on multiple data sets demonstrate the efficiency and effectiveness of the proposed approach.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                168. <h><a href=https://openreview.net/pdf?id=Cs3r5KLdoj>Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, Nitesh Chawla</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Cs3r5KLdoj>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose NOSMOG, a novel method to learn noise-robust and structure-aware MLPs on graphs, with superior effectiveness, outstanding robustness, and exceptional efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Cs3r5KLdoj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While Graph Neural Networks (GNNs) have demonstrated their efficacy in dealing with non-Euclidean structural data, they are difficult to be deployed in real applications due to the scalability constraint imposed by the multi-hop data dependency. Existing methods attempt to address this scalability issue by training student multi-layer perceptrons (MLPs) exclusively on node content features using labels derived from the teacher GNNs. However, the trained MLPs are neither effective nor robust. In this paper, we ascribe the lack of effectiveness and robustness to three significant challenges: 1) the misalignment between content feature and label spaces, 2) the strict hard matching to teacher&#39;s output, and 3) the sensitivity to node feature noises. To address the challenges, we propose NOSMOG, a novel method to learn NOise-robust Structure-aware MLPs On Graphs, with remarkable effectiveness, robustness, and efficiency. Specifically, we first address the misalignment by complementing node content with position features to capture the graph structural information. We then design an innovative representational similarity distillation strategy to inject soft node similarities into MLPs. Finally, we introduce adversarial feature augmentation to ensure stable learning against feature noises. Extensive experiments and theoretical analyses demonstrate the superiority of NOSMOG by comparing it to GNNs and the state-of-the-art method in both transductive and inductive settings across seven datasets. Codes are available at https://github.com/meettyj/NOSMOG.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                169. <h><a href=https://openreview.net/pdf?id=SJ0Lde3tRL>Extreme Q-Learning: MaxEnt RL without Entropy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Divyansh Garg, Joey Hejna, Matthieu Geist, Stefano Ermon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SJ0Lde3tRL>7.5 - [6, 10, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Introduce a novel framework for Q-learning that models the maximal soft-values without needing to sample from a policy and reaches SOTA performance on online and offline RL settings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SJ0Lde3tRL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern Deep Reinforcement Learning (RL) algorithms require estimates of the maximal Q-value, which are difficult to compute in continuous domains with an infinite number of possible actions. In this work, we introduce a new update rule for online and offline RL which directly models the maximal value using Extreme Value Theory (EVT), drawing inspiration from economics. By doing so, we avoid computing Q-values using out-of-distribution actions which is often a substantial source of error. Our key insight is to introduce an objective that directly estimates the optimal soft-value functions (LogSumExp) in the maximum entropy RL setting without needing to sample from a policy. Using EVT, we derive our \emph{Extreme Q-Learning} framework and consequently online and, for the first time, offline MaxEnt Q-learning algorithms, that do not explicitly require access to a policy or its entropy. Our method obtains consistently strong performance in the D4RL benchmark, outperforming prior works by \emph{10+ points} on the challenging Franka Kitchen tasks while offering moderate improvements over SAC and TD3 on online DM Control tasks. Visualizations and code can be found on our website.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                170. <h><a href=https://openreview.net/pdf?id=CWmvjOEhgH->MPCFORMER: FAST, PERFORMANT AND PRIVATE TRANSFORMER INFERENCE WITH MPC</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dacheng Li, Hongyi Wang, Rulin Shao, Han Guo, Eric Xing, Hao Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CWmvjOEhgH->7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a framework that allows fast, performant, and private inference with MPC for Transformer models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CWmvjOEhgH-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Enabling private inference is crucial for many cloud inference services that are based on Transformer models. However, existing private inference solutions can increase the inference latency by more than 60$\times$ or significantly compromise the inference quality. In this paper, we design the framework MPCFORMER as a practical solution, using Secure Multi-Party Computation (MPC) and Knowledge Distillation (KD). Through extensive evaluations, we show that MPCFORMER significantly speeds up Transformer inference in MPC settings while achieving similar ML performance to the input model. On the IMDb dataset, it achieves similar performance to $\text{BERT}_\text{BASE}$, while being 5.3$\times$ faster. On the GLUE benchmark, it achieves 97% performance of $\text{BERT}_\text{BASE}$ with a 2.2$\times$ speedup. MPCFORMER remains effective with different trained Transformer weights such as $\text{ROBERTA}_\text{BASE}$ and larger models including $\text{BERT}_\text{LARGE}$. Code is available at https://github.com/MccRee177/MPCFormer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                171. <h><a href=https://openreview.net/pdf?id=hWwY_Jq0xsN>Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eoin M. Kenny, Mycal Tucker, Julie Shah</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hWwY_Jq0xsN>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An &#34;interpretable-by-design&#34; deep reinforcement learning agent is proposed which uses prototypes for decision making.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hWwY_Jq0xsN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite recent success of deep learning models in research settings, their application in sensitive domains remains limited because of their opaque decision-making processes. Taking to this challenge, people have proposed various eXplainable AI (XAI) techniques designed to calibrate trust and understandability of black-box models, with the vast majority of work focused on supervised learning. Here, we focus on making an &#34;interpretable-by-design&#34; deep reinforcement learning agent which is forced to use human-friendly prototypes in its decisions, thus making its reasoning process clear. Our proposed method, dubbed Prototype-Wrapper Network (PW-Net), wraps around any neural agent backbone, and results indicate that it does not worsen performance relative to black-box models. Most importantly, we found in a user study that PW-Nets supported better trust calibration and task performance relative to standard interpretability approaches and black-boxes.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                172. <h><a href=https://openreview.net/pdf?id=iV9Cs8s8keU>Learning the Positions in CountSketch</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yi Li, Honghao Lin, Simin Liu, Ali Vakilian, David Woodruff</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iV9Cs8s8keU>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the first learning-based algorithms that also optimize the locations of the non-zero entries of CountSketch matrix.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iV9Cs8s8keU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider sketching algorithms which first compress data by multiplication with a random sketch matrix, and then apply the sketch to quickly solve an optimization problem, e.g., low-rank approximation and regression. In the learning-based sketching paradigm proposed by Indyk et al., the sketch matrix is found by choosing a random sparse matrix, e.g., CountSketch, and then the values of its non-zero entries are updated by running gradient descent on a training data set. Despite the growing body of work on this paradigm, a noticeable omission is that the locations of the non-zero entries of previous algorithms were fixed, and only their values were learned.
In this work, we propose the first learning-based algorithms that also optimize the locations of the non-zero entries. Our first proposed algorithm is based on a greedy algorithm. However, one drawback of the greedy algorithm is its slower training time. We fix this issue and propose approaches for learning a sketching matrix for both low-rank approximation and Hessian approximation for second-order optimization. The latter is helpful for a range of constrained optimization problems, such as LASSO and matrix estimation with a nuclear norm constraint. Both approaches achieve good accuracy with a fast running time.  Moreover, our experiments suggest that our algorithm can still reduce the error significantly even if we only have a very limited number of training matrices.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                173. <h><a href=https://openreview.net/pdf?id=1hKE9qjvz->gDDIM: Generalized denoising diffusion implicit models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qinsheng Zhang, Molei Tao, Yongxin Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1hKE9qjvz->7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> a small but delicate modification in parameterization to accelerate general diffusion models
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1hKE9qjvz-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Our goal is to extend the denoising diffusion implicit model (DDIM) to general diffusion models~(DMs) besides isotropic diffusions. Instead of constructing a non-Markov noising process as in the original DDIM, we examine the mechanism of DDIM from a numerical perspective.  We discover that the DDIM can be obtained by using some specific approximations of the score when solving the corresponding stochastic differential equation. We present an interpretation of the accelerating effects of DDIM that also explains the advantages of a deterministic sampling scheme over the stochastic one for fast sampling. Building on this insight, we extend DDIM to general DMs, coined generalized DDIM (gDDIM), with a small but delicate modification in parameterizing the score network. We validate gDDIM in two non-isotropic DMs: Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model (CLD). We observe more than 20 times acceleration in BDM. In the CLD, a diffusion model by augmenting the diffusion process with velocity, our algorithm achieves an FID score of 2.26, on CIFAR10, with only 50 number of score function evaluations~(NFEs) and an FID score of 2.86 with only 27 NFEs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                174. <h><a href=https://openreview.net/pdf?id=SJ1kSyO2jwu>Human Motion Diffusion Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, Amit Haim Bermano</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SJ1kSyO2jwu>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SJ1kSyO2jwu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Natural and expressive human motion generation is the holy grail of computer animation.
It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. 
Diffusion models are promising candidates for the human motion domain since they
have already shown remarkable generative capabilities in other domains, and their many-to-many nature. 
In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for human motion data.  MDM is transformer-based, combining insights from motion generation literature. 
A notable design-choice is that it predicts the sample itself rather than the noise in each step to facilitate the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion, action-to-motion, and unconditioned motion generation. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                175. <h><a href=https://openreview.net/pdf?id=S07feAlQHgM>A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Da-Wei Zhou, Qi-Wei Wang, Han-Jia Ye, De-Chuan Zhan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=S07feAlQHgM>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/S07feAlQHgM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Real-world applications require the classification model to adapt to new classes without forgetting old ones. Correspondingly, Class-Incremental Learning (CIL) aims to train a model with limited memory size to meet this requirement. Typical CIL methods tend to save representative exemplars from former classes to resist forgetting, while recent works find that storing models from history can substantially boost the performance. However, the stored models are not counted into the memory budget, which implicitly results in unfair comparisons. We find that when counting the model size into the total budget and comparing methods with aligned memory size, saving models do not consistently work, especially for the case with limited memory budgets. As a result, we need to holistically evaluate different CIL methods at different memory scales and simultaneously consider accuracy and memory size for measurement. On the other hand, we dive deeply into the construction of the memory buffer for memory efficiency. By analyzing the effect of different layers in the network, we find that shallow and deep layers have different characteristics in CIL. Motivated by this, we propose a simple yet effective baseline, denoted as MEMO for Memory-efficient Expandable MOdel. MEMO extends specialized layers based on the shared generalized representations, efficiently extracting diverse representations with modest cost and maintaining representative exemplars. Extensive experiments on benchmark datasets validate MEMO&#39;s competitive performance. Code is available at: https://github.com/wangkiw/ICLR23-MEMO
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                176. <h><a href=https://openreview.net/pdf?id=en9V5F8PR->Learning where and when to reason in neuro-symbolic inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Cristina Cornelio, Jan Stuehmer, Shell Xu Hu, Timothy Hospedales</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=en9V5F8PR->7.5 - [8, 6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/en9V5F8PR-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The integration of hard constraints on neural network outputs is a very desirable capability. This allows to instill trust in AI by guaranteeing the sanity of that neural network predictions with respect to domain knowledge. Recently, this topic has received a lot of attention. However, all the existing methods usually either impose the constraints in a &#34;weak&#34; form at training time, with no guarantees at inference, or fail to provide a general framework that supports different tasks and constraint types. We tackle this open problem from a neuro-symbolic perspective. Our pipeline enhances a conventional neural predictor with (1) a symbolic reasoning module capable of correcting structured prediction errors and (2) a neural attention module that learns to direct the reasoning effort to focus on potential prediction errors, while keeping other outputs unchanged. This framework provides an appealing trade-off between the efficiency of constraint-free neural inference and the prohibitive cost of exhaustive reasoning at inference time. We show that our method outperforms the state of the art on visual-Sudoku, and can also benefit visual scene graph prediction. Furthermore, it can improve the performance of existing neuro-symbolic systems that lack our explicit reasoning during inference.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                177. <h><a href=https://openreview.net/pdf?id=r8Mu7idxyF>Making Better Decision by Directly Planning in Continuous Control</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinhua Zhu, Yue Wang, Lijun Wu, Tao Qin, Wengang Zhou, Tie-Yan Liu, Houqiang Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=r8Mu7idxyF>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Directly using the environment model to do the planning might be an efficient way when making decision. We propose a novel POMP algorithm with a D3P planner module to achieve the efficient planning in the continuous action space control problem.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/r8Mu7idxyF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    By properly utilizing the learned environment model, model-based reinforcement learning methods can improve the sample efficiency for decision-making problems. Beyond using the learned environment model to train a policy, the success of MCTS-based methods shows that directly incorporating the learned environment model as a planner to make decisions might be more effective. However, when action space is of high dimension and continuous, directly planning according to the learned model is costly and non-trivial. Because of two challenges: (1) the infinite number of candidate actions and (2) the temporal dependency between actions in different timesteps. To address these challenges, inspired by Differential Dynamic Programming (DDP) in optimal control theory, we design a novel Policy Optimization with Model Planning (POMP) algorithm, which incorporates a carefully designed Deep Differential Dynamic Programming (D3P) planner into the model-based RL framework. In D3P planner, (1) to effectively plan in the continuous action space, we construct a locally quadratic programming problem that uses a gradient-based optimization process to replace search. (2) To take the temporal dependency of actions at different timesteps into account, we leverage the updated and latest actions of previous timesteps (i.e., step $1, \cdots, h-1$) to update the action of the current step (i.e., step $h$), instead of updating all actions simultaneously. We theoretically prove the convergence rate for our D3P planner and analyze the effect of the feedback term. In practice, to effectively apply the neural network based D3P planner in reinforcement learning, we leverage the policy network to initialize the action sequence and keep the action update conservative in the planning process. Experiments demonstrate that POMP consistently improves sample efficiency on widely used continuous control tasks. Our code is released at https://github.com/POMP-D3P/POMP-D3P. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                178. <h><a href=https://openreview.net/pdf?id=mRieQgMtNTQ>Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yinhuai Wang, Jiwen Yu, Jian Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mRieQgMtNTQ>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel zero-shot image restoration framework, achieving state-of-the-art performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mRieQgMtNTQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Most existing Image Restoration (IR) models are task-specific, which can not be generalized to different degradation operators. In this work, we propose the Denoising Diffusion Null-Space Model (DDNM), a novel zero-shot framework for arbitrary linear IR problems, including but not limited to image super-resolution, colorization, inpainting, compressed sensing, and deblurring. DDNM only needs a pre-trained off-the-shelf diffusion model as the generative prior, without any extra training or network modifications. By refining only the null-space contents during the reverse diffusion process, we can yield diverse results satisfying both data consistency and realness. We further propose an enhanced and robust version, dubbed DDNM+, to support noisy restoration and improve restoration quality for hard tasks. Our experiments on several IR tasks reveal that DDNM outperforms other state-of-the-art zero-shot IR methods. We also demonstrate that DDNM+ can solve complex real-world applications, e.g., old photo restoration. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                179. <h><a href=https://openreview.net/pdf?id=pxStyaf2oJ5>Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zihao Xu, Guang-Yuan Hao, Hao He, Hao Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pxStyaf2oJ5>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pxStyaf2oJ5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Previous studies have shown that leveraging &#34;domain index&#34; can significantly boost domain adaptation performance (Wang et al., 2020; Xu et al., 2022). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                180. <h><a href=https://openreview.net/pdf?id=6iDHce-0B-a>Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Arthur Jacot</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6iDHce-0B-a>7.5 - [8, 6, 8, 5, 8, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The representation cost of DNNs converges to a notion of nonlinear rank as the depth grows to infinity. This bias towards low-rank functions extends to large but finite widths.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6iDHce-0B-a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true&#39; rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                181. <h><a href=https://openreview.net/pdf?id=ZBUthI6wK9h>Robust Scheduling with GFlowNets</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>David W Zhang, Corrado Rainone, Markus Peschl, Roberto Bondesan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZBUthI6wK9h>7.5 - [8, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use GFlowNets for robust scheduling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZBUthI6wK9h.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Finding the best way to schedule operations in a computation graph is a classical NP-hard problem which is central to compiler optimization. However, evaluating the goodness of a schedule on the target hardware can be very time-consuming. Traditional approaches as well as previous machine learning ones typically optimize proxy metrics, which are fast to evaluate but can lead to bad schedules when tested on the target hardware. In this work, we propose a new approach to scheduling by sampling proportionally to the proxy metric using a novel GFlowNet method. We introduce a technique to control the trade-off between diversity and goodness of the proposed schedules at inference time and demonstrate empirically that the pure optimization baselines can lead to subpar performance with respect to our approach when tested on a target model. Furthermore, we show that conditioning the GFlowNet on the computation graph enables generalization to unseen scheduling problems for both synthetic and real-world compiler datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                182. <h><a href=https://openreview.net/pdf?id=_CDixzkzeyb>Prompt-to-Prompt Image Editing with Cross-Attention Control</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, Daniel Cohen-or</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_CDixzkzeyb>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_CDixzkzeyb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent large-scale text-driven synthesis diffusion models have attracted much attention thanks to their remarkable capabilities of generating highly diverse images that follow given text prompts. Therefore, it is only natural to build upon these synthesis models to provide text-driven image editing capabilities. However, Editing is challenging for these generative models, since an innate property of an editing technique is to preserve some content from the original image, while in the text-based models, even a small modification of the text prompt often leads to a completely different outcome. State-of-the-art methods mitigate this by requiring the users to provide a spatial mask to localize the edit, hence, ignoring the original structure and content within the masked region. In this paper, we pursue an intuitive prompt-to-prompt editing framework, where the edits are controlled by text only. We analyze a text-conditioned model in depth and observe that the cross-attention layers are the key to controlling the relation between the spatial layout of the image to each word in the prompt. With this observation, we propose to control the attention maps along the diffusion process. Our approach enables us to monitor the synthesis process by editing the textual prompt only, paving the way to a myriad of caption-based editing applications such as localized editing by replacing a word, global editing by adding a specification, and even controlling the extent to which a word is reflected in the image. We present our results over diverse images and prompts with different text-to-image models, demonstrating high-quality synthesis and fidelity to the edited prompts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                183. <h><a href=https://openreview.net/pdf?id=NPfDKT9OUJ3>H2RBox: Horizontal Box Annotation is All You Need for Oriented Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xue Yang, Gefan Zhang, Wentong Li, Yue Zhou, Xuehui Wang, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NPfDKT9OUJ3>7.5 - [10, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NPfDKT9OUJ3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Oriented object detection emerges in many applications from aerial images to autonomous driving, while many existing detection benchmarks are annotated with horizontal bounding box only which is also less costive than fine-grained rotated box, leading to a gap between the readily available training corpus and the rising demand for oriented object detection.  This paper proposes a simple yet effective oriented object detection approach called H2RBox merely using horizontal box annotation for weakly-supervised training, which closes the above gap and shows competitive performance even against those trained with rotated boxes.  The cores of our method are weakly- and self-supervised learning, which predicts the angle of the object by learning the consistency of two different views. To our best knowledge, H2RBox is the first horizontal box annotation-based oriented object detector. Compared to an alternative i.e. horizontal box-supervised instance segmentation with our post adaption to oriented object detection, our approach is not susceptible to the prediction quality of mask and can perform more robustly in complex scenes containing a large number of dense objects and outliers. Experimental results show that H2RBox has significant performance and speed advantages over horizontal box-supervised instance segmentation methods, as well as lower memory requirements. While compared to rotated box-supervised oriented object detectors, our method shows very close performance and speed. The source code is available at PyTorch-based \href{https://github.com/yangxue0827/h2rbox-mmrotate}{MMRotate} and Jittor-based \href{https://github.com/yangxue0827/h2rbox-jittor}{JDet}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                184. <h><a href=https://openreview.net/pdf?id=ejR4E1jaH9k>Solving stochastic weak Minty variational inequalities without increasing batch size</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thomas Pethick, Olivier Fercoq, Puya Latafat, Panagiotis Patrinos, Volkan Cevher</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ejR4E1jaH9k>7.5 - [8, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Weak MVIs can be solved with only stochastic feedback using extragradient-like algorithms by introducing a bias-correction term
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ejR4E1jaH9k.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper introduces a family of stochastic extragradient-type algorithms for a class of nonconvex-nonconcave problems characterized by the weak Minty variational inequality (MVI). Unlike existing results on extragradient methods in the monotone setting, employing diminishing stepsizes is no longer possible in the weak MVI setting. This has led to approaches such as increasing batch sizes per iteration which can however be prohibitively expensive. In contrast, our proposed methods involves two stepsizes and only requires one additional oracle evaluation per iteration. We show that it is possible to keep one fixed stepsize while it is only the second stepsize that is taken to be diminishing, making it interesting even in the monotone setting. Almost sure convergence is established and we provide a unified analysis for this family of schemes which contains a nonlinear generalization of the celebrated primal dual hybrid gradient algorithm.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                185. <h><a href=https://openreview.net/pdf?id=5N0wtJZ89r9>Embedding Fourier for Ultra-High-Definition Low-Light Image Enhancement</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chongyi Li, Chun-Le Guo, man zhou, Zhexin Liang, Shangchen Zhou, Ruicheng Feng, Chen Change Loy</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5N0wtJZ89r9>7.5 - [6, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper,  we propose a new solution for UHD LLIE based on the characteristics of the Fourier domain.  We also propose the first real UHD LLIE dataset with diverse data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5N0wtJZ89r9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Ultra-High-Definition (UHD) photo has gradually become the standard configuration in advanced imaging devices. The new standard unveils many issues in existing approaches for low-light image enhancement (LLIE), especially in dealing with the intricate issue of joint luminance enhancement and noise removal while remaining efficient. Unlike existing methods that address the problem in the spatial domain, we propose a new solution, UHDFour, that embeds Fourier transform into a cascaded network. Our approach is motivated by a few unique characteristics in the Fourier domain:  1) most luminance information concentrates on amplitudes while noise is closely related to phases, and 2) a high-resolution image and its low-resolution version share similar amplitude patterns. Through embedding Fourier into our network, the amplitude and phase of a low-light image are separately processed to avoid amplifying noise when enhancing luminance. Besides, UHDFour is scalable to UHD images by implementing amplitude and phase enhancement under the low-resolution regime and then adjusting the high-resolution scale with few computations. We also contribute the first real UHD LLIE dataset, UHD-LL, that contains 2,150 low-noise/normal-clear 4K image pairs with diverse darkness and noise levels captured in different scenarios. With this dataset, we systematically analyze the performance of existing LLIE methods for processing UHD images and demonstrate the advantage of our solution. We believe our new framework, coupled with the dataset, would push the frontier of LLIE towards UHD. The code and dataset are available at https://li-chongyi.github.io/UHDFour/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                186. <h><a href=https://openreview.net/pdf?id=vw-5EgYbJZr>A Non-monotonic Self-terminating Language Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eugene Choi, Kyunghyun Cho, Cheolhyoung Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vw-5EgYbJZr>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new method to prevent language models from non-terminating sequences resulting from incomplete decoding algorithms.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vw-5EgYbJZr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent large-scale neural autoregressive sequence models have shown impressive performances on a variety of natural language generation tasks. However, their generated sequences often exhibit degenerate properties such as non-termination, undesirable repetition, and premature termination, when generated with decoding algorithms such as greedy search, beam search, top-$k$ sampling, and nucleus sampling. In this paper, we focus on the problem of non-terminating sequences resulting from an incomplete decoding algorithm. We first define an incomplete probable decoding algorithm which includes greedy search, top-$k$ sampling, and nucleus sampling, beyond the incomplete decoding algorithm originally put forward by Welleck et al. (2020). We then propose a non-monotonic self-terminating language model, which significantly relaxes the constraint of monotonically increasing termination probability in the originally proposed self-terminating language model by Welleck et al. (2020), to address the issue of non-terminating sequences when using incomplete probable decoding algorithms. We prove that our proposed model prevents non-terminating sequences when using not only incomplete probable decoding algorithms but also beam search. We empirically validate our model on sequence completion tasks with various architectures.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                187. <h><a href=https://openreview.net/pdf?id=oJZ8bPtCar>Stochastic No-regret Learning for General Games with Variance Reduction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yichi Zhou, Fang Kong, Shuai Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oJZ8bPtCar>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oJZ8bPtCar.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We show that a stochastic version of optimistic mirror descent (OMD), a variant of mirror descent with recency bias, converges fast in general games. More specifically, with our algorithm, the individual regret of each player vanishes at a speed of $O(1/T^{3/4})$ and the sum of all players&#39; regret vanishes at a speed of $O(1/T)$, which is an improvement upon the $O(1/\sqrt{T})$ convergence rate of prior stochastic algorithms, where $T$ is the number of interaction rounds. Due to the advantage of stochastic methods in the computational cost, we significantly improve the time complexity over the deterministic algorithms to approximate coarse correlated equilibrium. To achieve lower time complexity, we equip the stochastic version of OMD in \cite{alacaoglu2021stochastic} with a novel low-variance Monte-Carlo estimator. Our algorithm extends previous works \cite{alacaoglu2021stochastic,carmon2019variance} from two-player zero-sum games to general games. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                188. <h><a href=https://openreview.net/pdf?id=_2bDpAtr7PI>Image to Sphere: Learning Equivariant Features for Efficient Pose Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>David Klee, Ondrej Biza, Robert Platt, Robin Walters</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_2bDpAtr7PI>7.5 - [8, 6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel architecture which efficiently describes uncertainty in pose estimation from images by using learned SO(3)-equivariant features to generate complex distributions over SO(3) with the Fourier basis.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_2bDpAtr7PI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Predicting the pose of objects from a single image is an important but difficult computer vision problem. Methods that predict a single point estimate do not predict the pose of objects with symmetries well and cannot represent uncertainty. Alternatively, some works predict a distribution over orientations in $\mathrm{SO}(3)$. However, training such models can be computation- and sample-inefficient. Instead, we propose a novel mapping of features from the image domain to the 3D rotation manifold. Our method then leverages $\mathrm{SO}(3)$ equivariant layers, which are more sample efficient, and outputs a distribution over rotations that can be sampled at arbitrary resolution. We demonstrate the effectiveness of our method at object orientation prediction, and achieve state-of-the-art performance on the popular PASCAL3D+ dataset. Moreover, we show that our method can model complex object symmetries, without any modifications to the parameters or loss function.  Code is available at \url{https://dmklee.github.io/image2sphere}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                189. <h><a href=https://openreview.net/pdf?id=vaxnu-Utr4l>WikiWhy: Answering and Explaining Cause-and-Effect Questions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthew Ho, Aditya Sharma, Justin Chang, Michael Saxon, Sharon Levy, Yujie Lu, William Yang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vaxnu-Utr4l>7.5 - [8, 8, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose WikiWhy, a dataset containing 9000+ &#34;why&#34; question-answer-rationale triplets to assess Large Language Models&#39; cause-effect reasoning capability.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vaxnu-Utr4l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As large language models (LLMs) grow larger and more sophisticated, assessing their &#34;reasoning&#34; capabilities in natural language grows more challenging. Recent question answering (QA) benchmarks that attempt to assess reasoning are often limited by a narrow scope of covered situations and subject matters. We introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language. WikiWhy contains over 9,000 &#34;why&#34; question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer. WikiWhy serves as a benchmark for the reasoning capabilities of LLMs because it demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer &amp; explain condition, leaving significant room for future improvements.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                190. <h><a href=https://openreview.net/pdf?id=5ktFNz_pJLK>Learning to Estimate Shapley Values with Vision Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ian Connick Covert, Chanwoo Kim, Su-In Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5ktFNz_pJLK>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A learning-based approach to efficiently calculate Shapley values for ViTs
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5ktFNz_pJLK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transformers have become a default architecture in computer vision, but understanding what drives their predictions remains a challenging problem. Current explanation approaches rely on attention values or input gradients, but these provide a limited view of a model’s dependencies. Shapley values offer a theoretically sound alternative, but their computational cost makes them impractical for large, high-dimensional models. In this work, we aim to make Shapley values practical for vision transformers (ViTs). To do so, we first leverage an attention masking approach to evaluate ViTs with partial information, and we then develop a procedure to generate Shapley value explanations via a separate, learned explainer model. Our experiments compare Shapley values to many baseline methods (e.g., attention rollout, GradCAM, LRP), and we find that our approach provides more accurate explanations than existing methods for ViTs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                191. <h><a href=https://openreview.net/pdf?id=aKcS3xojnwY>GEASS: Neural causal feature selection for high-dimensional biological data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingze Dong, Yuval Kluger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aKcS3xojnwY>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b>  We propose a new method (GEASS) to identify causally interacting features for high-dimensional spatial/temporal structured data, and apply it to several biological data to infer causal regulatory patterns.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aKcS3xojnwY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Identifying nonlinear causal relationships in high-dimensional biological data is an important task. However, current neural network based causality detection approaches for such data suffer from poor interpretability and cannot scale well to the high dimensional regime. Here we present GEASS (Granger fEAture Selection of Spatiotemporal data), which identifies sparse Granger causality mechanisms of high dimensional spatiotemporal data by a single neural network. GEASS maximizes sparsity-regularized modified transfer entropy with a theoretical guarantee of recovering features with spatial/temporal Granger causal relationships. The sparsity regularization is achieved by a novel combinatorial stochastic gate layer to select sparse non-overlapping feature subsets. We demonstrate the efficacy of GEASS in several synthetic datasets and real biological data from single-cell RNA sequencing and spatial transcriptomics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                192. <h><a href=https://openreview.net/pdf?id=tyZ1ChGZIKO>Selective Frequency Network for Image Restoration</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuning Cui, Yi Tao, Zhenshan Bing, Wenqi Ren, Xinwei Gao, Xiaochun Cao, Kai Huang, Alois Knoll</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tyZ1ChGZIKO>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel network to recover the most useful frequency component for image restoration via frequency selection.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tyZ1ChGZIKO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Image restoration aims to reconstruct the latent sharp image from its corrupted counterpart. Besides dealing with this long-standing task in the spatial domain, a few approaches seek solutions in the frequency domain in consideration of the large discrepancy between spectra of sharp/degraded image pairs. However, these works commonly utilize transformation tools, e.g., wavelet transform, to split features into several frequency parts, which is not flexible enough to select the most informative frequency component to recover. In this paper, we exploit a multi-branch and content-aware module to decompose features into separate frequency subbands dynamically and locally, and then accentuate the useful ones via channel-wise attention weights. In addition, to handle large-scale degradation blurs, we propose an extremely simple decoupling and modulation module to enlarge the receptive field via global and window-based average pooling. Integrating two developed modules into a U-Net backbone, the proposed Selective Frequency Network (SFNet) performs favorably against state-of-the-art algorithms on five image restoration tasks, including single-image defocus deblurring, image dehazing, image motion deblurring, image desnowing, and image deraining.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                193. <h><a href=https://openreview.net/pdf?id=SMa9EAovKMC>Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, Yuhuai Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SMa9EAovKMC>7.5 - [6, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SMa9EAovKMC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from $20.9\%$ to $39.3\%$ on a collection of mathematical competition problems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                194. <h><a href=https://openreview.net/pdf?id=cFuMmbWiN6>Relational Attention: Generalizing Transformers for Graph-Structured Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Cameron Diao, Ricky Loynd</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cFuMmbWiN6>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We generalize transformer attention to include edge vectors, which are then updated along with the standard node vectors in each layer of a transformer&#39;s computation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cFuMmbWiN6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. As set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention&#39;s inherent ability to leverage the greater expressivity of graphs over sets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                195. <h><a href=https://openreview.net/pdf?id=Bd7GueaTxUz>BAYES RISK CTC: CONTROLLABLE CTC ALIGNMENT IN SEQUENCE-TO-SEQUENCE TASKS</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinchuan Tian, Brian Yan, Jianwei Yu, CHAO WENG, Dong Yu, Shinji Watanabe</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Bd7GueaTxUz>7.5 - [8, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A Bayes risk function is applied to each CTC path to express the preference for selected paths and achieve controllable CTC alignment prediction
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Bd7GueaTxUz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sequence-to-Sequence (seq2seq) tasks transcribe the input sequence to a target sequence. The Connectionist Temporal Classification (CTC) criterion is widely used in multiple seq2seq tasks. Besides predicting the target sequence, a side product of CTC is to predict the alignment, which is the most probable input-long sequence that specifies a hard aligning relationship between the input and target units. As there are multiple potential aligning sequences (called paths) that are equally considered in CTC formulation, the choice of which path will be most probable and become the predicted alignment is always uncertain. In addition, it is usually observed that the alignment predicted by vanilla CTC will drift compared with its reference and rarely provides practical functionalities. Thus, the motivation of this work is to make the CTC alignment prediction controllable and thus equip CTC with extra functionalities. The Bayes risk CTC (BRCTC) criterion is then proposed in this work, in which a customizable Bayes risk function is adopted to enforce the desired characteristics of the predicted alignment. With the risk function, the BRCTC is a general framework to adopt some customizable preference over the paths in order to concentrate the posterior into a particular subset of the paths. In applications, we explore one particular preference which yields models with the down-sampling ability and reduced inference costs. By using BRCTC with another preference for early emissions, we obtain an improved performance-latency trade-off for online models. Experimentally, the proposed BRCTC reduces the inference cost of offline models by up to 47% without performance degradation and cuts down the overall latency of online systems to an unseen level.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                196. <h><a href=https://openreview.net/pdf?id=3YjQfCLdrzz>FoSR: First-order spectral rewiring for addressing oversquashing in GNNs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kedar Karhadkar, Pradeep Kr. Banerjee, Guido Montufar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3YjQfCLdrzz>7.5 - [8, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a graph rewiring algorithm that prevents oversquashing in GNNs via spectral expansion while retaining the original graph via a relational structure that prevents oversmoothing.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3YjQfCLdrzz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural networks (GNNs) are able to leverage the structure of graph data by passing messages along the edges of the graph. While this allows GNNs to learn features depending on the graph structure, for certain graph topologies it leads to inefficient information propagation and a problem known as oversquashing. This has recently been linked with the curvature and spectral gap of the graph. On the other hand, adding edges to the message-passing graph can lead to increasingly similar node representations and a problem known as oversmoothing. We propose a computationally efficient algorithm that prevents oversquashing by systematically adding edges to the graph based on spectral expansion. We combine this with a relational architecture, which lets the GNN preserve the original graph structure and provably prevents oversmoothing. We find experimentally that our algorithm outperforms existing graph rewiring methods in several graph classification tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                197. <h><a href=https://openreview.net/pdf?id=paGvsrl4Ntr>Transfer NAS with Meta-learned Bayesian Surrogates</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gresa Shala, Thomas Elsken, Frank Hutter, Josif Grabocka</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=paGvsrl4Ntr>7.5 - [8, 6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/paGvsrl4Ntr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While neural architecture search (NAS) is an intensely-researched area, approaches typically still suffer from either (i) high computational costs or (ii) lack of robustness across datasets and experiments. Furthermore, most methods start searching for an optimal architecture from scratch, ignoring prior knowledge. This is in contrast to the manual design process by researchers and engineers that leverage previous deep learning experiences by, e.g., transferring architectures from previously solved, related problems.
We propose to adopt this human design strategy and introduce a novel surrogate for NAS, that is meta-learned across prior architecture evaluations across different datasets. We utilizes Bayesian Optimization (BO) with deep-kernel Gaussian Processes,  graph neural networks for the architecture embeddings and a transformer-based set encoder of datasets. As a result, our method consistently achieves state-of-the-art results on six computer vision datasets, while being as fast as one-shot NAS methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                198. <h><a href=https://openreview.net/pdf?id=J6F3lLg4Kdp>Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin Huang, AJAY KUMAR JAISWAL, Zhangyang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=J6F3lLg4Kdp>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this work, we assemble a large-scale, difficult and diverse benchmark for sparse neural networks, on which current SOTA sparse networks are actually prone to significant performance degradation, sometimes even at trivial sparsity levels, e.g., 5%.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/J6F3lLg4Kdp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sparse Neural Networks (SNNs) have received voluminous attention predominantly due to growing computational and memory footprints of consistently exploding parameter count in large-scale models. Similar to their dense counterparts, recent SNNs generalize just as well and are equipped with numerous favorable benefits (e.g., low complexity, high scalability, and robustness), sometimes even better than the original dense networks. As research effort is focused on developing increasingly sophisticated sparse algorithms, it is startling that a comprehensive benchmark to evaluate the effectiveness of these algorithms has been highly overlooked. In absence of a carefully crafted evaluation benchmark, most if not all, sparse algorithms are evaluated against fairly simple and naive tasks (eg. CIFAR-10/100, ImageNet, GLUE, etc.), which can potentially camouflage many advantages as well unexpected predicaments of SNNs. In pursuit of a more general evaluation and unveiling the true potential of sparse algorithms, we introduce “Sparsity May Cry” Benchmark (SMC-Bench), a collection of carefully-curated 4 diverse tasks with 10 datasets, that accounts for capturing a wide range of domain-specific and sophisticated knowledge. Our systemic evaluation of the most representative sparse algorithms reveals an important obscured observation: the state-of-the-art magnitude- and/or gradient-based sparse algorithms seemingly fail to perform on SMC-Bench when applied out-of-the-box, sometimes at significantly trivial sparsity as low as 5%. The observations seek the immediate attention of the sparsity research community to reconsider the highly proclaimed benefits of SNNs. We further conduct a thorough investigation into the reasons for the failure of common SNNs. Our analysis points out that such failure is intimately related to the “lazy regime” of large model training, which hints us with stronger pruning recipes that alleviate the failure on SMC-Bench (though still more or less suffering). By incorporating these well-thought and diverse tasks, SMC-Bench is designed to favor and encourage the development of more scalable and generalizable sparse algorithms. We open-source SMC-Bench to assist researchers in building next-generation sparse algorithms that scale and generalize: https://github.com/VITA-Group/SMC-Bench.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                199. <h><a href=https://openreview.net/pdf?id=DJEEqoAq7to>RLx2: Training a Sparse Deep Reinforcement Learning Model from Scratch</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yiqin Tan, Pihe Hu, Ling Pan, Jiatai Huang, Longbo Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DJEEqoAq7to>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new framework for training an efficient DRL agent from scratch with an ultra-sparse network with strong performanc without performance degradation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DJEEqoAq7to.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training deep reinforcement learning (DRL) models usually requires high computation costs. Therefore, compressing DRL models possesses immense potential for training acceleration and model deployment. However, existing methods that generate small models mainly adopt the knowledge distillation-based approach by iteratively training a dense network. As a result, the training process still demands massive computing resources. Indeed, sparse training from scratch in DRL has not been well explored and is particularly challenging due to non-stationarity in bootstrap training. In this work, we propose a novel sparse DRL training framework, “the Rigged Reinforcement Learning Lottery” (RLx2), which builds upon gradient-based topology evolution and is capable of training a sparse DRL model based entirely on a sparse network. Specifically, RLx2 introduces a novel multi-step TD target mechanism with a dynamic-capacity replay buffer to achieve robust value learning and efficient topology exploration in sparse models. It also reaches state-of-the-art sparse training performance in several tasks, showing $7.5\times$-$20\times$ model compression with less than $3\%$ performance degradation and up to $20\times$ and $50\times$ FLOPs reduction for training and inference, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                200. <h><a href=https://openreview.net/pdf?id=E01k9048soZ>UNIFIED-IO: A Unified Model for Vision, Language, and Multi-modal Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, Aniruddha Kembhavi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=E01k9048soZ>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/E01k9048soZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and pre-trained models will be made publicly available.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                201. <h><a href=https://openreview.net/pdf?id=k71IGLC8cfc>A Unified Algebraic Perspective on Lipschitz Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexandre Araujo, Aaron J Havens, Blaise Delattre, Alexandre Allauzen, Bin Hu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k71IGLC8cfc>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, and show that AOL and CPL can be re-derived and generalized using exactly the same semidefinite programming (SDP) condition.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k71IGLC8cfc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical solutions of a common semidefinite programming (SDP) condition.  We also prove that AOL biases the scaled weight to the ones which are close to the set of orthogonal matrices in a certain mathematical manner. Moreover, our algebraic condition, combined with the Gershgorin circle theorem, readily leads to new and diverse parameterizations for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers (SLL), allows us to design non-trivial yet efficient generalization of convex potential layers.  Finally, the comprehensive set of experiments on image classification shows that SLLs outperform previous approaches on certified robust accuracy. Code is available at https://github.com/araujoalexandre/Lipschitz-SLL-Networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                202. <h><a href=https://openreview.net/pdf?id=99RpBVpLiX>Distilling Model Failures as Directions in Latent Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Saachi Jain, Hannah Lawrence, Ankur Moitra, Aleksander Madry</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=99RpBVpLiX>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a scalable method for automatically distilling and captioning a model&#39;s failure modes as directions in a latent space.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/99RpBVpLiX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model&#39;s failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model&#39;s failure modes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                203. <h><a href=https://openreview.net/pdf?id=NJENsJ37sQ>Empowering Networks With Scale and Rotation Equivariance Using A Similarity Convolution</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zikai Sun, Thierry Blu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NJENsJ37sQ>7.5 - [8, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NJENsJ37sQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The translational equivariant nature of Convolutional Neural Networks (CNNs) is a reason for its great success in computer vision. However, networks do not enjoy more general equivariance properties such as rotation or scaling, ultimately limiting their generalization performance. To address this limitation, we devise a method that endows CNNs with simultaneous equivariance with respect to translation, rotation, and scaling.  Our approach defines a convolution-like operation and ensures equivariance based on our proposed scalable Fourier-Argand representation. The method maintains similar efficiency as a traditional network and hardly introduces any additional learnable parameters, since it does not face the computational issue that often occurs in group-convolution operators. We validate the efficacy of our approach in the image classification task, demonstrating its robustness and the generalization ability to both scaled and rotated inputs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                204. <h><a href=https://openreview.net/pdf?id=N_g8TT9Cy7f>Human-Guided Fair Classification for Natural Language Processing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Florian E. Dorner, Momchil Peychev, Nikola Konstantinov, Naman Goel, Elliott Ash, Martin Vechev</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=N_g8TT9Cy7f>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide new methods for generating individual fairness specifications for NLP based on LLMs and validate them in a human study. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/N_g8TT9Cy7f.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Text classifiers have promising applications in high-stake tasks such as resume screening and content moderation. These classifiers must be fair and avoid discriminatory decisions by being invariant to perturbations of sensitive attributes such as gender or ethnicity. However, there is a gap between human intuition about these perturbations and the formal similarity specifications capturing them. While existing research has started to address this gap, current methods are based on hardcoded word replacements, resulting in specifications with limited expressivity or ones that fail to fully align with human intuition (e.g., in cases of asymmetric counterfactuals). This work proposes novel methods for bridging this gap by discovering expressive and intuitive individual fairness specifications. We show how to leverage unsupervised style transfer and GPT-3&#39;s zero-shot capabilities to automatically generate expressive candidate pairs of semantically similar sentences that differ along sensitive attributes. We then validate the generated pairs via an extensive crowdsourcing study, which confirms that a lot of these pairs align with human intuition about fairness in the context of toxicity classification. Finally, we show how limited amounts of human feedback can be leveraged to learn a similarity specification that can be used to train downstream fairness-aware models. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                205. <h><a href=https://openreview.net/pdf?id=cDYRS5iZ16f>Learning to Grow Pretrained Models for Efficient Transformer Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, Philip Greengard, Leonid Karlinsky, Rogerio Feris, David Daniel Cox, Zhangyang Wang, Yoon Kim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cDYRS5iZ16f>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Learning to grow smaller, extant models to enable faster training of newer, larger transformers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cDYRS5iZ16f.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Scaling transformers has led to significant breakthroughs in many domains, leading to a paradigm in which larger versions of existing models are trained and released on a periodic basis. New instances of such models are typically trained completely from scratch, despite the fact that they are often just scaled-up versions of their smaller counterparts. How can we use the implicit knowledge in the parameters of smaller, extant models to enable faster training of newer, larger models? This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where we learn to linearly map  the parameters of the smaller model to initialize the larger model. For tractable learning, we factorize the linear transformation as a composition of  (linear) width- and  depth-growth operators, and further employ a  Kronecker factorization of these growth operators to encode architectural knowledge. Extensive experiments across both language and vision transformers demonstrate that our learned Linear Growth Operator (LiGO)  can save up to 50% computational cost of training from scratch, while also consistently outperforming strong baselines that also reuse smaller pretrained models to initialize larger models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                206. <h><a href=https://openreview.net/pdf?id=KbYevcLjnc>PEER: A Collaborative Language Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, Sebastian Riedel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KbYevcLjnc>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce PEER, a language model trained to mimic the collaborative editing process by which humans often write text.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KbYevcLjnc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes.
Agnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions.
To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself. PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER&#39;s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                207. <h><a href=https://openreview.net/pdf?id=7oFuxtJtUMH>Certified Training: Small Boxes are All You Need</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mark Niklas Mueller, Franziska Eckert, Marc Fischer, Martin Vechev</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7oFuxtJtUMH>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel certified training method based on propagating small input regions, establishing a new state of the art for certified accuracy.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7oFuxtJtUMH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    To obtain, deterministic guarantees of adversarial robustness, specialized training methods are used. We propose, SABR, a novel such certified training method, based on the key insight that propagating interval bounds for a small but carefully selected subset of the adversarial input region is sufficient to approximate the worst-case loss over the whole region while significantly reducing approximation errors. We show in an extensive empirical evaluation that SABR outperforms existing certified defenses in terms of both standard and certifiable accuracies across perturbation magnitudes and datasets, pointing to a new class of certified training methods promising to alleviate the robustness-accuracy trade-off.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                208. <h><a href=https://openreview.net/pdf?id=jlAjNL8z5cs>Visual Classification via Description from Large Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sachit Menon, Carl Vondrick</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jlAjNL8z5cs>7.5 - [8, 6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We enhance zero-shot recognition with vision-language models by comparing to category descriptors from GPT-3, enabling better performance in an interpretable setting that also allows for incorporation of new concepts and bias mitigation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jlAjNL8z5cs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Vision-language models such as CLIP have shown promising performance on a variety of recognition tasks using the standard zero-shot classification procedure -- computing similarity between the query image and the embedded words for each category. By only using the category name, they neglect to make use of the rich context of additional information that language affords. The procedure gives no intermediate understanding of why a category is chosen, and furthermore provides no mechanism for adjusting the criteria used towards this decision. We present an alternative framework for classification with VLMs, which we call classification by description. We ask VLMs to check for descriptive features rather than broad categories: to find a tiger, look for its stripes; its claws; and more. By basing decisions on these descriptors, we can provide additional cues that encourage using the features we want to be used. In the process, we can get a clear idea of what the model ``thinks&#34; it is seeing to make its decision; it gains some level of inherent explainability. We query large language models (e.g., GPT-3) for these descriptors to obtain them in a scalable way. Extensive experiments show our framework has numerous advantages past interpretability. We show improvements in accuracy on ImageNet across distribution shifts; demonstrate the ability to adapt VLMs to recognize concepts unseen during training; and illustrate how descriptors can be edited to effectively mitigate bias compared to the baseline. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                209. <h><a href=https://openreview.net/pdf?id=fxjzKOdw9wb>Exploring Temporally Dynamic Data Augmentation for Video Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Taeoh Kim, Jinhyung Kim, Minho Shim, Sangdoo Yun, Myunggu Kang, Dongyoon Wee, Sangyoun Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fxjzKOdw9wb>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel data augmentation framework for video recognition that extends the static nature of image augmentations into temporally dynamic.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fxjzKOdw9wb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Data augmentation has recently emerged as an essential component of modern training recipes for visual recognition tasks.
However, data augmentation for video recognition has been rarely explored despite its effectiveness.
Few existing augmentation recipes for video recognition naively extend the image augmentation methods by applying the same operations to the whole video frames.
Our main idea is that the magnitude of augmentation operations for each frame needs to be changed over time to capture the real-world video&#39;s temporal variations.
These variations should be generated as diverse as possible using fewer additional hyper-parameters during training.
Through this motivation, we propose a simple yet effective video data augmentation framework, DynaAugment.
The magnitude of augmentation operations on each frame is changed by an effective mechanism, Fourier Sampling that parameterizes diverse, smooth, and realistic temporal variations.
DynaAugment also includes an extended search space suitable for video for automatic data augmentation methods.
DynaAugment experimentally demonstrates that there are additional performance rooms to be improved from static augmentations on diverse video models.
Specifically, we show the effectiveness of DynaAugment on various video datasets and tasks: large-scale video recognition (Kinetics-400 and Something-Something-v2), small-scale video recognition (UCF-101 and HMDB-51), fine-grained video recognition (Diving-48 and FineGym), video action segmentation on Breakfast, video action localization on THUMOS&#39;14, and video object detection on MOT17Det.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                210. <h><a href=https://openreview.net/pdf?id=PEgBEB74JjB>The Symmetric Generalized Eigenvalue Problem as a Nash Equilibrium</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ian Gemp, Charlie Chen, Brian McWilliams</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PEgBEB74JjB>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We formulate the solution to the generalized eigenvalue problem as the Nash of a game, design an unbiased streaming-style algorithm to solve it, and analyze neural representations 1000x larger than before.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PEgBEB74JjB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The symmetric generalized eigenvalue problem (SGEP) is a fundamental concept in numerical linear algebra. It captures the solution of many classical machine learning problems such as canonical correlation analysis, independent components analysis, partial least squares, linear discriminant analysis, principal components and others. Despite this, most general solvers are prohibitively expensive when dealing with *streaming data sets* (i.e., minibatches) and research has instead concentrated on finding efficient solutions to specific problem instances. In this work, we develop a game-theoretic formulation of the top-$k$ SGEP whose Nash equilibrium is the set of generalized eigenvectors. We also present a parallelizable algorithm with guaranteed asymptotic convergence to the Nash. Current state-of-the-art methods require $\mathcal{O}(d^2k)$ runtime complexity per iteration which is prohibitively expensive when the number of dimensions ($d$) is large. We show how to modify this parallel approach to achieve $\mathcal{O}(dk)$ runtime complexity. Empirically we demonstrate that this resulting algorithm is able to solve a variety of SGEP problem instances including a large-scale analysis of neural network activations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                211. <h><a href=https://openreview.net/pdf?id=mWVoBz4W0u>PaLI: A Jointly-Scaled Multilingual Language-Image Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mWVoBz4W0u>7.5 - [6, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mWVoBz4W0u.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Effective scaling and a flexible task interface enable large language models to excel at many tasks. We present PaLI, a model that extends this approach to the joint modeling of language and vision. PaLI generates text based on visual and textual inputs, and with this interface performs many vision, language, and multimodal tasks, in many languages. To train PaLI, we make use of large pretrained encoder-decoder language models and Vision Transformers (ViTs). This allows us to capitalize on their existing capabilities and leverage the substantial cost of training them. We find that joint scaling of the vision and language components is important. Since existing Transformers for language are much larger than their vision counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the benefits from even larger-capacity vision models. To train PaLI, we create a large multilingual mix of pretraining tasks, based on a new image-text training set containing 10B images and texts in over 100 languages. PaLI achieves state-of-the-art in multiple vision and language tasks (such as captioning, visual question-answering, scene-text understanding), while retaining a simple, modular, and scalable design.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                212. <h><a href=https://openreview.net/pdf?id=KDhFkA6MQsW>Faster Gradient-Free Methods for Escaping Saddle Points</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hualin Zhang, Bin Gu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KDhFkA6MQsW>7.5 - [6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KDhFkA6MQsW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Escaping from saddle points has become an important research topic in non-convex optimization. In this paper, we study the case when calculations of explicit gradients are expensive or even infeasible, and only function values are accessible. 
Currently, there have  two types of gradient-free (zeroth-order) methods based on  random perturbation and negative curvature finding  proposed to escape saddle points  efficiently and converge to an $\epsilon$-approximate second-order stationary point. 
Nesterov&#39;s accelerated gradient descent (AGD) method can escape saddle points faster than gradient descent (GD) which have been verified in first-order algorithms. However, whether  AGD could accelerate the gradient-free methods is still unstudied. To  unfold this mystery, in this paper, we propose two accelerated  variants for the two types of gradient-free methods of escaping saddle points. We show that our algorithms can find an $\epsilon$-approximate second-order stationary point with $\tilde{\mathcal{O}}(1/\epsilon^{1.75})$ iteration complexity and $\tilde{\mathcal{O}}(d/\epsilon^{1.75})$ oracle complexity, where $d$ is the problem dimension. Thus, our methods achieve a comparable convergence rate to their first-order counterparts and have fewer oracle complexity compared to prior derivative-free methods for finding second-order stationary points.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                213. <h><a href=https://openreview.net/pdf?id=D1Iqfm7WTkk>Neural ePDOs: Spatially Adaptive Equivariant Partial Differential Operator Based  Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lingshen He, Yuxuan Chen, Zhengyang Shen, Yibo Yang, Zhouchen Lin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=D1Iqfm7WTkk>7.5 - [8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel spatial adaptive equivariant PDOs-based network which achieves superior performance than previous works. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/D1Iqfm7WTkk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Endowing deep learning models with symmetry priors can lead to a considerable performance improvement. As an interesting bridge between physics and deep learning, the equivariant partial differential operators (PDOs) have drawn much researchers&#39; attention recently. However, to ensure the PDOs translation equivariance, previous works have to require coefficient matrices to be constant and spatially shared for their linearity, which could lead to the sub-optimal feature learning at each position. In this work, we propose a novel nonlinear PDOs scheme that is both spatially adaptive and translation equivariant. The coefficient matrices are obtained by local features through a generator rather than spatially shared. Besides, we establish a new theory on incorporating more equivariance like rotations for such PDOs. Based on our theoretical results, we efficiently implement the generator with an equivariant multilayer perceptron (EMLP). As such equivariant PDOs are generated by neural networks, we call them Neural ePDOs. In experiments, we show that our method can significantly improve previous works with smaller model size in various datasets. Especially, we achieve the state-of-the-art performance on the MNIST-rot dataset with only half parameters of the previous best model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                214. <h><a href=https://openreview.net/pdf?id=2QGJXyMNoPz>MocoSFL: enabling cross-client collaborative self-supervised learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jingtao Li, Lingjuan Lyu, Daisuke Iso, Chaitali Chakrabarti, Michael Spranger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2QGJXyMNoPz>7.5 - [6, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Existing collaborative SSL schemes are not suitable for cross-client applications because of their expensive computation and local data requirements. To address these issues, we propose MocoSFL based on Split Federated Learning and MoCo.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2QGJXyMNoPz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing collaborative self-supervised learning (SSL) schemes are not suitable for cross-client applications because of their expensive computation and large local data requirements. To address these issues, we propose MocoSFL, a collaborative SSL framework based on Split Federated Learning (SFL) and Momentum Contrast (MoCo). In MocoSFL, the large backbone model is split into a small client-side model and a large server-side model, and only the small client-side model is processed locally on the client&#39;s local devices. MocoSFL has three key components: (i) vector concatenation which enables the use of small batch size and reduces computation and memory requirements by orders of magnitude; (ii) feature sharing that helps achieve high accuracy regardless of the quality and volume of local data; (iii) frequent synchronization that helps achieve better non-IID performance because of smaller local model divergence. For a 1,000-client case with non-IID data (each client only has data from 2 random classes of CIFAR-10), MocoSFL can achieve over 84% accuracy with ResNet-18 model. Next we present TAResSFL module that significantly improves the resistance to privacy threats and communication overhead with small sacrifice in accuracy for a MocoSFL system. On a Raspberry Pi 4B device, the MocoSFL-based scheme requires less than 1MB of memory and less than 40MB of communication, and consumes less than 5W power. The code is available at https://github.com/SonyAI/MocoSFL.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                215. <h><a href=https://openreview.net/pdf?id=zKvm1ETDOq>Is Adversarial Training Really a Silver Bullet for Mitigating Data Poisoning?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rui Wen, Zhengyu Zhao, Zhuoran Liu, Michael Backes, Tianhao Wang, Yang Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zKvm1ETDOq>7.5 - [6, 10, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an indiscriminative feature-based poisoning approach to substantially degrade adversarial training, which was previously considered to be impossible.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zKvm1ETDOq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Indiscriminate data poisoning can decrease the clean test accuracy of a deep learning model by slightly perturbing its training samples.
There is a consensus that such poisons can hardly harm adversarially-trained (AT) models when the adversarial training budget is no less than the poison budget, i.e., $\epsilon_\mathrm{adv}\geq\epsilon_\mathrm{poi}$. This consensus, however, is challenged in this paper based on our new attack strategy that induces \textit{entangled features} (EntF). The existence of entangled features makes the poisoned data become less useful for training a model, no matter if AT is applied or not. We demonstrate that for attacking a CIFAR-10 AT model under a reasonable setting with $\epsilon_\mathrm{adv}=\epsilon_\mathrm{poi}=8/255$, our EntF yields an accuracy drop of $13.31\%$, which is $7\times$ better than existing methods and equal to discarding $83\%$ training data. We further show the generalizability of EntF to more challenging settings, e.g., higher AT budgets, partial poisoning, unseen model architectures, and stronger (ensemble or adaptive) defenses. We finally provide new insights into the distinct roles of non-robust vs. robust features in poisoning standard vs. AT models and demonstrate the possibility of using a hybrid attack to poison standard and AT models simultaneously. Our code is available at~\url{https://github.com/WenRuiUSTC/EntF}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                216. <h><a href=https://openreview.net/pdf?id=PXVGer7hmJ>Offline Congestion Games: How Feedback Type Affects Data Coverage Requirement</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haozhe Jiang, Qiwen Cui, Zhihan Xiong, Maryam Fazel, Simon Shaolei Du</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PXVGer7hmJ>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PXVGer7hmJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper investigates when one can efficiently recover an approximate Nash Equilibrium (NE) in offline congestion games. The existing dataset coverage assumption in offline general-sum games inevitably incurs a dependency on the number of actions, which can be exponentially large in congestion games. We consider three different types of feedback with decreasing revealed information. Starting from the facility-level (a.k.a., semi-bandit) feedback, we propose a novel one-unit deviation coverage condition and show a pessimism-type algorithm that can recover an approximate NE. For the agent-level (a.k.a., bandit) feedback setting, interestingly, we show the one-unit deviation coverage condition is not sufficient. On the other hand, we convert the game to multi-agent linear bandits and show that with a generalized data coverage assumption in offline linear bandits, we can efficiently recover the approximate NE. Lastly, we consider a novel type of feedback, the game-level feedback where only the total reward from all agents is revealed. Again, we show the coverage assumption for the agent-level feedback setting is insufficient in the game-level feedback setting, and with a stronger version of the data coverage assumption for linear bandits, we can recover an approximate NE. Together, our results constitute the first study of offline congestion games and imply formal separations between different types of feedback.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                217. <h><a href=https://openreview.net/pdf?id=75O7S_L4oY>Learning Group Importance using the Differentiable Hypergeometric Distribution</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thomas M. Sutter, Laura Manduchi, Alain Ryser, Julia E Vogt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=75O7S_L4oY>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the differentiable hypergeometric distribution and show the advantage of explicitly learning subset sizes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/75O7S_L4oY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Partitioning a set of elements into subsets of a priori unknown sizes is essential in many applications. These subset sizes are rarely explicitly learned - be it the cluster sizes in clustering applications or the number of shared versus independent generative latent factors in weakly-supervised learning. Probability distributions over correct combinations of subset sizes are non-differentiable due to hard constraints, which prohibit gradient-based optimization. In this work, we propose the differentiable hypergeometric distribution. The hypergeometric distribution models the probability of different group sizes based on their relative importance. We introduce reparameterizable gradients to learn the importance between groups and highlight the advantage of explicitly learning the size of subsets in two typical applications: weakly-supervised learning and clustering. In both applications, we outperform previous approaches, which rely on suboptimal heuristics to model the unknown size of groups.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                218. <h><a href=https://openreview.net/pdf?id=xzmqxHdZAwO>Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guoyang Xie, Jinbao Wang, Jiaqi Liu, Yaochu Jin, Feng Zheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xzmqxHdZAwO>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xzmqxHdZAwO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the area of few-shot anomaly detection (FSAD), efficient visual feature plays an essential role in the memory bank $\mathcal{M}$-based methods. However, these methods do not account for the relationship between the visual feature and its rotated visual feature, drastically limiting the anomaly detection performance. To push the limits, we reveal that rotation-invariant feature property has a significant impact on industrial-based FSAD. Specifically, we utilize graph representation in FSAD and provide a novel visual isometric invariant feature (VIIF) as an anomaly measurement feature. As a result, VIIF can robustly improve the anomaly discriminating ability and can further reduce the size of redundant features stored in $\mathcal{M}$ by a large amount. Besides, we provide a novel model GraphCore via VIIFs that can fast implement unsupervised FSAD training and improve the performance of anomaly detection. A comprehensive evaluation is provided for comparing GraphCore and other SOTA anomaly detection models under our proposed few-shot anomaly detection setting, which shows GraphCore can increase average AUC by 5.8%, 4.1%, 3.4%, and 1.6% on MVTec AD and by 25.5%, 22.0%, 16.9%, and 14.1% on MPDD for 1, 2, 4, and 8-shot cases, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                219. <h><a href=https://openreview.net/pdf?id=RVTOp3MwT3n>Diversify and Disambiguate: Out-of-Distribution Robustness via Disagreement</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yoonho Lee, Huaxiu Yao, Chelsea Finn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RVTOp3MwT3n>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Given underspecified data, (1) find a diverse set of solutions and (2) choose the best one.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RVTOp3MwT3n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Real-world machine learning problems often exhibit shifts between the source and target distributions, in which source data does not fully convey the desired behavior on target inputs. Different functions that achieve near-perfect source accuracy can make differing predictions on test inputs, and such ambiguity makes robustness to distribution shifts challenging. We propose DivDis, a simple two-stage framework for identifying and resolving ambiguity in data. DivDis first learns a diverse set of hypotheses that achieve low source loss but make differing predictions on target inputs. We then disambiguate by selecting one of the discovered functions using additional information, for example, a small number of target labels. Our experimental evaluation shows improved performance in subpopulation shift and domain generalization settings, demonstrating that DivDis can scalably adapt to distribution shifts in image and text classification benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                220. <h><a href=https://openreview.net/pdf?id=rqq6Dh8t4d>GNNInterpreter: A Probabilistic Generative Model-Level Explanation for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaoqi Wang, Han Wei Shen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rqq6Dh8t4d>7.5 - [8, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a model-level explanation method for GNNs, which is more general, flexible, and computationally efficient than the current SOTA.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rqq6Dh8t4d.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, Graph Neural Networks (GNNs) have significantly advanced the performance of machine learning tasks on graphs. However, this technological breakthrough makes people wonder: how does a GNN make such decisions, and can we trust its prediction with high confidence? When it comes to some critical fields, such as biomedicine, where making wrong decisions can have severe consequences, it is crucial to interpret the inner working mechanisms of GNNs before applying them. In this paper, we propose a model-agnostic model-level explanation method for different GNNs that follow the message passing scheme, GNNInterpreter, to explain the high-level decision-making process of the GNN model. More specifically, GNNInterpreter learns a probabilistic generative graph distribution that produces the most discriminative graph pattern the GNN tries to detect when making a certain prediction by optimizing a novel objective function specifically designed for the model-level explanation for GNNs. Compared to existing works, GNNInterpreter is more flexible and computationally efficient in generating explanation graphs with different types of node and edge features, without introducing another blackbox or requiring manually specified domain-specific rules. In addition, the experimental studies conducted on four different datasets demonstrate that the explanation graphs generated by GNNInterpreter match the desired graph pattern if the model is ideal; otherwise, potential model pitfalls can be revealed by the explanation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                221. <h><a href=https://openreview.net/pdf?id=I3HCE7Ro78H>Finding Actual Descent Directions for Adversarial Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fabian Latorre, Igor Krawczuk, Leello Tadesse Dadi, Thomas Pethick, Volkan Cevher</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=I3HCE7Ro78H>7.5 - [6, 6, 10, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> There is a subtle bug in the theory behind PGD. We show how to correct it and that it matters in practice
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/I3HCE7Ro78H.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Adversarial Training using a strong first-order adversary (PGD) is the gold standard for training Deep Neural Networks that are robust to adversarial examples. We show that, contrary to the general understanding of the method, the gradient at an optimal adversarial example may increase, rather than decrease, the adversarially robust loss. This holds independently of the learning rate. More precisely, we provide a counterexample to a corollary of Danskin&#39;s Theorem presented in the seminal paper of Madry et al. (2018) which states that a solution of the inner maximization problem can yield a descent direction for the adversarially robust loss. Based on a correct interpretation of Danskin&#39;s Theorem, we propose Danskin&#39;s Descent Direction (DDi) and we verify experimentally that it provides better directions than those obtained by a PGD adversary. Using the CIFAR10 dataset we further provide a real world example showing that our method achieves a steeper increase in robustness levels in the early stages of training, and is more stable than the PGD baseline. As a limitation, PGD training of ReLU+BatchNorm networks still performs better, but current theory is unable to explain this.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                222. <h><a href=https://openreview.net/pdf?id=ApF0dmi1_9K>NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruiqi Ni, Ahmed H Qureshi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ApF0dmi1_9K>7.5 - [8, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A physics-informed neural time fields model for robot motion planning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ApF0dmi1_9K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Motion Planners (NMPs) have emerged as a promising tool for solving robot navigation tasks in complex environments. However, these methods often require expert data for learning, which limits their application to scenarios where data generation is time-consuming. Recent developments have also led to physics-informed deep neural models capable of representing complex dynamical Partial Differential Equations (PDEs). Inspired by these developments, we propose Neural Time Fields (NTFields) for robot motion planning in cluttered scenarios. Our framework represents a wave propagation model generating continuous arrival time to find path solutions informed by a nonlinear first-order PDE called Eikonal Equation. We evaluate our method in various cluttered 3D environments, including the Gibson dataset, and demonstrate its ability to solve motion planning problems for 4-DOF and 6-DOF robot manipulators where the traditional grid-based Eikonal planners often face the curse of dimensionality. Furthermore, the results show that our method exhibits high success rates and significantly lower computational times than the state-of-the-art methods, including NMPs that require training data from classical planners.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                223. <h><a href=https://openreview.net/pdf?id=TdTGGj7fYYJ>Unsupervised Meta-learning via Few-shot Pseudo-supervised Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huiwon Jang, Hankook Lee, Jinwoo Shin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TdTGGj7fYYJ>7.5 - [8, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TdTGGj7fYYJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised meta-learning aims to learn generalizable knowledge across a distribution of tasks constructed from unlabeled data. Here, the main challenge is how to construct diverse tasks for meta-learning without label information; recent works have proposed to create, e.g., pseudo-labeling via pretrained representations or creating synthetic samples via generative models. However, such a task construction strategy is fundamentally limited due to heavy reliance on the immutable pseudo-labels during meta-learning and the quality of the representations or the generated samples. To overcome the limitations, we propose a simple yet effective unsupervised meta-learning framework, coined Pseudo-supervised Contrast (PsCo), for few-shot classification. We are inspired by the recent self-supervised learning literature; PsCo utilizes a momentum network and a queue of previous batches to improve pseudo-labeling and construct diverse tasks in a progressive manner. Our extensive experiments demonstrate that PsCo outperforms existing unsupervised meta-learning methods under various in-domain and cross-domain few-shot classification benchmarks. We also validate that PsCo is easily scalable to a large-scale benchmark, while recent prior-art meta-schemes are not.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                224. <h><a href=https://openreview.net/pdf?id=iUdSB2kK9GY>PandA: Unsupervised Learning of Parts and Appearances in the Feature Maps of GANs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>James Oldfield, Christos Tzelepis, Yannis Panagakis, Mihalis Nicolaou, Ioannis Patras</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iUdSB2kK9GY>7.5 - [6, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iUdSB2kK9GY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent advances in the understanding of Generative Adversarial Networks (GANs) have led to remarkable progress in visual editing and synthesis tasks, capitalizing on the rich semantics that are embedded in the latent spaces of pre-trained GANs. However, existing methods are often tailored to specific GAN architectures and are limited to either discovering global semantic directions that do not facilitate localized control, or require some form of supervision through manually provided regions or segmentation masks. In this light, we present an architecture-agnostic approach that jointly discovers factors representing spatial parts and their appearances in an entirely unsupervised fashion. These factors are obtained by applying a semi-nonnegative tensor factorization on the feature maps, which in turn enables context-aware local image editing with pixel-level control. In addition, we show that the discovered appearance factors correspond to saliency maps that localize concepts of interest, without using any labels. Experiments on a wide range of GAN architectures and datasets show that, in comparison to the state of the art, our method is far more efficient in terms of training time and, most importantly, provides much more accurate localized control. Our code is available at: https://github.com/james-oldfield/PandA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                225. <h><a href=https://openreview.net/pdf?id=p7EagBsMAEO>Understanding Edge-of-Stability Training Dynamics with a Minimalist Example</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xingyu Zhu, Zixuan Wang, Xiang Wang, Mo Zhou, Rong Ge</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p7EagBsMAEO>7.4 - [8, 8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p7EagBsMAEO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, researchers observed that gradient descent for deep neural networks operates in an ``edge-of-stability&#39;&#39; (EoS) regime: the sharpness (maximum eigenvalue of the Hessian) is often larger than stability threshold $2/\eta$ (where $\eta$ is the step size). Despite this, the loss oscillates and converges in the long run, and the sharpness at the end is just slightly below $2/\eta$. While many other well-understood nonconvex objectives such as matrix factorization or two-layer networks can also converge despite large sharpness, there is often a larger gap between sharpness of the endpoint and $2/\eta$. In this paper, we study EoS phenomenon by constructing a simple function that has the same behavior. We give rigorous analysis for its training dynamics in a large local region and explain why the final converging point has sharpness close to $2/\eta$. Globally we observe that the training dynamics for our example has an interesting bifurcating behavior, which was also observed in the training of neural nets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                226. <h><a href=https://openreview.net/pdf?id=xYlJRpzZtsY>ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, Asli Celikyilmaz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xYlJRpzZtsY>7.4 - [8, 5, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new taxonomy for reasoning errors and suite of metrics to score step-by-step reasoning in language models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xYlJRpzZtsY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality — among other traits — by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                227. <h><a href=https://openreview.net/pdf?id=iOc57X9KM54>Neuro-Symbolic Procedural Planning with Commonsense Prompting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yujie Lu, Weixi Feng, Wanrong Zhu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, William Yang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iOc57X9KM54>7.4 - [8, 5, 8, 6, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a neuro-symbolic procedural planner that elicits procedural planning knowledge from the large language models with commonsense-infused prompting. We achieve state-of-the-art performance on WikiHow and RobotHow.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iOc57X9KM54.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Procedural planning aims to implement complex high-level goals by decomposition into simpler low-level steps. Although procedural planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack a deep understanding of the cause-effect relations in procedures. Previous methods require manual exemplars to acquire procedural planning knowledge from LLMs in the zero-shot setting. However, such elicited pre-trained knowledge in LLMs induces spurious correlations between goals and steps, which impair the model generalization to unseen tasks. In contrast, this paper proposes a neuro-symbolic procedural PLANner (PLAN) that elicits procedural planning knowledge from the LLMs with commonsense-infused prompting. To mitigate spurious goal-step correlations, we use symbolic program executors on the latent procedural representations to formalize prompts from commonsense knowledge bases as a causal intervention toward the Structural Causal Model. Both automatic and human evaluations on WikiHow and RobotHow show the superiority of PLAN on procedural planning without further training or manual exemplars.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                228. <h><a href=https://openreview.net/pdf?id=-k7Lvk0GpBl>Localized Randomized Smoothing for Collective Robustness Certification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jan Schuchardt, Tom Wollschläger, Aleksandar Bojchevski, Stephan Günnemann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-k7Lvk0GpBl>7.33 - [6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel collective robustness certificate based on randomized smoothing that uses different anisotropic smoothign distribution for the different outputs of a multi-output model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-k7Lvk0GpBl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Models for image segmentation, node classification and many other tasks map a single input to multiple labels. By perturbing this single shared input (e.g. the image) an adversary can manipulate several predictions (e.g. misclassify several pixels). Collective robustness certification is the task of provably bounding the number of robust predictions under this threat model. The only dedicated method that goes beyond certifying each output independently is limited to strictly local models, where each prediction is associated with a small receptive field. We propose a more general collective robustness certificate for all types of models. We further show that this approach is beneficial for the larger class of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their proximity in the image). The certificate is based on our novel localized randomized smoothing approach, where the random perturbation strength for different input regions is proportional to their importance for the outputs. Localized smoothing Pareto-dominates existing certificates on both image segmentation and node classification tasks, simultaneously offering higher accuracy and stronger certificates.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                229. <h><a href=https://openreview.net/pdf?id=dJruFeSRym1>Efficient Conditionally Invariant Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Roman Pogodin, Namrata Deka, Yazhe Li, Danica J. Sutherland, Victor Veitch, Arthur Gretton</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dJruFeSRym1>7.33 - [8, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Batch-efficient conditional independence regularization
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dJruFeSRym1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce the Conditional Independence Regression CovariancE (CIRCE), a measure of conditional independence for multivariate continuous-valued variables. CIRCE applies as a regularizer in settings where we wish to learn neural features $\varphi(X)$ of data $X$ to estimate a target $Y$, while being conditionally independent of a distractor $Z$ given $Y$. Both $Z$ and $Y$ are assumed to be continuous-valued but relatively low dimensional, whereas $X$ and its features may be complex and high dimensional. Relevant settings include domain-invariant learning, fairness, and causal learning. The procedure requires just a single ridge regression from $Y$ to kernelized features of $Z$, which can be done in advance. It is then only necessary to enforce independence of $\varphi(X)$ from residuals of this regression, which is possible with attractive estimation properties and consistency guarantees. By contrast, earlier measures of conditional feature dependence require multiple regressions for each step of feature learning, resulting in more severe bias and variance, and greater computational cost. When sufficiently rich features are used, we establish that CIRCE is zero if and only if $\varphi(X) \perp \!\!\! \perp Z \mid Y$. In experiments, we show superior performance to previous methods on challenging benchmarks, including learning conditionally invariant image features. Code for image data experiments is available at github.com/namratadeka/circe.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                230. <h><a href=https://openreview.net/pdf?id=ZIkHSXzd9O7>Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nicolai Dorka, Tim Welschehold, Wolfram Burgard</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZIkHSXzd9O7>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZIkHSXzd9O7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari 100k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search which is not feasible for many applications. Our method eliminates the need to set the UTD hyperparameter by hand and even leads to a higher robustness with regard to other learning-related hyperparameters further reducing the amount of necessary tuning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                231. <h><a href=https://openreview.net/pdf?id=mCmerkTCG2S>Brain-like representational straightening of natural movies in robust feedforward neural networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tahereh Toosi, Elias Issa</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mCmerkTCG2S>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Brain-like temporal straightening of natural movies emerge in robust neural networks trained on static images 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mCmerkTCG2S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Representational straightening refers to a decrease in curvature of visual feature representations of a sequence of frames taken from natural movies. Prior work established straightening in neural representations of the primate primary visual cortex (V1) and perceptual straightening in human behavior as a hallmark of biological vision in contrast to artificial feedforward neural networks which did not demonstrate this phenomenon as they were not explicitly optimized to produce temporally predictable movie representations. Here, we show robustness to noise in the input image can produce representational straightening in feedforward neural networks. Both adversarial training (AT) and base classifiers for Random Smoothing (RS) induced remarkably straightened feature codes. Demonstrating their utility within the domain of natural movies, these codes could be inverted to generate intervening movie frames by linear interpolation in the feature space even though they were not trained on these trajectories. Demonstrating their biological utility, we found that AT and RS training improved predictions of neural data in primate V1 over baseline models providing a parsimonious, bio-plausible mechanism -- noise in the sensory input stages -- for generating representations in early visual cortex. Finally, we compared the geometric properties of frame representations in these networks to better understand how they produced representations that mimicked the straightening phenomenon from biology. Overall, this work elucidating emergent properties of robust neural networks demonstrates that it is not necessary to utilize predictive objectives or train directly on natural movie statistics to achieve models supporting straightened movie representations similar to human perception that also predict V1 neural responses.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                232. <h><a href=https://openreview.net/pdf?id=r90KYcuB7JS>Approximation and non-parametric estimation of functions over high-dimensional spheres via deep ReLU networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Namjoon Suh, Tian-Yi Zhou, Xiaoming Huo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=r90KYcuB7JS>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the approximation and statistical estimation of deep ReLU feed-forward neural networks, when functions of interests are from Sobolev spaces over high-dimensional sphere.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/r90KYcuB7JS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We develop a new approximation and estimation analysis of deep feed-forward neural networks (FNNs) with the Rectified Linear Unit (ReLU) activation. The functions of interests for the approximation and estimation are assumed to be from Sobolev spaces defined over the $d$-dimensional unit sphere with smoothness index $r&gt;0$. In the regime where $r$ is in the constant order (i.e., $r=\mathcal{O}(1)$), it is shown that at most $d^d$ active parameters are required for getting $d^{-C}$ approximation rate for some constant $C&gt;0$. In contrast, in the regime where the index $r$ grows in the order of $d$ (i.e., $r=\mathcal{O}(d)$) asymptotically, we prove the approximation error decays in the rate $d^{-d^{\beta}}$ with $0&lt;\beta&lt;1$ up to some constant factor independent of $d$. The required number of active parameters in the networks for the approximation increases polynomially in $d$ as $d\rightarrow{\infty}$. In addition to this, it is shown that bound on the excess risk has a $d^d$ factor, when $r=\mathcal{O}(1)$, whereas it has $d^{\mathcal{O}(1)}$ factor, when $r=\mathcal{O}(d)$. We emphasize our findings by making comparisons to the results on approximation and estimation errors of deep ReLU FNN when functions are from Sobolev spaces defined over $d$-dimensional cube. Here, we show that with the current state-of-the-art result, $d^{d}$ factor remain both in the approximation and estimation error, regardless of the order of $r$. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                233. <h><a href=https://openreview.net/pdf?id=14-kr46GvP->Efficient Deep Reinforcement Learning Requires Regulating Overfitting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qiyang Li, Aviral Kumar, Ilya Kostrikov, Sergey Levine</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=14-kr46GvP->7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/14-kr46GvP-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep reinforcement learning algorithms that learn policies by trial-and-error must learn from limited amounts of data collected by actively interacting with the environment. While many prior works have shown that proper regularization techniques are crucial for enabling data-efficient RL, a general understanding of the bottlenecks in data-efficient RL has remained unclear. Consequently, it has been difficult to devise a universal technique that works well across all domains. In this paper, we attempt to understand the primary bottleneck in sample-efficient deep RL by examining several potential hypotheses such as non-stationarity, excessive action distribution shift, and overfitting. We perform thorough empirical analysis on state-based DeepMind control suite (DMC) tasks in a controlled and systematic way to show that high temporal-difference (TD) error on the validation set of transitions is the main culprit that severely affects the performance of deep RL algorithms, and prior methods that lead to good performance do in fact, control the validation TD error to be low. This observation gives us a robust principle for making deep RL efficient: we can hill-climb on the validation TD error by utilizing any form of regularization techniques from supervised learning. We show that a simple online model selection method that targets the validation TD error is effective across state-based DMC and Gym tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                234. <h><a href=https://openreview.net/pdf?id=VBTJqqWjxMv>A Message Passing Perspective on Learning Dynamics of Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yifei Wang, Qi Zhang, Tianqi Du, Jiansheng Yang, Zhouchen Lin, Yisen Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VBTJqqWjxMv>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VBTJqqWjxMv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In recent years, contrastive learning achieves impressive results on self-supervised visual representation learning, but there still lacks a rigorous understanding of its learning dynamics. In this paper, we show that if we cast a contrastive objective equivalently into the feature space, then its learning dynamics admits an interpretable form. Specifically, we show that its gradient descent corresponds to a specific message passing scheme on the corresponding augmentation graph. Based on this perspective, we theoretically characterize how contrastive learning gradually learns discriminative features with the alignment update and the uniformity update. Meanwhile, this perspective also establishes an intriguing connection between contrastive learning and Message Passing Graph Neural Networks (MP-GNNs). This connection not only provides a unified understanding of many techniques independently developed in each community, but also enables us to borrow techniques from MP-GNNs to design new contrastive learning variants, such as graph attention, graph rewiring, jumpy knowledge techniques, etc. We believe that our message passing perspective not only provides a new theoretical understanding of contrastive learning dynamics, but also bridges the two seemingly independent areas together, which could inspire more interleaving studies to benefit from each other. The code is available at https://github.com/PKU-ML/Message-Passing-Contrastive-Learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                235. <h><a href=https://openreview.net/pdf?id=v69itrHLEu>Outcome-directed Reinforcement Learning by Uncertainty \&amp; Temporal Distance-Aware Curriculum Goal Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daesol Cho, Seungjae Lee, H. Jin Kim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=v69itrHLEu>7.33 - [6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/v69itrHLEu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Current reinforcement learning (RL) often suffers when solving a challenging exploration problem where the desired outcomes or high rewards are rarely observed. Even though curriculum RL, a framework that solves complex tasks by proposing a sequence of surrogate tasks, shows reasonable results, most of the previous works still have difficulty in proposing curriculum due to the absence of a mechanism for obtaining calibrated guidance to the desired outcome state without any prior domain knowledge. To alleviate it, we propose an uncertainty \&amp; temporal distance-aware curriculum goal generation method for the outcome-directed RL via solving a bipartite matching problem. It could not only provide precisely calibrated guidance of the curriculum to the desired outcome states but also bring much better sample efficiency and geometry-agnostic curriculum goal proposal capability compared to previous curriculum RL methods. We demonstrate that our algorithm significantly outperforms these prior methods in a variety of challenging navigation tasks and robotic manipulation tasks in a quantitative and qualitative way.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                236. <h><a href=https://openreview.net/pdf?id=yHIIM9BgOo>Graph-based Deterministic Policy Gradient for Repetitive Combinatorial Optimization Problems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhongyuan Zhao, Ananthram Swami, Santiago Segarra</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yHIIM9BgOo>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A general learning framework is proposed to learn reusable node or edge representations that can reduce the optimality gap of fast heuristics for repetitive combinatorial optimization problems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yHIIM9BgOo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose an actor-critic framework for graph-based machine learning pipelines with non-differentiable blocks, and apply it to repetitive combinatorial optimization problems (COPs) under hard constraints. Repetitive COP refers to problems to be solved repeatedly on graphs of the same or slowly changing topology but rapidly changing node or edge weights. Compared to one-shot COPs, repetitive COPs often rely on fast heuristics to solve one instance of the problem before the next one arrives, at the cost of a relatively large optimality gap. Through numerical experiments on several discrete optimization problems, we show that our approach can learn reusable node or edge representations to reduce the optimality gap of fast heuristics for independent repetitive COPs, and can optimize the long-term objectives for repetitive COPs embedded in graph-based Markov decision processes. Source code at https://github.com/XzrTGMu/twin-nphard 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                237. <h><a href=https://openreview.net/pdf?id=lZOUQQvwI3q>Measuring axiomatic soundness of counterfactual image models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Miguel Monteiro, Fabio De Sousa Ribeiro, Nick Pawlowski, Daniel C. Castro, Ben Glocker</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lZOUQQvwI3q>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use the axiomatic definition of counterfactual to derive metrics that enable quantifying the correctness of approximate counterfactual inference models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lZOUQQvwI3q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a general framework for evaluating image counterfactuals. The power and flexibility of deep generative models make them valuable tools for learning mechanisms in structural causal models. However, their flexibility makes counterfactual identifiability impossible in the general case.
Motivated by these issues, we revisit Pearl&#39;s axiomatic definition of counterfactuals to determine the necessary constraints of any counterfactual inference model: composition, reversibility, and effectiveness. We frame counterfactuals as functions of an input variable, its parents, and counterfactual parents and use the axiomatic constraints to restrict the set of functions that could represent the counterfactual, thus deriving distance metrics between the approximate and ideal functions. We demonstrate how these metrics can be used to compare and choose between different approximate counterfactual inference models and to provide insight into a model&#39;s shortcomings and trade-offs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                238. <h><a href=https://openreview.net/pdf?id=xnsg4pfKb7>Bispectral Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sophia Sanborn, Christian A Shewmake, Bruno Olshausen, Christopher J. Hillar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xnsg4pfKb7>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xnsg4pfKb7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a neural network architecture, Bispectral Neural Networks (BNNs) for learning representations that are invariant to the actions of compact commutative groups on the space over which a signal is defined. The model incorporates the ansatz of the bispectrum, an analytically defined group invariant that is complete -- that is, it preserves all signal structure while removing only the variation due to group actions. Here, we demonstrate that BNNs are able to simultaneously learn groups, their irreducible representations, and corresponding equivariant and complete-invariant maps purely from the symmetries implicit in data.  Further, we demonstrate that the completeness property endows these networks with strong invariance-based adversarial robustness. This work establishes Bispectral Neural Networks as a powerful computational primitive for robust invariant representation learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                239. <h><a href=https://openreview.net/pdf?id=svCcui6Drl>Why (and When) does Local SGD Generalize Better than SGD?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xinran Gu, Kaifeng Lyu, Longbo Huang, Sanjeev Arora</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=svCcui6Drl>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We derive a Stochastic Differential Equation (SDE) that captures the long-term behavior of Local SGD and provide a theoretical explanation why Local SGD generalizes better than SGD.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/svCcui6Drl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Local SGD is a communication-efficient variant of SGD for large-scale training, where multiple GPUs perform SGD independently and average the model parameters periodically. It has been recently observed that Local SGD can not only achieve the design goal of reducing the communication overhead but also lead to higher test accuracy than the corresponding SGD baseline (Lin et al., 2020b), though the training regimes for this to happen are still in debate (Ortiz et al., 2021). This paper aims to understand why (and when) Local SGD generalizes better based on Stochastic Differential Equation (SDE) approximation. The main contributions of this paper include (i) the derivation of an SDE that captures the long-term behavior of Local SGD in the small learning rate regime, showing how noise drives the iterate to drift and diffuse after it has reached close to the manifold of local minima, (ii) a comparison between the SDEs of Local SGD and SGD, showing that Local SGD induces a stronger drift term that can result in a stronger effect of regularization, e.g., a faster reduction of sharpness, and (iii) empirical evidence validating that having a small learning rate and long enough training time enables the generalization improvement over SGD but removing either of the two conditions leads to no improvement.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                240. <h><a href=https://openreview.net/pdf?id=uKiE0VIluA->GFlowNets and variational inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward J Hu, Katie E Everett, Dinghuai Zhang, Yoshua Bengio</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uKiE0VIluA->7.33 - [6, 6, 10] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We theoretically and empirically compare and contrast GFlowNets with hierarchical variational inference.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uKiE0VIluA-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper builds bridges between two families of probabilistic algorithms: (hierarchical) variational inference (VI), which is typically used to model distributions over continuous spaces, and generative flow networks (GFlowNets), which have been used for distributions over discrete structures such as graphs. We demonstrate that, in certain cases, VI algorithms are equivalent to special cases of GFlowNets in the sense of equality of expected gradients of their learning objectives. We then point out the differences between the two families and show how these differences emerge experimentally. Notably, GFlowNets, which borrow ideas from reinforcement learning, are more amenable than VI to off-policy training without the cost of high gradient variance induced by importance sampling. We argue that this property of GFlowNets can provide advantages for capturing diversity in multimodal target distributions. Code: https://github.com/GFNOrg/GFN_vs_HVI.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                241. <h><a href=https://openreview.net/pdf?id=3ZPESALKXO>Approximate Vanishing Ideal Computations at Scale</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Elias Samuel Wirth, Hiroshi Kera, Sebastian Pokutta</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3ZPESALKXO>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study approximate vanishing ideal algorithms at scale.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3ZPESALKXO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The vanishing ideal of a set of points $X = \{\mathbf{x}_1, \ldots, \mathbf{x}_m\}\subseteq \mathbb{R}^n$ is the set of polynomials that evaluate to $0$ over all points $\mathbf{x} \in X$ and admits an efficient representation by a finite subset of generators. In practice, to accommodate noise in the data, algorithms that construct generators of the approximate vanishing ideal are widely studied but their computational complexities remain expensive. In this paper, we scale up the oracle approximate vanishing ideal algorithm (OAVI), the only generator-constructing algorithm with known learning guarantees. We prove that the computational complexity of OAVI is not superlinear, as previously claimed, but linear in the number of samples $m$. In addition, we propose two modifications that accelerate OAVI&#39;s training time: Our analysis reveals that replacing the pairwise conditional gradients algorithm, one of the solvers used in OAVI, with the faster blended pairwise conditional gradients algorithm leads to an exponential speed-up in the number of features $n$. Finally, using a new inverse Hessian boosting approach, intermediate convex optimization problems can be solved almost instantly, improving OAVI&#39;s training time by multiple orders of magnitude in a variety of numerical experiments.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                242. <h><a href=https://openreview.net/pdf?id=SmufNDN90G>Policy-Based Self-Competition for Planning Problems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jonathan Pirnay, Quirin Göttl, Jakob Burger, Dominik Gerhard Grimm</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SmufNDN90G>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Solving deterministic single-agent problems through self-competition by including a historical policy in the planning process of Gumbel AlphaZero.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SmufNDN90G.jpg loading="lazy">
                </div>
                <div class="abstract">
                    AlphaZero-type algorithms may stop improving on single-player tasks in case the value network guiding the tree search is unable to approximate the outcome of an episode sufficiently well. One technique to address this problem is transforming the single-player task through self-competition. The main idea is to compute a scalar baseline from the agent’s historical performances and to reshape an episode’s reward into a binary output, indicating whether the baseline has been exceeded or not. However, this baseline only carries limited information for the agent about strategies how to improve. We leverage the idea of self-competition and directly incorporate a historical policy into the planning process instead of its scalar performance. Based on the recently introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ ‘Play-to-Plan’ (GAZ PTP), in which the agent learns to find strong trajectories by planning against possible strategies of its past self. We show the effectiveness of our approach in two well-known combinatorial optimization problems, the Traveling Salesman Problem and the Job-Shop Scheduling Problem. With only half of the simulation budget for search, GAZ PTP consistently outperforms all selected single-player variants of GAZ.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                243. <h><a href=https://openreview.net/pdf?id=mXPoBtnpMnuy>Unsupervised 3D Object Learning through Neuron Activity aware Plasticity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Beomseok Kang, Biswadeep Chakraborty, Saibal Mukhopadhyay</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mXPoBtnpMnuy>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mXPoBtnpMnuy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present an unsupervised deep learning model for 3D object classification. Conventional Hebbian learning, a well-known unsupervised model, suffers from loss of local features leading to reduced performance for tasks with complex geometric objects. We present a deep network with a novel Neuron Activity Aware (NeAW) Hebbian learning rule that dynamically switches the neurons to be governed by Hebbian learning or anti-Hebbian learning, depending on its activity. We analytically show that NeAW Hebbian learning relieves the bias in neuron activity, allowing more neurons to attend to the representation of the 3D objects. Empirical results show that the NeAW Hebbian learning outperforms other variants of Hebbian learning and shows higher accuracy over fully supervised models when training data is limited.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                244. <h><a href=https://openreview.net/pdf?id=UazgYBMS9-W>Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingxu Tao, Yansong Feng, Dongyan Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UazgYBMS9-W>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UazgYBMS9-W.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large pre-trained language models have helped to achieve state of the art on a variety of NLP tasks, nevertheless, they still suffer from forgetting when incrementally learning a series of sequential tasks. To alleviate this problem, recent works propose several models enhanced by sparse experience replay and local adaption, which yield satisfactory performance. However, in this paper we find that pre-trained language models like BERT have a potential ability to learn sequentially, even without any sparse memory replay. To verify the ability of BERT to maintain old knowledge, we adopt and re-finetune single-layer probe networks with the parameters of BERT fixed. We investigate the models on two typical kinds of NLP tasks, text classification and extractive question answering. And our experiments reveal that BERT can actually generate high quality representations for previous tasks in a long term, under extremely sparse replay or even no replay. We further introduce a series of methods to interpret the mechanism of forgetting and how memory rehearsal plays a significant role in task incremental learning, which bridges the gap between our new discovery and previous studies about catastrophic forgetting. Additionally, we provide both quantified and visualized results demonstrating that the representation space of BERT is always topologically organised, which guarantees its performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                245. <h><a href=https://openreview.net/pdf?id=77lSWa-Tm3Z>Variational Information Pursuit for Interpretable Predictions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aditya Chattopadhyay, Kwan Ho Ryan Chan, Benjamin David Haeffele, Donald Geman, Rene Vidal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=77lSWa-Tm3Z>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A Framework for Interpretable ML
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/77lSWa-Tm3Z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There is a growing interest in the machine learning community in developing predictive algorithms that are interpretable by design. To this end, recent work proposes to sequentially ask interpretable queries about data until a high confidence prediction can be made based on the answers obtained (the history). To promote short query-answer chains, a greedy procedure called Information Pursuit (IP) is used, which adaptively chooses queries in order of information gain. Generative models are employed to learn the distribution of query-answers and labels, which is in turn used to estimate the most informative query. However, learning and inference with a full generative model of the data is often intractable for complex tasks. In this work, we propose Variational Information Pursuit (V-IP), a variational characterization of IP which bypasses the need to learn generative models. V-IP is based on finding a query selection strategy and a classifier that minimize the expected cross-entropy between true and predicted labels. We prove that the IP strategy is the optimal solution to this problem. Therefore, instead of learning generative models, we can use our optimal strategy to directly pick the most informative query given any history. We then develop a practical algorithm by defining a finite-dimensional parameterization of our strategy and classifier using deep networks and train them end-to-end using our objective. Empirically, V-IP is 10-100x faster than IP on different Vision and NLP tasks with competitive performance. Moreover, V-IP finds much shorter query chains when compared to reinforcement learning which is typically used in sequential-decision-making problems. Finally, we demonstrate the utility of V-IP on challenging tasks like medical diagnosis where the performance is far superior to the generative modeling approach.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                246. <h><a href=https://openreview.net/pdf?id=o0LFPcoFKnr>SCALE-UP: An Efficient Black-box Input-level Backdoor Detection via Analyzing Scaled Prediction Consistency</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, Cong Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=o0LFPcoFKnr>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We reveal an intriguing phenomenon that the predictions of poisoned samples are significantly more consistent when amplifying all pixel values, based on which we design a simple yet effective black-box input-level backdoor detection.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/o0LFPcoFKnr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks (DNNs) are vulnerable to backdoor attacks, where adversaries embed a hidden backdoor trigger during the training process for malicious prediction manipulation. These attacks pose great threats to the applications of DNNs under the real-world machine learning as a service (MLaaS) setting, where the deployed model is fully black-box while the users can only query and obtain its predictions. Currently, there are many existing defenses to reduce backdoor threats. However, almost all of them cannot be adopted in MLaaS scenarios since they require getting access to or even modifying the suspicious models. In this paper, we propose a simple yet effective black-box input-level backdoor detection, called SCALE-UP, which requires only the predicted labels to alleviate this problem. Specifically, we identify and filter malicious testing samples by analyzing their prediction consistency during the pixel-wise amplification process. Our defense is motivated by an intriguing observation (dubbed \emph{scaled prediction consistency}) that the predictions of poisoned samples are significantly more consistent compared to those of benign ones when amplifying all pixel values. Besides, we also provide theoretical foundations to explain this phenomenon. Extensive experiments are conducted on benchmark datasets, verifying the effectiveness and efficiency of our defense and its resistance to potential adaptive attacks. Our codes are available at \url{https://github.com/JunfengGo/SCALE-UP}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                247. <h><a href=https://openreview.net/pdf?id=pBBsrPzq7aF>Combinatorial Pure Exploration of Causal Bandits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nuoya Xiong, Wei Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pBBsrPzq7aF>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Combinatorial pure exploration algorithm of causal bandits on two different models
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pBBsrPzq7aF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The combinatorial pure exploration of causal bandits is the following online learning task: given a causal graph with unknown causal inference distributions, in each round we choose a subset of variables to intervene or do no intervention, and observe the random outcomes of all random variables, with the goal that using as few rounds as possible, we can output an intervention that gives the best (or almost best) expected outcome on the reward variable $Y$ with probability at least $1-\delta$, where $\delta$ is a given confidence level. We provide the first gap-dependent and fully adaptive pure exploration algorithms on two types of causal models --- the binary generalized linear model (BGLM) and general graphs. For BGLM, our algorithm is the first to be designed specifically for this setting and achieves polynomial sample complexity, while all existing algorithms for general graphs have either sample complexity exponential to the graph size or some unreasonable assumptions. For general graphs, our algorithm provides a significant improvement on sample complexity, and it nearly matches the lower bound we prove. Our algorithms achieve such improvement by a novel integration of prior causal bandit algorithms and prior adaptive pure exploration algorithms, the former of which utilize the rich observational feedback in causal bandits but are not adaptive to reward gaps, while the latter of which have the issue in reverse.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                248. <h><a href=https://openreview.net/pdf?id=uVcDssQff_>REVISITING PRUNING AT INITIALIZATION THROUGH THE LENS OF RAMANUJAN GRAPH</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Duc N.M Hoang, Shiwei Liu, Radu Marculescu, Zhangyang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uVcDssQff_>7.33 - [6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uVcDssQff_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pruning neural networks at initialization (PaI) has received an upsurge of interest due to its end-to-end saving potential. PaI is able to find sparse subnetworks at initialization that can achieve comparable performance to the full networks. These methods can surpass the trivial baseline of random pruning but suffer from a significant performance gap compared to post-training pruning. Previous approaches firmly rely on weights, gradients, and sanity checks as primary signals when conducting PaI analysis. To better understand the underlying mechanism of PaI, we propose to interpret it through the lens of the Ramanujan Graph - a class of expander graphs that are sparse while being highly connected. It is often believed there should be a strong correlation between the Ramanujan graph and PaI since both are about finding sparse and well-connected neural networks. However, the finer-grained link relating highly sparse and connected networks to their relative performance (i.e., ranking of difference sparse structures at the same specific global sparsity) is still missing. We observe that not only the Ramanujan property for sparse networks shows no significant relationship to PaI’s relative performance, but maximizing it can also lead to the formation of pseudo-random graphs with no structural meanings. We reveal the underlying cause to be Ramanujan Graph’s strong assumption on the upper bound of the largest nontrivial eigenvalue (µˆ) of layers belonging to highly sparse networks. We hence propose Iterative Mean Difference of Bound (IMDB) as a mean to relax the µˆ upper bound. Likewise, we also show there exists a lower bound for µˆ, which we call the Normalized Random Coefficient (NaRC), that gives us an accurate assessment for when sparse but highly connected structure degenerates into naive randomness. Finally, we systematically analyze the behavior of various PaI methods and demonstrate the utility of our proposed metrics in characterizing PaI performance. We show that subnetworks preserving better the IMDB property correlate higher in performance, while NaRC provides us with a possible mean to locate the region where highly connected, highly sparse, and non-trivial Ramanujan expanders exist. Our code is available at: https://github.com/VITA-Group/ramanujan-on-pai.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                249. <h><a href=https://openreview.net/pdf?id=6OphWWAE3cS>Particle-based Variational Inference with Preconditioned Functional Gradient Flow</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hanze Dong, Xi Wang, LIN Yong, Tong Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6OphWWAE3cS>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6OphWWAE3cS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Particle-based variational inference (VI) minimizes the KL divergence between model samples and the target posterior with gradient flow estimates. With the popularity of Stein variational gradient descent (SVGD), the focus of particle-based VI algorithms has been on the properties of functions in Reproducing Kernel Hilbert Space (RKHS) to approximate the gradient flow. However, the requirement of RKHS restricts the function class and algorithmic flexibility. This paper offers a general solution to this problem by introducing a functional regularization term that encompasses the RKHS norm as a special case.  This allows us to propose a new particle-based VI algorithm called  preconditioned functional gradient flow (PFG). Compared to SVGD, PFG has several advantages. It has a larger function class, improved scalability in large particle-size scenarios, better adaptation to ill-conditioned distributions, and provable continuous-time convergence in KL divergence. Additionally, non-linear function classes such as neural networks can be incorporated to estimate the gradient flow. Our theory and experiments demonstrate the effectiveness of the proposed framework.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                250. <h><a href=https://openreview.net/pdf?id=TKIFuQHHECj>Can CNNs Be More Robust Than Transformers?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyu Wang, Yutong Bai, Yuyin Zhou, Cihang Xie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TKIFuQHHECj>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> we show CNNs can be as robust as, or even more robust than, Transformers
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TKIFuQHHECj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The recent success of Vision Transformers is shaking the long dominance of Convolutional Neural Networks (CNNs) in image recognition for a decade. Specifically, in terms of robustness on out-of-distribution samples, recent research finds that Transformers are inherently more robust than CNNs, regardless of different training setups. Moreover, it is believed that such superiority of Transformers should largely be credited to their \emph{self-attention-like architectures per se}. In this paper, we question that belief by closely examining the design of Transformers. Our findings lead to three highly effective architecture designs for boosting robustness, yet simple enough to be implemented in several lines of code, namely a) patchifying input images, b) enlarging kernel size, and c) reducing activation layers and normalization layers. Bringing these components together, we are able to build pure CNN architectures without any attention-like operations that are as robust as, or even more robust than, Transformers. We hope this work can help the community better understand the design of robust neural architectures. The code is publicly available at https://github.com/UCSC-VLAA/RobustCNN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                251. <h><a href=https://openreview.net/pdf?id=w0QXrZ3N-s>The Modality Focusing Hypothesis: Towards Understanding Crossmodal Knowledge Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zihui Xue, Zhengqi Gao, Sucheng Ren, Hang Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=w0QXrZ3N-s>7.33 - [6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a thorough investigation of crossmodal knowledge transfer
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/w0QXrZ3N-s.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Crossmodal knowledge distillation (KD) extends traditional knowledge distillation to the area of multimodal learning and demonstrates great success in various applications. To achieve knowledge transfer across modalities, a pretrained network from one modality is adopted as the teacher to provide supervision signals to a student network learning from the other modality. In contrast to the empirical success reported in prior works, the working mechanism of crossmodal KD remains a mystery. In this paper, we present a thorough understanding of crossmodal KD. We begin by providing two failure cases and demonstrate that KD is not a universal cure in crossmodal knowledge transfer. We then present the modality Venn diagram to understand modality relationships and the modality focusing hypothesis revealing the decisive factor in the efficacy of crossmodal KD. Experimental results on 6 multimodal datasets help justify our hypothesis, diagnose failure cases, and point directions to improve crossmodal knowledge transfer in the future.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                252. <h><a href=https://openreview.net/pdf?id=Mvetq8DO05O>A Laplace-inspired Distribution on SO(3) for Probabilistic Rotation Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yingda Yin, Yang Wang, He Wang, Baoquan Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Mvetq8DO05O>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Mvetq8DO05O.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. Probabilistic rotation regression has raised more and more attention with the benefit of expressing uncertainty information along with the prediction. Though modeling noise using Gaussian-resembling Bingham distribution and matrix Fisher distribution is natural, they are shown to be sensitive to outliers for the nature of quadratic punishment to deviations. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel Rotation Laplace distribution on SO(3). Rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region, resulting in a better convergence. Our extensive experiments show that our proposed distribution achieves state-of-the-art performance for rotation regression tasks over both probabilistic and non-probabilistic baselines. Our project page is at pku-epic.github.io/RotationLaplace.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                253. <h><a href=https://openreview.net/pdf?id=qs2YCziX2o->On The Relative Error of Random Fourier Features for Preserving Kernel Distance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kuan Cheng, Shaofeng H.-C. Jiang, Luojian Wei, Zhide Wei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qs2YCziX2o->7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We characterize for what kernels the random Fourier features method, proposed in a seminal paper by Rahimi and Recht, preserves the relative-error for the kernel distance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qs2YCziX2o-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The method of random Fourier features (RFF), proposed in a seminal paper by Rahimi and Recht (NIPS&#39;07), is a powerful technique to find approximate low-dimensional representations of points in (high-dimensional) kernel space, for shift-invariant kernels. While RFF has been analyzed under various notions of error guarantee, the ability to preserve the kernel distance with \emph{relative} error is less understood. We show that for a significant range of kernels, including the well-known Laplacian kernels, RFF cannot approximate the kernel distance with small relative error using low dimensions. We complement this by showing as long as the shift-invariant kernel is analytic, RFF with $\mathrm{poly}(\epsilon^{-1} \log n)$ dimensions achieves $\epsilon$-relative error for pairwise kernel distance of $n$ points, and the dimension bound is improved to $\mathrm{poly}(\epsilon^{-1}\log k)$ for the specific application of kernel $k$-means. Finally, going beyond RFF, we make the first step towards data-oblivious dimension-reduction for general shift-invariant kernels, and we obtain a similar $\mathrm{poly}(\epsilon^{-1} \log n)$ dimension bound for Laplacian kernels. We also validate the dimension-error tradeoff of our methods on simulated datasets, and they demonstrate superior performance compared with other popular methods including random-projection and Nystr\&#34;{o}m methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                254. <h><a href=https://openreview.net/pdf?id=MofT9KEF0kw>Pushing the Accuracy-Group Robustness Frontier with Introspective Self-play</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeremiah Zhe Liu, Krishnamurthy Dj Dvijotham, Jihyeon Lee, Quan Yuan, Balaji Lakshminarayanan, Deepak Ramachandran</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MofT9KEF0kw>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Principled training method to improve deep model&#39;s uncertainty and active learning performance under dataset bias.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MofT9KEF0kw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Standard empirical risk minimization (ERM) training can produce deep neural network (DNN) models that are accurate on average but under-perform in under-represented population subgroups, especially when there are imbalanced group distributions in the long-tailed training data. Therefore, approaches that improve the accuracy - group robustness trade-off frontier of a DNN model (i.e. improving worst-group accuracy without sacrificing average accuracy, or vice versa) is of crucial importance.   Uncertainty-based active learning (AL) can potentially improve the frontier by preferentially sampling underrepresented subgroups to create a more balanced training dataset.  However, the quality of uncertainty estimates from modern DNNs tend to degrade in the presence of spurious correlations and dataset bias, compromising the effectiveness of AL for sampling tail groups. In this work, we propose Introspective Self-play (ISP), a simple approach to improve the uncertainty estimation of a deep neural network under dataset bias, by adding an auxiliary introspection task requiring a model to predict the bias for each data point in addition to the label. We show that ISP provably improves the bias-awareness of the model representation and the resulting uncertainty estimates. On two real-world tabular and language tasks,ISP serves as a simple “plug-in” for AL model training, consistently improving both the tail-group sampling rate and the final accuracy-fairness trade-off frontier of popular AL methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                255. <h><a href=https://openreview.net/pdf?id=rLwC0_MG-4w>Denoising Diffusion Error Correction Codes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yoni Choukroun, Lior Wolf</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rLwC0_MG-4w>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel SOTA Neural error correction decoder based on a new diffusion model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rLwC0_MG-4w.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Error correction code (ECC) is an integral part of the physical communication layer, ensuring reliable data transfer over noisy channels. 
Recently, neural decoders have demonstrated their advantage over classical decoding techniques. 
However, recent state-of-the-art neural decoders suffer from high complexity and lack the important iterative scheme characteristic of many legacy decoders. 
In this work, we propose to employ denoising diffusion models for the soft decoding of linear codes at arbitrary block lengths. 
Our framework models the forward channel corruption as a series of diffusion steps that can be reversed iteratively. 
Three contributions are made: (i) a diffusion process suitable for the decoding setting is introduced, (ii) the neural diffusion decoder is conditioned on the number of parity errors, which indicates the level of corruption at a given step, (iii) a line search procedure based on the code&#39;s syndrome obtains the optimal reverse diffusion step size. 
The proposed approach demonstrates the power of diffusion models for ECC and is able to achieve state-of-the-art accuracy, outperforming the other neural decoders by sizable margins, even for a single reverse diffusion step. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                256. <h><a href=https://openreview.net/pdf?id=SbR9mpTuBn>Bag of Tricks for Unsupervised Text-to-Speech</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yi Ren, Chen Zhang, Shuicheng YAN</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SbR9mpTuBn>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a bag of tricks to enable effective unsupervised TTS using low-quality and multi-speaker unpaired data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SbR9mpTuBn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised text-to-speech (TTS) aims to train TTS models for a specific language without any paired speech-text training data in that language. Existing methods either use speech and corresponding pseudo text generated by an unsupervised automatic speech recognition (ASR) model as training data, or employ the back-translation technique. Though effective, they suffer from low robustness to low-quality data and heavy dependence on the lexicon of a language that is sometimes unavailable, leading to difficulty in convergence, especially in low-resource language scenarios. In this work, we introduce a bag of tricks to enable effective unsupervised TTS. Specifically, 1) we carefully design a voice conversion model to normalize the variable and noisy information in the low-quality speech data while preserving the pronunciation information; 2) we employ the non-autoregressive TTS model to overcome the robustness issue; and 3) we explore several tricks applied in back-translation, including curriculum learning, length augmentation and auxiliary supervised loss to stabilize the back-translation and improve its effectiveness. Through experiments, it has been demonstrated that our method achieves better intelligibility and audio quality than all previous methods, and that these tricks are very essential to the performance gain.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                257. <h><a href=https://openreview.net/pdf?id=hVVUY7p64WL>Learnable Topological Features For Phylogenetic Inference via Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Cheng Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hVVUY7p64WL>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Novel phylogenetic inference methods based on learnable topological features via graph neural networks
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hVVUY7p64WL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Structural information of phylogenetic tree topologies plays an important role in phylogenetic inference. However, finding appropriate topological structures for specific phylogenetic inference tasks often requires significant design effort and domain expertise. In this paper, we propose a novel structural representation method for phylogenetic inference based on learnable topological features. By combining the raw node features that minimize the Dirichlet energy with modern graph representation learning techniques, our learnable topological features can provide efficient structural information of phylogenetic trees that automatically adapts to different downstream tasks without requiring domain expertise. We demonstrate the effectiveness and efficiency of our method on a simulated data tree probability estimation task and a benchmark of challenging real data variational Bayesian phylogenetic inference problems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                258. <h><a href=https://openreview.net/pdf?id=S31oTB72m0G>Provable Sim-to-real Transfer in Continuous Domain with Partial Observations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiachen Hu, Han Zhong, Chi Jin, Liwei Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=S31oTB72m0G>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/S31oTB72m0G.jpg loading="lazy">
                </div>
                <div class="abstract">
                        Sim-to-real transfer, which trains RL agents in the simulated environments and then deploys them in the real world, has been widely used to overcome the limitations of gathering samples in the real world. Despite the empirical success of the sim-to-real transfer, its theoretical foundation is much less understood. In this paper, we study the sim-to-real transfer in continuous domain with partial observations, where the simulated environments and real-world environments are modeled by linear quadratic Gaussian (LQG) systems. We show that a popular robust adversarial training algorithm is capable of learning a policy from the simulated environment that is competitive to the optimal policy in the real-world environment. To achieve our results, we design a new algorithm for infinite-horizon average-cost LQGs and establish a regret bound that depends on the intrinsic complexity of the model class. Our algorithm crucially relies on a novel history clipping scheme, which might be of independent interest.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                259. <h><a href=https://openreview.net/pdf?id=ZTMuZ68B1g>Active Learning in Bayesian Neural Networks with Balanced Entropy Learning Principle</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jae Oh Woo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZTMuZ68B1g>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new bayesian active learning principle.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZTMuZ68B1g.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Acquiring labeled data is challenging in many machine learning applications with limited budgets. Active learning gives a procedure to select the most informative data points and improve data efficiency by reducing the cost of labeling. The info-max learning principle maximizing mutual information such as BALD has been successful and widely adapted in various active learning applications. However, this pool-based specific objective inherently introduces a redundant selection and further requires a high computational cost for batch selection. In this paper, we design and propose a new uncertainty measure, Balanced Entropy Acquisition (BalEntAcq), which captures the information balance between the uncertainty of underlying softmax probability and the label variable. To do this, we approximate each marginal distribution by Beta distribution. Beta approximation enables us to formulate BalEntAcq as a ratio between an augmented entropy and the marginalized joint entropy. The closed-form expression of BalEntAcq facilitates parallelization by estimating two parameters in each marginal Beta distribution. BalEntAcq is a purely standalone measure without requiring any relational computations with other data points. Nevertheless, BalEntAcq captures a well-diversified selection near the decision boundary with a margin, unlike other existing uncertainty measures such as BALD, Entropy, or Mean Standard Deviation (MeanSD). Finally, we demonstrate that our balanced entropy learning principle with BalEntAcq consistently outperforms well-known linearly scalable active learning methods, including a recently proposed PowerBALD, a simple but diversified version of BALD, by showing experimental results obtained from MNIST, CIFAR-100, SVHN, and TinyImageNet datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                260. <h><a href=https://openreview.net/pdf?id=4F1gvduDeL>Few-Shot Domain Adaptation For End-to-End Communication</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jayaram Raghuram, Yijing Zeng, Dolores Garcia, Rafael Ruiz, Somesh Jha, Joerg Widmer, Suman Banerjee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4F1gvduDeL>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a sample-efficient domain adaptation method for the autoencoder based end-to-end communication problem
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4F1gvduDeL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The problem of end-to-end learning of a communication system using an autoencoder -- consisting of an encoder, channel, and decoder modeled using neural networks -- has recently been shown to be an effective approach. A challenge faced in the practical adoption of this learning approach is that under changing channel conditions (e.g. a wireless link), it requires frequent retraining of the autoencoder in order to maintain a low decoding error rate. Since retraining is both time consuming and requires a large number of samples, it becomes impractical when the channel distribution is changing quickly. We propose to address this problem using a fast and sample-efficient (few-shot) domain adaptation method that does not change the encoder and decoder networks. Different from conventional training-time unsupervised or semi-supervised domain adaptation, here we have a trained autoencoder from a source distribution that we want to adapt (at test time) to a target distribution using only a small labeled dataset, and no unlabeled data. We focus on a generative channel model based on the Gaussian mixture density network (MDN), and propose a regularized, parameter-efficient adaptation of the MDN using a set of affine transformations. The learned affine transformations are then used to design an optimal transformation at the decoder input to compensate for the distribution shift, and effectively present to the decoder inputs close to the source distribution. Experiments on many simulated distribution changes common to the wireless setting, and a real mmWave FPGA testbed demonstrate the effectiveness of our method at adaptation using very few target domain samples~\footnote{Code for our work: \url{https://github.com/jayaram-r/domain-adaptation-autoencoder}}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                261. <h><a href=https://openreview.net/pdf?id=vTb1JI0Gps_>Automated Data Augmentations for Graph Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Youzhi Luo, Michael Curtis McThrow, Wing Yee Au, Tao Komikado, Kanji Uchino, Koji Maruhashi, Shuiwang Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vTb1JI0Gps_>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose GraphAug, a novel automated data augmentation method aiming at computing label-invariant augmentations for graph classification.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vTb1JI0Gps_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Data augmentations are effective in improving the invariance of learning machines. We argue that the core challenge of data augmentations lies in designing data transformations that preserve labels. This is relatively straightforward for images, but much more challenging for graphs. In this work, we propose GraphAug, a novel automated data augmentation method aiming at computing label-invariant augmentations for graph classification. Instead of using uniform transformations as in existing studies, GraphAug uses an automated augmentation model to avoid compromising critical label-related information of the graph, thereby producing label-invariant augmentations at most times. To ensure label-invariance, we develop a training method based on reinforcement learning to maximize an estimated label-invariance probability. Experiments show that GraphAug outperforms previous graph augmentation methods on various graph classification tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                262. <h><a href=https://openreview.net/pdf?id=I_YZANaz5X>DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Manipulation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yan Zhao, Ruihai Wu, Zhehuan Chen, Yourong Zhang, Qingnan Fan, Kaichun Mo, Hao Dong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=I_YZANaz5X>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel learning framework to learn collaborative affordance for dual-gripper manipulation tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/I_YZANaz5X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    It is essential yet challenging for future home-assistant robots to understand and manipulate diverse 3D objects in daily human environments. Towards building scalable systems that can perform diverse manipulation tasks over various 3D shapes, recent works have advocated and demonstrated promising results learning visual actionable affordance, which labels every point over the input 3D geometry with an action likelihood of accomplishing the downstream task (e.g., pushing or picking-up). However, these works only studied single-gripper manipulation tasks, yet many real-world tasks require two hands to achieve collaboratively. In this work, we propose a novel learning framework, DualAfford, to learn collaborative affordance for dual-gripper manipulation tasks. The core design of the approach is to reduce the quadratic problem for two grippers into two disentangled yet interconnected subtasks for efficient learning. Using the large-scale PartNet-Mobility and ShapeNet datasets, we set up four benchmark tasks for dual-gripper manipulation. Experiments prove the effectiveness and superiority of our method over three baselines. We will release code and data upon acceptance. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                263. <h><a href=https://openreview.net/pdf?id=EUrxG8IBCrC>Mutual Partial Label Learning with Competitive Label Noise</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yan Yan, Yuhong Guo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EUrxG8IBCrC>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EUrxG8IBCrC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Partial label learning (PLL) is an important weakly supervised learning problem, where each training instance is associated with a set of candidate labels that include both the true label and additional noisy labels. Most existing PLL methods assume the candidate noisy labels are randomly chosen, which hardly holds in real-world learning scenarios. In this paper, we consider a more realistic PLL scenario with competitive label noise that is more difficult to distinguish from the true label than the random label noise. We propose a novel Mutual Learning based PLL approach named ML-PLL to address this challenging problem. ML-PLL learns a prediction network based classifier and a class-prototype based classifier cooperatively through interactive mutual learning and label correction. Moreover, we use a transformation network to model the association relationships between the true label and candidate labels, and learn it together with the prediction network to match the observed candidate labels in the training data and enhance label correction. Extensive experiments are conducted on several benchmark PLL datasets, and the proposed ML-PLL approach demonstrates state-of-the-art performance for partial label learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                264. <h><a href=https://openreview.net/pdf?id=tYIMtogyee>Pre-training via Denoising for Molecular Property Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter Battaglia, Razvan Pascanu, Jonathan Godwin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tYIMtogyee>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We describe a technique for pre-training models for molecular property prediction from 3D structures based on denoising and show that it achieves SOTA results for various tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tYIMtogyee.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many important problems involving molecular property prediction from 3D structures have limited data, posing a generalization challenge for neural networks. In this paper, we describe a pre-training technique based on denoising that achieves a new state-of-the-art in molecular property prediction by utilizing large datasets of 3D molecular structures at equilibrium to learn meaningful representations for downstream tasks. Relying on the well-known link between denoising autoencoders and score-matching, we show that the denoising objective corresponds to learning a molecular force field -- arising from approximating the Boltzmann distribution with a mixture of Gaussians -- directly from equilibrium structures. Our experiments demonstrate that using this pre-training objective significantly improves performance on multiple benchmarks, achieving a new state-of-the-art on the majority of targets in the widely used QM9 dataset. Our analysis then provides practical insights into the effects of different factors -- dataset sizes, model size and architecture, and the choice of upstream and downstream datasets -- on pre-training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                265. <h><a href=https://openreview.net/pdf?id=TFbwV6I0VLg>SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, Animesh Garg</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TFbwV6I0VLg>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a general Transformer-based dynamic model to enable consistent future prediction in object-centric models
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TFbwV6I0VLg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Understanding dynamics from visual observations is a challenging problem that requires disentangling individual objects from the scene and learning their interactions. While recent object-centric models can successfully decompose a scene into objects, modeling their dynamics effectively still remains a challenge. We address this problem by introducing SlotFormer -- a Transformer-based autoregressive model operating on learned object-centric representations. Given a video clip, our approach reasons over object features to model spatio-temporal relationships and predicts accurate future object states. In this paper, we successfully apply SlotFormer to perform video prediction on datasets with complex object interactions. Moreover, the unsupervised SlotFormer&#39;s dynamics model can be used to improve the performance on supervised downstream tasks, such as Visual Question Answering (VQA), and goal-conditioned planning. Compared to past works on dynamics modeling, our method achieves significantly better long-term synthesis of object dynamics, while retaining high quality visual generation. Besides, SlotFormer enables VQA models to reason about the future without object-level labels, even outperforming counterparts that use ground-truth annotations. Finally, we show its ability to serve as a world model for model-based planning, which is competitive with methods designed specifically for such tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                266. <h><a href=https://openreview.net/pdf?id=Tvms8xrZHyR>Characterizing the spectrum of the NTK via a power series expansion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Murray, Hui Jin, Benjamin Bowman, Guido Montufar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Tvms8xrZHyR>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We characterize the NTK spectrum via a power series representation in terms of the Hermite coefficients of the activation function, the depth, and the effective rank of the input Gram. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Tvms8xrZHyR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Under mild conditions on the network initialization we derive a power series expansion for the Neural Tangent Kernel (NTK) of arbitrarily deep feedforward networks in the infinite width limit. We provide expressions for the coefficients of this power series which depend on both the Hermite coefficients of the activation function as well as the depth of the network. We observe faster decay of the Hermite coefficients leads to faster decay in the NTK coefficients and explore the role of depth. Using this series, first we relate the effective rank of the NTK to the effective rank of the input-data Gram. Second, for data drawn uniformly on the sphere we study the eigenvalues of the NTK, analyzing the impact of the choice of activation function. Finally, for generic data and activation functions with sufficiently fast Hermite coefficient decay, we derive an asymptotic upper bound on the spectrum of the NTK.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                267. <h><a href=https://openreview.net/pdf?id=C0q9oBc3n4>Temporal Dependencies in Feature Importance for Time Series Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kin Kwan Leung, Clayton Rooke, Jonathan Smith, Saba Zuberi, Maksims Volkovs</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=C0q9oBc3n4>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> New explainability method for multivariate time series predictions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/C0q9oBc3n4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Time series data introduces two key challenges for explainability methods: firstly, observations of the same feature over subsequent time steps are not independent, and secondly, the same feature can have varying importance to model predictions over time. In this paper, we propose Windowed Feature Importance in Time (WinIT), a feature removal based explainability approach to address these issues. Unlike existing feature removal explanation methods, WinIT explicitly accounts for the temporal dependence between different observations of the same feature in the construction of its importance score. Furthermore, WinIT captures the varying importance of a feature over time, by summarizing its importance over a window of past time steps. We conduct an extensive empirical study on synthetic and real-world data, compare against a wide range of leading explainability methods, and explore the impact of various evaluation strategies. Our results show that WinIT achieves significant gains over existing methods, with more consistent performance across different evaluation metrics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                268. <h><a href=https://openreview.net/pdf?id=kUI41mY8bHl>Robustness to corruption in pre-trained Bayesian neural networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xi Wang, Laurence Aitchison</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kUI41mY8bHl>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kUI41mY8bHl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We develop ShiftMatch, a new training-data-dependent likelihood for robustness to corruption in Bayesian neural networks (BNNs). ShiftMatch is inspired by the training-data-dependent “EmpCov” priors from Izmailov et al. (2021a), and efficiently matches test-time spatial correlations to those at training time. Critically, ShiftMatch is designed to leave the neural network’s training time likelihood unchanged, allowing it to use publicly available samples from pre-trained BNNs. Using pre-trained HMC samples, ShiftMatch gives strong performance improvements on CIFAR-10-C, outperforms EmpCov priors (though ShiftMatch uses extra information from a minibatch of corrupted test points), and is perhaps the first Bayesian method capable of convincingly outperforming plain deep ensembles.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                269. <h><a href=https://openreview.net/pdf?id=vouQcZS8KfW>Neural Causal Models for Counterfactual Identification and Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kevin Muyuan Xia, Yushu Pan, Elias Bareinboim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vouQcZS8KfW>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We solve the two problems of counterfactual identification and estimation from arbitrary surrogate experiments using a Generative Adversarial Network implementation of the Neural Causal Model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vouQcZS8KfW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Evaluating hypothetical statements about how the world would be had a different course of action been taken is arguably one key capability expected from modern AI systems. Counterfactual reasoning underpins discussions in fairness, the determination of blame and responsibility, credit assignment, and regret. In this paper, we study the evaluation of counterfactual statements through neural models. Specifically, we tackle two causal problems required to make such evaluations, i.e., counterfactual identification and estimation from an arbitrary combination of observational and experimental data. First, we show that neural causal models (NCMs) are expressive enough and encode the structural constraints necessary for performing counterfactual reasoning. Second, we develop an algorithm for simultaneously identifying and estimating counterfactual distributions. We show that this algorithm is sound and complete for deciding counterfactual identification in general settings. Third, considering the practical implications of these results, we introduce a new strategy for modeling NCMs using generative adversarial networks. Simulations corroborate with the proposed methodology.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                270. <h><a href=https://openreview.net/pdf?id=0ypGZvm0er0>View Synthesis with Sculpted Neural Points</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yiming Zuo, Jia Deng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0ypGZvm0er0>7.33 - [8, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0ypGZvm0er0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We address the task of view synthesis, generating novel views of a scene given a set of images as input. In many recent works such as NeRF (Mildenhall et al., 2020), the scene geometry is parameterized using neural implicit representations (i.e., MLPs). Implicit neural representations have achieved impressive visual quality but have drawbacks in computational efficiency. In this work, we propose a new approach that performs view synthesis using point clouds. It is the first point-based method that achieves better visual quality than NeRF while being 100× faster in rendering speed. Our approach builds on existing works on differentiable point-based rendering but introduces a novel technique we call “Sculpted Neural Points (SNP)”, which significantly improves the robustness to errors and holes in the reconstructed point cloud. We further propose to use view-dependent point features based on spherical harmonics to capture non-Lambertian surfaces, and new designs in the point-based rendering pipeline that further boost the performance. Finally, we show that our system supports fine-grained scene editing. Code is available at https://github.com/princeton-vl/SNP.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                271. <h><a href=https://openreview.net/pdf?id=d8CBRlWNkqH>Neural Optimal Transport</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexander Korotin, Daniil Selikhanovych, Evgeny Burnaev</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=d8CBRlWNkqH>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/d8CBRlWNkqH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a novel neural-networks-based algorithm to compute optimal transport maps and plans for strong and weak transport costs. To justify the usage of neural networks, we prove that they are universal approximators of transport plans between probability distributions. We evaluate the performance of our optimal transport algorithm on toy examples and on the unpaired image-to-image translation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                272. <h><a href=https://openreview.net/pdf?id=sKc6fgce1zs>Learning About Progress From Experts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jake Bruce, Ankit Anand, Bogdan Mazoure, Rob Fergus</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sKc6fgce1zs>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We learn a model of long-term progress using expert demonstrations, and show that it can be used to form an exploration reward that allows reinforcement learning agents to solve very challenging sparse tasks in NetHack.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sKc6fgce1zs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many important tasks involve some notion of long-term progress in multiple phases: e.g. to clean a shelf it must be cleared of items, cleaning products applied, and then the items placed back on the shelf. In this work, we explore the use of expert demonstrations in long-horizon tasks to learn a monotonically increasing function that summarizes progress. This function can then be used to aid agent exploration in environments with sparse rewards. As a case study we consider the NetHack environment, which requires long-term progress at a variety of scales and is far from being solved by existing approaches. In this environment, we demonstrate that by learning a model of long-term progress from expert data containing only observations, we can achieve efficient exploration in challenging sparse tasks, well beyond what is possible with current state-of-the-art approaches. We have made the curated gameplay dataset used in this work available at https://github.com/deepmind/nao_top10.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                273. <h><a href=https://openreview.net/pdf?id=KemSBwOYJC>Statistical Inference for Fisher Market Equilibrium</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Luofeng Liao, Yuan Gao, Christian Kroer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KemSBwOYJC>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a statistical inference framework for Fisher market equilibrium.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KemSBwOYJC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Statistical inference under market equilibrium effects has attracted increasing attention recently. In this paper we focus on the specific case of linear Fisher markets. They have been widely use in fair resource allocation of food/blood donations and budget management in large-scale Internet ad auctions. In resource allocation, it is crucial to quantify the variability of the resource received by the agents (such as blood banks and food banks) in addition to fairness and efficiency properties of the systems. For ad auction markets, it is important to establish statistical properties of the platform&#39;s revenues in addition to their expected values. To this end, we propose a statistical framework based on the concept of infinite-dimensional Fisher markets. In our framework, we observe a market formed by a finite number of items sampled from an underlying distribution (the ``observed market&#39;&#39;) and aim to infer several important equilibrium quantities of the underlying long-run market. These equilibrium quantities include individual utilities, social welfare, and pacing multipliers. Through the lens of sample average approximation (SSA), we derive a collection of statistical results and show that the observed market provides useful statistical information of the long-run market. In other words, the equilibrium quantities of the observed market converge to the true ones of the long-run market with strong statistical guarantees. These include consistency, finite sample bounds, asymptotics, and confidence. As an extension, we discuss revenue inference in quasilinear Fisher markets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                274. <h><a href=https://openreview.net/pdf?id=ymFhZxw70uz>Classically Approximating Variational Quantum Machine Learning with Random Fourier Features</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jonas Landman, Slimane Thabet, Constantin Dalyac, Hela Mhiri, Elham Kashefi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ymFhZxw70uz>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show theoretically and experimentally that models built from exponentially large quantum feature space can be classically reproduced by sampling a few frequencies to build an equivalent low dimensional kernel
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ymFhZxw70uz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many applications of quantum computing in the near term rely on variational quantum circuits (VQCs). They have been showcased as a promising model for reaching a quantum advantage in machine learning with current noisy intermediate scale quantum computers (NISQ). It is often believed that the power of VQCs relies on their exponentially large feature space, and extensive works have explored the expressiveness and trainability of VQCs in that regard. In our work, we propose a classical sampling method that can closely approximate most VQCs with Hamiltonian encoding, given only the description of their architecture. It uses the seminal proposal of Random Fourier Features (RFF) and the fact that VQCs can be seen as large Fourier series. We show theoretically and experimentally that models built from exponentially large quantum feature space can be classically reproduced by sampling a few frequencies to build an equivalent low dimensional kernel. Precisely, we show that the number of required samples grows favourably with the size of the quantum spectrum. This tool therefore questions the hope for quantum advantage from VQCs in many cases, but conversely helps to narrow the conditions for their potential success. We expect VQCs with various and complex encoding Hamiltonians, or with large input dimension, to become more robust to classical approximations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                275. <h><a href=https://openreview.net/pdf?id=cw8FeirkIfU>Distributed Differential Privacy in Multi-Armed Bandits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sayak Ray Chowdhury, Xingyu Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cw8FeirkIfU>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We achieve pure DP for the first time in the distributed trust model while maintaining the same regret under the central model
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cw8FeirkIfU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the standard $K$-armed bandit problem under a distributed trust model of differential privacy (DP), which enables to guarantee privacy without a trustworthy server.  Under this trust model, previous work largely focus on achieving privacy using a shuffle protocol, where a batch of users data are randomly permuted before sending to a central server. This protocol achieves ($\epsilon,\delta$) or approximate-DP guarantee by sacrificing an additive $O\!\left(\!\frac{K\log T\sqrt{\log(1/\delta)}}{\epsilon}\!\right)\!$ factor in $T$-step cumulative regret. In contrast, the optimal privacy cost to achieve a stronger ($\epsilon,0$) or pure-DP guarantee under the widely used central trust model is only $\Theta\!\left(\!\frac{K\log T}{\epsilon}\!\right)\!$, where, however, a trusted server is required. In this work, we aim to obtain a pure-DP guarantee under distributed trust model while sacrificing no more regret than that under central trust model. We achieve this by designing a generic bandit algorithm based on successive arm elimination, where privacy is guaranteed by corrupting rewards with an equivalent discrete Laplace noise ensured by a secure computation protocol. We also show that our algorithm, when instantiated with Skellam noise and the secure protocol, ensures \emph{R\&#39;{e}nyi differential privacy} -- a stronger notion than approximate DP -- under distributed trust model with a privacy cost of $O\!\left(\!\frac{K\sqrt{\log T}}{\epsilon}\!\right)\!$. Finally, as a by-product of our techniques, we also recover the best-known regret bounds for bandits under central and local models while using only \emph{discrete privacy noise}, which can avoid the privacy leakage due to floating point arithmetic of continuous noise on finite computers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                276. <h><a href=https://openreview.net/pdf?id=n05upKp02kQ>Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fan Chen, Yu Bai, Song Mei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n05upKp02kQ>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a unified structural condition for sample-efficient partially observable RL (POMDPs/PSRs), and establish substantially sharper learning results than existing ones.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n05upKp02kQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Partial Observability---where agents can only observe partial information about the true underlying state of the system---is ubiquitous in real-world applications of Reinforcement Learning (RL). Theoretically, learning a near-optimal policy under partial observability is known to be hard in the worst case due to an exponential sample complexity lower bound. Recent work has identified several tractable subclasses that are learnable with polynomial samples, such as Partially Observable Markov Decision Processes (POMDPs) with certain revealing or decodability conditions. However, this line of research is still in its infancy, where (1) unified structural conditions enabling sample-efficient learning are lacking; (2) existing sample complexities for known tractable subclasses are far from sharp; and (3) fewer sample-efficient algorithms are available than in fully observable RL.

This paper advances all three aspects above for Partially Observable RL in the general setting of Predictive State Representations (PSRs). First, we propose a natural and unified structural condition for PSRs called \emph{B-stability}. B-stable PSRs encompasses the vast majority of known tractable subclasses such as weakly revealing POMDPs, low-rank future-sufficient POMDPs, decodable POMDPs, and regular PSRs. Next, we show that any B-stable PSR can be learned with polynomial samples in relevant problem parameters. When instantiated in the aforementioned subclasses, our sample complexities improve substantially over the current best ones. Finally, our results are achieved by three algorithms simultaneously: Optimistic Maximum Likelihood Estimation, Estimation-to-Decisions, and Model-Based Optimistic Posterior Sampling. The latter two algorithms are new for sample-efficient learning of POMDPs/PSRs.
We additionally design a variant of the Estimation-to-Decisions algorithm to perform sample-efficient \emph{all-policy model estimation} for B-stable PSRs, which also yields guarantees for reward-free learning as an implication.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                277. <h><a href=https://openreview.net/pdf?id=fk7RbGibe1>Domain Generalization via Heckman-type Selection Models </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyungu Kahng, Hyungrok Do, Judy Zhong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fk7RbGibe1>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A non-random sample selection framework for solving domain generalization, and a set of Heckman-type estimators for various types of outcomes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fk7RbGibe1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The domain generalization (DG) setup considers the problem where models are trained on data sampled from multiple domains and evaluated on test domains unseen during training. In this paper, we formulate DG as a sample selection problem where each domain is sampled from a common underlying population through non-random sampling probabilities that correlate with both the features and the outcome. Under this setting, the fundamental iid assumption of the empirical risk minimization (ERM) is violated, so it often performs worse on test domains whose non-random sampling probabilities differ from the domains in the training dataset. We propose a Selection-Guided DG (SGDG) framework to learn the selection probability of each domain and the joint distribution of the outcome and domain selection variables. The proposed SGDG is domain generalizable as it intends to minimize the risk under the population distribution. We theoretically proved that, under certain regular conditions, SGDG can achieve smaller risk than ERM. Furthermore, we present a class of parametric SGDG (HeckmanDG) estimators applicable to continuous, binary, and multinomial outcomes. We also demonstrated its efficacy empirically through simulations and experiments on a set of benchmark datasets comparing with other well-known DG methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                278. <h><a href=https://openreview.net/pdf?id=_ruvo2KCL2x>Deep Ranking Ensembles for Hyperparameter Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Abdus Salam Khazi, Sebastian Pineda Arango, Josif Grabocka</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_ruvo2KCL2x>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Meta-learn Deep Ensembles using Ranking Losses to improve the performance on Hyperparameter Optimization
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_ruvo2KCL2x.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Automatically optimizing the hyperparameters of Machine Learning algorithms is one of the primary open questions in AI. Existing work in Hyperparameter Optimization (HPO) trains surrogate models for approximating the response surface of hyperparameters as a regression task. In contrast, we hypothesize that the optimal strategy for training surrogates is to preserve the ranks of the performances of hyperparameter configurations as a Learning to Rank problem. As a result, we present a novel method that meta-learns neural network surrogates optimized for ranking the configurations&#39; performances while modeling their uncertainty via ensembling. In a large-scale experimental protocol comprising 12 baselines, 16 HPO search spaces and 86 datasets/tasks, we demonstrate that our method achieves new state-of-the-art results in HPO.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                279. <h><a href=https://openreview.net/pdf?id=YPKBIILy-Kt>Confidence-Based Feature Imputation for Graphs with Partially Known Features</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daeho Um, Jiwoong Park, Seulki Park, Jin young Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YPKBIILy-Kt>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> For graphs with missing features, we define a novel concept of confidence and propose a pseudo-confidence-based feature imputation (PCFI) scheme.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YPKBIILy-Kt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper investigates a missing feature imputation problem for graph learning tasks. Several methods have previously addressed learning tasks on graphs with missing features. However, in cases of high rates of missing features, they were unable to avoid significant performance degradation. To overcome this limitation, we introduce a novel concept of channel-wise confidence in a node feature, which is assigned to each imputed channel feature of a node for reflecting the certainty of the imputation. We then design pseudo-confidence using the channel-wise shortest path distance between a missing-feature node and its nearest known-feature node to replace unavailable true confidence in an actual learning process. Based on the pseudo-confidence, we propose a novel feature imputation scheme that performs channel-wise inter-node diffusion and node-wise inter-channel propagation. The scheme can endure even at an exceedingly high missing rate (e.g., 99.5\%) and it achieves state-of-the-art accuracy for both semi-supervised node classification and link prediction on various datasets containing a high rate of missing features. Codes are available at https://github.com/daehoum1/pcfi.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                280. <h><a href=https://openreview.net/pdf?id=sckjveqlCZ>Broken Neural Scaling Laws</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sckjveqlCZ>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a functional form that accurately models the scaling behaviors for each task from a very large and diverse set of downstream (and upstream) tasks, even scaling behaviors that were previously believed to be &#34;unpredictable&#34;.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sckjveqlCZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a smoothly broken power law functional form (referred to by us as a broken neural scaling law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning (single agent and multi-agent). When compared to other functional forms for neural scaling behavior, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. See arXiv for longer version of this paper. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                281. <h><a href=https://openreview.net/pdf?id=p7G8t5FVn2h>One-Pixel Shortcut: On the Learning Preference of Deep Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shutong Wu, Sizhe Chen, Cihang Xie, Xiaolin Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p7G8t5FVn2h>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a model-free method to craft unlearnable example by perturbing only one pixel, and construct a benchmark containing images that are unlearnable by various existing methods to avoid shortcut learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p7G8t5FVn2h.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unlearnable examples (ULEs) aim to protect data from unauthorized usage for training DNNs. Existing work adds $\ell_\infty$-bounded perturbations to the original sample so that the trained model generalizes poorly. Such perturbations, however, are easy to eliminate by adversarial training and data augmentations. In this paper, we resolve this problem from a novel perspective by perturbing only one pixel in each image. Interestingly, such a small modification could effectively degrade model accuracy to almost an untrained counterpart. Moreover, our produced \emph{One-Pixel Shortcut (OPS)} could not be erased by adversarial training and strong augmentations. To generate OPS, we perturb in-class images at the same position to the same target value that could mostly and stably deviate from all the original images. Since such generation is only based on images, OPS needs significantly less computation cost than the previous methods using DNN generators. Based on OPS, we introduce an unlearnable dataset called CIFAR-10-S, which is indistinguishable from CIFAR-10 by humans but induces the trained model to extremely low accuracy. Even under adversarial training, a ResNet-18 trained on CIFAR-10-S has only 10.61% accuracy, compared to 83.02% by the existing error-minimizing method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                282. <h><a href=https://openreview.net/pdf?id=6taykzqcPD>Neural Networks Efficiently Learn Low-Dimensional Representations with SGD</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, Murat A Erdogdu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6taykzqcPD>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove that SGD on neural networks can learn low-dimensional features in certain settings, and use this to derive novel generalization and excess risk bounds.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6taykzqcPD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the problem of training a two-layer neural network (NN) of arbitrary width using stochastic gradient descent (SGD) where the input $\boldsymbol{x}\in \mathbb{R}^d$ is Gaussian and the target $y \in \mathbb{R}$ follows a multiple-index model, i.e., $y=g(\langle\boldsymbol{u_1},\boldsymbol{x}\rangle,...,\langle\boldsymbol{u_k},\boldsymbol{x}\rangle)$ with a noisy link function $g$. We prove that the first-layer weights in the NN converge to the $k$-dimensional principal subspace spanned by the vectors $\boldsymbol{u_1},...,\boldsymbol{u_k}$ of the true model, when online SGD with weight decay is used for training. This phenomenon has several important consequences when $k \ll d$. First, by employing uniform convergence on this smaller subspace, we establish a generalization error bound of $\mathcal{O}(\sqrt{{kd}/{T}})$ after $T$ iterations of SGD, which is independent of the width of the NN. We further demonstrate that, by recovering the principal direction, SGD-trained ReLU NNs can learn a single-index target of the form $y=f(\langle\boldsymbol{u},\boldsymbol{x}\rangle) + \epsilon$ with a sample complexity linear in $d$ (up to log factors), where $f$ is a monotonic function with at most polynomial growth, and $\epsilon$ is the noise. This is in contrast to the known $d^{\Omega(p)}$ samples required to learn any degree $p$ polynomial in the kernel regime, and shows that SGD-trained NNs can outperform the Neural Tangent Kernel at initialization. Finally, we establish compressibility guarantees for NNs using that SGD produces an approximately rank-$k$ first-layer weight matrix.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                283. <h><a href=https://openreview.net/pdf?id=LIV7-_7pYPl>DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sizhe Li, Zhiao Huang, Tao Chen, Tao Du, Hao Su, Joshua B. Tenenbaum, Chuang Gan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LIV7-_7pYPl>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We investigate the problem of learning dexterous manipulation of deformable objects using multi-fingered hands.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LIV7-_7pYPl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, we aim to learn dexterous manipulation of deformable objects using multi-fingered hands. Reinforcement learning approaches for dexterous rigid object manipulation would struggle in this setting due to the complexity of physics interaction with deformable objects. At the same time, previous trajectory optimization approaches with differentiable physics for deformable manipulation would suffer from local optima caused by the explosion of contact modes from hand-object interactions. To address these challenges, we propose DexDeform, a principled framework that abstracts dexterous manipulation skills from human demonstration, and refines the learned skills with differentiable physics. Concretely, we first collect a small set of human demonstrations using teleoperation. And we then train a skill model using demonstrations for planning over action abstractions in imagination. To explore the goal space, we further apply augmentations to the existing deformable shapes in demonstrations and use a gradient optimizer to refine the actions planned by the skill model. Finally, we adopt the refined trajectories as new demonstrations for finetuning the skill model. To evaluate the effectiveness of our approach, we introduce a suite of six challenging dexterous deformable object manipulation tasks. Compared with baselines, DexDeform is able to better explore and generalize across novel goals unseen in the initial human demonstrations. Additional materials can be found at our project website: https://sites.google.com/view/dexdeform.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                284. <h><a href=https://openreview.net/pdf?id=PolHquob8M7>Continual Transformers: Redundancy-Free Attention for Online Inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lukas Hedegaard, Arian Bakhtiarnia, Alexandros Iosifidis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PolHquob8M7>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A Transformer Decorder acceleration for online stream processing validated with experiments in Online Action Detection and Audio Classification.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PolHquob8M7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transformers in their common form are inherently limited to operate on whole token sequences rather than on one token at a time. Consequently, their use during online inference on time-series data entails considerable redundancy due to the overlap in successive token sequences. In this work, we propose novel formulations of the Scaled Dot-Product Attention, which enable Transformers to perform efficient online token-by-token inference on a continual input stream. Importantly, our modifications are purely to the order of computations, while the outputs and learned weights are identical to those of the original Transformer Encoder. We validate our Continual Transformer Encoder with experiments on the THUMOS14, TVSeries and GTZAN datasets with remarkable results: Our Continual one- and two-block architectures reduce the floating point operations per prediction by up to 63x and 2.6x, respectively, while retaining predictive performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                285. <h><a href=https://openreview.net/pdf?id=Mpa3tRJFBb>Where to Begin? On the Impact of Pre-Training and Initialization in Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>John Nguyen, Jianyu Wang, Kshitiz Malik, Maziar Sanjabi, Michael Rabbat</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Mpa3tRJFBb>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Stop worrying about heterogeneity and start from pre-trained weights.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Mpa3tRJFBb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    An oft-cited challenge of federated learning is the presence of heterogeneity. \emph{Data heterogeneity} refers to the fact that data from different clients may follow very different distributions. \emph{System heterogeneity} refers to client devices having different system capabilities. A considerable number of federated optimization methods address this challenge. In the literature, empirical evaluations usually start federated training from random initialization. However, in many practical applications of federated learning, the server has access to proxy data for the training task that can be used to pre-train a model before starting federated training. Using four standard federated learning benchmark datasets, we empirically study the impact of starting from a pre-trained model in federated learning. Unsurprisingly, starting from a pre-trained model reduces the training time required to reach a target error rate and enables the training of more accurate models (up to 40\%) than is possible when starting from random initialization. Surprisingly, we also find that starting federated learning from a pre-trained initialization reduces the effect of both data and system heterogeneity. We recommend future work proposing and evaluating federated optimization methods to evaluate the performance when starting from random and pre-trained initializations. This study raises several questions for further work on understanding the role of heterogeneity in federated optimization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                286. <h><a href=https://openreview.net/pdf?id=47KG_AvNqeZ>Online Low Rank Matrix Completion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Soumyabrata Pal, Prateek Jain</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=47KG_AvNqeZ>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel algorithm for solving online low-rank matrix completion problem with optimal regret for rank-one case.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/47KG_AvNqeZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                      We study the problem of online low-rank matrix completion with $\mathsf{M}$ users, $\mathsf{N}$ items and $\mathsf{T}$ rounds. In each round, the algorithm recommends one item per user, for which it gets a (noisy) reward sampled from a low-rank user-item preference matrix. The goal is to design a method with sub-linear regret (in $\mathsf{T}$) and nearly optimal dependence on $\mathsf{M}$ and $\mathsf{N}$. The problem can be easily mapped to the standard multi-armed bandit problem where each item is an independent arm, but that leads to poor regret as the correlation between arms and users is not exploited. On the other hand, exploiting the low-rank structure of reward matrix is challenging due to non-convexity of the low-rank manifold. We first demonstrate that the low-rank structure can be exploited  using a simple  explore-then-commit (ETC) approach that ensures a regret of $O(\mathsf{polylog} (\mathsf{M}+\mathsf{N}) \mathsf{T}^{2/3})$. That is, roughly only $\mathsf{polylog} (\mathsf{M}+\mathsf{N})$ item recommendations are required per user to get a non-trivial solution. We then improve our result for the  rank-$1$ setting which in itself is quite challenging and encapsulates some of the key issues. Here, we propose OCTAL (Online Collaborative filTering using iterAtive user cLustering) that guarantees nearly optimal regret of $O(\mathsf{polylog} (\mathsf{M}+\mathsf{N}) \mathsf{T}^{1/2})$. OCTAL is based on a novel technique of clustering users that allows iterative elimination of items and leads to a nearly optimal minimax rate. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                287. <h><a href=https://openreview.net/pdf?id=APuPRxjHvZ>Surgical Fine-Tuning Improves Adaptation to Distribution Shifts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, Chelsea Finn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=APuPRxjHvZ>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Selectively fine-tuning a subset of layers outperforms full fine-tuning when transferring to tasks with various distribution shifts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/APuPRxjHvZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shift.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                288. <h><a href=https://openreview.net/pdf?id=Z4s73sJYQM>Evolve Smoothly, Fit Consistently: Learning Smooth Latent Dynamics For Advection-Dominated Systems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhong Yi Wan, Leonardo Zepeda-Nunez, Anudhyan Boral, Fei Sha</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Z4s73sJYQM>7.33 - [6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Z4s73sJYQM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a data-driven, space-time continuous framework to learn surrogate models for complex physical systems described by advection-dominated partial differential equations. Those systems have slow-decaying Kolmogorov n-width that hinders standard methods, including reduced order modeling, from producing high-fidelity simulations at low cost. In this work, we construct hypernetwork-based latent dynamical models directly on the parameter space of a compact representation network. We leverage the expressive power of the network and a specially designed consistency-inducing regularization to obtain latent trajectories that are both low-dimensional and smooth. These properties render our surrogate models highly efficient at inference time. We show the efficacy of our framework by learning models that generate accurate multi-step rollout predictions at much faster inference speed compared to competitors, for several challenging examples.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                289. <h><a href=https://openreview.net/pdf?id=7YfHla7IxBJ>Encoding Recurrence into Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Feiqing Huang, Kexin Lu, Yuxi CAI, Zhen Qin, Yanwen Fang, Guangjian Tian, Guodong Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7YfHla7IxBJ>7.33 - [6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new module to encode the recurrent dynamics of an RNN layer into Transformers and higher sample efficiency can be achieved.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7YfHla7IxBJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper novelly breaks down with ignorable loss an RNN layer into a sequence of simple RNNs, each of which can be further rewritten into a lightweight positional encoding matrix of a self-attention, named the Recurrence Encoding Matrix (REM). Thus, recurrent dynamics introduced by the RNN layer can be encapsulated into the positional encodings of a multihead self-attention, and this makes it possible to seamlessly incorporate these recurrent dynamics into a Transformer, leading to a new module, Self-Attention with Recurrence (RSA). The proposed module can leverage the recurrent inductive bias of REMs to achieve a better sample efficiency than its corresponding baseline Transformer, while the self-attention is used to model the remaining non-recurrent signals. The relative proportions of these two components are controlled by a data-driven gated mechanism, and the effectiveness of RSA modules are demonstrated by four sequential learning tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                290. <h><a href=https://openreview.net/pdf?id=xjb563TH-GH>Representational Dissimilarity Metric Spaces for Stochastic Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lyndon Duong, Jingyang Zhou, Josue Nassar, Jules Berman, Jeroen Olieslagers, Alex H Williams</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xjb563TH-GH>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Representational dissimilarity metrics that account for noise geometry in biological and artificial neural responses. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xjb563TH-GH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Quantifying similarity between neural representations---e.g. hidden layer activation vectors---is a perennial problem in deep learning and neuroscience research. Existing methods compare deterministic responses (e.g. artificial networks that lack stochastic layers) or averaged responses (e.g., trial-averaged firing rates in biological data). However, these measures of _deterministic_ representational similarity ignore the scale and geometric structure of noise, both of which play important roles in neural computation. To rectify this, we generalize previously proposed shape metrics (Williams et al. 2021) to quantify differences in _stochastic_ representations. These new distances satisfy the triangle inequality, and thus can be used as a rigorous basis for many supervised and unsupervised analyses. Leveraging this novel framework, we find that the stochastic geometries of neurobiological representations of oriented visual gratings and naturalistic scenes respectively resemble untrained and trained deep network representations. Further, we are able to more accurately predict certain network attributes (e.g. training hyperparameters) from its position in stochastic (versus deterministic) shape space.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                291. <h><a href=https://openreview.net/pdf?id=6fuPIe9tbnC>Multifactor Sequential Disentanglement via Structured Koopman Autoencoders</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nimrod Berman, Ilan Naiman, Omri Azencot</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6fuPIe9tbnC>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new method for learning multifactor disentangled representations of sequential data
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6fuPIe9tbnC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disentangling abilities such as swapping of individual static factors between characters, and an incremental swap of disentangled factors from the source to the target. Moreover, we evaluate our method extensively on two factor standard benchmark tasks where we significantly improve over competing unsupervised approaches, and we perform competitively in comparison to weakly- and self-supervised state-of-the-art approaches. The code is available at https://github.com/azencot-group/SKD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                292. <h><a href=https://openreview.net/pdf?id=l9vM_PaUKz>Soft Neighbors are Positive Supporters in Contrastive Visual Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chongjian GE, Jiangliu Wang, Zhan Tong, Shoufa Chen, Yibing Song, Ping Luo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=l9vM_PaUKz>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We leverage the soft neighbors to sufficiently explore the correlation information among samples in cotrastive learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/l9vM_PaUKz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contrastive learning methods train visual encoders by comparing views (e.g., often created via a group of data augmentations on the same instance) from one instance to others. Typically, the views created from one instance are set as positive, while views from other instances are negative. This binary instance discrimination is studied extensively to improve feature representations in self-supervised learning. In this paper, we rethink the instance discrimination framework and find the binary instance labeling insufficient to measure correlations between different samples. For an intuitive example, given a random image instance, there may exist other images in a mini-batch whose content meanings are the same (i.e., belonging to the same category) or partially related (i.e., belonging to a similar category). How to treat the images that correlate similarly to the current image instance leaves an unexplored problem. We thus propose to support the current image by exploring other correlated instances (i.e., soft neighbors). We first carefully cultivate a candidate neighbor set, which will be further utilized to explore the highly-correlated instances. A cross-attention module is then introduced to predict the correlation score (denoted as positiveness) of other correlated instances with respect to the current one. The positiveness score quantitatively measures the positive support from each correlated instance, and is encoded into the objective for pretext training. To this end, our proposed method benefits in discriminating uncorrelated instances while absorbing correlated instances for SSL. We evaluate our soft neighbor contrastive learning method (SNCLR) on standard visual recognition benchmarks, including image classification, object detection, and instance segmentation. The state-of-the-art recognition performance shows that SNCLR is effective in improving feature representations from both ViT and CNN encoders. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                293. <h><a href=https://openreview.net/pdf?id=n7CPzMPKQl>Integrating Symmetry into Differentiable Planning with Steerable Convolutions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Linfeng Zhao, Xupeng Zhu, Lingzhi Kong, Robin Walters, Lawson L.S. Wong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n7CPzMPKQl>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n7CPzMPKQl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    To achieve this, we draw inspiration from equivariant convolution networks and model the path planning problem as a set of signals over grids. We demonstrate that value iteration can be treated as a linear equivariant operator, which is effectively a steerable convolution. Building upon Value Iteration Networks (VIN), we propose a new Symmetric Planning (SymPlan) framework that incorporates rotation and reflection symmetry using steerable convolution networks. We evaluate our approach on four tasks: 2D navigation, visual navigation, 2 degrees of freedom (2-DOF) configuration space manipulation, and 2-DOF workspace manipulation. Our experimental results show that our symmetric planning algorithms significantly improve training efficiency and generalization performance compared to non-equivariant baselines, including VINs and GPPN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                294. <h><a href=https://openreview.net/pdf?id=Xyme9p1rpZw>SoftZoo: A Soft Robot Co-design Benchmark For Locomotion In Diverse Environments</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tsun-Hsuan Wang, Pingchuan Ma, Andrew Everett Spielberg, Zhou Xian, Hao Zhang, Joshua B. Tenenbaum, Daniela Rus, Chuang Gan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Xyme9p1rpZw>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a new virtual environment for soft robot co-design.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Xyme9p1rpZw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While significant research progress has been made in robot learning for control, unique challenges arise when simultaneously co-optimizing morphology. Existing work has typically been tailored for particular environments or representations. In order to more fully understand inherent design and performance tradeoffs and accelerate the development of new breeds of soft robots, a comprehensive virtual platform — with well-established tasks, environments, and evaluation metrics — is needed. In this work, we introduce SoftZoo, a soft robot co-design platform for locomotion in diverse environments. SoftZoo supports an extensive, naturally-inspired material set, including the ability to simulate environments such as flat ground, desert, wetland, clay, ice, snow, shallow water, and ocean. Further, it provides a variety of tasks relevant for soft robotics, including fast locomotion, agile turning, and path following, as well as differentiable design representations for morphology and control. Combined, these elements form a feature-rich platform for analysis and development of soft robot co-design algorithms. We benchmark prevalent representations and co-design algorithms, and shed light on 1) the interplay between environment, morphology, and behavior (2) the importance of design space representations 3) the ambiguity in muscle formation and controller synthesis and 4) the value of differentiable physics. We envision that SoftZoo will serve as a standard platform and template an approach toward the development of novel representations and algorithms for co-designing soft robots’ behavioral and morphological intelligence. Demos are available on our project page.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                295. <h><a href=https://openreview.net/pdf?id=ejHUr4nfHhD>Temperature Schedules for self-supervised contrastive methods on long-tail data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anna Kukleva, Moritz Böhle, Bernt Schiele, Hilde Kuehne, Christian Rupprecht</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ejHUr4nfHhD>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Simple temperature schedules in self-supervised contrastive learning improve representation learning on long-tail distributions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ejHUr4nfHhD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Most approaches for self-supervised learning (SSL) are optimised on curated balanced datasets, e.g. ImageNet, despite the fact that natural data usually exhibits long-tail distributions. In this paper, we analyse the behaviour of one of the most popular variants of SSL, i.e. contrastive methods, on imbalanced data. In particular, we investigate the role of the temperature parameter $\tau$ in the contrastive loss, by analysing the loss through the lens of average distance maximisation, and find that a large $\tau$ emphasises group-wise discrimination, whereas a small $\tau$ leads to a higher degree of instance discrimination. While $\tau$ has thus far been treated exclusively as a constant hyperparameter, in this work, we propose to employ a dynamic $\tau$ and show that a simple cosine schedule can yield significant improvements in the learnt representations. Such a schedule results in a constant `task switching&#39; between an emphasis on instance discrimination and group-wise discrimination and thereby ensures that the model learns both group-wise features, as well as instance-specific details. Since frequent classes benefit from the former, while infrequent classes require the latter, we find this method to consistently improve separation between the classes in long-tail data without any additional computational cost. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                296. <h><a href=https://openreview.net/pdf?id=_nF5imFKQI>How I Learned to Stop Worrying and Love Retraining</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Max Zimmer, Christoph Spiegel, Sebastian Pokutta</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_nF5imFKQI>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_nF5imFKQI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many Neural Network Pruning approaches consist of several iterative training and pruning steps, seemingly losing a significant amount of their performance after pruning and then recovering it in the subsequent retraining phase. Recent works of Renda et al. (2020) and Le &amp; Hua (2021) demonstrate the significance of the learning rate schedule during the retraining phase and propose specific heuristics for choosing such a schedule for IMP (Han et al., 2015). We place these findings in the context of the results of Li et al. (2020) regarding the training of models within a fixed training budget and demonstrate that, consequently, the retraining phase can be massively shortened using a simple linear learning rate schedule. Improving on existing retraining approaches, we additionally propose a method to adaptively select the initial value of the linear schedule. Going a step further, we propose similarly imposing a budget on the initial dense training phase and show that the resulting simple and efficient method is capable of outperforming significantly more complex or heavily parameterized state-of-the-art approaches that attempt to sparsify the network during training. These findings not only advance our understanding of the retraining phase, but more broadly question the belief that one should aim to avoid the need for retraining and reduce the negative effects of ‘hard’ pruning by incorporating the sparsification process into the standard training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                297. <h><a href=https://openreview.net/pdf?id=_geIwiOyUhZ>Bayes-MIL: A New Probabilistic Perspective on Attention-based Multiple Instance Learning for Whole Slide Images</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yufei CUI, Ziquan Liu, Xiangyu Liu, Xue Liu, Cong Wang, Tei-Wei Kuo, Chun Jason Xue, Antoni B. Chan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_geIwiOyUhZ>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Bayesian modeling of multiple instance learning addresses untrustworthy and unsatisfactory interpretability problem of related methods. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_geIwiOyUhZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multiple instance learning (MIL) is a popular weakly-supervised learning model on the whole slide image (WSI) for AI-assisted pathology diagnosis. The recent advance in attention-based MIL allows the model to find its region-of-interest (ROI) for interpretation by learning the attention weights for image patches of WSI slides. However, we empirically find that the interpretability of some related methods is either untrustworthy as the principle of MIL is violated or unsatisfactory as the high-attention regions are not consistent with experts&#39; annotations. In this paper, we propose Bayes-MIL to address the problem from a probabilistic perspective. The induced patch-level uncertainty is proposed as a new measure of MIL interpretability, which outperforms previous methods in matching doctors annotations. We design a slide-dependent patch regularizer (SDPR) for the attention, imposing constraints derived from the MIL assumption, on the attention distribution. SDPR explicitly constrains the model to generate correct attention values. The spatial information is further encoded by an approximate convolutional conditional random field (CRF), for better interpretability. Experimental results show Bayes-MIL outperforms the related methods in patch-level and slide-level metrics and provides much better interpretable ROI on several large-scale WSI datasets. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                298. <h><a href=https://openreview.net/pdf?id=KXRSh0sdVTP>Meta-learning Adaptive Deep Kernel Gaussian Processes for Molecular Property Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenlin Chen, Austin Tripp, José Miguel Hernández-Lobato</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KXRSh0sdVTP>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a meta-learning approach for fitting deep kernel GPs via implicit differentiation, which outperforms previous SOTA methods on a variety of real-world chemical tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KXRSh0sdVTP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose Adaptive Deep Kernel Fitting with Implicit Function Theorem (ADKF-IFT), a novel framework for learning deep kernel Gaussian processes (GPs) by interpolating between meta-learning and conventional deep kernel learning. Our approach employs a bilevel optimization objective where we meta-learn generally useful feature representations across tasks, in the sense that task-specific GP models estimated on top of such features achieve the lowest possible predictive loss on average. We solve the resulting nested optimization problem using the implicit function theorem (IFT). We show that our ADKF-IFT framework contains previously proposed Deep Kernel Learning (DKL) and Deep Kernel Transfer (DKT) as special cases. Although ADKF-IFT is a completely general method, we argue that it is especially well-suited for drug discovery problems and demonstrate that it significantly outperforms previous state-of-the-art methods on a variety of real-world few-shot molecular property prediction tasks and out-of-domain molecular property prediction and optimization tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                299. <h><a href=https://openreview.net/pdf?id=ayPPc0SyLv1>Do We Really Need Complicated Model Architectures For Temporal Networks?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, Mehrdad Mahdavi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ayPPc0SyLv1>7.33 - [6, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper propose a conceptually and technically simple method for temporal graph link prediction
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ayPPc0SyLv1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recurrent neural network (RNN) and self-attention mechanism (SAM) are the de facto methods to extract spatial-temporal information for temporal graph learning. Interestingly, we found that although both RNN and SAM could lead to a good performance, in practice neither of them is always necessary. In this paper, we propose GraphMixer, a conceptually and technically simple architecture that consists of three components: (1) a link-encoder that is only based on multi-layer perceptrons (MLP) to summarize the information from temporal links, (2) a node-encoder that is only based on neighbor mean-pooling to summarize node information, and (3) an MLP-based link classifier that performs link prediction based on the outputs of the encoders. Despite its simplicity, GraphMixer attains an outstanding performance on temporal link prediction benchmarks with faster convergence and better generalization performance. These results motivate us to rethink the importance of simpler model architecture.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                300. <h><a href=https://openreview.net/pdf?id=rGeZuBRahju>Learning Language Representations with Logical Inductive Bias</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianshu Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rGeZuBRahju>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a novel neural architecture for learning language representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rGeZuBRahju.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transformer architectures have achieved great success in solving natural language tasks, which learn strong language representations from large-scale unlabeled texts. In this paper, we seek to go further beyond and explore a new logical inductive bias for better language representation learning. Logic reasoning is known as a formal methodology to reach answers from given knowledge and facts. Inspired by such a view, we develop a novel neural architecture named FOLNet (First-Order Logic Network), to encode this new inductive bias. We construct a set of neural logic operators as learnable Horn clauses, which are further forward-chained into a fully differentiable neural architecture (FOLNet). Interestingly, we find that the self-attention module in transformers can be composed by two of our neural logic operators, which probably explains their strong reasoning performance. Our proposed FOLNet has the same input and output interfaces as other pretrained models and thus could be pretrained/finetuned by using similar losses. It also allows FOLNet to be used in a plug-and-play manner when replacing other pretrained models. With our logical inductive bias, the same set of ``logic deduction skills&#39;&#39; learned through pretraining are expected to be equally capable of solving diverse downstream tasks. For this reason, FOLNet learns language representations that have much stronger transfer capabilities. Experimental results on several language understanding tasks show that our pretrained FOLNet model outperforms the existing strong transformer-based approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                301. <h><a href=https://openreview.net/pdf?id=Ubc74gTVo3>Self-supervision through Random Segments with Autoregressive Coding (RandSAC)</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianyu Hua, Yonglong Tian, Sucheng Ren, Michalis Raptis, Hang Zhao, Leonid Sigal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Ubc74gTVo3>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Ubc74gTVo3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inspired by the success of self-supervised autoregressive representation learning in natural language (GPT and its variants), and advances in recent visual architecture design with Vision Transformers (ViTs), in this paper, we explore the effects various design choices have on the success of applying such training strategies for visual feature learning. Specifically, we introduce a novel strategy that we call Random Segments with Autoregressive Coding (RandSAC). In RandSAC, we group patch representations (image tokens) into hierarchically arranged segments; within each segment, tokens are predicted in parallel, similar to BERT, while across segment predictions are sequential, similar to GPT. We illustrate that randomized serialization of the segments significantly improves the performance and results in distribution over spatially-long (across-segments) and -short (within-segment) predictions which are effective for feature learning. We illustrate the pertinence of these design choices and explore alternatives on a number of datasets (e.g., CIFAR10, ImageNet). While our pre-training strategy works with vanilla Transformer, we also propose a conceptually simple, but highly effective, addition to the decoder that allows learnable skip-connections to encoder feature layers, which further improves the performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                302. <h><a href=https://openreview.net/pdf?id=u-RuvyDYqCM>The In-Sample Softmax for Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, Martha White</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=u-RuvyDYqCM>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel Bellman operator that avoids bootstrapping on out-of-sample actions. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/u-RuvyDYqCM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reinforcement learning (RL) agents can leverage batches of previously collected data to extract a reasonable control policy. An emerging issue in this offline RL setting, however, is that the bootstrapping update underlying many of our methods suffers from insufficient action-coverage: standard max operator may select a maximal action that has not been seen in the dataset. Bootstrapping from these inaccurate values can lead to overestimation and even divergence. There are a growing number of methods that attempt to approximate an in-sample max, that only uses actions well-covered by the dataset. We highlight a simple fact: it is more straightforward to approximate an in-sample softmax using only actions in the dataset. We show that policy iteration based on the in-sample softmax converges, and that for decreasing temperatures it approaches the in-sample max. We derive an In-Sample Actor-Critic (AC), using this in-sample softmax, and show that it is consistently better or comparable to existing offline RL methods, and is also well-suited to fine-tuning. We release the code at github.com/hwang-ua/inac_pytorch.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                303. <h><a href=https://openreview.net/pdf?id=nG9RF9z1yy3>DiffusER: Diffusion via Edit-based Reconstruction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Machel Reid, Vincent Josua Hellendoorn, Graham Neubig</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nG9RF9z1yy3>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a generally applicable text generative model which takes inspiration from diffusion models and parameterises generation steps as text editing steps without compromising performance and adding flexibility.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nG9RF9z1yy3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DiffusER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models -- a class of models that use a Markov chain of denoising steps to incrementally generate data. DiffusER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DiffusER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                304. <h><a href=https://openreview.net/pdf?id=C2fsSj3ZGiU>Neural Episodic Control with State Abstraction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhuo Li, Derui Zhu, Yujing Hu, Xiaofei Xie, Lei Ma, YAN ZHENG, Yan Song, Yingfeng Chen, Jianjun Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=C2fsSj3ZGiU>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose NECSA, a simple and effective state abstraction-based episodic control containing a more comprehensive episodic memory, a novel state measurement, and a multi-step state analysis.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/C2fsSj3ZGiU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing Deep Reinforcement Learning (DRL) algorithms suffer from sample inefficiency. Generally, episodic control-based approaches are solutions that leverage highly rewarded past experiences to improve sample efficiency of DRL algorithms. However, previous episodic control-based approaches fail to utilize the latent information from the historical behaviors (\eg, state transitions, topological similarities, \etc) and lack scalability during DRL training. This work introduces Neural Episodic Control with State Abstraction (NECSA), a simple but effective state abstraction-based episodic control containing a more comprehensive episodic memory, a novel state evaluation, and a multi-step state analysis. We evaluate our approach to the MuJoCo and Atari tasks in OpenAI gym domains. The experimental results indicate that NECSA achieves higher sample efficiency than the state-of-the-art episodic control-based approaches. Our data and code are available at the project website\footnote{\url{https://sites.google.com/view/drl-necsa}}. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                305. <h><a href=https://openreview.net/pdf?id=oze0clVGPeX>Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Yin Tat Lee, Arturs Backurs, Nenghai Yu, Jiang Bian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oze0clVGPeX>7.33 - [8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Explore the limit of the efficiency of DP-SGD with group-wise clipping
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oze0clVGPeX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of \emph{group-wise clipping}.  To reduce the compute time overhead of private learning, we show that \emph{per-layer clipping}, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest.  While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3.  We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with \emph{per-device clipping} that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with per-device clipping achieves a task performance at $\epsilon=1$ better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                306. <h><a href=https://openreview.net/pdf?id=dCOL0inGl3e>Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yanchao Sun, Ruijie Zheng, Parisa Hassanzadeh, Yongyuan Liang, Soheil Feizi, Sumitra Ganesh, Furong Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dCOL0inGl3e>7.33 - [6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a defense method such that an agent receiving communication in an multi-agent system can be certifiably robust when a subset of communication messages get (arbitrarily) perturbed.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dCOL0inGl3e.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Communication is important in many multi-agent reinforcement learning (MARL) problems for agents to share information and make good decisions. However, when deploying trained communicative agents in a real-world application where noise and potential attackers exist, the safety of communication-based policies becomes a severe issue that is underexplored. Specifically, if communication messages are manipulated by malicious attackers, agents relying on untrustworthy communication may take unsafe actions that lead to catastrophic consequences. Therefore, it is crucial to ensure that agents will not be misled by corrupted communication, while still benefiting from benign communication. In this work, we consider an environment with $N$ agents, where the attacker may arbitrarily change the communication from any $C&lt;\frac{N-1}{2}$ agents to a victim agent. For this strong threat model, we propose a certifiable defense by constructing a message-ensemble policy that aggregates multiple randomly ablated message sets. Theoretical analysis shows that this message-ensemble policy can utilize benign communication while being certifiably robust to adversarial communication, regardless of the attacking algorithm. Experiments in multiple environments verify that our defense significantly improves the robustness of trained policies against various types of attacks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                307. <h><a href=https://openreview.net/pdf?id=OKcJhpQiGiX>Disentanglement of Correlated Factors via Hausdorff Factorized Support</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, Diane Bouchacourt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OKcJhpQiGiX>7.33 - [8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a method that allows for disentangled representation learning not only under the assumption of independent factors of variation but instead fundamentally allows for much more realistic correlations during training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OKcJhpQiGiX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts.
Disentanglement is one promising direction aimed at aligning a model&#39;s representation with the underlying factors generating the data (e.g. color or background). Existing disentanglement methods, however, rely on an often unrealistic assumption: that factors are statistically independent. In reality, factors (like object color and shape) are correlated. To address this limitation, we consider the use of a relaxed disentanglement criterion -- the Hausdorff Factorized Support (HFS) criterion -- that encourages only pairwise factorized support, rather than a factorial distribution, by minimizing a Hausdorff distance. This allows for arbitrary distributions of the factors over their support, including correlations between them. We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over +60% in relative improvement over existing disentanglement methods. In addition, we find that leveraging HFS for representation learning can even facilitate transfer to downstream tasks such as classification under distribution shifts. We hope our original approach and positive empirical results inspire further progress on the open problem of robust generalization. Code available at https://github.com/facebookresearch/disentangling-correlated-factors.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                308. <h><a href=https://openreview.net/pdf?id=kJUS5nD0vPB>Out-of-Distribution Detection and Selective Generation for Conditional Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, Peter J Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kJUS5nD0vPB>7.33 - [8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A simple, fast, effective method for out-of-distribution detection and selective generation for conditional language models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kJUS5nD0vPB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning algorithms typically assume independent and identically distributed samples in training and at test time (IID). Much work has shown that high-performing ML classifiers can degrade significantly and provide overly-confident, wrong classification predictions,  particularly for out-of-distribution (OOD) inputs. Conditional language models (CLMs) are predominantly trained to classify the next token in an output sequence, and may suffer even worse degradation on OOD inputs as the prediction is done auto-regressively over many steps. Furthermore, the space of potential low-quality outputs is larger as arbitrary text can be generated and it is important to know when to trust the generated output. We present a highly accurate and lightweight OOD detection method for CLMs, and demonstrate its effectiveness on abstractive summarization and translation. We also show how our method can be used under the common and realistic setting of distribution shift for selective generation (analogous to selective prediction for classification) of high-quality outputs, while  automatically abstaining from low-quality ones, enabling safer deployment of generative language models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                309. <h><a href=https://openreview.net/pdf?id=4eJ43EN2g6l>SketchKnitter: Vectorized Sketch Generation with Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qiang Wang, Haoge Deng, Yonggang Qi, Da Li, Yi-Zhe Song</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4eJ43EN2g6l>7.33 - [8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4eJ43EN2g6l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We show vectorized sketch generation can be identified as a reversal of the stroke deformation process. This relationship was established by means of a diffusion model that learns data distributions over the stroke-point locations and pen states of real human sketches. Given randomly scattered stroke-points, sketch generation becomes a process of deformation-based denoising, where the generator rectifies positions of stroke points at each timestep to converge at a recognizable sketch. A key innovation was to embed recognizability into the reverse time diffusion process. It was observed that the estimated noise during the reversal process is strongly correlated with sketch classification accuracy. An auxiliary recurrent neural network (RNN) was consequently used to quantify recognizability during data sampling. It follows that, based on the recognizability scores, a sampling shortcut function can also be devised that renders better quality sketches with fewer sampling steps. Finally it is shown that the model can be easily extended to a conditional generation framework, where given incomplete and unfaithful sketches, it yields one that is more visually appealing and with higher recognizability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                310. <h><a href=https://openreview.net/pdf?id=Z3IClM_bzvP>Multi-skill Mobile Manipulation for Object Rearrangement</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiayuan Gu, Devendra Singh Chaplot, Hao Su, Jitendra Malik</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Z3IClM_bzvP>7.25 - [5, 6, 10, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Z3IClM_bzvP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study a modular approach to tackle long-horizon mobile manipulation tasks for object rearrangement, which decomposes a full task into a sequence of subtasks. To tackle the entire task, prior work chains multiple stationary manipulation skills with a point-goal navigation skill, which are learned individually on subtasks. Although more effective than monolithic end-to-end RL policies, this framework suffers from compounding errors in skill chaining, e.g., navigating to a bad location where a stationary manipulation skill can not reach its target to manipulate. To this end, we propose that the manipulation skills should include mobility to have flexibility in interacting with the target object from multiple locations and at the same time the navigation skill could have multiple end points which lead to successful manipulation. We operationalize these ideas by implementing mobile manipulation skills rather than stationary ones and training a navigation skill trained with region goal instead of point goal. We evaluate our multi-skill mobile manipulation method M3 on 3 challenging long-horizon mobile manipulation tasks in the Home Assistant Benchmark (HAB), and show superior performance as compared to the baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                311. <h><a href=https://openreview.net/pdf?id=WlbG820mRH->Fundamental Limits in Formal Verification of Message-Passing Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marco Sälzer, Martin Lange</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WlbG820mRH->7.25 - [8, 10, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove that certain safety properties of MPNN can not be verified formally.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WlbG820mRH-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Output reachability and adversarial robustness are among the most relevant safety properties of neural networks. 
We show that in the context of Message Passing Neural Networks (MPNN), a common Graph Neural Network (GNN) model, 
formal verification is impossible. In particular, we show that output reachability of graph-classifier MPNN, 
working over graphs of unbounded size, non-trivial degree and sufficiently expressive node labels, cannot be verified formally: there
is no algorithm that answers correctly (with yes or no), given an MPNN, whether there exists some valid input to 
the MPNN such that the corresponding output satisfies a given specification. However, we also show that 
output reachability and adversarial robustness of node-classifier MPNN can be verified formally when a limit on
the degree of input graphs is given a priori. We discuss the implications of these results, for the purpose of
obtaining a complete picture of the principle possibility to formally verify GNN, depending on 
the expressiveness of the involved GNN models and input-output specifications.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                312. <h><a href=https://openreview.net/pdf?id=GrpU6dxFmMN>Improving the imputation of missing data with Markov Blanket discovery</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yang Liu, Anthony Constantinou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GrpU6dxFmMN>7.25 - [8, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GrpU6dxFmMN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The process of imputation of missing data typically relies on generative and regression models. These approaches often operate on the unrealistic assumption that all of the data features are directly related with one another, and use all of the available features to impute missing values. In this paper, we propose a novel Markov Blanket discovery approach to determine the optimal feature set for a given variable by considering both observed variables and missingness of partially observed variables to account for systematic missingness. We then incorporate this method to the learning process of the state-of-the-art MissForest imputation algorithm, such that it informs MissForest which features to consider to impute missing values, depending on the variable the missing value belongs to. Experiments across different case studies and multiple imputation algorithms show that the proposed solution improves imputation accuracy, both under random and systematic missingness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                313. <h><a href=https://openreview.net/pdf?id=Zy350cRstc6>Continual evaluation for lifelong learning: Identifying the stability gap</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthias De Lange, Gido M van de Ven, Tinne Tuytelaars</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Zy350cRstc6>7.25 - [8, 8, 8, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Proposing an iteration-based continual evaluation framework for CL, we discover, quantify, and analyse the &#34;stability gap&#34;, a phenomenon where upon learning new tasks, past tasks exhibit substantial but transient performance loss for SOTA CL methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Zy350cRstc6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Time-dependent data-generating distributions have proven to be difficult for gradient-based training of neural networks, as the greedy updates result in catastrophic forgetting of previously learned knowledge. Despite the progress in the field of continual learning to overcome this forgetting, we show that a set of common state-of-the-art methods still suffers from substantial forgetting upon starting to learn new tasks, except that this forgetting is temporary and followed by a phase of performance recovery. We refer to this intriguing but potentially problematic phenomenon as the stability gap. The stability gap had likely remained under the radar due to standard practice in the field of evaluating continual learning models only after each task. Instead, we establish a framework for continual evaluation that uses per-iteration evaluation and we define a new set of metrics to quantify worst-case performance. Empirically we show that experience replay, constraint-based replay, knowledge-distillation, and parameter regularization methods are all prone to the stability gap; and that the stability gap can be observed in class-, task-, and domain-incremental learning benchmarks. Additionally, a controlled experiment shows that the stability gap increases when tasks are more dissimilar. Finally, by disentangling gradients into plasticity and stability components, we propose a conceptual explanation for the stability gap.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                314. <h><a href=https://openreview.net/pdf?id=t9Zd7Oi5JPl>PatchDCT: Patch Refinement for High Quality Instance Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qinrou Wen, Jirui Yang, Xue Yang, Kewei Liang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=t9Zd7Oi5JPl>7.25 - [8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/t9Zd7Oi5JPl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    High-quality instance segmentation has shown emerging importance in computer vision. Without any refinement, DCT-Mask directly generates high-resolution masks by compressed vectors. To further refine masks obtained by compressed vectors, we propose for the first time a compressed vector based multi-stage refinement framework.  However, the vanilla combination does not bring significant gains, because changes in some elements of the DCT vector will affect the prediction of the entire mask. Thus, we propose a simple and novel method named PatchDCT, which separates the mask decoded from a DCT vector into several patches and refines each patch by the designed classifier and regressor. Specifically, the classifier is used to distinguish mixed patches from all patches, and to correct previously mispredicted foreground and background patches. In contrast, the regressor is used for DCT vector prediction of mixed patches, further refining the segmentation quality at boundary locations. Experiments on COCO show that our method achieves 2.0\%, 3.2\%, 4.5\% AP and 3.4\%, 5.3\%, 7.0\% Boundary AP improvements over Mask-RCNN on COCO, LVIS, and Cityscapes, respectively. It also surpasses DCT-Mask by 0.7\%, 1.1\%, 1.3\% AP and 0.9\%, 1.7\%, 4.2\% Boundary AP on COCO, LVIS and Cityscapes. Besides, the performance of PatchDCT is also competitive with other state-of-the-art methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                315. <h><a href=https://openreview.net/pdf?id=SXZr8aDKia>Personalized Federated Learning with Feature Alignment and Classifier Collaboration</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jian Xu, Xinyi Tong, Shao-Lun Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SXZr8aDKia>7.25 - [8, 8, 5, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SXZr8aDKia.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Data heterogeneity is one of the most challenging issues in federated learning, which motivates a variety of approaches to learn personalized models for participating clients. One such approach in deep neural networks based tasks is employing a shared feature representation and learning a customized classifier head for each client. However, previous works do not utilize the global knowledge during local representation learning and also neglect the fine-grained collaboration between local classifier heads, which limits the model generalization ability. In this work, we conduct explicit local-global feature alignment by leveraging global semantic knowledge for learning a better representation. Moreover, we quantify the benefit of classifier combination for each client as a function of the combining weights and derive an optimization problem for estimating optimal weights. Finally, extensive evaluation results on benchmark datasets with various heterogeneous data scenarios demonstrate the effectiveness of our proposed method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                316. <h><a href=https://openreview.net/pdf?id=qNLe3iq2El>Mega: Moving Average Equipped Gated Attention</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, Luke Zettlemoyer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qNLe3iq2El>7.25 - [8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Moving Average Equipped Gated Attention
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qNLe3iq2El.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.  We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                317. <h><a href=https://openreview.net/pdf?id=dKkMnCWfVmm>Multi-Objective Online Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiyan Jiang, Wenpeng Zhang, Shiji Zhou, Lihong Gu, Xiaodong Zeng, Wenwu Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dKkMnCWfVmm>7.25 - [8, 8, 8, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dKkMnCWfVmm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents a systematic study of multi-objective online learning. We first formulate the framework of Multi-Objective Online Convex Optimization, which encompasses a novel multi-objective regret. This regret is built upon a sequence-wise extension of the commonly used discrepancy metric Pareto suboptimality gap in zero-order multi-objective bandits. We then derive an equivalent form of the regret, making it amenable to be optimized via first-order iterative methods. To motivate the algorithm design, we give an explicit example in which equipping OMD with the vanilla min-norm solver for gradient composition will incur a linear regret, which shows that merely regularizing the iterates, as in single-objective online learning, is not enough to guarantee sublinear regrets in the multi-objective setting. To resolve this issue, we propose a novel min-regularized-norm solver that regularizes the composite weights. Combining min-regularized-norm with OMD results in the Doubly Regularized Online Mirror Multiple Descent algorithm. We further derive the multi-objective regret bound for the proposed algorithm, which matches the optimal bound in the single-objective setting. Extensive experiments on several real-world datasets verify the effectiveness of the proposed algorithm.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                318. <h><a href=https://openreview.net/pdf?id=h5OpjGd_lo6>Self-Guided Noise-Free Data Generation for Efficient Zero-Shot Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng Ye, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan Liang, Zhenguo Li, Lingpeng Kong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=h5OpjGd_lo6>7.25 - [5, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a framework to automatically enhance the quality of PLM-generated data for efficient zero-shot learning, without relying on any human annotation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/h5OpjGd_lo6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There is a rising interest in further exploring the zero-shot learning potential of large pre-trained language models (PLMs). A new paradigm called data-generation-based zero-shot learning has achieved impressive success. In this paradigm, the synthesized data from the PLM acts as the carrier of knowledge, which is used to train a task-specific model with orders of magnitude fewer parameters than the PLM, achieving both higher performance and efficiency than prompt-based zero-shot learning methods on PLMs. The main hurdle of this approach is that the synthesized data from PLM usually contains a significant portion of low-quality samples. Fitting on such data will greatly hamper the performance of the task-specific model, making it unreliable for deployment. Previous methods remedy this issue mainly by filtering synthetic data using heuristic metrics(e.g., output confidence), or refining the data with the help of a human expert, which comes with excessive manual tuning or expensive costs. In this paper, we propose a novel noise-robust re-weighting framework SunGen to automatically construct high-quality data for zero-shot classification problems. Our framework features the ability to learn the sample weights indicating data quality without requiring any human annotation. We theoretically and empirically verify the ability of our method to help construct good-quality synthetic datasets. Notably, SunGen-LSTM yields a 9.8% relative improvement than the baseline on average accuracy across eight different established text classification tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                319. <h><a href=https://openreview.net/pdf?id=1mNssCWt_v>STaSy: Score-based Tabular data Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jayoung Kim, Chaejeong Lee, Noseong Park</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1mNssCWt_v>7.25 - [8, 8, 8, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design a score-based generative model for tabular data and apply two training strategies, including the self-paced learning and the proposed fine-tuning method, to stabilize the denoising score matching training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1mNssCWt_v.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Tabular data synthesis is a long-standing research topic in machine learning. Many different methods have been proposed over the past decades, ranging from statistical methods to deep generative methods. However, it has not always been successful due to the complicated nature of real-world tabular data. In this paper, we present a new model named $\textbf{S}$core-based $\textbf{Ta}$bular data $\textbf{Sy}$nthesis ($\texttt{STaSy}$) and its training strategy based on the paradigm of score-based generative modeling. Despite the fact that score-based generative models have resolved many issues in generative models, there still exists room for improvement in tabular data synthesis. Our proposed training strategy includes a self-paced learning technique and a fine-tuning strategy, which further increases the sampling quality and diversity by stabilizing the denoising score matching training. Furthermore, we also conduct rigorous experimental studies in terms of the generative task trilemma: sampling quality, diversity, and time. In our experiments with 15 benchmark tabular datasets and 7 baselines, our method outperforms existing methods in terms of task-dependant evaluations and diversity.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                320. <h><a href=https://openreview.net/pdf?id=FZdJQgy05rz>Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Takashi Ishida, Ikko Yamane, Nontawat Charoenphakdee, Gang Niu, Masashi Sugiyama</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FZdJQgy05rz>7.25 - [8, 8, 5, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A simple and direct Bayes error estimator that just takes the mean of the labels that show uncertainty of the classes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FZdJQgy05rz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer may have reached, or is about to reach, the Bayes error for benchmark datasets. Finally, we discuss how we can study the inherent difficulty of the acceptance/rejection decision for scientific articles, by estimating the Bayes error of the ICLR papers from 2017 to 2023.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                321. <h><a href=https://openreview.net/pdf?id=8JCg5xJCTPR>Provable Memorization Capacity of Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junghwan Kim, Michelle Kim, Barzan Mozafari</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8JCg5xJCTPR>7.25 - [8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide the memorization capacity of Transformer architecture in sequence input.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8JCg5xJCTPR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Quantifying memorization capacity is essential for understanding the expressiveness and generalizability of deep learning model architectures. However, the memorization capacity of the Transformer architecture has yet to be explored. In this work, we present the first study of the memorization capacity of the Transformer architecture. We prove that Transformers are capable of memorizing $N$ sequence-to-sequence mappings of length $n$ with $d$-dimensional input tokens using $\tilde{O}(d + n + \sqrt{nN})$ parameters. Our theory supports memorization both with and without permutation equivariance, utilizing positional encodings in the latter case. Building on our theory, we also analyze the memorization capacity of Transformers in the sequence classification and language modeling tasks. To verify these theoretical findings, we conduct experiments analyzing the memorization capacity of Transformers in the natural language domain.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                322. <h><a href=https://openreview.net/pdf?id=3aQs3MCSexD>How Much Data Are Augmentations Worth?  An Investigation into Scaling Laws, Invariance, and Implicit Regularization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jonas Geiping, Micah Goldblum, Gowthami Somepalli, Ravid Shwartz-Ziv, Tom Goldstein, Andrew Gordon Wilson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3aQs3MCSexD>7.25 - [8, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We uncover mechanisms by which data augmentations regularize training and inform the relationship between augmentations and extra data, invariance, stochasticity, and flatness.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3aQs3MCSexD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the clear performance benefits of data augmentations, little is known about why they are so effective. In this paper, we disentangle several key mechanisms through which data augmentations operate. Establishing an exchange rate between augmented and additional real data, we find that in out-of-distribution testing scenarios, augmentations which yield samples that are diverse, but inconsistent with the data distribution can be even more valuable than additional training data. Moreover, we find that data augmentations which encourage invariances can be more valuable than invariance alone, especially on small and medium sized training sets. Following this observation, we show that augmentations induce additional stochasticity during training, effectively flattening the loss landscape.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                323. <h><a href=https://openreview.net/pdf?id=QAV2CcLEDh>MaskViT: Masked Visual Pre-Training for Video Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Agrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Martín-Martín, Li Fei-Fei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QAV2CcLEDh>7.25 - [5, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose to learn a Transformer based video prediction model via masked visual modeling. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QAV2CcLEDh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to predict future visual observations conditioned on past observations and motor commands can enable embodied agents to plan solutions to a variety of tasks in complex environments. This work shows that we can create good video prediction models by pre-training transformers via masked visual modeling. Our approach, named MaskViT, is based on two simple design decisions. First, for memory and training efficiency, we use two types of window attention: spatial and spatiotemporal. Second, during training, we mask a variable percentage of tokens instead of a fixed mask ratio. For inference, MaskViT generates all tokens via iterative refinement where we incrementally decrease the masking ratio following a mask scheduling function. On several datasets we demonstrate that MaskViT outperforms prior works in video prediction, is parameter efficient, and can generate high resolution videos ($256 \times $256). Further, we demonstrate the benefits of inference speedup (up to $512 \times$) due to iterative decoding by using MaskViT for planning on a real robot. Our work suggests that we can endow embodied agents with powerful predictive models by leveraging the general framework of masked visual modeling with minimal domain knowledge. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                324. <h><a href=https://openreview.net/pdf?id=HcUf-QwZeFh>A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hiroki Furuta, Yusuke Iwasawa, Yutaka Matsuo, Shixiang Shane Gu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HcUf-QwZeFh>7.25 - [5, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We explore a method for learning a single policy that manipulates various forms of agents to various goal positions by distilling a large amount of proficient behavioral data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HcUf-QwZeFh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other domains such as continuous control. In this work, we explore a method for learning a single policy that manipulates various forms of agents to solve various tasks by distilling a large amount of proficient behavioral data. In order to align input-output (IO) interface among multiple tasks and diverse agent morphologies while preserving essential 3D geometric relations, we introduce morphology-task graph, which treats observations, actions and goals/task in a unified graph representation. We also develop MxT-Bench for fast large-scale behavior generation, which supports procedural generation of diverse morphology-task combinations with a minimal blueprint and hardware-accelerated simulator. Through efficient representation and architecture selection on MxT-Bench, we find out that a morphology-task graph representation coupled with Transformer architecture improves the multi-task performances compared to other baselines including recent discrete tokenization, and provides better prior knowledge for zero-shot transfer or sample efficiency in downstream multi-task imitation learning. Our work suggests large diverse offline datasets, unified IO representation, and policy representation and architecture selection through supervised learning form a promising approach for studying and advancing morphology-task generalization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                325. <h><a href=https://openreview.net/pdf?id=8Oun8ZUVe8N>Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, Kaisheng Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8Oun8ZUVe8N>7.25 - [5, 10, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper shows that pretrained 2D image Transformers can help self-supervised 3D representation learning by training autoencoders as cross-modal teachers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8Oun8ZUVe8N.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The success of deep learning heavily relies on large-scale data with comprehensive labels, which is more expensive and time-consuming to fetch in 3D compared to 2D images or natural languages. This promotes the potential of utilizing models pretrained with data more than 3D as teachers for cross-modal knowledge transferring. In this paper, we revisit masked modeling in a unified fashion of knowledge distillation, and we show that foundational Transformers pretrained with 2D images or natural languages can help self-supervised 3D representation learning through training Autoencoders as Cross-Modal Teachers (ACT). The pretrained Transformers are transferred as cross-modal 3D teachers using discrete variational autoencoding self-supervision, during which the Transformers are frozen with prompt tuning for better knowledge inheritance. The latent features encoded by the 3D teachers are used as the target of masked point modeling, wherein the dark knowledge is distilled to the 3D Transformer students as foundational geometry understanding. Our ACT pretrained 3D learner achieves state-of-the-art generalization capacity across various downstream benchmarks, e.g., 88.21% overall accuracy on ScanObjectNN. Codes have been released at https://github.com/RunpeiDong/ACT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                326. <h><a href=https://openreview.net/pdf?id=FKXVK9dyMM>LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xuheng Cai, Chao Huang, Lianghao Xia, Xubin Ren</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FKXVK9dyMM>7.25 - [8, 8, 8, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new lightweight graph contrastive learning approach to enhance recommender systems
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FKXVK9dyMM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural network (GNN) is a powerful learning approach for graph-based recommender systems. Recently, GNNs integrated with contrastive learning have shown superior performance in recommendation with their data augmentation schemes, aiming at dealing with highly sparse data. Despite their success, most existing graph contrastive learning methods either perform stochastic augmentation (e.g., node/edge perturbation) on the user-item interaction graph, or rely on the heuristic-based augmentation techniques (e.g., user clustering) for generating contrastive views. We argue that these methods cannot well preserve the intrinsic semantic structures and are easily biased by the noise perturbation. In this paper, we propose a simple yet effective graph contrastive learning paradigm LightGCL that mitigates these issues impairing the generality and robustness of CL-based recommenders. Our model exclusively utilizes singular value decomposition for contrastive augmentation, which enables the unconstrained structural refinement with global collaborative relation modeling. Experiments conducted on several benchmark datasets demonstrate the significant improvement in performance of our model over the state-of-the-arts. Further analyses demonstrate the superiority of LightGCL&#39;s robustness against data sparsity and popularity bias. The source code of our model is available at https://github.com/HKUDS/LightGCL.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                327. <h><a href=https://openreview.net/pdf?id=rZ-wylY5VI>Programmatically Grounded, Compositionally Generalizable Robotic Manipulation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Renhao Wang, Jiayuan Mao, Joy Hsu, Hang Zhao, Jiajun Wu, Yang Gao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rZ-wylY5VI>7.25 - [5, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We parse and execute semantically grounded neural programs for robotic manipulation, enabling better zero-shot and compositional generalizable to new manipulation behaviors.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rZ-wylY5VI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Robots operating in the real world require both rich manipulation skills as well as the ability to semantically reason about when to apply those skills. Towards this goal, recent works have integrated semantic representations from large-scale pretrained vision-language (VL) models into manipulation models, imparting them with more general reasoning capabilities. However, we show that the conventional {\it pretraining-finetuning} pipeline for integrating such representations entangles the learning of domain-specific action information and domain-general visual information, leading to less data-efficient training and poor generalization to unseen objects and tasks. To this end, we propose \ours, a {\it modular} approach to better leverage pretrained VL models by exploiting the syntactic and semantic structures of language instructions. Our framework uses a semantic parser to recover an executable program, composed of functional modules grounded on vision and action across different modalities. Each functional module is realized as a combination of deterministic computation and learnable neural networks. Program execution produces parameters to general manipulation primitives for a robotic end-effector. The entire modular network can be trained with end-to-end imitation learning objectives. Experiments show that our model successfully disentangles action and perception, translating to improved zero-shot and compositional generalization in a variety of manipulation behaviors. Project webpage at: \url{https://progport.github.io}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                328. <h><a href=https://openreview.net/pdf?id=ZCTvSF_uVM4>A Theoretical Framework for Inference and Learning in Predictive Coding Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Beren Millidge, Yuhang Song, Tommaso Salvatori, Thomas Lukasiewicz, Rafal Bogacz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZCTvSF_uVM4>7.25 - [8, 10, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a comprehensive mathematical framework for understanding predictive coding networks including novel links with target propagation and expectation maximisation and prove that they converge to the same minima as backdrop
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZCTvSF_uVM4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Predictive coding (PC) is an influential theory in computational neuroscience, which argues that the cortex forms unsupervised world models by implementing a hierarchical process of prediction error minimization. PC networks (PCNs) are trained in two phases. First, neural activities are updated to optimize the network&#39;s response to external stimuli. Second, synaptic weights are updated to consolidate this change in activity --- an algorithm called \emph{prospective configuration}. While previous work has shown how in various limits, PCNs can be found to approximate backpropagation (BP), recent work has demonstrated that PCNs operating in this standard regime, which does not approximate BP, nevertheless obtain competitive training and generalization performance to BP-trained networks while outperforming them on various tasks. However, little is understood theoretically about the properties and dynamics of PCNs in this regime. In this paper, we provide a comprehensive theoretical analysis of the properties of PCNs trained with prospective configuration. We first derive analytical results concerning the inference equilibrium for PCNs and a previously unknown close connection relationship to target propagation (TP). Secondly, we provide a theoretical analysis of learning in PCNs as a variant of generalized expectation-maximization and use that to prove the convergence of PCNs to critical points of the BP loss function, thus showing that deep PCNs can, in theory, achieve the same generalization performance as BP, while maintaining their unique advantages.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                329. <h><a href=https://openreview.net/pdf?id=HnlCZATopvr>Transformer Meets Boundary Value Inverse Problems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruchi Guo, Shuhao Cao, Long Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HnlCZATopvr>7.25 - [8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We argue that, from both theoretical and experimental perspective, the attention mechanism is a structure-conforming neural architecture for learning the PDE-based boundary value inverse problems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HnlCZATopvr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A Transformer-based deep direct sampling method is proposed for electrical impedance tomography, a well-known severely ill-posed nonlinear boundary value inverse problem. A real-time reconstruction is achieved by evaluating the learned inverse operator between carefully designed data and the reconstructed images. An effort is made to give a specific example to a fundamental question: whether and how one can benefit from the theoretical structure of a mathematical problem to develop task-oriented and structure-conforming deep neural networks? Specifically, inspired by direct sampling methods for inverse problems, the 1D boundary data in different frequencies are preprocessed by a partial differential equation-based feature map to yield 2D harmonic extensions as different input channels. Then, by introducing learnable non-local kernels, the direct sampling is recast to a modified attention mechanism. The new method achieves superior accuracy over its predecessors and contemporary operator learners and shows robustness to noises in benchmarks. 
This research shall strengthen the insights that, despite being invented for natural language processing tasks, the attention mechanism offers great flexibility to be modified in conformity with the a priori mathematical knowledge, which ultimately leads to the design of more physics-compatible neural architectures. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                330. <h><a href=https://openreview.net/pdf?id=PvLnIaJbt9>Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, Sara Hooker</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PvLnIaJbt9>7.25 - [8, 5, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our work provides a unified and efficient framework for Metadata Archaeology -- uncovering and inferring metadata of examples in a dataset
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PvLnIaJbt9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern machine learning research relies on relatively few carefully curated datasets. Even in these datasets, and typically in `untidy&#39; or raw data, practitioners are faced with significant issues of data quality and diversity which can be prohibitively labor intensive to address. Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels. Our work is orthogonal to these methods: we instead focus on providing a unified and efficient framework for Metadata Archaeology -- uncovering and inferring metadata of examples in a dataset. We curate different subsets of data that might exist in a dataset (e.g. mislabeled, atypical, or out-of-distribution examples) using simple transformations, and leverage differences in learning dynamics between these probe suites to infer metadata of interest. Our method is on par with far more sophisticated mitigation methods across different tasks: identifying and correcting mislabeled examples, classifying minority-group samples, prioritizing points relevant for training and enabling scalable human auditing of relevant examples.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                331. <h><a href=https://openreview.net/pdf?id=7jk5gWjC18M>Chasing All-Round Graph Representation Robustness: Model, Training, and Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chunhui Zhang, Yijun Tian, Mingxuan Ju, Zheyuan Liu, Yanfang Ye, Nitesh Chawla, Chuxu Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7jk5gWjC18M>7.25 - [8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We identify a fundamental issue in graph adversarial learning and then propose a novel method to enlarge the model capacity and enrich the representation diversity of adversarial samples.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7jk5gWjC18M.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph Neural Networks (GNNs) have achieved state-of-the-art results on a variety of graph learning tasks, however, it has been demonstrated that they are vulnerable to adversarial attacks, raising serious security concerns. A lot of studies have been developed to train GNNs in a noisy environment and increase their robustness against adversarial attacks. However, existing methods have not uncovered a principled difficulty: the convoluted mixture distribution between clean and attacked data samples, which leads to sub-optimal model design and limits their frameworks’ robustness. In this work, we first begin by identifying the root cause of mixture distribution, then, for tackling it, we propose a novel method GAME - Graph Adversarial Mixture of Experts to enlarge the model capacity and enrich the representation diversity of adversarial samples, from three perspectives of model, training, and optimization. Specifically, we first propose a plug-and- play GAME layer that can be easily incorporated into any GNNs and enhance their adversarial learning capabilities. Second, we design a decoupling-based graph adversarial training in which the component of the model used to generate adversarial graphs is separated from the component used to update weights. Third, we introduce a graph diversity regularization that enables the model to learn diverse representation and further improves model performance. Extensive experiments demonstrate the effectiveness and advantages of GAME over the state-of-the-art adversarial training methods across various datasets given different attacks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                332. <h><a href=https://openreview.net/pdf?id=HTJE5Krui0g>Softened Symbol Grounding for Neuro-symbolic Systems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxing Ma, Jian L\&#34;{u}</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HTJE5Krui0g>7.25 - [10, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HTJE5Krui0g.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neuro-symbolic learning generally consists of two separated worlds, i.e., neural network training and symbolic constraint solving, 
whose success hinges on symbol grounding, a fundamental problem in AI. This paper presents a novel, softened symbol grounding process, bridging the gap between the two worlds, and resulting in an effective and efficient neuro-symbolic learning framework. Technically, the framework features (1) modeling of symbol solution states as a Boltzmann distribution, which avoids expensive state searching and facilitates mutually beneficial interactions between network training and symbolic reasoning; (2) a new MCMC technique leveraging projection and SMT solvers, which efficiently samples from disconnected symbol solution spaces; (3) an annealing mechanism that can escape from sub-optimal symbol groundings. Experiments with three representative neuro-symbolic learning tasks demonstrate that, owing to its superior symbol grounding capability, our framework successfully solves problems well beyond the frontier of the existing proposals. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                333. <h><a href=https://openreview.net/pdf?id=z9C5dGip90>Phase2vec: dynamical systems embedding with a physics-informed convolutional network</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matt Ricci, Noa Moriel, Zoe Piran, Mor Nitzan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=z9C5dGip90>7.25 - [8, 8, 8, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Unsupervised framework for learning high-quality, physically-meaningful embeddings of dynamical systems. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/z9C5dGip90.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dynamical systems are found in innumerable forms across the physical and biological sciences, yet all these systems fall naturally into equivalence classes: conservative or dissipative, stable or unstable, compressible or incompressible. Predicting these classes from data remains an essential open challenge in computational physics on which existing time-series classification methods struggle. Here, we propose, phase2vec, an embedding method that learns high-quality, physically-meaningful representations of low-dimensional dynamical systems without supervision. Our embeddings are produced by a convolutional backbone that extracts geometric features from flow data and minimizes a physically-informed vector field reconstruction loss. The trained architecture can not only predict the equations of unseen data, but also produces embeddings that encode meaningful physical properties of input data (e.g. stability of fixed points, conservation of energy, and the incompressibility of flows) more faithfully than standard blackbox classifiers and state-of-the-art time series classification techniques. We additionally apply our embeddings to the analysis of meteorological data, showing we can detect climatically meaningful features. Collectively, our results demonstrate the viability of embedding approaches for the discovery of dynamical features in physical systems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                334. <h><a href=https://openreview.net/pdf?id=MYEap_OcQI>Does Zero-Shot Reinforcement Learning Exist?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ahmed Touati, Jérémy Rapin, Yann Ollivier</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MYEap_OcQI>7.25 - [10, 8, 3, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We revisit zero-shot RL based on successor representations, we introduce improved losses and new models and evaluate them systematically on the unsupervised RL benchmark.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MYEap_OcQI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A zero-shot RL agent is an agent that can solve any RL task in a given environment, instantly with no additional planning or learning, after an initial reward-free learning phase. This marks a shift from the reward-centric RL paradigm towards controllable agents that can follow arbitrary instructions in an environment. Current RL agents can solve families of related tasks at best, or require planning anew for each task. Strategies for approximate zero-shot RL have been suggested using successor features (SFs) (Borsa et al., 2018) or forward-backward (FB) representations (Touati &amp; Ollivier, 2021), but testing has been limited. 
After clarifying the relationships between these schemes, we introduce improved losses and new SF models, and test the viability of zero-shot RL schemes systematically on tasks from the Unsupervised RL benchmark (Laskin et al., 2021). To disentangle universal representation learning from exploration, we work in an offline setting and repeat the tests on several existing replay buffers.
SFs appear to suffer from the choice of the elementary state features. SFs with Laplacian eigenfunctions do well, while SFs based on auto-encoders, inverse curiosity, transition models, low-rank transition matrix, contrastive learning, or diversity (APS),  perform unconsistently. In contrast, FB representations jointly learn the elementary and successor features from a single, principled criterion. They perform best and consistently across the board, reaching $85\%$ of supervised RL performance with a good replay buffer, in a zero-shot manner.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                335. <h><a href=https://openreview.net/pdf?id=vSVLM2j9eie>Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yunhao Zhang, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vSVLM2j9eie>7.25 - [8, 5, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose Crossformer, a Transformer-based model that explicitly utilizes cross-dimension dependency for multivariate time series forecasting.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vSVLM2j9eie.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently many deep models have been proposed for multivariate time series (MTS) forecasting. In particular, Transformer-based models have shown great potential because they can capture long-term dependency. However, existing Transformer-based models mainly focus on modeling the temporal dependency (cross-time dependency) yet often omit the dependency among different variables (cross-dimension dependency), which is critical for MTS forecasting. To fill the gap, we propose Crossformer, a Transformer-based model utilizing cross-dimension dependency for MTS forecasting. In Crossformer, the input MTS is embedded into a 2D vector array through the Dimension-Segment-Wise (DSW) embedding to preserve time and dimension information. Then the Two-Stage Attention (TSA) layer is proposed to efficiently capture the cross-time and cross-dimension dependency. Utilizing DSW embedding and TSA layer, Crossformer establishes a Hierarchical Encoder-Decoder (HED) to use the information at different scales for the final forecasting. Extensive experimental results on six real-world datasets show the effectiveness of Crossformer against previous state-of-the-arts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                336. <h><a href=https://openreview.net/pdf?id=g2YraF75Tj>Towards Stable Test-time Adaptation in Dynamic Wild World</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, Mingkui Tan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=g2YraF75Tj>7.25 - [5, 8, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Propose a Sharpness-aware and Reliable entropy minimization method to make online test-time adaptation stable under wild test scenarios 1) small batch sizes; 2) mixed distribution shifts; 3) imbalanced online label distribution shifts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/g2YraF75Tj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, i.e., group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, i.e., assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably than prior methods and is computationally efficient under the above wild test scenarios. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                337. <h><a href=https://openreview.net/pdf?id=HmPOzJQhbwg>ResAct: Reinforcing Long-term Engagement in Sequential Recommendation with Residual Actor</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wanqi Xue, Qingpeng Cai, Ruohan Zhan, Dong Zheng, Peng Jiang, Kun Gai, Bo An</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HmPOzJQhbwg>7.25 - [5, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel paradigm to reinforce long-term engagement in sequential recommendation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HmPOzJQhbwg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Long-term engagement is preferred over immediate engagement in sequential recommendation as it directly affects product operational metrics such as daily active users (DAUs) and dwell time. Meanwhile, reinforcement learning (RL) is widely regarded as a promising framework for optimizing long-term engagement in sequential recommendation. However, due to expensive online interactions, it is very difficult for RL algorithms to perform state-action value estimation, exploration and feature extraction when optimizing long-term engagement. In this paper, we propose ResAct which seeks a policy that is close to, but better than, the online-serving policy. In this way, we can collect sufficient data near the learned policy so that state-action values can be properly estimated, and there is no need to perform online exploration. ResAct optimizes the policy by first reconstructing the online behaviors and then improving it via a Residual Actor. To extract long-term information, ResAct utilizes two information-theoretical regularizers to confirm the expressiveness and conciseness of features. We conduct experiments on a benchmark dataset and a large-scale industrial dataset which consists of tens of millions of recommendation requests. Experimental results show that our method significantly outperforms the state-of-the-art baselines in various long-term engagement optimization tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                338. <h><a href=https://openreview.net/pdf?id=6H_uOfcwiVh>A Kernel Perspective of Skip Connections in Convolutional Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Barzilai, Amnon Geifman, Meirav Galun, Ronen Basri</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6H_uOfcwiVh>7.25 - [8, 8, 8, 5] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6H_uOfcwiVh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Over-parameterized residual networks (ResNets) are amongst the most successful convolutional neural architectures for image processing. Here we study their properties through their Gaussian Process and Neural Tangent kernels. We derive explicit formulas for these kernels, analyze their spectra, and provide bounds on their implied condition numbers. Our results indicate that (1) with ReLU activation, the eigenvalues of these residual kernels decay polynomially at a similar rate compared to the same kernels when skip connections are not used, thus maintaining a similar frequency bias; (2) however, residual kernels are more locally biased. Our analysis further shows that the matrices obtained by these residual kernels yield favorable condition numbers at finite depths than those obtained without the skip connections, enabling therefore faster convergence of training with gradient descent.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                339. <h><a href=https://openreview.net/pdf?id=hy0a5MMPUv>In-context Reinforcement Learning with Algorithm Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, maxime gazeau, Himanshu Sahni, Satinder Singh, Volodymyr Mnih</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hy0a5MMPUv>7.25 - [5, 6, 8, 10] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present Algorithm Distillation, a method that outputs an in-context RL algorithm by treating learning to reinforcement learn as a sequential prediction problem.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hy0a5MMPUv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                340. <h><a href=https://openreview.net/pdf?id=UKr0MwZM6fL>Building a Subspace of Policies for Scalable Continual Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer, Roberta Raileanu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UKr0MwZM6fL>7.2 - [6, 6, 8, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a continual reinforcement learning method that incrementally builds a subspace of policies and adaptively prune it to preserve a good trade-off between model size and performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UKr0MwZM6fL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to continuously acquire new knowledge and skills is crucial for autonomous agents. Existing methods are typically based on either fixed-size models that struggle to learn a large number of diverse behaviors, or growing-size models that scale poorly with the number of tasks. In this work, we aim to strike a better balance between scalability and performance by designing a method whose size grows adaptively depending on the task sequence. We introduce Continual Subspace of Policies (CSP), a new approach that incrementally builds a subspace of policies for training a reinforcement learning agent on a sequence of tasks. The subspace&#39;s high expressivity allows CSP to perform well for many different tasks while growing more slowly than the number of tasks. Our method does not suffer from forgetting and also displays positive transfer to new tasks. CSP outperforms a number of popular baselines on a wide range of scenarios from two challenging domains, Brax (locomotion) and Continual World (robotic manipulation). Interactive visualizations of the subspace can be found at https://share.streamlit.io/continual-subspace/policies/main.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                341. <h><a href=https://openreview.net/pdf?id=uzFQpkEzOo>Depth Separation with Multilayer Mean-Field Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yunwei Ren, Mo Zhou, Rong Ge</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uzFQpkEzOo>7.2 - [8, 8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that, using gradient flow, 3-layer networks can efficiently learn a function that no 2-layer networks can efficiently approximate.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uzFQpkEzOo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Depth separation—why a deeper network is more powerful than a shallow one—has been a major problem in deep learning theory. Previous results often focus on representation power, for example, Safran et al. (2019) constructed a function that is easy to approximate using a 3-layer network but not approximable by any 2-layer network. In this paper, we show that this separation is in fact algorithmic: one can learn the function constructed by Safran et al. (2019) using an overparametrized network with polynomially many neurons efﬁciently. Our result relies on a new way of extending the mean-ﬁeld limit to multilayer networks, and a decomposition of loss that factors out the error introduced by the discretization of inﬁnite-width mean-ﬁeld networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                342. <h><a href=https://openreview.net/pdf?id=i9UlAr1T_xl>SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sheng Li, Geng Yuan, Yue Dai, Youtao Zhang, Yanzhi Wang, Xulong Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=i9UlAr1T_xl>7.2 - [8, 6, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/i9UlAr1T_xl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user&#39;s demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation&#39;&#39; layer freezing for different networks during training processes. To this end, we propose a generic and efficient training framework (SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer freezing, which can automatically select the appropriate layers to freeze without compromising accuracy. Experimental results show that SmartFRZ effectively reduces the amount of computation in training and achieves significant training acceleration, and outperforms the state-of-the-art layer freezing approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                343. <h><a href=https://openreview.net/pdf?id=WbxHAzkeQcn>Neural Networks and the Chomsky Hierarchy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, Pedro A Ortega</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WbxHAzkeQcn>7.2 - [6, 8, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Large-scale empirical study to determine the computational complexity class of a number of neural network architectures, which allows forecasting limitations on generalization capabilities.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WbxHAzkeQcn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (20&#39;910 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                344. <h><a href=https://openreview.net/pdf?id=q8vgHfPdoQP>When to Make and Break Commitments?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alihan Hüyük, Zhaozhi Qian, Mihaela van der Schaar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=q8vgHfPdoQP>7.2 - [8, 8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/q8vgHfPdoQP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In many scenarios, decision-makers must commit to long-term actions until their resolution before receiving the payoff of said actions, and usually, staying committed to such actions incurs continual costs. For instance, in healthcare, a newly-discovered treatment cannot be marketed to patients until a clinical trial is conducted, which both requires time and is also costly. Of course in such scenarios, not all commitments eventually pay off. For instance, a clinical trial might end up failing to show efficacy. Given the time pressure created by the continual cost of keeping a commitment, we aim to answer: When should a decision-maker break a commitment that is likely to fail—either to make an alternative commitment or to make no further commitments at all? First, we formulate this question as a new type of optimal stopping/switching problem called the optimal commitment problem (OCP). Then, we theoretically analyze OCP, and based on the insights we gain, propose a practical algorithm for solving it. Finally, we empirically evaluate the performance of our algorithm in running clinical trials with subpopulation selection.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                345. <h><a href=https://openreview.net/pdf?id=aFzaXRImWE>A Holistic View of Label Noise Transition Matrix in Deep Learning and Beyond</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>LIN Yong, Renjie Pi, WEIZHONG ZHANG, Xiaobo Xia, Jiahui Gao, Xiao Zhou, Tongliang Liu, Bo Han</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aFzaXRImWE>7.2 - [8, 6, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aFzaXRImWE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we explore learning statistically consistent classifiers under label noise by estimating the noise transition matrix T. We first provide a holistic view of existing T-estimation methods including those with or without anchor point assumptions.  We unified them into the Minimum Geometric Envelope Operator (MGEO) framework, which tries to find the smallest T (in terms of a certain metric) that elicits a convex hull to enclose the posteriors of all the training data. Although MGEO methods show appealing theoretical properties and empirical results, we find them prone to failing when the noisy posterior estimation is imperfect, which is inevitable in practice. Specifically, we show that MGEO methods are in-consistent even with infinite samples if the noisy posterior is not estimated accurately. In view of this, we make the first effort to address this issue by proposing a novel T-estimation framework via the lens of bilevel optimization, and term it RObust Bilevel OpTimzation (ROBOT). ROBOT paves a new road beyond MGEO framework, which enjoys strong theoretical properties: identifibility, consistency and finite-sample generalization guarantees. Notably, ROBOT neither requires the perfect posterior estimation nor assumes the existence of anchor points. We further theoretically demonstrate that ROBOT is more robust in the case where MGEO methods fail. Experimentally, our framework also shows superior performance across multiple benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                346. <h><a href=https://openreview.net/pdf?id=7L2mgi0TNEP>$\rm A^2Q$: Aggregation-Aware Quantization for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyu Zhu, Fanrong Li, Zitao Mo, Qinghao Hu, Gang Li, Zejian Liu, Xiaoyao Liang, Jian Cheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7L2mgi0TNEP>7.2 - [8, 6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an Aggregation-Aware mixed-precision Quantization method that fully utilizes the  property of GNNs, achieving up to $2\times$ speedup and $11.4\%$ accuracy improvement compared to the state-of-the-art quantization method on GNNs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7L2mgi0TNEP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As graph data size increases, the vast latency and memory consumption during inference pose a significant challenge to the real-world deployment of Graph Neural Networks (GNNs). While quantization is a powerful approach to reducing GNNs complexity, most previous works on GNNs quantization fail to exploit the unique characteristics of GNNs, suffering from severe accuracy degradation. Through an in-depth analysis of the topology of GNNs, we observe that the topology of the graph leads to significant differences between nodes, and most of the nodes in a graph appear to have a small aggregation value. Motivated by this, in this paper, we propose the Aggregation-Aware mixed-precision Quantization ($\rm A^2Q$) for GNNs, where an appropriate bitwidth is automatically learned and assigned to each node in the graph. To mitigate the vanishing gradient problem caused by sparse connections between nodes, we propose a Local Gradient method to serve the quantization error of the node features as the supervision during training. We also develop a Nearest Neighbor Strategy to deal with the generalization on unseen graphs. Extensive experiments on eight public node-level and graph-level datasets demonstrate the generality and robustness of our proposed method. Compared to the FP32 models, our method can achieve up to $18.8\times$ (i.e., 1.70bits) compression ratio with negligible accuracy degradation. Moreover, compared to the state-of-the-art quantization method, our method can achieve up to $11.4\%$ and $9.5\%$ accuracy improvements on the node-level and graph-level tasks, respectively, and up to $2\times$ speedup on a dedicated hardware accelerator.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                347. <h><a href=https://openreview.net/pdf?id=aRTKuscKByJ>Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuancheng Xu, Yanchao Sun, Micah Goldblum, Tom Goldstein, Furong Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aRTKuscKByJ>7.2 - [6, 8, 10, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aRTKuscKByJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The robustness of a deep classifier can be characterized by its margins: the decision boundary&#39;s distances to natural data points. However, it is unclear whether existing robust training methods effectively increase the margin for each vulnerable point during training. To understand this, we propose a continuous-time framework for quantifying the relative speed of the decision boundary with respect to each individual point. Through visualizing the moving speed of the decision boundary under Adversarial Training, one of the most effective robust training algorithms, a surprising moving-behavior is revealed: the decision boundary moves away from some vulnerable points but simultaneously moves closer to others, decreasing their margins. To alleviate these conflicting dynamics of the decision boundary, we propose Dynamics-aware Robust Training (DyART), which encourages the decision boundary to engage in movement that prioritizes increasing smaller margins. In contrast to prior works, DyART directly operates on the margins rather than their indirect approximations, allowing for more targeted and effective robustness improvement. Experiments on the CIFAR-10 and Tiny-ImageNet datasets verify that DyART alleviates the conflicting dynamics of the decision boundary and obtains improved robustness under various perturbation sizes compared to the state-of-the-art defenses. Our code is available at https://github.com/Yuancheng-Xu/Dynamics-Aware-Robust-Training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                348. <h><a href=https://openreview.net/pdf?id=U_T8-5hClV>A Primal-Dual Framework for Transformers and Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tan Minh Nguyen, Tam Minh Nguyen, Nhat Ho, Andrea L. Bertozzi, Richard Baraniuk, Stanley Osher</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=U_T8-5hClV>7.2 - [8, 8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that the self-attention corresponds to the support vector expansion derived from a support vector regression problem and provide a principled framework for constructing new attention mechanisms from popular neural network layers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/U_T8-5hClV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-attention is key to the remarkable success of transformers in sequence modeling tasks including many applications in natural language processing and computer vision. Like neural network layers, these attention mechanisms are often developed by heuristics and experience. To provide a principled framework for constructing attention layers in transformers, we show that the self-attention corresponds to the support vector expansion derived from a support vector regression problem, whose primal formulation has the form of a neural network layer. Using our framework, we derive popular attention layers used in practice and propose two new attentions: 1) the Batch Normalized Attention (Attention-BN) derived from the batch normalization layer and 2)  the Attention with Scaled Head (Attention-SH) derived from using less training data to fit the SVR model. We empirically demonstrate the advantages of the Attention-BN and Attention-SH in reducing head redundancy, increasing the model&#39;s accuracy, and improving the model&#39;s efficiency in a variety of practical applications including image and time-series classification. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                349. <h><a href=https://openreview.net/pdf?id=ZMz-sW6gCLF>Energy-Inspired Self-Supervised Pretraining for Vision Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ze Wang, Jiang Wang, Zicheng Liu, Qiang Qiu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZMz-sW6gCLF>7.17 - [8, 6, 10, 8, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZMz-sW6gCLF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Motivated by the fact that forward and backward passes of a deep network naturally form symmetric mappings between input and output representations, we introduce a simple yet effective self-supervised vision model pretraining framework inspired by energy-based models (EBMs). In the proposed framework, we model energy estimation and data restoration as the forward and backward passes of a single network without any auxiliary components, e.g., an extra decoder. For the forward pass, we fit a network to an energy function that assigns low energy scores to samples that belong to an unlabeled dataset, and high energy otherwise. For the backward pass, we restore data from corrupted versions iteratively using gradient-based optimization along the direction of energy minimization. In this way, we naturally fold the encoder-decoder architecture widely used in masked image modeling into the forward and backward passes of a single vision model. Our framework accepts a wide range of pretext tasks with different data corruption methods, and permits models to be pretrained from masked image modeling, patch sorting, and image restoration, including super-resolution, denoising, and colorization. We support our findings with extensive experiments, and show the proposed method delivers comparable and even better performance with remarkably fewer epochs of training compared to the state-of-the-art self-supervised vision model pretraining methods. Our findings shed light on further exploring self-supervised vision model pretraining and pretext tasks beyond masked image modeling. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                350. <h><a href=https://openreview.net/pdf?id=sb-IkS8DQw2>Accurate Bayesian Meta-Learning by Accurate Task Posterior Inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Volpp, Philipp Dahlinger, Philipp Becker, Christian Daniel, Gerhard Neumann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sb-IkS8DQw2>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that accurate inference of the task posterior is all you need for accurate Bayesian meta-learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sb-IkS8DQw2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Bayesian meta-learning (BML) enables fitting expressive generative models to small datasets by incorporating inductive priors learned from a set of related tasks. The Neural Process (NP) is a prominent deep neural network-based BML architecture, which has shown remarkable results in recent years. In its standard formulation, the NP encodes epistemic uncertainty in an amortized, factorized, Gaussian variational (VI) approximation to the BML task posterior (TP), using reparametrized gradients. Prior work studies a range of architectural modifications to boost performance, such as attentive computation paths or improved context aggregation schemes, while the influence of the VI scheme remains under-explored. We aim to bridge this gap by introducing GMM-NP, a novel BML model, which builds on recent work that enables highly accurate, full-covariance Gaussian mixture (GMM) TP approximations by combining VI with natural gradients and trust regions. We show that GMM-NP yields tighter evidence lower bounds, which increases the efficiency of marginal likelihood optimization, leading to improved epistemic uncertainty estimation and accuracy. GMM-NP does not require complex architectural modifications, resulting in a powerful, yet conceptually simple BML model, which outperforms the state of the art on a range of challenging experiments, highlighting its applicability to settings where data is scarce.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                351. <h><a href=https://openreview.net/pdf?id=2nLeOOfAjK>Versatile Neural Processes for Learning Implicit Neural Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zongyu Guo, Cuiling Lan, Zhizheng Zhang, Yan Lu, Zhibo Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2nLeOOfAjK>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new neural process framework for efficient learning of the implicit neural representations w.r.t. various signals, including complex 3D scenes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2nLeOOfAjK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Representing a signal as a continuous function parameterized by neural network (a.k.a. Implicit Neural Representations, INRs) has attracted increasing attention in recent years. Neural Processes (NPs), which model the distributions over functions conditioned on partial observations (context set), provide a practical solution for fast inference of continuous functions. However, existing NP architectures suffer from inferior modeling capability for complex signals. In this paper, we propose an efficient NP framework dubbed Versatile Neural Processes (VNP), which largely increases the capability of approximating functions. Specifically, we introduce a bottleneck encoder that produces fewer and informative context tokens, relieving the high computational cost while providing high modeling capability. At the decoder side, we hierarchically learn multiple global latent variables that jointly model the global structure and the uncertainty of a function, enabling our model to capture the distribution of complex signals. We demonstrate the effectiveness of the proposed VNP on a variety of tasks involving 1D, 2D and 3D signals. Particularly, our method shows promise in learning accurate INRs w.r.t. a 3D scene without further finetuning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                352. <h><a href=https://openreview.net/pdf?id=vVJZtlZB9D>A General Framework For Proving The Equivariant Strong Lottery Ticket Hypothesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Damien Ferbach, Christos Tsirigotis, Gauthier Gidel, Joey Bose</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vVJZtlZB9D>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We extend the strong lottery ticket hypothesis to Equivariant Networks and show optimal pruning strategies in theory and practice for Steerable CNNs, Higher Order GNNs, and Message Passing GNNs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vVJZtlZB9D.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The Strong Lottery Ticket Hypothesis (SLTH) stipulates the existence of a subnetwork within a sufficiently overparameterized (dense) neural network that---when initialized randomly and without any training---achieves the accuracy of a fully trained target network. Recent works by Da Cunha et. al 2022, Burkholz 2022 demonstrate that the SLTH can be extended to translation equivariant networks---i.e. CNNs---with the same level of overparametrization as needed for the SLTs in dense networks. However, modern neural networks are capable of incorporating more than just translation symmetry, and developing general equivariant architectures such as rotation and permutation has been a powerful design principle. In this paper, we generalize the SLTH to functions that preserve the action of the group $G$---i.e. $G$-equivariant network---and prove, with high probability, that one can approximate any $G$-equivariant network of fixed width and depth by pruning a randomly initialized overparametrized $G$-equivariant network to a $G$-equivariant subnetwork. We further prove that our prescribed overparametrization scheme is optimal and provide a lower bound on the number of effective parameters as a function of the error tolerance. We develop our theory for a large range of groups, including subgroups of the Euclidean $\text{E}(2)$ and Symmetric group $G \leq \mathcal{S}_n$---allowing us to find SLTs for MLPs, CNNs, $\text{E}(2)$-steerable CNNs, and permutation equivariant networks as specific instantiations of our unified framework. Empirically, we verify our theory by pruning overparametrized $\text{E}(2)$-steerable CNNs, $k$-order GNNs, and message passing GNNs to match the performance of trained target networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                353. <h><a href=https://openreview.net/pdf?id=JTGimap_-F>Rarity Score : A New Metric to Evaluate the Uncommonness of Synthesized Images</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiyeon Han, Hwanil Choi, Yunjey Choi, Junho Kim, Jung-Woo Ha, Jaesik Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JTGimap_-F>7.0 - [6, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JTGimap_-F.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Evaluation metrics in image synthesis play a key role to measure performances of generative models. However, most metrics mainly focus on image fidelity. Existing diversity metrics are derived by comparing distributions, and thus they cannot quantify the diversity or rarity degree of each generated image. In this work, we propose a new evaluation metric, called `rarity score&#39;, to measure both image-wise uncommonness and model-wise diversified generation performance.   
We first show empirical observation that typical samples are close to each other and distinctive samples are far from each other in nearest-neighbor distances on latent spaces represented by feature extractor networks such as VGG16. We then show that one can effectively filter typical or distinctive samples with the proposed metric. We also use our metric to demonstrate that the extent to which different generative models produce rare images can be effectively compared. Further, our metric can be used to compare rarities between datasets that share the same concept such as CelebA-HQ and FFHQ. Finally, we analyze the use of metrics in different designs of feature extractors to better understand the relationship between feature spaces and resulting high-rarity images. Code will be publicly available for the research community.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                354. <h><a href=https://openreview.net/pdf?id=Vk-34OQ7rFo>Model-based Causal Bayesian Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Scott Sussex, Anastasia Makarova, Andreas Krause</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Vk-34OQ7rFo>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A principled algorithm for causal bayesian optimization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Vk-34OQ7rFo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    How should we intervene on an unknown structural equation model to maximize a downstream variable of interest? This setting, also known as causal Bayesian optimization (CBO), has important applications in medicine, ecology, and manufacturing. Standard Bayesian optimization algorithms fail to effectively leverage the underlying causal structure. Existing CBO approaches assume noiseless measurements and do not come with guarantees. We propose the {\em model-based causal Bayesian optimization algorithm (MCBO)} that learns a full system model instead of only modeling intervention-reward pairs. MCBO propagates epistemic uncertainty about the causal mechanisms through the graph and trades off exploration and exploitation via the optimism principle. We bound its cumulative regret, and obtain the first non-asymptotic bounds for CBO. Unlike in standard Bayesian optimization, our acquisition function cannot be evaluated in closed form, so we show how the reparameterization trick can be used to apply gradient-based optimizers. The resulting practical implementation of MCBO compares favorably with state-of-the-art approaches empirically.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                355. <h><a href=https://openreview.net/pdf?id=4LMIZY7gt7h>Robust Fair Clustering: A Novel Fairness Attack and Defense Framework</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anshuman Chhabra, Peizhao Li, Prasant Mohapatra, Hongfu Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4LMIZY7gt7h>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a highly effective &amp; novel fairness attack against state-of-the-art fair clustering models, &amp; for self-completeness, we propose a defense framework based on consensus clustering &amp; graph representation learning that is robust to our attack.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4LMIZY7gt7h.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Clustering algorithms are widely used in many societal resource allocation applications, such as loan approvals and candidate recruitment, among others, and hence, biased or unfair model outputs can adversely impact individuals that rely on these applications. To this end, many $\textit{fair}$ clustering approaches have been recently proposed to counteract this issue. Due to the potential for significant harm, it is essential to ensure that fair clustering algorithms provide consistently fair outputs even under adversarial influence. However, fair clustering algorithms have not been studied from an adversarial attack perspective. In contrast to previous research, we seek to bridge this gap and conduct a robustness analysis against fair clustering by proposing a novel $\textit{black-box fairness attack}$. Through comprehensive experiments, we find that state-of-the-art models are highly susceptible to our attack as it can reduce their fairness performance significantly. Finally, we propose Consensus Fair Clustering (CFC), the first $\textit{robust fair clustering}$ approach that transforms consensus clustering into a fair graph partitioning problem, and iteratively learns to generate fair cluster outputs. Experimentally, we observe that CFC is highly robust to the proposed attack and is thus a truly robust fair clustering alternative.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                356. <h><a href=https://openreview.net/pdf?id=8uu6JStuYm>Self-supervised learning with rotation-invariant kernels</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Léon Zheng, Gilles Puy, Elisa Riccietti, Patrick Perez, Rémi Gribonval</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8uu6JStuYm>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A regularization loss based on kernel mean embeddings with rotation-invariant kernels on the hypersphere for self-supervised learning of image representations
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8uu6JStuYm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce a regularization loss based on kernel mean embeddings with rotation-invariant kernels on the hypersphere (also known as dot-product kernels) for self-supervised learning of image representations. Besides being fully competitive with the state of the art, our method significantly reduces time and memory complexity for self-supervised training, making it implementable for very large embedding dimensions on existing devices and more easily adjustable than previous methods to settings with limited resources. Our work follows the major paradigm where the model learns to be invariant to some predefined image transformations (cropping, blurring, color jittering, etc.), while avoiding a degenerate solution by regularizing the embedding distribution. Our particular contribution is to propose a loss family promoting the embedding distribution to be close to the uniform distribution on the hypersphere, with respect to the maximum mean discrepancy pseudometric. We demonstrate that this family encompasses several regularizers of former methods, including uniformity-based and information-maximization methods, which are variants of our flexible regularization loss with different kernels. Beyond its practical consequences for state of the art self-supervised learning with limited resources, the proposed generic regularization approach opens perspectives to leverage more widely the literature on kernel methods in order to improve self-supervised learning methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                357. <h><a href=https://openreview.net/pdf?id=YJ7o2wetJ2>VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash Kumar, Amy Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YJ7o2wetJ2>7.0 - [6, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method for pre-training a goal-conditioned value function on human videos that can be effectively used as zero-shot visual reward and representation for unseen robotics tasks in simulation and real-world.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YJ7o2wetJ2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reward and representation learning are two long-standing challenges for learning an expanding set of robot manipulation skills from sensory observations. Given the inherent cost and scarcity of in-domain, task-specific robot data, learning from large, diverse, offline human videos has emerged as a promising path towards acquiring a generally useful visual representation for control; however, how these human videos can be used for general-purpose reward learning remains an open question. We introduce $\textbf{V}$alue-$\textbf{I}$mplicit $\textbf{P}$re-training (VIP), a self-supervised pre-trained visual representation capable of generating dense and smooth reward functions for unseen robotic tasks. VIP casts representation learning from human videos as an offline goal-conditioned reinforcement learning problem and derives a self-supervised dual goal-conditioned value-function objective that does not depend on actions, enabling pre-training on unlabeled human videos. Theoretically, VIP can be understood as a novel implicit time contrastive objective that generates a temporally smooth embedding, enabling the value function to be implicitly defined via the embedding distance, which can then be used to construct the reward for any goal-image specified downstream task. Trained on large-scale Ego4D human videos and without any fine-tuning on in-domain, task-specific data, VIP can provide dense visual reward for an extensive set of simulated and $\textbf{real-robot}$ tasks, enabling diverse reward-based visual control methods and significantly outperforming all prior pre-trained representations. Notably, VIP can enable simple, few-shot offline RL on a suite of real-world robot tasks with as few as 20 trajectories.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                358. <h><a href=https://openreview.net/pdf?id=5aT4ganOd98>CLARE: Conservative Model-Based Reward Learning for Offline Inverse Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju Ren, Junshan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5aT4ganOd98>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper introduces a principled algorithm to approach the reward extrapolation error in offline inverse reinforcement learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5aT4ganOd98.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work aims to tackle a major challenge in offline Inverse Reinforcement Learning (IRL), namely the reward extrapolation error, where the learned reward function may fail to explain the task correctly and misguide the agent in unseen environments due to the intrinsic covariate shift. Leveraging both expert data and lower-quality diverse data, we devise a principled algorithm (namely CLARE) that solves offline IRL efficiently via integrating &#34;conservatism&#34; into a learned reward function and utilizing an estimated dynamics model. Our theoretical analysis provides an upper bound on the return gap between the learned policy and the expert policy, based on which we characterize the impact of covariate shift by examining subtle two-tier tradeoffs between the exploitation (on both expert and diverse data) and exploration (on the estimated dynamics model). We show that CLARE can provably alleviate the reward extrapolation error by striking the right exploitation-exploration balance therein. Extensive experiments corroborate the significant performance gains of CLARE over existing state-of-the-art algorithms on MuJoCo continuous control tasks (especially with a small offline dataset), and the learned reward is highly instructive for further learning. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                359. <h><a href=https://openreview.net/pdf?id=o8xdgmwCP8l>Time to augment self-supervised visual representation learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Arthur Aubret, Markus R. Ernst, Céline Teulière, Jochen Triesch</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=o8xdgmwCP8l>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that time-based augmentations resulting from ego-motion and object manipulations improve over standard data-augmentations methods on the ability to visually recognize object categories.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/o8xdgmwCP8l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Biological vision systems are unparalleled in their ability to learn visual representations without supervision. In machine learning, self-supervised learning (SSL) has led to major advances in forming object representations in an unsupervised fashion. Such systems learn representations invariant to augmentation operations over images, like cropping or flipping. In contrast, biological vision systems exploit the temporal structure of the visual experience during natural interactions with objects. This gives access to “augmentations” not commonly used in SSL, like watching the same object from multiple viewpoints or against different backgrounds. Here, we systematically investigate and compare the potential benefits of such time-based augmentations during natural interactions for learning object categories. Our results show that incorporating time-based augmentations achieves large performance gains over state-of-the-art image augmentations. Specifically, our analyses reveal that: 1) 3-D object manipulations drastically improve the learning of object categories; 2) viewing objects against changing backgrounds is important for learning to discard background-related information from the latent representation. Overall, we conclude that time-based augmentations during natural interactions with objects can substantially improve self-supervised learning, narrowing the gap between artificial and biological vision systems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                360. <h><a href=https://openreview.net/pdf?id=kUf4BcWXGJr>HypeR: Multitask Hyper-Prompted Training Enables Large-Scale Retrieval Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>ZeFeng Cai, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, Xin Alex Lin, Liang He, Daxin Jiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kUf4BcWXGJr>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A multitask hyper-prompted training mechanism that enables a neural retriever to dynamically process different types of queries with different hyper-prompts and transfer learned knowledge across different domains and tasks. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kUf4BcWXGJr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, large-scale text retrieval has made impressive progress, facilitating both information retrieval and downstream knowledge-intensive tasks (e.g., open-domain QA and dialogue). With a moderate amount of data, a neural text retriever can outperform traditional methods such as BM25 by a large step. However, while being applied to out-of-domain data, the performance of a neural retriever degrades considerably. Therefore, how to enable a retriever to perform more robustly across different domains or tasks  and even show strong zero-shot transfer ability is critical for building scalable IR systems. To this end, we propose HypeR, a hyper-prompted training mechanism to enable uniform retrieval across tasks of different domains. Specifically, our approach jointly trains the query encoder with a shared prompt-based parameter pool and a prompt synthesizer that dynamically composes hyper-prompt for encoding each query from different tasks or domains. Besides, to avoid the mode collapse of prompt attention distribution for different queries, we design a contrastive prompt regularization that promotes the mode of prompt attention to be aligned and uniform. Through multi-task hyper-prompted training, our retriever can master the ability to dynamically represent different types of queries and transfer knowledge across different domains and tasks. Extensive experiments show our model attains better retrieval performance across different tasks and better zero-shot transfer ability compared with various previous methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                361. <h><a href=https://openreview.net/pdf?id=i9ogGQHYbkY>Near-Optimal Adversarial Reinforcement Learning with Switching Costs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ming Shi, Yingbin Liang, Ness Shroff</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=i9ogGQHYbkY>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper provides the first algorithms with near-optimal regrets for adversarial reinforcement learning with switching costs, and a matching lower bound on the regret.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/i9ogGQHYbkY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Switching costs, which capture the costs for changing policies, are regarded as a critical metric in reinforcement learning (RL), in addition to the standard metric of losses (or rewards). However, existing studies on switching costs (with a coefficient that is strictly positive and is independent of the time horizon) have mainly focused on static RL, where the loss distribution is assumed to be fixed during the learning process, and thus practical scenarios where the loss distribution could be non-stationary or even adversarial are not considered. While adversarial RL better models this type of practical scenarios, an open problem remains: how to develop a provably efficient algorithm for adversarial RL with switching costs? This paper makes the first effort towards solving this problem. First, we provide a regret lower-bound that shows that the regret of any algorithm must be larger than $\tilde{\Omega}( ( H S A )^{1/3} T^{2/3} )$, where $T$, $S$, $A$ and $H$ are the number of episodes, states, actions and layers in each episode, respectively. Our lower bound indicates that, due to the fundamental challenge of switching costs in adversarial RL, the best achieved regret (whose dependency on $T$ is $\tilde{O}(\sqrt{T})$) in static RL with switching costs (as well as adversarial RL without switching costs) is no longer achievable. Moreover, we propose two novel switching-reduced algorithms with regrets that match our lower bound when the transition function is known, and match our lower bound within a small factor of $\tilde{O}( H^{1/3} )$ when the transition function is unknown. Our regret analysis demonstrates the near-optimal performance of them.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                362. <h><a href=https://openreview.net/pdf?id=-CA8yFkPc7O>Why adversarial training can hurt robust accuracy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jacob Clarysse, Julia Hörrmann, Fanny Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-CA8yFkPc7O>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Adversarial training can hurt robust generalization for perceptible perturbations when the sample size is small
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-CA8yFkPc7O.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning classifiers with high test accuracy often perform poorly under adversarial attacks. It is commonly believed that adversarial training alleviates this issue. In this paper, we demonstrate that, surprisingly, the opposite can be true for a natural class of perceptible perturbations --- even though adversarial training helps when enough data is  available, it may in fact hurt robust generalization in the small sample size regime. We first prove this phenomenon for a high-dimensional linear classification setting with noiseless observations. Using intuitive insights from the proof, we could surprisingly find perturbations on standard image datasets for which this behavior persists. Specifically, it occurs for perceptible attacks that effectively reduce class information such as object occlusions or corruptions. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                363. <h><a href=https://openreview.net/pdf?id=9ImtNIZ7bYx>Finding the Global Semantic Representation in GAN through Fréchet Mean</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jaewoong Choi, Geonho Hwang, Hyunsoo Cho, Myungjoo Kang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9ImtNIZ7bYx>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the global basis for semantics in the latent space of GAN through Fréchet Mean.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9ImtNIZ7bYx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ideally disentangled latent space in GAN involves the global representation of latent space using semantic attribute coordinates. In other words,  in this disentangled space, there exists the global semantic basis as a vector space where each basis component describes one attribute of generated images. In this paper, we propose an unsupervised method for finding this global semantic basis in the intermediate latent space in GANs. This semantic basis represents sample-independent meaningful perturbations that change the same semantic attribute of an image on the entire latent space. The proposed global basis, called Fréchet basis, is derived by introducing Fréchet mean to the local semantic perturbations in a latent space. Fréchet basis is discovered in two stages. First, the global semantic subspace is discovered by the Fréchet mean in the Grassmannian manifold of the local semantic subspaces. Second, Fréchet basis is found by optimizing a basis of the semantic subspace via the Fréchet mean in the Special Orthogonal Group. Experimental results demonstrate that Fréchet basis provides better semantic factorization and robustness compared to the previous methods. Moreover, we suggest the basis refinement scheme for the previous methods. The quantitative experiments show that the refined basis achieves better semantic factorization while constrained on the same semantic subspace given by the previous method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                364. <h><a href=https://openreview.net/pdf?id=XrMWUuEevr>Context-enriched molecule representations improve few-shot drug discovery</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp Hochreiter, Günter Klambauer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XrMWUuEevr>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a new architecture for few-shot learning in drug discovery that enriches molecule representations by retrieving from a large set of known molecules.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XrMWUuEevr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A central task in computational drug discovery is to construct models from known active molecules to find further promising molecules for subsequent screening. However, typically only very few active molecules are known. Therefore, few-shot learning methods have the potential to improve the effectiveness of this critical phase of the drug discovery process. We introduce a new method for few-shot drug discovery. Its main idea is to enrich a molecule representation by knowledge about known context or reference molecules. Our novel concept for molecule representation enrichment is to associate molecules from both the support set and the query set with a large set of reference (context) molecules through a modern Hopfield network. Intuitively, this enrichment step is analogous to a human expert who would associate a given molecule with familiar molecules whose properties are known. The enrichment step reinforces and amplifies the covariance structure of the data, while simultaneously removing spurious correlations arising from the decoration of molecules. Our approach is compared with other few-shot methods for drug discovery on the FS-Mol benchmark dataset. On FS-Mol, our approach outperforms all compared methods and therefore sets a new state-of-the art for few-shot learning in drug discovery. An ablation study shows that the enrichment step of our method is the key to improve the predictive quality. In a domain shift experiment, we further demonstrate the robustness of our method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                365. <h><a href=https://openreview.net/pdf?id=FkSp8VW8RjH>Language Modelling with Pixels</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Phillip Rust, Jonas F. Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, Desmond Elliott</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FkSp8VW8RjH>7.0 - [8, 6, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train PIXEL, a language model that operates solely on images of rendered text, and show that it is possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FkSp8VW8RjH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                366. <h><a href=https://openreview.net/pdf?id=PfpEtB3-csK>LexMAE: Lexicon-Bottlenecked Pretraining for Large-Scale Retrieval</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PfpEtB3-csK>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new pre-training framework, dubbed lexicon-bottlenecked masked autoencoder, is proposed to learn importance-aware lexicon representations in line with the lexicon-weighting paradigm for large-scale retrieval. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PfpEtB3-csK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In large-scale retrieval, the lexicon-weighting paradigm, learning weighted sparse representations in vocabulary space, has shown promising results with high quality and low latency. Despite it deeply exploiting the lexicon-representing capability of pre-trained language models, a crucial gap remains between language modeling and lexicon-weighting retrieval -- the former preferring certain or low-entropy words whereas the latter favoring pivot or high-entropy words -- becoming the main barrier to lexicon-weighting performance for large-scale retrieval. To bridge this gap, we propose a brand-new pre-training framework, lexicon-bottlenecked masked autoencoder (LexMAE), to learn importance-aware lexicon representations. Essentially, we present a lexicon-bottlenecked module between a normal language modeling encoder and a weakened decoder, where a continuous bag-of-words bottleneck is constructed to learn a lexicon-importance distribution in an unsupervised fashion. The pre-trained LexMAE is readily transferred to the lexicon-weighting retrieval via fine-tuning. On the ad-hoc retrieval benchmark, MS-Marco, it achieves 42.6% MRR@10 with 45.8 QPS for the passage dataset and 44.4% MRR@100 with 134.8 QPS for the document dataset, by a CPU machine. And LexMAE shows state-of-the-art zero-shot transfer capability on BEIR benchmark with 12 datasets. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                367. <h><a href=https://openreview.net/pdf?id=PQ2zoIZqvm>Switch-NeRF: Learning Scene Decomposition with Mixture of Experts for Large-scale Neural Radiance Fields</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhenxing MI, Dan Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PQ2zoIZqvm>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b>  We propose an applicable end-to-end sparse NeRF network with learning-based decomposition for large-scale scenes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PQ2zoIZqvm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The Neural Radiance Fields (NeRF) have been recently applied to reconstruct building-scale and even city-scale scenes. To model a large-scale scene efficiently, a dominant strategy is to employ a divide-and-conquer paradigm via performing scene decomposition, which decomposes a complex scene into parts that are further processed by different sub-networks. Existing large-scale NeRFs mainly use heuristic hand-crafted scene decomposition, with regular 3D-distance-based or physical-street-block-based schemes. Although achieving promising results, the hand-crafted schemes limit the capabilities of NeRF in large-scale scene modeling in several aspects. Manually designing a universal scene decomposition rule for different complex scenes is challenging, leading to adaptation issues for different scenarios. The decomposition procedure is not learnable, hindering the network from jointly optimizing the scene decomposition and the radiance fields in an end-to-end manner.  The different sub-networks are typically optimized independently, and thus hand-crafted rules are required to composite them to achieve a better consistency.  To tackle these issues, we propose Switch-NeRF, a novel end-to-end large-scale NeRF with learning-based scene decomposition. We design a gating network to dispatch 3D points to different NeRF sub-networks. The gating network can be optimized together with the NeRF sub-networks for different scene partitions, by a design with the Sparsely Gated Mixture of Experts (MoE). The outputs from different sub-networks can also be fused in a learnable way in the unified framework to effectively guarantee the consistency of the whole scene. Furthermore, the proposed MoE-based Switch-NeRF model is carefully implemented and optimized to achieve both high-fidelity scene reconstruction and efficient computation. Our method establishes clear state-of-the-art performances on several large-scale datasets. To the best of our knowledge, we are the first to propose an applicable end-to-end sparse NeRF network  with learning-based decomposition for large-scale scenes. Codes are released at https://github.com/MiZhenxing/Switch-NeRF.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                368. <h><a href=https://openreview.net/pdf?id=Pv1GPQzRrC8>Imitating Human Behaviour with Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, Sam Devlin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Pv1GPQzRrC8>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Pv1GPQzRrC8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models have emerged as powerful generative models in the text-to-image domain. This paper studies their application as observation-to-action models for imitating human behaviour in sequential environments. Human behaviour is stochastic and multimodal, with structured correlations between action dimensions. Meanwhile, standard modelling choices in behaviour cloning are limited in their expressiveness and may introduce bias into the cloned policy. We begin by pointing out the limitations of these choices. We then propose that diffusion models are an excellent fit for imitating human behaviour, since they learn an expressive distribution over the joint action space. We introduce several innovations to make diffusion models suitable for sequential environments; designing suitable architectures, investigating the role of guidance, and developing reliable sampling strategies. Experimentally, diffusion models closely match human demonstrations in a simulated robotic control task and a modern 3D gaming environment.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                369. <h><a href=https://openreview.net/pdf?id=NeDc-Ak-H_>Learning What and Where: Disentangling Location and Identity Tracking Without Supervision</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Manuel Traub, Sebastian Otte, Tobias Menge, Matthias Karlbauer, Jannik Thuemmel, Martin V. Butz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NeDc-Ak-H_>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Loci: an unsupervised disentangled LOCation and Identity tracking system, which excels on the CATER and related object tracking challenges featuring emergent object permanence and stable entity disentanglement via fully unsupervised learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NeDc-Ak-H_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Our brain can almost effortlessly decompose visual data streams into background and salient objects. Moreover, it can anticipate object motion and interactions, which are crucial abilities for conceptual planning and reasoning. Recent object reasoning datasets, such as CATER, have revealed fundamental shortcomings of current vision-based AI systems, particularly when targeting explicit object representations, object permanence, and object reasoning. Here we introduce a self-supervised LOCation and Identity tracking system (Loci), which excels on the CATER tracking challenge. Inspired by the dorsal and ventral pathways in the brain, Loci tackles the binding problem by processing separate, slot-wise encodings of &#39;what&#39; and &#39;where&#39;. Loci&#39;s predictive coding-like processing encourages active error minimization, such that individual slots tend to encode individual objects. Interactions between objects and object dynamics are processed in the disentangled latent space. Truncated backpropagation through time combined with forward eligibility accumulation significantly speeds up learning and improves memory efficiency. Besides exhibiting superior performance in current benchmarks, Loci effectively extracts objects from video streams and separates them into location and Gestalt components. We believe that this separation offers a representation that will facilitate effective planning and reasoning on conceptual levels.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                370. <h><a href=https://openreview.net/pdf?id=kUmdmHxK5N>Koopman Neural Operator Forecaster for Time-series with Temporal Distributional Shifts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rui Wang, Yihe Dong, Sercan O Arik, Rose Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kUmdmHxK5N>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kUmdmHxK5N.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Temporal distributional shifts, with underlying dynamics changing over time, frequently occur in real-world time series and pose a fundamental challenge for deep neural networks (DNNs). In this paper, we propose a novel deep sequence model based on the Koopman theory for time series forecasting: Koopman Neural Forecaster (KNF) that leverages DNNs to learn the linear Koopman space and the coefficients of chosen measurement functions. KNF imposes appropriate inductive biases for improved robustness against distributional shifts, employing both a global operator to learn shared characteristics and a local operator to capture changing dynamics, as well as a specially-designed feedback loop to continuously update the learnt operators over time for rapidly varying behaviors. We demonstrate that KNF achieves superior performance compared to the alternatives, on multiple time series datasets that are shown to suffer from distribution shifts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                371. <h><a href=https://openreview.net/pdf?id=bvpkw7UIRdU>On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tiago Pimentel, Clara Isabel Meister, Ryan Cotterell</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bvpkw7UIRdU>7.0 - [6, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a theoretical and empirical analysis of why a recently-proposed automatic evaluation metric for language generators correlates well with human judgments. We identify its use of embeddings from pretrained language models as the main reason.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bvpkw7UIRdU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality.  Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is  the recently proposed Mauve. In theory, Mauve measures an information-theoretic divergence between two probability distributions over strings: one representing the language generator under evaluation; the other representing the true natural language distribution. Mauve&#39;s authors argue that its success comes from the qualitative properties of their proposed divergence.  Yet in practice, as this divergence is uncomputable, Mauve approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pretrained language model&#39;s embeddings. As we show, however, this is not a tight approximation---in either theory or practice. This begs the question: why does Mauve work so well? In this work, we show that \mauve was right for the wrong reasons, and that its newly proposed divergence is not necessary for its high performance. In fact, classical divergences paired with its proposed cluster-based approximation may actually serve as better evaluation metrics. We finish the paper with a probing analysis; this analysis leads us to conclude that---by encoding syntactic- and coherence-level features of text, while ignoring surface-level features---such cluster-based approximations to string distributions may simply be better for evaluating state-of-the-art language generators.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                372. <h><a href=https://openreview.net/pdf?id=yYbhKqdi7Hz>Continuized Acceleration for Quasar Convex Functions  in Non-Convex Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jun-Kun Wang, Andre Wibisono</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yYbhKqdi7Hz>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yYbhKqdi7Hz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Quasar convexity is a condition that allows some first-order methods to efficiently minimize a function even when the optimization landscape is non-convex. Previous works develop near-optimal accelerated algorithms for minimizing this class of functions, however, they require a subroutine of binary search which results in multiple calls to gradient evaluations in each iteration, and consequently the total number of gradient evaluations does not match a known lower bound. In this work, we show that a recently proposed continuized Nesterov acceleration can be applied to minimizing quasar convex functions and achieves the optimal bound with a high probability. Furthermore, we find that the objective functions of training generalized linear models (GLMs) satisfy quasar convexity, which broadens the applicability of the relevant algorithms, while known practical examples of quasar convexity in non-convex learning are sparse in the literature. We also show that if a smooth and one-point strongly convex, Polyak-Lojasiewicz, or quadratic-growth function satisfies quasar convexity, then attaining an accelerated linear rate for minimizing the function is possible under certain conditions, while acceleration is not known in general for these classes of functions.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                373. <h><a href=https://openreview.net/pdf?id=OoOIW-3uadi>Deep Learning on Implicit Neural Representations of Shapes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Luca De Luigi, Adriano Cardace, Riccardo Spezialetti, Pierluigi Zama Ramirez, Samuele Salti, Luigi di Stefano</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OoOIW-3uadi>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OoOIW-3uadi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Implicit Neural Representations (INRs) have emerged in the last few years as a powerful tool to encode continuously  a variety of different signals like images, videos, audio and 3D shapes. When applied to 3D shapes, INRs allow to overcome the fragmentation and shortcomings of the popular discrete representations used so far. Yet, considering that INRs consist in neural networks, it is not clear whether and how it may be possible to feed them into deep learning pipelines aimed at solving a downstream task. In this paper, we put forward this research problem and propose inr2vec, a framework that can compute a compact latent representation for an input INR in a single inference pass. We verify that inr2vec can embed effectively the 3D shapes represented by the input INRs and show how the produced embeddings can be fed into deep learning pipelines to solve several tasks by processing exclusively INRs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                374. <h><a href=https://openreview.net/pdf?id=xE-LtsE-xx>Is Attention All That NeRF Needs?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, Zhangyang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xE-LtsE-xx>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present Generalizable NeRF Transformer (GNT), a pure, unified transformer-based architecture that efficiently reconstructs Neural Radiance Fields (NeRFs) on the fly.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xE-LtsE-xx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present Generalizable NeRF Transformer (GNT), a transformer-based architecture that reconstructs Neural Radiance Fields (NeRFs) and learns to render novel views on the fly from source views. While prior works on NeRFs optimize a scene representation by inverting a handcrafted rendering equation, GNT achieves neural representation and rendering that generalizes across scenes using transformers at two stages. (1) The view transformer leverages multi-view geometry as an inductive bias for attention-based scene representation, and predicts coordinate-aligned features by aggregating information from epipolar lines on the neighboring views. (2) The ray transformer renders novel views using attention to decode the features from the view transformer along the sampled points during ray marching. Our experiments demonstrate that when optimized on a single scene, GNT can successfully reconstruct NeRF without an explicit rendering formula due to the learned ray renderer. When trained on multiple scenes, GNT consistently achieves state-of-the-art performance when transferring to unseen scenes and outperform all other methods by ~10% on average. Our analysis of the learned attention maps to infer depth and occlusion indicate that attention enables learning a physically-grounded rendering. Our results show the promise of transformers as a universal modeling tool for graphics. Please refer to our project page for video results: https://vita-group.github.io/GNT/
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                375. <h><a href=https://openreview.net/pdf?id=9F_xlC7sk9>Generalization and Estimation Error Bounds for Model-based Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Avner Shultzman, Eyar Azar, Miguel R. D. Rodrigues, Yonina C. Eldar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9F_xlC7sk9>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9F_xlC7sk9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Model-based neural networks provide unparalleled performance for various tasks, such as sparse coding and compressed sensing problems. Due to the strong connection with the sensing model, these networks are interpretable and inherit prior structure of the problem. In practice, model-based neural networks exhibit higher generalization capability compared to ReLU neural networks. However, this phenomenon was not addressed theoretically. Here, we leverage complexity measures including the global and local Rademacher complexities, in order to provide upper bounds on the generalization and estimation errors of model-based networks. We show that the generalization abilities of model-based networks for sparse recovery outperform those of regular ReLU networks, and derive practical design rules that allow to construct model-based networks with guaranteed high generalization. We demonstrate through a series of experiments that our theoretical insights shed light on a few behaviours experienced in practice, including the fact that ISTA and ADMM networks exhibit higher generalization abilities (especially for small number of training samples), compared to ReLU networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                376. <h><a href=https://openreview.net/pdf?id=E8mzu3JbdR>ChordMixer: A Scalable Neural Attention Model for Sequences with Different Length</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruslan Khalitov, Tong Yu, Lei Cheng, Zhirong Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=E8mzu3JbdR>7.0 - [8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/E8mzu3JbdR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                377. <h><a href=https://openreview.net/pdf?id=6xXtM8bFFJ>SGDA with shuffling: faster convergence for nonconvex-PŁ minimax optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hanseul Cho, Chulhee Yun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6xXtM8bFFJ>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the convergence bounds of (mini-batch) SGDA with random reshuffling for nonconvex-PŁ and primal-PŁ-PŁ problems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6xXtM8bFFJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Stochastic gradient descent-ascent (SGDA) is one of the main workhorses for solving finite-sum minimax optimization problems. Most practical implementations of SGDA randomly reshuffle components and sequentially use them (i.e., without-replacement sampling); however, there are few theoretical results on this approach for minimax algorithms, especially outside the easier-to-analyze (strongly-)monotone setups. To narrow this gap, we study the convergence bounds of SGDA with random reshuffling (SGDA-RR) for smooth nonconvex-nonconcave objectives with Polyak-{\L}ojasiewicz (P{\L}) geometry. We analyze both simultaneous and alternating SGDA-RR for nonconvex-P{\L} and primal-P{\L}-P{\L} objectives, and obtain convergence rates faster than with-replacement SGDA. Our rates extend to mini-batch SGDA-RR, recovering known rates for full-batch gradient descent-ascent (GDA). Lastly, we present a comprehensive lower bound for GDA with an arbitrary step-size ratio, which matches the full-batch upper bound for the primal-P{\L}-P{\L} case.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                378. <h><a href=https://openreview.net/pdf?id=WUWJIV2Yxtp>Re-calibrating Feature Attributions for Model Interpretation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Peiyu Yang, NAVEED AKHTAR, Zeyi Wen, Mubarak Shah, Ajmal Saeed Mian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WUWJIV2Yxtp>7.0 - [5, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a re-calibration technique to calibrate existing integral-based attribution methods with valid references for a consistent explanation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WUWJIV2Yxtp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to interpret machine learning models is critical for high-stakes applications. Due to its desirable theoretical properties, path integration is a widely used scheme for feature attribution to interpret model predictions. However, the methods implementing this scheme currently rely on absolute attribution scores to eventually provide sensible interpretations. This not only contradicts the premise that the features with larger attribution scores are more relevant to the model prediction, but also conflicts with the theoretical settings for which the desirable properties of the attributions are proven. We address this by devising a method to first compute an appropriate reference for the path integration scheme. This reference further helps in identifying valid interpolation points on a desired integration path. The reference is computed in a gradient ascending direction on the model&#39;s loss surface, while the interpolations are performed by analyzing the model gradients and variations between the reference and the input. The eventual integration is effectively performed along a non-linear path. Our scheme can be incorporated into the existing integral-based attribution methods. We also devise an effective sampling and integration procedure that enables employing our scheme with multi-reference path integration efficiently. We achieve a marked performance boost for a range of integral-based attribution methods on both local and global evaluation metrics by enhancing them with our scheme. Our extensive results also show improved sensitivity, sanity preservation and model robustness with the proposed re-calibration of the attribution techniques with our method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                379. <h><a href=https://openreview.net/pdf?id=27uBgHuoSQ>Data Continuity Matters: Improving Sequence Modeling with Lipschitz Regularizer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eric Qu, Xufang Luo, Dongsheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=27uBgHuoSQ>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/27uBgHuoSQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sequence modeling is a core problem in machine learning, and various neural networks have been designed to process different types of sequence data. However, few attempts have been made to understand the inherent data property of sequence data, neglecting the critical factor that may significantly affect the performance of sequence modeling. In this paper, we theoretically and empirically analyze a generic property of sequence data, i.e., continuity, and connect this property with the performance of deep models. First, we empirically observe that different kinds of models for sequence modeling prefer data with different continuity. Then, we theoretically analyze the continuity preference of different models in both time and frequency domains. To further utilize continuity to improve sequence modeling, we propose a simple yet effective Lipschitz Regularizer, that can flexibly adjust data continuity according to model preferences, and bring very little extra computational cost. Extensive experiments on various tasks demonstrate that altering data continuity via Lipschitz Regularizer can largely improve the performance of many deep models for sequence modeling.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                380. <h><a href=https://openreview.net/pdf?id=Dk7QQp8jHEo>Batch Multivalid Conformal Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Christopher Jung, Georgy Noarov, Ramya Ramalingam, Aaron Roth</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Dk7QQp8jHEo>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We give algorithms for conformal prediction in the batch setting that have coverage guarantees even when conditioning on group membership for intersecting groups and on the threshold used to produce the prediction set.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Dk7QQp8jHEo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We develop  fast distribution-free conformal prediction algorithms for obtaining multivalid coverage on exchangeable data in the batch setting. Multivalid coverage guarantees are stronger than marginal coverage guarantees in two ways: (1) They hold even conditional on group membership---that is, the target coverage level $1-\alpha$ holds conditionally on membership in each of an arbitrary (potentially intersecting) group in a finite collection $\mathcal{G}$ of regions in the feature space. (2) They hold even conditional on the value of the threshold used to produce the prediction set on a given example. In fact multivalid coverage guarantees hold even when conditioning on group membership and threshold value simultaneously.

We give two algorithms: both take as input an arbitrary non-conformity score and an arbitrary collection of possibly intersecting groups $\mathcal{G}$, and then can equip arbitrary black-box predictors with prediction sets.  Our first algorithm is a direct extension of quantile regression, needs to solve only a single convex minimization problem, and produces an estimator which has group-conditional guarantees for each group in $\mathcal{G}$. Our second algorithm is iterative, and gives the full guarantees of multivalid conformal prediction: prediction sets that are valid conditionally both on group membership and non-conformity threshold. We evaluate the performance of both of our algorithms in an extensive set of experiments. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                381. <h><a href=https://openreview.net/pdf?id=uHaWaNhCvZD>Meta-Learning in Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Keegan Harris, Ioannis Anagnostides, Gabriele Farina, Mikhail Khodak, Steven Wu, Tuomas Sandholm</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uHaWaNhCvZD>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We formalize and study the problem of meta-learning across a wide range of fundamental multi-agent settings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uHaWaNhCvZD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the literature on game-theoretic equilibrium finding, focus has mainly been on solving a single game in isolation. In practice, however, strategic interactions—ranging from routing problems to online advertising auctions—evolve dynamically, thereby leading to many similar games to be solved. To address this gap, we introduce meta-learning for equilibrium finding and learning to play games. We establish the first meta-learning guarantees for a variety of fundamental and well-studied games, including two-player zero-sum games, general-sum games, Stackelberg games, and multiple extensions thereof. In particular, we obtain rates of convergence to different game-theoretic equilibria that depend on natural notions of similarity between the sequence of games encountered, while at the same time recovering the known single-game guarantees when the sequence of games is arbitrary. Along the way, we prove a number of new results in the single-game regime through a simple and unified framework, which may be of independent interest. Finally, we evaluate our meta-learning algorithms on endgames faced by the poker agent Libratus against top human professionals. The experiments show that games with varying stack sizes can be solved significantly faster using our meta-learning techniques than by solving them separately, often by an order of magnitude.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                382. <h><a href=https://openreview.net/pdf?id=NEtep2C7yD>Learning Simultaneous Navigation and Construction in Grid Worlds </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenyu Han, Haoran Wu, Eisuke Hirota, Alexander Gao, Lerrel Pinto, Ludovic Righetti, Chen Feng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NEtep2C7yD>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Position-related representation learning improves DRL consistently when addressing the localization-planning interdependence challenge in the proposed mobile construction tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NEtep2C7yD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose to study a new learning task, mobile construction, to enable an agent to build designed structures in 1/2/3D grid worlds while navigating in the same evolving environments. Unlike existing robot learning tasks such as visual navigation and object manipulation, this task is challenging because of the interdependence between accurate localization and strategic construction planning. In pursuit of generic and adaptive solutions to this partially observable Markov decision process (POMDP) based on deep reinforcement learning (RL), we design
a Deep Recurrent Q-Network (DRQN) with explicit recurrent position estimation in this dynamic grid world. Our extensive experiments show that pre-training this position estimation module before Q-learning can significantly improve the construction performance measured by the intersection-over-union score, achieving the best results in our benchmark of various baselines including model-free and model-based RL, a handcrafted SLAM-based policy, and human players. Our code is available at: https://ai4ce.github.io/SNAC/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                383. <h><a href=https://openreview.net/pdf?id=iPWiwWHc1V>CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tuomas Oikarinen, Tsui-Wei Weng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iPWiwWHc1V>7.0 - [8, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an automated method for generating descriptions of the representation learned by hidden layer neurons, leveraging the multimodal CLIP-model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iPWiwWHc1V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10$\times$ faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                384. <h><a href=https://openreview.net/pdf?id=w-x7U26GM7j>Advancing Radiograph Representation Learning with Masked Record Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hong-Yu Zhou, Chenyu Lian, Liansheng Wang, Yizhou Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=w-x7U26GM7j>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose to learn radiograph representations via masked record modeling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/w-x7U26GM7j.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern studies in radiograph representation learning (R$^2$L) rely on either self-supervision to encode invariant semantics or associated radiology reports to incorporate medical expertise, while the complementarity between them is barely noticed. To explore this, we formulate the self- and report-completion as two complementary objectives and present a unified framework based on masked record modeling (MRM). In practice, MRM reconstructs masked image patches and masked report tokens following a multi-task scheme to learn knowledge-enhanced semantic representations. With MRM pre-training, we obtain pre-trained models that can be well transferred to various radiography tasks. Specifically, we find that MRM offers superior performance in label-efficient fine-tuning. For instance, MRM achieves 88.5% mean AUC on CheXpert using 1% labeled data, outperforming previous R$^2$L methods with 100% labels. On NIH ChestX-ray, MRM outperforms the best performing counterpart by about 3% under small labeling ratios. Besides, MRM surpasses self- and report-supervised pre-training in identifying the pneumonia type and the pneumothorax area, sometimes by large margins.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                385. <h><a href=https://openreview.net/pdf?id=ZcnzsHC10Y>Learning to CROSS exchange to solve min-max vehicle routing problems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minjun Kim, Junyoung Park, Jinkyoo Park</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZcnzsHC10Y>7.0 - [8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZcnzsHC10Y.jpg loading="lazy">
                </div>
                <div class="abstract">
                    CROSS exchange (CE), a meta-heuristic that solves various vehicle routing problems (VRPs), improves the solutions of VRPs by swapping the sub-tours of the vehicles. Inspired by CE, we propose Neuro CE (NCE), a fundamental operator of \textit{learned} meta-heuristic, to solve various min-max VRPs while overcoming the limitations of CE, i.e., the expensive $\mathcal{O}(n^4)$ search cost. NCE employs graph neural network to predict the cost-decrements (i.e., results of CE searches) and utilizes the predicted cost-decrements to guide the selection of sub-tours for swapping, while reducing the search cost to $\mathcal{O}(n^2)$. As the learning objective of NCE is to predict the cost-decrement, the training can be simply done in a supervised fashion, whose training samples can be easily collected. Despite the simplicity of NCE, numerical results show that the NCE trained with min-max flexible multi-depot VRP (min-max FMDVRP) outperforms the meta-heuristic baselines. More importantly, it significantly outperforms the neural baselines when solving distinctive special cases of min-max FMDVRP (e.g., min-max MDVRP, min-max mTSP, min-max CVRP) without additional training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                386. <h><a href=https://openreview.net/pdf?id=OnD9zGAGT0k>Diffusion Posterior Sampling for General Noisy Inverse Problems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, Jong Chul Ye</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OnD9zGAGT0k>7.0 - [8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a diffusion model-based general inverse problem solver that scales to nonlinear problems and different noise statistics.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OnD9zGAGT0k.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models have been recently studied as powerful generative inverse problem solvers, owing to their high quality reconstructions and the ease of combining existing iterative solvers. However, most works focus on solving simple linear inverse problems in noiseless settings, which significantly under-represents the complexity of real-world problems. In this work, we extend diffusion solvers to efficiently handle general noisy (non)linear inverse problems via the Laplace approximation of the posterior sampling. Interestingly, the resulting posterior sampling scheme is a blended version of diffusion sampling with the manifold constrained gradient without a strict measurement consistency projection step, yielding a more desirable generative path in noisy settings compared to the previous studies. Our method demonstrates that diffusion models can incorporate various measurement noise statistics such as Gaussian and Poisson, and also efficiently handle noisy nonlinear inverse problems such as Fourier phase retrieval and non-uniform deblurring.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                387. <h><a href=https://openreview.net/pdf?id=IxmWsm4xrua>Toeplitz Neural Network for Sequence Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, Yiran Zhong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IxmWsm4xrua>7.0 - [8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An efficient method that uses Toeplitz matrices to model sequences.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IxmWsm4xrua.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sequence modeling has important applications in natural language processing and computer vision. Recently, the transformer-based models have shown strong performance on various sequence modeling tasks, which rely on attention to capture pairwise token relations, and position embedding to inject positional information. While showing good performance, the transformer models are inefficient to scale to long input sequences, mainly due to the quadratic space-time complexity of attention. To overcome this inefficiency, we propose to model sequences with a relative position encoded Toeplitz matrix and use a Toeplitz matrix-vector production trick to reduce the space-time complexity of the sequence modeling to log linear. A lightweight sub-network called relative position encoder is proposed to generate relative position coefficients with a fixed budget of parameters, enabling the proposed Toeplitz neural network to deal with varying sequence lengths. In addition, despite being trained on 512-token sequences, our model can extrapolate input sequence length up to 14K tokens in inference with consistent performance. Extensive experiments on autoregressive and bidirectional language modeling, image modeling, and the challenging Long-range Arena Benchmark show that our method achieves better performance than its competitors in most downstream tasks while being significantly faster.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                388. <h><a href=https://openreview.net/pdf?id=p8hMBcPtvju>Scalable Subset Sampling with Neural Conditional Poisson Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Adeel Pervez, Phillip Lippe, Efstratios Gavves</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p8hMBcPtvju>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p8hMBcPtvju.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A number of problems in learning can be formulated in terms of the basic primitive of sampling $k$ elements out of a universe of $n$ elements. This subset sampling operation cannot directly be included in differentiable models and approximations are essential. Current approaches take an \emph{order sampling} approach to sampling subsets and depend on differentiable approximations of the Top-$k$ operator for selecting the largest $k$ elements from a set. We present a simple alternative method for sampling subsets based on \emph{conditional Poisson sampling}. Unlike order sampling approaches, the parallel complexity of the proposed method is independent of the subset size which makes the method scalable to large subset sizes. We adapt the procedure to make it efficient and amenable to discrete gradient approximations for use in differentiable models. Furthermore, the method also allows the subset size parameter $k$ to be differentiable. We demonstrate our approach on model explanation, image sub-sampling and stochastic $k$-nearest neighbor tasks outperforming existing methods in accuracy, efficiency and scalability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                389. <h><a href=https://openreview.net/pdf?id=ovZE0KsbM3S>Pitfalls of Gaussians as a noise distribution in NCE</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Holden Lee, Chirag Pabbaraju, Anish Prasad Sevekari, Andrej Risteski</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ovZE0KsbM3S>7.0 - [8, 5, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that using Gaussians as the noise distribution in Noise Contrastive Estimation can lead to exponentially bad statistical and algorithmic complexity.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ovZE0KsbM3S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Noise Contrastive Estimation (NCE) is a popular approach for learning probability density functions parameterized up to a constant of proportionality. The main idea is to design a classification problem for distinguishing training data from samples from an (easy-to-sample) noise distribution $q$, in a manner that avoids having to calculate a partition function. It is well-known that the choice of $q$ can severely impact the computational and statistical efficiency of NCE. In practice, a common choice for $q$ is a Gaussian which matches the mean and covariance of the data.

In this paper, we show that such a choice can result in an exponentially bad (in the ambient dimension) conditioning of the Hessian of the loss - even for very simple data distributions. As a consequence, both the statistical and algorithmic complexity for such a choice of $q$ will be problematic in practice - suggesting that more complex noise distributions are essential to the success of NCE.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                390. <h><a href=https://openreview.net/pdf?id=hQwb-lbM6EL>InCoder: A Generative Model for Code Infilling and Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, Mike Lewis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hQwb-lbM6EL>7.0 - [8, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An infilling-capable code completion model, evaluated on tasks including language-to-code, type inference, and comment generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hQwb-lbM6EL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Code is seldom written in a single left-to-right pass and is instead repeatedly edited and refined. We introduce InCoder, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via masking and infilling). InCoder is trained to generate code files from a large corpus of permissively licensed code, where regions of code have been randomly masked and moved to the end of each file, allowing code infilling with bidirectional context. Our model is the first large generative code model that is able to infill arbitrary regions of code, which we evaluate in a zero-shot setting on challenging tasks such as type inference, comment generation, and variable re-naming. We find that the ability to condition on bidirectional context substantially improves performance on these tasks, while still performing comparably on standard program synthesis benchmarks in comparison to left-to-right only models pretrained at similar scale. Our models and code will be publicly released.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                391. <h><a href=https://openreview.net/pdf?id=PDrUPTXJI_A>FreeMatch: Self-adaptive Thresholding for Semi-supervised Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, Xing Xie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PDrUPTXJI_A>7.0 - [8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose FreeMatch to define and adjust the confidence threshold in a self-adaptive manner for semi-supervised learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PDrUPTXJI_A.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Semi-supervised Learning (SSL) has witnessed great success owing to the impressive performances brought by various methods based on pseudo labeling and consistency regularization. However, we argue that existing methods might fail to utilize the unlabeled data more effectively since they either use a pre-defined / fixed threshold or an ad-hoc threshold adjusting scheme, resulting in inferior performance and slow convergence. We first analyze a motivating example to obtain intuitions on the relationship between the desirable threshold and model&#39;s learning status. Based on the analysis, we hence propose FreeMatch to adjust the confidence threshold in a self-adaptive manner according to the model&#39;s learning status. We further introduce a self-adaptive class fairness regularization penalty to encourage the model for diverse predictions during the early training stage. Extensive experiments indicate the superiority of FreeMatch especially when the labeled data are extremely rare. FreeMatch achieves 5.78%, 13.59%, and 1.28% error rate reduction over the latest state-of-the-art method FlexMatch on CIFAR-10 with 1 label per class, STL-10 with 4 labels per class, and ImageNet with 100 labels per class, respectively. Moreover, FreeMatch can also boost the performance of imbalanced SSL. The codes can be found at https://github.com/microsoft/Semi-supervised-learning.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                392. <h><a href=https://openreview.net/pdf?id=FBMLeaXpZN>Spectral Decomposition Representation for Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E. Gonzalez, Dale Schuurmans, Bo Dai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FBMLeaXpZN>7.0 - [5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new spectral representation learning method that gets rid of the policy dependency and can be easily applied in downstream tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FBMLeaXpZN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Representation learning often plays a critical role in avoiding the curse of dimensionality in reinforcement learning. A representative class of algorithms exploits spectral decomposition of the stochastic transition dynamics to construct representations that enjoy strong theoretical properties in idealized settings. However, current spectral methods suffer from limited applicability because they are constructed for
state-only aggregation and are derived from a policy-dependent transition kernel, without considering the issue of exploration. To address these issues, we propose an alternative spectral method, Spectral Decomposition Representation (SPEDER), that extracts a state-action abstraction from the dynamics without inducing spurious dependence on the data collection policy, while also balancing the exploration-versus-exploitation trade-off during learning. A theoretical analysis establishes the sample efficiency of the proposed algorithm in both the online and offline settings. In addition, an experimental investigation demonstrates superior performance over current state-of-the-art algorithms across several RL benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                393. <h><a href=https://openreview.net/pdf?id=fYzLpCsGZVf>On Accelerated Perceptrons and Beyond</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guanghui Wang, Rafael Hanashiro, Etash Kumar Guha, Jacob Abernethy</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fYzLpCsGZVf>7.0 - [5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a unified analysis for accelerated Perceptrons, and obtain improved results for a series of other problems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fYzLpCsGZVf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The classical Perceptron algorithm of Rosenblatt can be used to find a linear threshold function to correctly classify $n$ linearly separable data points, assuming the classes are separated by some margin $\gamma &gt; 0$. A foundational result is that Perceptron converges after  $\Omega(1/\gamma^{2})$ iterations. There have been several recent works that managed to improve this rate by a quadratic factor, to $\Omega(\sqrt{\log n}/\gamma)$, with more sophisticated algorithms. In this paper, we unify these existing results under one framework by showing that they can all be described through the lens of solving min-max problems using modern acceleration techniques, mainly through \emph{optimistic} online learning.  We then show that the proposed framework also leads to improved results for a series of problems beyond the standard Perceptron setting. Specifically, a) for the margin maximization problem, we improve the state-of-the-art result from $O(\log t/t^2)$ to $O(1/t^2)$, where $t$ is the number of iterations; b) we provide the first result on identifying the implicit bias property of the classical Nesterov&#39;s accelerated gradient descent (NAG) algorithm, and show NAG can maximize the margin with an $O(1/t^2)$ rate; c) for the classical $p$-norm Perceptron problem, we provide an algorithm with $\Omega(\sqrt{(p-1)\log n}/\gamma)$ convergence rate, while existing algorithms suffer the $\Omega({(p-1)}/\gamma^2)$ convergence rate.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                394. <h><a href=https://openreview.net/pdf?id=3k5CUGDLNdd>Benchmarking Offline Reinforcement Learning on Real-Robot Hardware</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nico Gürtler, Sebastian Blaes, Pavel Kolev, Felix Widmaier, Manuel Wuthrich, Stefan Bauer, Bernhard Schölkopf, Georg Martius</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3k5CUGDLNdd>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose new robotics datasets for dexterous manipulation and benchmark offline RL algorithms on them.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3k5CUGDLNdd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning policies from previously recorded data is a promising direction for real-world robotics tasks, as online learning is often infeasible. Dexterous manipulation in particular remains an open problem in its general form. The combination of offline reinforcement learning with large diverse datasets, however, has the potential to lead to a breakthrough in this challenging domain analogously to the rapid progress made in supervised learning in recent years. To coordinate the efforts of the research community toward tackling this problem, we propose a benchmark including: i) a large collection of data for offline learning from a dexterous manipulation platform on two tasks, obtained with capable RL agents trained in simulation; ii) the option to execute learned policies on a real-world robotic system and a simulation for efficient debugging. We evaluate prominent open-sourced offline reinforcement learning algorithms on the datasets and provide a reproducible experimental setup for offline reinforcement learning on real systems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                395. <h><a href=https://openreview.net/pdf?id=bn0GZZdDfI1>Decentralized Optimistic Hyperpolicy Mirror Descent: Provably No-Regret Learning in Markov Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenhao Zhan, Jason D. Lee, Zhuoran Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bn0GZZdDfI1>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bn0GZZdDfI1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study decentralized policy learning in Markov games where we control a single agent to play with nonstationary and possibly adversarial opponents. Our goal is to develop a no-regret online learning algorithm that (i) takes actions based on the local information observed by the agent and (ii) is able to find the best policy in hindsight. For such a problem, the nonstationary state transitions due to the varying opponent pose a significant challenge. In light of a recent hardness result (Liu et al., 2022), we focus on the setting where the opponent&#39;s previous policies are revealed to the agent for decision making. With such an information structure, we propose a new algorithm, Decentralized Optimistic hypeRpolicy mIrror deScent (DORIS), which achieves $\sqrt{K}$-regret in the context of general function approximation, where $K$ is the number of episodes. Moreover, when all the agents adopt DORIS, we prove that their mixture policy constitutes an approximate coarse correlated equilibrium. In particular, DORIS maintains a hyperpolicy which is a distribution over the policy space. The hyperpolicy is updated via mirror descent, where the update direction is obtained by an optimistic variant of least-squares policy evaluation. Furthermore, to illustrate the power of our method, we apply DORIS to constrained and vector-valued MDPs, which can be formulated as zero-sum Markov games with a fictitious opponent. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                396. <h><a href=https://openreview.net/pdf?id=mjzm6btqgV>Efficiently Computing Nash Equilibria in Adversarial Team Markov Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fivos Kalogiannis, Ioannis Anagnostides, Ioannis Panageas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Vaggos Chatziafratis, Stelios Andrew Stavroulakis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mjzm6btqgV>7.0 - [5, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Nash equlibrium can be computed effieciently in Markov games where a single player competes against multiple agents who share a common interest.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mjzm6btqgV.jpg loading="lazy">
                </div>
                <div class="abstract">
                        Computing Nash equilibrium policies is a central problem in multi-agent reinforcement learning that has received extensive attention both in theory and in practice. However, in light of computational intractability barriers in general-sum games, provable guarantees have been thus far either limited to fully competitive or cooperative scenarios or impose strong assumptions that are difficult to meet in most practical applications.
    
    In this work, we depart from those prior results by investigating infinite-horizon \emph{adversarial team Markov games}, a natural and well-motivated class of games in which a team of identically-interested players---in the absence of any explicit coordination or communication---is competing against an adversarial player. This setting allows for a unifying treatment of zero-sum Markov games and Markov potential games, and serves as a step to model more realistic strategic interactions that feature both competing and cooperative interests. Our main contribution is the first algorithm for computing stationary $\epsilon$-approximate Nash equilibria in adversarial team Markov games with computational complexity that is polynomial in all the natural parameters of the game, as well as $1/\epsilon$.
    
    The proposed algorithm is based on performing independent policy gradient steps for each player in the team, in tandem with best responses from the side of the adversary; in turn, the policy for the adversary is then obtained by solving a carefully constructed linear program. Our analysis leverages non-standard techniques to establish the KKT optimality conditions for a nonlinear program with nonconvex constraints, thereby leading to a natural interpretation of the induced Lagrange multipliers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                397. <h><a href=https://openreview.net/pdf?id=okwxL_c4x84>Clifford Neural Layers for PDE Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Johannes Brandstetter, Rianne van den Berg, Max Welling, Jayesh K Gupta</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=okwxL_c4x84>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce neural network layers on composite objects of scalars, vectors, and higher order objects such as bivectors.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/okwxL_c4x84.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Partial differential equations (PDEs) see widespread use in sciences and engineering to describe simulation of physical processes as scalar and vector fields interacting and coevolving over time. Due to the computationally expensive nature of their standard solution methods, neural PDE surrogates have become an active research topic to accelerate these simulations. However, current methods do not explicitly take into account the relationship between different fields and their internal components, which are often correlated. Viewing the time evolution of such correlated fields through the lens of multivector fields allows us to overcome these limitations. Multivector fields consist of scalar, vector, as well as higher-order components, such as bivectors and trivectors. Their algebraic properties, such as multiplication, addition and other arithmetic operations can be described by Clifford algebras. To our knowledge, this paper presents the first usage of such multivector representations together with Clifford convolutions and Clifford Fourier transforms in the context of deep learning. The resulting Clifford neural layers are universally applicable and will find direct use in the areas of fluid dynamics, weather forecasting, and the modeling of physical systems in general. We empirically evaluate the benefit of Clifford neural layers by replacing convolution and Fourier operations in common neural PDE surrogates by their Clifford counterparts on 2D Navier-Stokes and weather modeling tasks, as well as 3D Maxwell equations. For similar parameter count, Clifford neural layers consistently improve generalization capabilities of the tested neural PDE surrogates.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                398. <h><a href=https://openreview.net/pdf?id=9IaN4FkVSR1>Deconstructing Distributions: A Pointwise Framework of Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gal Kaplun, Nikhil Ghosh, Saurabh Garg, Boaz Barak, Preetum Nakkiran</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9IaN4FkVSR1>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new lens for studying the pointwise performance of learning algorithms which reveals new insights into their behavior and goes beyond traditional notions of in-distribution and &#34;out-of-distribution&#34; learning. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9IaN4FkVSR1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In machine learning, we traditionally evaluate the performance of a single model, averaged over a collection of test inputs. In this work, we propose a new approach: we measure the performance of a collection of models when evaluated at *single input point*. Specifically, we study a point&#39;s *profile*: the relationship between models&#39; average performance on the test distribution and their pointwise performance on this individual point. We find that profiles can yield new insights into the structure of both models and data---in and out-of-distribution. For example, we empirically show that real data distributions consist of points with qualitatively different profiles. On one hand, there are ``compatible&#39;&#39; points with strong correlation between the pointwise and average performance. On the other hand, there are points with weak and even *negative* correlation: cases where improving overall model accuracy actually *hurts* performance on these inputs. As an application, we use profiles to construct a dataset we call CIFAR-10-NEG: a subset of CINIC-10 such that for standard models, accuracy on CIFAR-10-NEG is *negatively correlated* with CIFAR-10 accuracy. Illustrating for the first time an OOD dataset that completely inverts ``accuracy-on-the-line&#39;&#39; (Miller et al., 2021).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                399. <h><a href=https://openreview.net/pdf?id=ik91mY-2GN>Diffusion Probabilistic Fields</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Peiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M. Susskind, Miguel Ángel Bautista</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ik91mY-2GN>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A diffusion model that can learn distribution over fields
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ik91mY-2GN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion probabilistic models have quickly become a major approach for generative modeling of images, 3D geometry, video and other domains. However, to adapt diffusion generative modeling to these domains the denoising network needs to be carefully designed for each domain independently, oftentimes under the assumption that data lives in a Euclidean grid. In this paper we introduce Diffusion Probabilistic Fields (DPF), a diffusion model that can learn distributions over continuous functions defined over metric spaces, commonly known as fields. We extend the formulation of diffusion probabilistic models to deal with this field parametrization in an explicit way, enabling us to define an end-to-end learning algorithm that side-steps the requirement of representing fields with latent vectors as in previous approaches (Dupont et al., 2022a; Du et al., 2021). We empirically show that, while using the same denoising network, DPF effectively deals with different modalities like 2D images and 3D geometry, in addition to modeling distributions over fields defined on non-Euclidean metric spaces.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                400. <h><a href=https://openreview.net/pdf?id=QCrw0u9LQ7>Iterative Patch Selection for High-Resolution Image Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Benjamin Bergner, Christoph Lippert, Aravindh Mahendran</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QCrw0u9LQ7>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple, memory-efficient method that selects the most salient patches from a high-resolution image and then aggregates them into a global representation for image recognition.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QCrw0u9LQ7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    High-resolution images are prevalent in various applications, such as autonomous driving and computer-aided diagnosis. However, training neural networks on such images is computationally challenging and easily leads to out-of-memory errors even on modern GPUs. We propose a simple method, Iterative Patch Selection (IPS), which decouples the memory usage from the input size and thus enables the processing of arbitrarily large images under tight hardware constraints. IPS achieves this by selecting only the most salient patches, which are then aggregated into a global representation for image recognition. For both patch selection and aggregation, a cross-attention based transformer is introduced, which exhibits a close connection to Multiple Instance Learning. Our method demonstrates strong performance and has wide applicability across different domains, training regimes and image sizes while using minimal accelerator memory. For example, we are able to finetune our model on whole-slide images consisting of up to 250k patches (&gt;16 gigapixels) with only 5 GB of GPU VRAM at a batch size of 16.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                401. <h><a href=https://openreview.net/pdf?id=jpq0qHggw3t>Partial Label Unsupervised Domain Adaptation with Class-Prototype Alignment</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yan Yan, Yuhong Guo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jpq0qHggw3t>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This is the first partial label learning method that handles partial label learning and unsupervised domain adaptation simultaneously.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jpq0qHggw3t.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Partial label learning (PLL) tackles the problem where each instance is associated with a set of candidate labels, only one of which is the ground-truth label. Most existing PLL approaches assume that both the training and test sets share an identical data distribution. However, this assumption does not hold in many real-world scenarios where the training and test data come from different distributions. In this paper, we formalize this learning scenario as a new problem called partial label unsupervised domain adaptation (PLUDA). To address this challenging PLUDA problem, we propose a novel Prototype Alignment based PLUDA method named PAPLUDA, which dynamically refines the pseudo-labels of instances from both the source and target domains by consulting the outputs of a teacher-student model in a moving-average manner, and bridges the cross-domain discrepancy through inter-domain class-prototype alignment. In addition, a teacher-student model based contrastive regularization is deployed to enhance prediction stability and hence improve the class-prototypes in both domains for PLUDA. Comprehensive experimental results demonstrate that PAPLUDA achieves state-of-the-art performance on the widely used benchmark datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                402. <h><a href=https://openreview.net/pdf?id=F_EhNDSamN>Parametrizing Product Shape Manifolds by Composite Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Josua Sassen, Klaus Hildebrandt, Martin Rumpf, Benedikt Wirth</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=F_EhNDSamN>7.0 - [5, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/F_EhNDSamN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Parametrizations of data manifolds in shape spaces can be computed using the rich toolbox of Riemannian geometry. This, however, often comes with high computational costs, which raises the question if one can learn an efficient neural network approximation. We show that this is indeed possible for shape spaces with a special product structure, namely those smoothly approximable by a direct sum of low-dimensional manifolds. Our proposed architecture leverages this structure by separately learning approximations for the low-dimensional factors and a subsequent combination. After developing the approach as a general framework, we apply it to a shape space of triangular surfaces. Here, typical examples of data manifolds are given through datasets of articulated models and can be factorized, for example, by a Sparse Principal Geodesic Analysis (SPGA). We demonstrate the effectiveness of our proposed approach with experiments on synthetic data as well as manifolds extracted from data via SPGA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                403. <h><a href=https://openreview.net/pdf?id=o58JtGDs6y>The Surprising Computational Power of Nondeterministic Stack RNNs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Brian DuSell, David Chiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=o58JtGDs6y>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that nondeterministic stack RNNs can learn non-CFLs and languages with surprisingly large alphabets, and we propose a new version that models a stack of vector embeddings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/o58JtGDs6y.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Traditional recurrent neural networks (RNNs) have a fixed, finite number of memory cells. In theory (assuming bounded range and precision), this limits their formal language recognition power to regular languages, and in practice, RNNs have been shown to be unable to learn many context-free languages (CFLs). In order to expand the class of languages RNNs recognize, prior work has augmented RNNs with a nondeterministic stack data structure, putting them on par with pushdown automata and increasing their language recognition power to CFLs. Nondeterminism is needed for recognizing all CFLs (not just deterministic CFLs), but in this paper, we show that nondeterminism and the neural controller interact to produce two more unexpected abilities. First, the nondeterministic stack RNN can recognize not only CFLs, but also many non-context-free languages. Second, it can recognize languages with much larger alphabet sizes than one might expect given the size of its stack alphabet. Finally, to increase the information capacity in the stack and allow it to solve more complicated tasks with large alphabet sizes, we propose a new version of the nondeterministic stack that simulates stacks of vectors rather than discrete symbols. We demonstrate perplexity improvements with this new model on the Penn Treebank language modeling benchmark.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                404. <h><a href=https://openreview.net/pdf?id=rVM8wD2G7Dy>Imbalanced Semi-supervised Learning with Bias Adaptive Classifier</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Renzhen Wang, Xixi Jia, Quanziang Wang, Yichen Wu, Deyu Meng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rVM8wD2G7Dy>7.0 - [5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work proposes a bi-level learning framework to learn a tailored classifier for imbalanced semi-supervised learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rVM8wD2G7Dy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pseudo-labeling has proven to be a promising semi-supervised learning (SSL) paradigm. Existing pseudo-labeling methods commonly assume that the class distributions of training data are balanced. However, such an assumption is far from realistic scenarios and thus severely limits the performance of current pseudo-labeling methods under the context of class-imbalance. To alleviate this problem, we design a bias adaptive classifier that targets the imbalanced SSL setups. The core idea is to automatically assimilate the training bias caused by class imbalance via the bias adaptive classifier, which is composed of a novel bias attractor and the original linear classifier. The bias attractor is designed as a light-weight residual network and learned through a bi-level learning framework, which enables the bias adaptive classifier to fit imbalanced training data, while the linear classifier can provide unbiased label prediction for each class. We conduct extensive experiments under various imbalanced semi-supervised setups, and the results demonstrate that our method can be applied to different pseudo-labeling models and is superior to current state-of-the-art methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                405. <h><a href=https://openreview.net/pdf?id=MkbcAHIYgyS>Mass-Editing Memory in a Transformer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, David Bau</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MkbcAHIYgyS>7.0 - [8, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An algorithm that can make tens of thousands of edits to an autoregressive transformer&#39;s memory.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MkbcAHIYgyS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by an order of magnitude. Our code and data will be open-sourced upon publication.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                406. <h><a href=https://openreview.net/pdf?id=U9HW6vyNClg>Towards Minimax Optimal Reward-free Reinforcement Learning in Linear MDPs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pihe Hu, Yu Chen, Longbo Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=U9HW6vyNClg>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a computationally-efficient algorithm for reward-free exploration in linear MDPs reaching a minimax optimal sample complexity up to an $H$ and logarithm factor.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/U9HW6vyNClg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). In this setting, an agent first interacts with the environment without accessing the reward function in the exploration phase. In the subsequent planning phase, it is given a reward function and asked to output an $\epsilon$-optimal policy. We propose a novel algorithm LSVI-RFE under the linear MDP setting, where the transition probability and reward functions are linear in a feature mapping. We prove an $\widetilde{O}(H^{4} d^{2}/\epsilon^2)$ sample complexity upper bound for LSVI-RFE, where $H$ is the episode length and $d$ is the feature dimension. We also establish a sample complexity lower bound of $\Omega(H^{3} d^{2}/\epsilon^2)$. To the best of our knowledge, LSVI-RFE is the first computationally efficient algorithm that achieves the minimax optimal sample complexity in linear MDP settings up to an $H$ and logarithmic factors. Our LSVI-RFE algorithm is based on a novel variance-aware exploration mechanism to avoid overly-conservative exploration in prior works. Our sharp bound relies on the decoupling of UCB bonuses during two phases, and a Bernstein-type self-normalized bound, which remove the extra dependency of sample complexity on $H$ and $d$, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                407. <h><a href=https://openreview.net/pdf?id=xtbog7cfsr>The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mor Shpigel Nacson, Rotem Mulayoff, Greg Ongie, Tomer Michaeli, Daniel Soudry</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xtbog7cfsr>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A study of multivariate two layer ReLU nets via dynamical stability, showing bias to smooth functions, depth separation and stable approximation guarantees
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xtbog7cfsr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the type of solutions to which stochastic gradient descent converges when used to train a single hidden-layer multivariate ReLU network with the quadratic loss. Our results are based on a dynamical stability analysis. In the univariate case, it was shown that linearly stable minima correspond to network functions (predictors), whose second derivative has a bounded weighted $L^1$ norm. Notably, the bound gets smaller as the step size increases, implying that training with a large step size leads to `smoother&#39; predictors. Here we generalize this result to the multivariate case, showing that a similar result applies to the Laplacian of the predictor. We demonstrate the tightness of our bound on the MNIST dataset, and show that it accurately captures the behavior of the solutions as a function of the step size. Additionally, we prove a depth separation result on the approximation power of ReLU networks corresponding to stable minima of the loss. Specifically, although shallow ReLU networks are universal approximators, we prove that stable shallow networks are not. Namely, there is a function that cannot be well-approximated by stable single hidden-layer ReLU networks trained with a non-vanishing step size. This is while the same function can be realized as a stable two hidden-layer ReLU network. Finally, we prove that if a function is sufficiently smooth (in a Sobolev sense) then it can be approximated arbitrarily well using shallow ReLU networks that correspond to stable solutions of gradient descent.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                408. <h><a href=https://openreview.net/pdf?id=8U4joMeLRF>Rethinking Self-Supervised Visual Representation Learning in Pre-training for 3D Human Pose and Shape Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongsuk Choi, Hyeongjin Nam, Taeryung Lee, Gyeongsik Moon, Kyoung Mu Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8U4joMeLRF>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Empirical Study of Pre-training a Backbone for 3D Human Pose and Shape Estimation
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8U4joMeLRF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, a few self-supervised representation learning (SSL) methods have outperformed the ImageNet classification pre-training for vision tasks such as object detection. However, its effects on 3D human body pose and shape estimation (3DHPSE) are open to question, whose target is fixed to a unique class, the human, and has an inherent task gap with SSL. We empirically study and analyze the effects of SSL and further compare it with other pre-training alternatives for 3DHPSE. The alternatives are 2D annotation-based pre-training and synthetic data pre-training, which share the motivation of SSL that aims to reduce the labeling cost. They have been widely utilized as a source of weak-supervision or fine-tuning, but have not been remarked as a pre-training source. SSL methods underperform the conventional ImageNet classification pre-training on multiple 3DHPSE benchmarks by 7.7% on average. In contrast, despite a much less amount of pre-training data, the 2D annotation-based pre-training improves accuracy on all benchmarks and shows faster convergence during fine-tuning. Our observations challenge the naive application of the current SSL pre-training to 3DHPSE and relight the value of other data types in the pre-training aspect.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                409. <h><a href=https://openreview.net/pdf?id=3VO1y5N7K1H>StableDR: Stabilized Doubly Robust Learning for Recommendation on Data Missing Not at Random</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoxuan Li, Chunyuan Zheng, Peng Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3VO1y5N7K1H>7.0 - [8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a theoretically guaranteed stabilized doubly robust learning approach that overcomes the shortcomings due to the presence of extremely small propensities in debiased recommendations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3VO1y5N7K1H.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In recommender systems, users always choose the favorite items to rate, which leads to data missing not at random and poses a great challenge for unbiased evaluation and learning of prediction models. Currently, the doubly robust (DR) methods have been widely studied and demonstrate superior performance. However, in this paper, we show that DR methods are unstable and have unbounded bias, variance, and generalization bounds to extremely small propensities. Moreover, the fact that DR relies more on extrapolation will lead to suboptimal performance. To address the above limitations while retaining double robustness, we propose a stabilized doubly robust (StableDR) learning approach with a weaker reliance on extrapolation. Theoretical analysis shows that StableDR has bounded bias, variance, and generalization error bound simultaneously under inaccurate imputed errors and arbitrarily small propensities. In addition, we propose a novel learning approach for StableDR that updates the imputation, propensity, and prediction models cyclically, achieving more stable and accurate predictions. Extensive experiments show that our approaches significantly outperform the existing methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                410. <h><a href=https://openreview.net/pdf?id=aBIpZvMdS56>Over-parameterized Model Optimization with Polyak-{\L}ojasiewicz Condition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yixuan Chen, Yubin Shi, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert Dick, Qin Lv, Yingying Zhao, Fan Yang, Ning Gu, Li Shang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aBIpZvMdS56>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work proposes a new regularized risk minimization for over-parameterized models with a novel PL regularization and implements it via network pruning guided by PL-based condition number. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aBIpZvMdS56.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work pursues the optimization of over-parameterized deep models for superior training efficiency and test performance. We first theoretically emphasize the importance of two properties of over-parameterized models, i.e., the convergence gap and the generalization gap. Subsequent analyses unveil that these two gaps can be upper-bounded by the ratio of the Lipschitz constant and the Polyak-{\L}ojasiewicz (PL) constant, a crucial term abbreviated as the \emph{condition number}. Such discoveries have led to a structured pruning method with a novel pruning criterion. That is, we devise a gating network that dynamically detects and masks out those poorly-behaved nodes of a deep model during the training session. To this end, this gating network is learned via minimizing the \emph{condition number} of the target model, and this process can be implemented as an extra regularization loss term. Experimental studies demonstrate that the proposed method outperforms the baselines in terms of both training efficiency and test performance, exhibiting the potential of generalizing to a variety of deep network architectures and tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                411. <h><a href=https://openreview.net/pdf?id=5-Df3tljit7>Defending against Adversarial Audio  via Diffusion Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shutong Wu, Jiongxiao Wang, Wei Ping, Weili Nie, Chaowei Xiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5-Df3tljit7>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a defense method based based on diffusion models for acoustic systems against diverse audio adversarial examples.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5-Df3tljit7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning models have been widely used in commercial acoustic systems in recent years. However, adversarial audio examples can cause abnormal behaviors for those acoustic systems, while being hard for humans to perceive. Various methods, such as transformation-based defenses and adversarial training, have been proposed to protect acoustic systems from adversarial attacks, but they are less effective against adaptive attacks. Furthermore, directly applying the methods from the image domain can lead to suboptimal results because of the unique properties of audio data. In this paper, we propose an adversarial purification-based defense pipeline, AudioPure, for acoustic systems via off-the-shelf diffusion models. Taking advantage of the strong generation ability of diffusion models, AudioPure first adds a small amount of noise to the adversarial audio and then runs the reverse sampling step to purify the noisy audio and recover clean audio. AudioPure is a plug-and-play method that can be directly applied to any pretrained classifier without any fine-tuning or re-training. We conduct extensive experiments on the speech command recognition task to evaluate the robustness of AudioPure. Our method is effective against diverse adversarial attacks (e.g. L2 or L∞-norm). It outperforms the existing methods under both strong adaptive white-box and black-box attacks bounded by L2 or L∞-norm (up to +20% in robust accuracy). Besides, we also evaluate the certified robustness for perturbations bounded by L2-norm via randomized smoothing. Our pipeline achieves a higher certified accuracy than baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                412. <h><a href=https://openreview.net/pdf?id=Jifob4dSh99>Theoretical Characterization of the Generalization Performance of Overfitted Meta-Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Peizhong Ju, Yingbin Liang, Ness Shroff</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Jifob4dSh99>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Jifob4dSh99.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Meta-learning has arisen as a successful method for improving training performance by training over many similar tasks, especially with deep neural networks (DNNs). However, the theoretical understanding of when and why overparameterized models such as DNNs can generalize well in meta-learning is still limited. As an initial step towards addressing this challenge, this paper studies the generalization performance of overfitted meta-learning under a linear regression model with Gaussian features. In contrast to a few recent studies along the same line, our framework allows the number of model parameters to be arbitrarily larger than the number of features in the ground truth signal, and hence naturally captures the overparameterized regime in practical deep meta-learning. We show that the overfitted min $\ell_2$-norm solution of model-agnostic meta-learning (MAML) can be beneficial, which is similar to the recent remarkable findings on &#34;benign overfitting&#34; and &#34;double descent&#34; phenomenon in the classical (single-task) linear regression. However, due to the uniqueness of meta-learning such as task-specific gradient descent inner training and the diversity/fluctuation of the ground-truth signals among training tasks, we find new and interesting properties that do not exist in single-task linear regression. We first provide a high-probability upper bound (under reasonable tightness) on the generalization error, where certain terms decrease when the number of features increases. Our analysis suggests that benign overfitting is more significant and easier to observe when the noise and the diversity/fluctuation of the ground truth of each training task are large. Under this circumstance, we show that the overfitted min $\ell_2$-norm solution can achieve an even lower generalization error than the underparameterized solution.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                413. <h><a href=https://openreview.net/pdf?id=dYHYXZ3uGdQ>A General Rank Preserving Framework for Asymmetric Image Retrieval</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hui Wu, Min Wang, Wengang Zhou, Houqiang Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dYHYXZ3uGdQ>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a rank preserving framework to achieve the consistency of the ranking lists returned by asymmetric and symmetric retrieval. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dYHYXZ3uGdQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Asymmetric image retrieval aims to deploy compatible models on platforms of different resources to achieve a balance between computational efficiency and retrieval accuracy. The most critical issue is how to align the output features of different models. Despite the great progress, existing approaches apply strong constraints so that features or neighbor structures are strictly aligned across different models. However, such a one-to-one constraint is too strict to be well preserved for the query models with low capacity. Considering that the primary concern of the users is the rank of the returned images, we propose a generic rank preserving framework, which achieves feature compatibility and the order consistency between query and gallery models simultaneously. Specifically, we propose two alternatives to instantiate the framework. One realizes straightforward rank order preservation by directly preserving the consistency of the sorting results. To make sorting process differentiable, the Heaviside step function in sorting is approximated by the sigmoid function. The other aims to preserve a learnable monotonic mapping relationship between the returned similarity scores of query and gallery models. The mapped similarity scores of gallery model are considered as pseudo-supervision to guide the query model training. Extensive experiments on various large-scale datasets demonstrate the superiority of our two proposed methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                414. <h><a href=https://openreview.net/pdf?id=Vf6WcUDnY7c>Optimizing Spca-based Continual Learning: A Theoretical Approach</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chunchun Yang, Malik Tiomoko, Zengfu Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Vf6WcUDnY7c>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a theoretical analysis of a simple but efficient continual learning algorithm
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Vf6WcUDnY7c.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Catastrophic forgetting and the stability-plasticity dilemma are two major obstacles to continual learning. In this paper we first propose a theoretical analysis of a SPCA-based continual learning algorithm using high dimensional statistics. Second, we design OSCL  (Optimized Spca-based Continual Learning) which builds on a flexible task optimization based on the theory. By optimizing a single task, catastrophic forgetting can be prevented theoretically. While optimizing multi-tasks, the trade-off between integrating knowledge from the new task and retaining previous knowledge of the old task can be achieved by assigning appropriate weights to corresponding tasks in compliance with the objectives. Experimental results confirm that the various theoretical conclusions are robust to a wide range of data distributions. Besides, several applications on synthetic and real data show that the proposed method while being computationally efficient, achieves comparable results with some state of the art.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                415. <h><a href=https://openreview.net/pdf?id=4yqxDCbzS98>Weakly Supervised Knowledge Transfer with Probabilistic Logical Reasoning for Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Martijn Oldenhof, Adam Arany, Yves Moreau, Edward De Brouwer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4yqxDCbzS98>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this work, we propose ProbKT, a framework based on probabilistic logical reasoning to train object detection models with weak supervision, by transferring knowledge from a source domain where instance-level annotations are available.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4yqxDCbzS98.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training object detection models usually requires instance-level annotations, such as the positions and labels of all objects present in each image. Such supervision is unfortunately not always available and, more often, only image-level information is provided, also known as weak supervision. 
Recent works have addressed this limitation by leveraging knowledge from a richly annotated domain. However, the scope of weak supervision supported by these approaches has been very restrictive, preventing them to use all available information. In this work, we propose ProbKT, a framework based on probabilistic logical reasoning to train object detection models with arbitrary types of weak supervision. We empirically show on different datasets that using all available information is beneficial as our ProbKT leads to significant improvement on target domain and better generalisation compared to existing baselines. We also showcase the ability of our approach to handle complex logic statements as supervision signal.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                416. <h><a href=https://openreview.net/pdf?id=-P7G-8dmSh4>Formal Mathematics Statement Curriculum Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-P7G-8dmSh4>7.0 - [8, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-P7G-8dmSh4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We explore the use of expert iteration in the context of language modeling applied to formal mathematics. We show that at same compute budget, expert iteration, by which we mean proof search interleaved with learning, dramatically outperforms proof search only. We also observe that when applied to a collection of formal statements of sufficiently varied difficulty, expert iteration is capable of finding and solving a curriculum of increasingly difficult problems, without the need for associated ground-truth proofs. Finally, by applying this expert iteration to a manually curated set of problem statements, we surpass previous state-of-the-art on the miniF2F benchmark, automatically solving multiple challenging problems drawn from high school olympiads.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                417. <h><a href=https://openreview.net/pdf?id=XFSCKELP3bp>Your Contrastive Learning Is Secretly Doing Stochastic Neighbor Embedding</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianyang Hu, Zhili LIU, Fengwei Zhou, Wenjia Wang, Weiran Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XFSCKELP3bp>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work proposes a novel perspective that interprets SSCL methods as a type of SNE methods, which facilitates both deeper theoretical understandings of SSCL, and methodological guidelines for practical improvement.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XFSCKELP3bp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contrastive learning, especially self-supervised contrastive learning (SSCL), has achieved great success in extracting powerful features from unlabeled data. In this work, we contribute to the theoretical understanding of SSCL and uncover its connection to the classic data visualization method, stochastic neighbor embedding (SNE), whose goal is to preserve pairwise distances. From the perspective of preserving neighboring information, SSCL can be viewed as a special case of SNE with the input space pairwise similarities specified by data augmentation. The established correspondence facilitates deeper theoretical understanding of learned features of SSCL, as well as methodological guidelines for practical improvement. Specifically, through the lens of SNE, we provide novel analysis on domain-agnostic augmentations, implicit bias and robustness of learned features. To illustrate the practical advantage, we demonstrate that the modifications from SNE to $t$-SNE can also be adopted in the SSCL setting, achieving significant improvement in both in-distribution and out-of-distribution generalization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                418. <h><a href=https://openreview.net/pdf?id=NPrsUQgMjKK>Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew Brock, Samuel L Smith, Yee Whye Teh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NPrsUQgMjKK>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Understanding and improving signal propagation in self-attention layers to train deep transformers without skip connections and/or normalisation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NPrsUQgMjKK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Skip connections and normalisation layers form two standard architectural components that are ubiquitous for the training of Deep Neural Networks (DNNs), but whose precise roles are poorly understood. Recent approaches such as Deep Kernel Shaping have made progress towards reducing our reliance on them, using insights from wide NN kernel theory to improve signal propagation in vanilla DNNs (which we define as networks without skips or normalisation). However, these approaches are incompatible with the self-attention layers present in transformers, whose kernels are intrinsically more complicated to analyse and control.  And so the question remains: \emph{is it possible to train deep vanilla transformers?} We answer this question in the affirmative by designing several approaches that use combinations of parameter initialisations, bias matrices and location-dependent rescaling to achieve faithful signal propagation in vanilla transformers. Our methods address various intricacies specific to signal propagation in transformers, including the interaction with positional encoding and causal masking. In experiments on WikiText-103 and C4, our approaches enable deep transformers without normalisation to train at speeds matching their standard counterparts, and deep vanilla transformers to reach the same performance as standard ones after about 5 times more iterations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                419. <h><a href=https://openreview.net/pdf?id=9gfir3fSy3J>NeRN: Learning Neural Representations for Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Maor Ashkenazi, Zohar Rimon, Ron Vainshtein, Shir Levi, Elad Richardson, Pinchas Mintz, Eran Treister</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9gfir3fSy3J>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper we present NerN: a neural representation for the weights of a pretrained neural network, which is obtained by applying smoothness over the reconstructed weights and various knowledge distillation techniques
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9gfir3fSy3J.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Representations have recently been shown to effectively reconstruct a wide range of signals from 3D meshes and shapes to images and videos. We show that, when adapted correctly, neural representations can be used to directly represent the weights of a pre-trained convolutional neural network, resulting in a Neural Representation for Neural Networks (NeRN). Inspired by coordinate inputs of previous neural representation methods, we assign a coordinate to each convolutional kernel in our network based on its position in the architecture, and optimize a predictor network to map coordinates to their corresponding weights. Similarly to the spatial smoothness of visual scenes, we show that incorporating a smoothness constraint over the original network&#39;s weights aids NeRN towards a better reconstruction. In addition, since slight perturbations in pre-trained model weights can result in a considerable accuracy loss, we employ techniques from the field of knowledge distillation to stabilize the learning process. We demonstrate the effectiveness of NeRN in reconstructing widely used architectures on CIFAR-10, CIFAR-100, and ImageNet. Finally, we present two applications using NeRN, demonstrating the capabilities of the learned representations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                420. <h><a href=https://openreview.net/pdf?id=QVcDQJdFTG>Ensuring DNN Solution Feasibility for Optimization Problems with Linear Constraints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianyu Zhao, Xiang Pan, Minghua Chen, Steven Low</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QVcDQJdFTG>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a preventive learning framework to ensure DNN solution feasibility for optimization problems with linear constraints without post-processing.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QVcDQJdFTG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose preventive learning as the first framework to guarantee Deep Neural Network (DNN) solution feasibility for optimization problems with linear constraints without post-processing, upon satisfying a mild condition on constraint calibration. Without loss of generality, we focus on problems with only inequality constraints. We systematically calibrate the inequality constraints used in training, thereby anticipating DNN prediction errors and ensuring the obtained solutions remain feasible. We characterize the calibration rate and a critical DNN size, based on which we can directly construct a DNN with provable solution feasibility guarantee. We further propose an Adversarial-Sample Aware training algorithm to improve its optimality performance. We apply the framework to develop DeepOPF+ for solving essential DC optimal power flow problems in grid operation. Simulation results over IEEE test cases show that it outperforms existing strong DNN baselines in ensuring 100\% feasibility and attaining consistent optimality loss (&lt;0.19%) and speedup (up to x228) in both light-load and heavy-load regimes, as compared to a state-of-the-art solver. We also apply our framework to a non-convex problem and show its performance advantage over existing schemes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                421. <h><a href=https://openreview.net/pdf?id=c9lAOPvQHS>Fisher-Legendre (FishLeg) optimization of deep neural networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jezabel R Garcia, Federica Freddi, Stathi Fotiadis, Maolin Li, Sattar Vakili, Alberto Bernacchia, Guillaume Hennequin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=c9lAOPvQHS>7.0 - [6, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a new approach to estimate the natural gradient via Legendre-Fenchel duality, provide a convergence proof, and show competitive performance on a number of benchmarks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/c9lAOPvQHS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Incorporating second-order gradient information (curvature) into optimization can dramatically reduce the number of iterations required to train machine learning models. In natural gradient descent, such information comes from the Fisher information matrix which yields a number of desirable properties. As exact natural gradient updates are intractable for large models, successful methods such as KFAC and sequels approximate the Fisher in a structured form that can easily be inverted. However, this requires model/layer-specific tensor algebra and certain approximations that are often difficult to justify. Here, we use ideas from Legendre-Fenchel duality to learn a direct and efficiently evaluated model for the product of the inverse Fisher with any vector, in an online manner, leading to natural gradient steps that get progressively more accurate over time despite noisy gradients. We prove that the resulting “Fisher-Legendre” (FishLeg) optimizer converges to a (global) minimum of non-convex functions satisfying the PL condition, which applies in particular to deep linear networks. On standard auto-encoder benchmarks, we show empirically that FishLeg outperforms standard first-order optimization methods, and performs on par with or better than other second-order methods, especially when using small batches. Thanks to its generality, we expect our approach to facilitate the handling of a variety  neural network layers in future work.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                422. <h><a href=https://openreview.net/pdf?id=2mvALOAWaxY>Lower Bounds on the Depth of Integral ReLU Neural Networks via Lattice Polytopes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Christian Alexander Haase, Christoph Hertrich, Georg Loho</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2mvALOAWaxY>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We derive lower bounds on the depth of integral ReLU neural networks using volume arguments for lattice polytopes arising from connections to tropical geometry.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2mvALOAWaxY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We prove that the set of functions representable by ReLU neural networks with integer weights strictly increases with the network depth while allowing arbitrary width. More precisely, we show that $\lceil\log_2(n)\rceil$ hidden layers are indeed necessary to compute the maximum of $n$ numbers, matching known upper bounds. Our results are based on the known duality between neural networks and Newton polytopes via tropical geometry. The integrality assumption implies that these Newton polytopes are lattice polytopes. Then, our depth lower bounds follow from a parity argument on the normalized volume of faces of such polytopes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                423. <h><a href=https://openreview.net/pdf?id=3mRwyG5one>DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, Heung-Yeung Shum</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3mRwyG5one>7.0 - [6, 8, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a state-of-the-art end-to-end object detector, the first DETR-like model on top of the COCO detection leader board.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3mRwyG5one.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present  DINO (DETR with Improved deNoising anchOr boxes), a strong end-to-end object detector. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a look forward twice scheme for box prediction, and a mixed query selection method for anchor initialization. DINO achieves 49.4AP in 12 epochs and 51.3AP in 24 epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of +6.0AP and +2.7AP, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO val2017 (63.2AP) and test-dev (63.3AP) with model size under 1 billion parameters. Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. The code will be available.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                424. <h><a href=https://openreview.net/pdf?id=gu-SC0dpkvw>Almost Linear Constant-Factor Sketching for $\ell_1$ and Logistic Regression</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexander Munteanu, Simon Omlor, David Woodruff</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gu-SC0dpkvw>7.0 - [5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We give the first constant factor approximate sketches for l1 and logistic regression in a turnstile stream with almost linear sketching dimension that result in an efficient optimization problem in the sketch space.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gu-SC0dpkvw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We improve upon previous oblivious sketching and turnstile streaming results for $\ell_1$ and logistic regression, giving a much smaller sketching dimension achieving $O(1)$-approximation and yielding an efficient optimization problem in the sketch space. Namely, we achieve for any constant $c&gt;0$ a sketching dimension of $\tilde{O}(d^{1+c})$ for $\ell_1$ regression and $\tilde{O}(\mu d^{1+c})$ for logistic regression, where $\mu$ is a standard measure that captures the complexity of compressing the data. For $\ell_1$-regression our sketching dimension is near-linear and improves previous work which either required $\Omega(\log d)$-approximation with this sketching dimension, or required a larger $\operatorname{poly}(d)$ number of rows. Similarly, for logistic regression previous work had worse $\operatorname{poly}(\mu d)$ factors in its sketching dimension. We also give a tradeoff that yields a $1+\varepsilon$ approximation in input sparsity time by increasing the total size to $(d\log(n)/\varepsilon)^{O(1/\varepsilon)}$ for $\ell_1$ and to $(\mu d\log(n)/\varepsilon)^{O(1/\varepsilon)}$ for logistic regression. Finally, we show that our sketch can be extended to approximate a regularized version of logistic regression where the data-dependent regularizer corresponds to the variance of the individual logistic losses.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                425. <h><a href=https://openreview.net/pdf?id=qBvBycTqVJ>Generalized Precision Matrix for Scalable Estimation of Nonparametric Markov Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yujia Zheng, Ignavier Ng, Yewen Fan, Kun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qBvBycTqVJ>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We investigate scalable estimation of nonparametric Markov networks with general distributions for all data types (i.e., continuous, discrete, and mixed-type).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qBvBycTqVJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A Markov network characterizes the conditional independence structure, or Markov property, among a set of random variables. Existing work focuses on specific families of distributions (e.g., exponential families) and/or certain structures of graphs, and most of them can only handle variables of a single data type (continuous or discrete). In this work, we characterize the conditional independence structure in general distributions for all data types (i.e., continuous, discrete, and mixed-type) with a Generalized Precision Matrix (GPM). Besides, we also allow general functional relations among variables, thus giving rise to a Markov network structure learning algorithm in one of the most general settings. To deal with the computational challenge of the problem, especially for large graphs, we unify all cases under the same umbrella of a regularized score matching framework. We validate the theoretical results and demonstrate the scalability empirically in various settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                426. <h><a href=https://openreview.net/pdf?id=PzBGIu-llo7>Learning Proximal Operators to Discover Multiple Optima</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lingxiao Li, Noam Aigerman, Vladimir Kim, Jiajin Li, Kristjan Greenewald, Mikhail Yurochkin, Justin Solomon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PzBGIu-llo7>7.0 - [5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PzBGIu-llo7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Finding multiple solutions of non-convex optimization problems is a ubiquitous yet challenging task. Most past algorithms either apply single-solution optimization methods from multiple random initial guesses or search in the vicinity of found solutions using ad hoc heuristics. We present an end-to-end method to learn the proximal operator of a family of training problems so that multiple local minima can be quickly obtained from initial guesses by iterating the learned operator, emulating the proximal-point algorithm that has fast convergence. The learned proximal operator can be further generalized to recover multiple optima for unseen problems at test time, enabling applications such as object detection. The key ingredient in our formulation is a proximal regularization term, which elevates the convexity of our training loss: by applying recent theoretical results, we show that for weakly-convex objectives with Lipschitz gradients, training of the proximal operator converges globally with a practical degree of over-parameterization. We further present an exhaustive benchmark for multi-solution optimization to demonstrate the effectiveness of our method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                427. <h><a href=https://openreview.net/pdf?id=nYWqxUwFc3x>Learning Vortex Dynamics for Fluid Inference and Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yitong Deng, Hong-Xing Yu, Jiajun Wu, Bo Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nYWqxUwFc3x>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nYWqxUwFc3x.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a novel differentiable vortex particle (DVP) method to infer and predict fluid dynamics from a single video. Lying at its core is a particle-based latent space to encapsulate the hidden, Lagrangian vortical evolution underpinning the observable, Eulerian flow phenomena. Our differentiable vortex particles are coupled with a learnable, vortex-to-velocity dynamics mapping to effectively capture the complex flow features in a physically-constrained, low-dimensional space. This representation facilitates the learning of a fluid simulator tailored to the input video that can deliver robust, long-term future predictions. The value of our method is twofold: first, our learned simulator enables the inference of hidden physics quantities (e.g., velocity field) purely from visual observation; secondly, it also supports future prediction, constructing the input video&#39;s sequel along with its future dynamics evolution. We compare our method with a range of existing methods on both synthetic and real-world videos, demonstrating improved reconstruction quality, visual plausibility, and physical integrity.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                428. <h><a href=https://openreview.net/pdf?id=txlWziuCE5W>MEDICAL IMAGE UNDERSTANDING WITH PRETRAINED VISION LANGUAGE MODELS: A COMPREHENSIVE STUDY</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziyuan Qin, Hua Hui Yi, Qicheng Lao, Kang Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=txlWziuCE5W>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper discuss about how to leverage the trending vision language model to transfer to the medical domain, showing exciting performance on zero-shot and few-shot learning tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/txlWziuCE5W.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The large-scale pre-trained vision language models (VLM) have shown remarkable domain transfer capability on natural images. However, it remains unknown whether this capability can also apply to the medical image domain. This paper thoroughly studies the knowledge transferability of pre-trained VLMs to the medical domain, where we show that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs. We demonstrate that by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization. This mechanism empowers VLMs to recognize novel objects with fewer or without image samples. Furthermore, to avoid the laborious manual designing process, we develop three approaches for automatic generation of medical prompts, which can inject expert-level medical knowledge and image-specific information into the prompts for fine-grained grounding. We conduct extensive experiments on thirteen different medical datasets across various modalities, showing that our well-designed prompts greatly improve the zero-shot performance compared to the default prompts, and our fine-tuned models surpass the supervised models by a significant margin.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                429. <h><a href=https://openreview.net/pdf?id=3itjR9QxFw>Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ting Chen, Ruixiang ZHANG, Geoffrey Hinton</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3itjR9QxFw>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Generating discrete/categorical data with (continuous) diffusion models; also presents a technique that improves diffusion models in general.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3itjR9QxFw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present Bit Diffusion: a simple and generic approach for generating discrete data with continuous state and continuous time diffusion models. The main idea behind our approach is to first represent the discrete data as binary bits, and then train a continuous diffusion model to model these bits as real numbers which we call analog bits. To generate samples, the model first generates the analog bits, which are then thresholded to obtain the bits that represent the discrete variables. We further propose two simple techniques, namely Self-Conditioning and Asymmetric Time Intervals, which lead to a significant improvement in sample quality. Despite its simplicity, the proposed approach can achieve strong performance in both discrete image generation and image captioning tasks. For discrete image generation, we significantly improve previous state-of-the-art on both CIFAR-10 (which has 3K discrete 8-bit tokens) and ImageNet-64x64 (which has 12K discrete 8-bit tokens), outperforming the best autoregressive model in both sample quality (measured by FID) and efficiency. For image captioning on MS-COCO dataset, our approach achieves competitive results compared to autoregressive models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                430. <h><a href=https://openreview.net/pdf?id=QC10RmRbZy9>Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ping-yeh Chiang, Renkun Ni, David Yu Miller, Arpit Bansal, Jonas Geiping, Micah Goldblum, Tom Goldstein</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QC10RmRbZy9>7.0 - [8, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We empirically showed that a random optimizer performs just as well as SGD
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QC10RmRbZy9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    It is commonly believed that the implicit regularization of optimizers is needed for neural networks to generalize in the overparameterized regime. In this paper, we observe experimentally that this implicit regularization behavior is {\em generic}, i.e. it does not depend strongly on the choice of optimizer. We demonstrate this by training neural networks using several gradient-free optimizers, which do not benefit from properties that are often attributed to gradient-based optimizers.   This includes a guess-and-check optimizer that generates uniformly random parameter vectors until finding one that happens to achieve perfect train accuracy, and a zeroth-order Pattern Search optimizer that uses no gradient computations. In the low sample and few-shot regimes, where zeroth order optimizers are most computationally tractable, we find that these non-gradient optimizers achieve test accuracy comparable to SGD. The code to reproduce results can be found at https://github.com/Ping-C/optimizer .
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                431. <h><a href=https://openreview.net/pdf?id=HgQR0mXQ1_a>Write and Paint: Generative Vision-Language Models are Unified Modal Learners</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shizhe Diao, Wangchunshu Zhou, Xinsong Zhang, Jiawei Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HgQR0mXQ1_a>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The paper proposes a simple, scalable, and versatile seq2seq foundation model, which is capable of vision-language understanding, image-to-text generation, and text-to-image generation with a single unified architecture
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HgQR0mXQ1_a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent advances in vision-language pre-training have pushed the state-of-the-art on various vision-language tasks, making machines more capable of multi-modal writing (image-to-text generation) and painting (text-to-image generation). However, few studies investigate if these two essential capabilities can be learned together and boost each other, making a versatile and powerful multi-modal foundation model. In this work, we disclose the potential of symmetric generative vision-language pre-training in learning to write and paint concurrently, and propose a new unified modal model, named DaVinci, trained with prefix language modeling and prefix image modeling, a simple generative self-supervised objective on image-text pairs. Thanks to the proposed prefix multi-modal modeling framework, DaVinci is simple to train, scalable to huge data, adaptable to both writing and painting tasks, and also strong on other vision, text, and multi-modal understanding tasks. DaVinci achieves competitive performance on a wide range of 27 generation/understanding tasks and demonstrates the superiority of combining vision/language generative pre-training. Furthermore, we carefully benchmark the performance of different vision-language pre-training objectives on different scales of pre-training datasets on a heterogeneous and broad distribution coverage. Our results demonstrate the potential of exploiting self-supervision in both language and vision inputs, and establish new, stronger baselines for future comparisons at different data scales. The code and pre-trained models are available at https://github.com/shizhediao/DaVinci.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                432. <h><a href=https://openreview.net/pdf?id=1_jFneF07YC>Unsupervised Semantic Segmentation with Self-supervised Object-centric Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu, Francesco Locatello, Thomas Brox</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1_jFneF07YC>7.0 - [8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Strong and simple baseline for unsupervised segmentation methods obtained by leveraging and combining object-centric priors.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1_jFneF07YC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we show that recent advances in self-supervised representation learning enable unsupervised object discovery and semantic segmentation with a performance that matches the state of the field on supervised semantic segmentation 10 years ago. We propose a methodology based on unsupervised saliency masks and self-supervised feature clustering to kickstart object discovery followed by training a semantic segmentation network on pseudo-labels to bootstrap the system on images with multiple objects. We show that while being conceptually simple our proposed baseline is surprisingly strong. We present results on PASCAL VOC that go far beyond the current state of the art (50.0 mIoU), and we report for the first time results on MS COCO for the whole set of 81 classes: our method discovers 34 categories with more than 20% IoU, while obtaining an average IoU of 19.6 for all 81 categories.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                433. <h><a href=https://openreview.net/pdf?id=mMNimwRb7Gr>Turning the Curse of Heterogeneity in Federated Learning into a Blessing for Out-of-Distribution Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuyang Yu, Junyuan Hong, Haotao Wang, Zhangyang Wang, Jiayu Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mMNimwRb7Gr>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mMNimwRb7Gr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks have witnessed huge successes in many challenging prediction tasks and yet they often suffer from out-of-distribution (OoD) samples, misclassifying them with high confidence. Recent advances show promising OoD detection performance for centralized training, and however, OoD detection in federated learning (FL) is largely overlooked, even though many security sensitive applications such as autonomous driving and voice recognition authorization are commonly trained using FL for data privacy concerns. The main challenge that prevents previous state-of-the-art OoD detection methods from being incorporated to FL is that they require large amount of real OoD samples. However, in real-world scenarios, such large-scale OoD training data can be costly or even infeasible to obtain, especially for resource-limited local devices. On the other hand, a notorious challenge in FL is data heterogeneity where each client collects non-identically and independently distributed (non-iid) data. We propose to take advantage of such heterogeneity and turn the curse into a blessing that facilitates OoD detection in FL. The key is that for each client, non-iid data from other clients (unseen external classes) can serve as an alternative to real OoD samples. Specifically, we propose a novel Federated Out-of-Distribution Synthesizer (FOSTER), which learns a class-conditional generator to synthesize virtual external-class OoD samples, and maintains data confidentiality and communication efficiency required by FL. Experimental results show that our method outperforms the state-of-the-art by 2.49%, 2.88%, 1.42% AUROC, and 0.01%, 0.89%, 1.74% ID accuracy, on CIFAR-10, CIFAR-100, and STL10, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                434. <h><a href=https://openreview.net/pdf?id=8IN-qLkl215>Visually-Augmented Language Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8IN-qLkl215>7.0 - [6, 10, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel pre-trained framework, to Visually-augment text tokens with retrieved relevant images for multimodal grounded Language Modeling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8IN-qLkl215.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on the text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VaLM, to Visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VaLM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VaLM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending on both text context and visual knowledge in images. We evaluate VaLM on various visual knowledge intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VaLM outperforms all strong language-only and vision-language baselines with substantial gains on reasoning object commonsense including color, size, and shape.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                435. <h><a href=https://openreview.net/pdf?id=LfdEuhjR5GV>Adversarial Training of Self-supervised Monocular Depth Estimation against Physical-World Attacks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhiyuan Cheng, James Chenhao Liang, Guanhong Tao, Dongfang Liu, Xiangyu Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LfdEuhjR5GV>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Use self-supervised adversarial training to harden monocular depth estimation models against physical-world adversarial attacks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LfdEuhjR5GV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Monocular Depth Estimation (MDE) is a critical component in applications such as autonomous driving. There are various attacks against MDE networks. These attacks, especially the physical ones, pose a great threat to the security of such systems.  Traditional adversarial training method requires ground-truth labels and hence cannot be directly applied to self-supervised MDE that does not have depth ground truth. Some self-supervised model hardening technique (e.g., contrastive learning) ignores the domain knowledge of MDE and can hardly achieve optimal performance. In this work, we propose a novel adversarial training method for self-supervised MDE models based on view synthesis without using the depth ground truth. We improve adversarial robustness against physical-world attacks using $L_0$-norm-bounded perturbation in training. We compare our method with supervised learning-based and contrastive learning-based methods that are tailored for MDE. Results on two representative MDE networks show that we achieve better robustness against various adversarial attacks with nearly no benign performance degradation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                436. <h><a href=https://openreview.net/pdf?id=JAezPMehaUu>Mosaic Representation Learning for Self-supervised Visual Pre-training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhaoqing Wang, Ziyu Chen, Yaqian Li, Yandong Guo, Jun Yu, Mingming Gong, Tongliang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JAezPMehaUu>7.0 - [8, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple and effective mosaic representation learning framework consisting of a new data augmentation strategy, which aims to adequately learn discriminative feature representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JAezPMehaUu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-supervised learning has achieved significant success in learning visual representations without the need for manual annotation. To obtain generalizable representations, a meticulously designed data augmentation strategy is one of the most crucial parts. Recently, multi-crop strategies utilizing a set of small crops as positive samples have been shown to learn spatially structured features. However, it overlooks the diverse contextual backgrounds, which reduces the variance of the input views and degenerates the performance. To address this problem, we propose a mosaic representation learning framework (MosRep), consisting of a new data augmentation strategy that enriches the backgrounds of each small crop and improves the quality of visual representations. Specifically, we randomly sample numbers of small crops from different input images and compose them into a mosaic view, which is equivalent to introducing different background information for each small crop. Additionally, we further jitter the mosaic view to prevent memorizing the spatial locations of each crop. Along with optimization, our MosRep gradually extracts more discriminative features. Extensive experimental results demonstrate that our method improves the performance far greater than the multi-crop strategy on a series of downstream tasks, e.g., +7.4% and +4.9% than the multi-crop strategy on ImageNet-1K with 1% label and 10% label, respectively. Code is available at https://github.com/DerrickWang005/MosRep.git.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                437. <h><a href=https://openreview.net/pdf?id=bXNl-myZkJl>More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao Xiao, Boqian Wu, Tommi Kärkkäinen, Mykola Pechenizkiy, Decebal Constantin Mocanu, Zhangyang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bXNl-myZkJl>7.0 - [5, 6, 10, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose Sparse Large Kernel Network (SLaK), a pure CNN architectures equipped with 51x51 kernels that can perform on par with or better than the state-of-the-art hierarchical Transformers and modern ConvNets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bXNl-myZkJl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO. Codes are available at https://github.com/VITA-Group/SLaK.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                438. <h><a href=https://openreview.net/pdf?id=TUBpc5rqGA>Neural Design for Genetic Perturbation Experiments</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aldo Pacchiano, Drausin Wulsin, Robert A Barton, Luis Voloch</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TUBpc5rqGA>7.0 - [8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce and analyze many tractable methods for noiseless optimistic arm elimination with applications in genetic perturbation experiments.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TUBpc5rqGA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The problem of how to genetically modify cells in order to maximize a certain cellular phenotype has taken center stage in drug development over the last few years (with, for example, genetically edited CAR-T, CAR-NK, and CAR-NKT cells entering cancer clinical trials). Exhausting the search space for all possible genetic edits (perturbations) or combinations thereof is infeasible due to cost and experimental limitations. This work provides a theoretically sound framework for iteratively exploring the space of perturbations in pooled batches in order to maximize a target phenotype under an experimental budget. Inspired by this application domain, we study the problem of batch query bandit optimization and introduce the Optimistic Arm Elimination ($\mathrm{OAE}$) principle designed to find an almost optimal arm under different functional relationships between the queries (arms) and the outputs (rewards). We analyze the convergence properties of $\mathrm{OAE}$ by relating it to the Eluder dimension of the algorithm&#39;s function class and validate that $\mathrm{OAE}$ outperforms other strategies in finding optimal actions in experiments on simulated problems, public datasets well-studied in bandit contexts, and in genetic perturbation datasets when the regression model is a deep neural network. OAE also outperforms the benchmark algorithms in 3 of 4 datasets in the GeneDisco experimental planning challenge. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                439. <h><a href=https://openreview.net/pdf?id=ATLEl_izD87>Enhancing the Inductive Biases of Graph Neural ODE for Modeling Physical Systems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Suresh Bishnoi, Ravinder Bhattoo, Jayadeva Jayadeva, Sayan Ranu, N M Anoop Krishnan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ATLEl_izD87>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Inferring the dynamics of physical systems can be significantly enhanced by Graph neural ODEs with appropriate inductive biases
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ATLEl_izD87.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks with physics-based inductive biases such as Lagrangian neural networks (LNNs), and Hamiltonian neural networks (HNNs) learn the dynamics of physical systems by encoding strong inductive biases. Alternatively, Neural ODEs with appropriate inductive biases have also been shown to give similar performances. However, these models, when applied to particle-based systems, are transductive in nature and hence, do not generalize to large system sizes. In this paper, we present a graph-based neural ODE, GNODE, to learn the time evolution of dynamical systems. Further, we carefully analyze the role of different inductive biases on the performance of GNODE. We show that similar to LNN and HNN, encoding the constraints explicitly can significantly improve the training efficiency and performance of GNODE significantly. Our experiments also assess the value of additional inductive biases, such as Newton’s third law, on the final performance of the model. We demonstrate that inducing these biases can enhance the performance of the model by orders of magnitude in terms of both energy violation and rollout error. Interestingly, we observe that the GNODE trained with the most effective inductive biases, namely MCGNODE, outperforms the graph versions of LNN and HNN, namely, Lagrangian graph networks (LGN) and Hamiltonian graph networks (HGN) in terms of energy violation error by ∼4 orders of magnitude for a pendulum system, and ∼2 orders of magnitude for spring systems. These results suggest that NODE-based systems can give competitive performances with energy-conserving neural networks by employing appropriate inductive biases.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                440. <h><a href=https://openreview.net/pdf?id=0eTTKOOOQkV>HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, Yongfeng Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0eTTKOOOQkV>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0eTTKOOOQkV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The success of large-scale contrastive vision-language pretraining (CLIP) has benefited both visual recognition and multimodal content understanding. The concise design brings CLIP the advantage in inference efficiency against other vision-language models with heavier cross-attention fusion layers, making it a popular choice for a wide spectrum of downstream tasks. However, CLIP does not explicitly capture the hierarchical nature of high-level and fine-grained semantics conveyed in images and texts, which is arguably critical to vision-language understanding and reasoning. To this end, we equip both the visual and language branches in CLIP with hierarchy-aware attentions, namely Hierarchy-aware CLIP (HiCLIP), to progressively discover semantic hierarchies layer-by-layer from both images and texts in an unsupervised manner. As a result, such hierarchical aggregation significantly improves the cross-modal alignment. To demonstrate the advantages of HiCLIP, we conduct qualitative analysis on its unsupervised hierarchy induction during inference, as well as extensive quantitative experiments on both visual recognition and vision-language downstream tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                441. <h><a href=https://openreview.net/pdf?id=p_jIy5QFB7>Taking a Step Back with KCal: Multi-Class Kernel-Based Calibration for Deep Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhen Lin, Shubhendu Trivedi, Jimeng Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p_jIy5QFB7>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Provable full calibration for neural network classifiers using kernel density estimation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p_jIy5QFB7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural network (DNN) classifiers are often overconfident, producing miscalibrated class probabilities. In high-risk applications like healthcare, practitioners require fully calibrated probability predictions for decision-making. That is, conditioned on the prediction vector, every class’ probability should be close to the predicted value. Most existing calibration methods either lack theoretical guarantees for producing calibrated outputs, reduce classification accuracy in the process, or only calibrate the predicted class. This paper proposes a new Kernel-based calibration method called KCal. Unlike existing calibration procedures, KCal does not operate directly on the logits or softmax outputs of the DNN. Instead, KCal learns a metric space on the penultimate-layer latent embedding and generates predictions using kernel density estimates on a calibration set. We first analyze KCal theoretically, showing that it enjoys a provable full calibration guarantee. Then, through extensive experiments across a variety of datasets, we show that KCal consistently outperforms baselines as measured by the calibration error and by proper scoring rules like the Brier Score.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                442. <h><a href=https://openreview.net/pdf?id=M_MvkWgQSt>Real-time variational method for learning neural trajectory and its dynamics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthew Dowling, Yuan Zhao, Il Memming Park</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=M_MvkWgQSt>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A real-time variational Bayesian method aimed at uncovering latent neural trajectories and their dynamical systems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/M_MvkWgQSt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Latent variable models have become instrumental in computational neuroscience for reasoning about neural computation.  This has fostered the development of powerful offline algorithms for extracting latent neural trajectories from neural recordings.  However, despite the potential of real-time alternatives to give immediate feedback to experimentalists, and enhance experimental design, they have received markedly less attention.  In this work, we introduce the exponential family variational Kalman filter (eVKF), an online recursive Bayesian method aimed at inferring latent trajectories while simultaneously learning the dynamical system generating them.  eVKF works for arbitrary likelihoods and utilizes the constant base measure exponential family to model the latent state stochasticity. We derive a closed-form variational analog to the predict step of the Kalman filter which leads to a provably tighter bound on the ELBO compared to another online variational method. We validate our method on synthetic and real-world data, and, notably, show that it achieves competitive performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                443. <h><a href=https://openreview.net/pdf?id=ZzdBhtEH9yB>Implicit regularization in Heavy-ball momentum accelerated stochastic gradient descent</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Avrajit Ghosh, He Lyu, Xitong Zhang, Rongrong Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZzdBhtEH9yB>7.0 - [8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZzdBhtEH9yB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    It is well known that the finite step-size ($h$) in Gradient descent (GD) implicitly regularizes solutions to flatter minimas. A natural question to ask is \textit{Does the momentum parameter $\beta$ (say) play a role in implicit regularization in Heavy-ball (H.B) momentum accelerated gradient descent (GD+M)?}. To answer this question, first, we show that  the trajectory traced by discrete H.B momentum update (GD+M) is $O(h^2)$ close to a continuous trajectory induced by a modified loss, which consists of an original loss and an implicit regularizer. This implicit regularizer for (GD+M) is indeed stronger than that of (GD) by factor of $(\frac{1+\beta}{1-\beta})$, thus explaining why (GD+M) shows better generalization performance and higher test accuracy than (GD). Furthermore, we extend our analysis to stochastic version of gradient descent with momentum (SGD+M) and propose a deterministic continuous trajectory that is $O(h^2)$ close to the discrete update of (SGD+M) in a strong approximation sense. We explore the implicit regularization in (SGD+M) and (GD+M) through a series of experiments validating our theory. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                444. <h><a href=https://openreview.net/pdf?id=d7Q0vVfJ0wO>Implicit Regularization for Group Sparsity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiangyuan Li, Thanh V Nguyen, Chinmay Hegde, Raymond K. W. Wong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=d7Q0vVfJ0wO>7.0 - [5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the implicit regularization of gradient descent towards structured sparsity via a novel neural reparameterization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/d7Q0vVfJ0wO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the implicit regularization of gradient descent towards structured sparsity via a novel neural reparameterization, which we call a diagonally grouped linear neural network. We show the following intriguing property of our reparameterization: gradient descent over the squared regression loss, without any explicit regularization, biases towards solutions with a group sparsity structure. In contrast to many existing works in understanding implicit regularization, we prove that our training trajectory cannot be simulated by mirror descent. We analyze the gradient dynamics of the corresponding regression problem in the general noise setting and obtain minimax-optimal error rates. Compared to existing bounds for implicit sparse regularization using diagonal linear networks, our analysis with the new reparameterization shows improved sample complexity. In the degenerate case of size-one groups, our approach gives rise to a new algorithm for sparse linear regression. Finally, we demonstrate the efficacy of our approach with several numerical experiments.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                445. <h><a href=https://openreview.net/pdf?id=Bo7eeXm6An8>Multi-lingual Evaluation of Code Generation Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Bo7eeXm6An8>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Bo7eeXm6An8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present two new benchmarks, MBXP and Multilingual HumanEval, designed to evaluate code completion models in over 10 programming languages. These datasets are generated using a conversion framework that transpiles prompts and test cases from the original MBPP and HumanEval datasets into the corresponding data in the target language. By using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of  few-shot prompting to teach the model new languages, and zero-shot translation abilities. In addition, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                446. <h><a href=https://openreview.net/pdf?id=-jTaz3CMk72>Breaking Correlation Shift via Conditional Invariant Regularizer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingyang Yi, Ruoyu Wang, Jiacheng Sun, Zhenguo Li, Zhi-Ming Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-jTaz3CMk72>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes an algorithm to make the model to generalize on data with spurious correlation, the method can be implemented without information on spurious feature. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-jTaz3CMk72.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, generalization on out-of-distribution (OOD) data with correlation shift has attracted great attentions. The correlation shift is caused by the spurious attributes that correlate to the class label, as the correlation between them may vary in training and test data. For such a problem, we show that given the class label, the models that are conditionally independent of spurious attributes are OOD generalizable. Based on this, a metric Conditional Spurious Variation (CSV) which controls the OOD generalization error, is proposed to measure such conditional independence. To improve the OOD generalization, we regularize the training process with the proposed CSV. Under mild assumptions, our training objective can be formulated as a nonconvex-concave mini-max problem. An algorithm with a provable convergence rate is proposed to solve the problem. Extensive empirical results verify our algorithm&#39;s efficacy in improving OOD generalization.  
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                447. <h><a href=https://openreview.net/pdf?id=uyqks-LILZX>Modeling the Data-Generating Process is Necessary for Out-of-Distribution Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jivat Neet Kaur, Emre Kiciman, Amit Sharma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uyqks-LILZX>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uyqks-LILZX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent empirical studies on domain generalization (DG) have shown that DG algorithms that perform well on some distribution shifts fail on others, and no state-of-the-art DG algorithm performs consistently well on all shifts. Moreover, real-world data often has multiple distribution shifts over different attributes; hence we introduce multi-attribute distribution shift datasets and find that the accuracy of existing DG algorithms falls even further. To explain these results, we provide a formal characterization of generalization under multi-attribute shifts using a canonical causal graph. Based on the relationship between spurious attributes and the classification label, we obtain realizations of the canonical causal graph that characterize common distribution shifts and show that each shift entails different independence constraints over observed variables. As a result, we prove that any algorithm based on a single, fixed constraint cannot work well across all shifts, providing theoretical evidence for mixed empirical results on DG algorithms. Based on this insight, we develop Causally Adaptive Constraint Minimization (CACM), an algorithm that uses knowledge about the data-generating process to adaptively identify and apply the correct independence constraints for regularization. Results on fully synthetic, MNIST, small NORB, and Waterbirds datasets, covering binary and multi-valued attributes and labels, show that adaptive dataset-dependent constraints lead to the highest accuracy on unseen domains whereas incorrect constraints fail to do so. Our results demonstrate the importance of modeling the causal relationships inherent in the data-generating process.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                448. <h><a href=https://openreview.net/pdf?id=nIMifqu2EO>Backpropagation at the Infinitesimal Inference Limit of Energy-Based Models: Unifying Predictive Coding, Equilibrium Propagation, and Contrastive Hebbian Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Beren Millidge, Yuhang Song, Tommaso Salvatori, Thomas Lukasiewicz, Rafal Bogacz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nIMifqu2EO>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We unify and provide a single limit for many papers in the literature concerning when energy based models approximate backdrop, typically in the context of biologically plausible learning algorithms
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nIMifqu2EO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    How the brain performs credit assignment is a fundamental unsolved problem in neuroscience. Many `biologically plausible&#39; algorithms have been proposed, which compute gradients that approximate those computed by backpropagation (BP), and which operate in ways that more closely satisfy the constraints imposed by neural circuitry. Many such algorithms utilize the framework of energy-based models (EBMs), in which all free variables in the model are optimized to minimize a global energy function. However, in the literature, these algorithms exist in isolation and no unified theory exists linking them together. Here, we provide a comprehensive theory of the conditions under which EBMs can approximate BP, which lets us unify many of the BP approximation results in the literature (namely,  predictive coding, equilibrium propagation, and contrastive Hebbian learning) and demonstrate that their approximation to BP arises from a simple and general mathematical property of EBMs at free-phase equilibrium. This property can then be exploited in different ways with different energy functions, and these specific choices yield a family of BP-approximating algorithms, which both includes the known results in the literature and can be used to derive new ones.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                449. <h><a href=https://openreview.net/pdf?id=F8VKQyDgRVj>Neural Compositional Rule Learning for Knowledge Graph Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kewei Cheng, Nesreen Ahmed, Yizhou Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=F8VKQyDgRVj>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we propose an end-to-end neural model for learning compositional logic rules called NCRL. NCRL treats logic rules as a hierarchical tree, and breaks the rule body into small atomic compositions during inference.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/F8VKQyDgRVj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning logical rules is critical to improving reasoning in KGs. This is due to their ability to provide logical and interpretable explanations when used for predictions, as well as their ability to generalize to other tasks, domains, and data. While recent methods have been proposed to learn logical rules, the majority of these methods are either restricted by their computational complexity and can not handle the large search space of large-scale KGs, or show poor generalization when exposed to data outside the training set. In this paper, we propose an end-to-end neural model for learning compositional logical rules called NCRL. NCRL detects the best compositional structure of a rule body, and breaks it into small compositions in order to infer the rule head. By recurrently merging compositions in the rule body with a recurrent attention unit, NCRL finally predicts a single rule head. Experimental results show that NCRL learns high-quality rules, as well as being generalizable. Specifically, we show that NCRL is scalable, efficient, and yields state-of-the-art results for knowledge graph completion on large-scale KGs. Moreover, we test NCRL for systematic generalization by learning to reason on small-scale observed graphs and evaluating on larger unseen ones.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                450. <h><a href=https://openreview.net/pdf?id=-ENYHCE8zBp>Unsupervised Learning for Combinatorial Optimization Needs Meta Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoyu Peter Wang, Pan Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-ENYHCE8zBp>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-ENYHCE8zBp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A general framework of unsupervised learning for combinatorial optimization (CO) is to train a neural network whose output gives a problem solution by directly optimizing the CO objective. Albeit with some advantages over traditional solvers, current frameworks optimize an averaged performance over the distribution of historical problem instances, which misaligns with the actual goal of CO that looks for a good solution to every future encountered instance. With this observation, we propose a new objective of unsupervised learning for CO where the goal of learning is to search for good initialization for future problem instances rather than give direct solutions. We propose a meta-learning-based training pipeline for this new objective. Our method achieves good performance. We observe that even the initial solution given by our model before fine-tuning can significantly outperform the baselines under various evaluation settings including evaluation across multiple datasets, and the case with big shifts in the problem scale. The reason we conjecture is that meta-learning-based training lets the model be loosely tied to each local optimum for a training instance while being more adaptive to the changes of optimization landscapes across instances.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                451. <h><a href=https://openreview.net/pdf?id=Kpdewuy7RU6>Reparameterization through Spatial Gradient Scaling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexander Detkov, Mohammad Salameh, Muhammad Fetrat, Jialin Zhang, Robin Luwei, SHANGLING JUI, Di Niu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Kpdewuy7RU6>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Kpdewuy7RU6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reparameterization aims to improve the generalization of deep neural networks by transforming a convolution operation into equivalent multi-branched structures during training. However, there exists a gap in understanding how reparameterization may change and benefit learning processes for neural networks. In this paper, we present a novel spatial gradient scaling method to redistribute learning focus among weights in convolutional neural networks. We prove that spatial gradient scaling achieves the same learning dynamics as a branched reparameterization yet without introducing structural changes into the network. We further propose an analytical approach that dynamically learns scalings for each convolutional layer based on the spatial characteristics of its input feature map gauged by mutual information. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that without searching for reparameterized structures, our proposed scaling method outperforms the state-of-the-art reparameterization methods at a lower computational cost.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                452. <h><a href=https://openreview.net/pdf?id=HZf7UbpWHuA>Diffusion-GAN: Training GANs with Diffusion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HZf7UbpWHuA>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HZf7UbpWHuA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice.  In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the adaptive diffusion process via different noise-to-data ratios at each timestep. The timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data at each diffusion timestep. The generator learns from the discriminator&#39;s feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the discriminator&#39;s timestep-dependent strategy gives consistent and helpful guidance to the generator, enabling it to match the true data distribution. We demonstrate the advantages of Diffusion-GAN over strong GAN baselines on various datasets, showing that it can produce more realistic images with higher stability and data efficiency than state-of-the-art GANs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                453. <h><a href=https://openreview.net/pdf?id=b0RuGUYo8pA>Transfer Learning with Deep Tabular Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Roman Levin, Valeriia Cherepanova, Avi Schwarzschild, Arpit Bansal, C. Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, Micah Goldblum</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b0RuGUYo8pA>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We find that transfer learning with deep tabular models provides a definitive advantage over gradient boosted decision tree methods when downstream data is limited.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b0RuGUYo8pA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent work on deep learning for tabular data demonstrates the strong performance of deep tabular models, often bridging the gap between gradient boosted decision trees and neural networks. Accuracy aside, a major advantage of neural models is that they are easily fine-tuned in new domains and learn reusable features. This property is often exploited in computer vision and natural language applications, where transfer learning is indispensable when task-specific training data is scarce. In this work, we explore the benefits that representation learning provides for knowledge transfer in the tabular domain. We conduct experiments in a realistic medical diagnosis test bed with limited amounts of downstream data and find that transfer learning with deep tabular models provides a definitive advantage over gradient boosted decision tree methods. We further compare the supervised and self-supervised pretraining strategies and provide practical advice on transfer learning with tabular models. Finally, we propose a pseudo-feature method for cases where the upstream and downstream feature sets differ, a tabular-specific problem widespread in real-world applications.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                454. <h><a href=https://openreview.net/pdf?id=DSy8tP4WctmZ>Voxurf: Voxel-based Efficient and Accurate Neural Surface Reconstruction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tong Wu, Jiaqi Wang, Xingang Pan, Xudong XU, Christian Theobalt, Ziwei Liu, Dahua Lin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DSy8tP4WctmZ>7.0 - [6, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present Voxurf, a voxel-based approach for efficient and accurate neural surface reconstruction.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DSy8tP4WctmZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural surface reconstruction aims to reconstruct accurate 3D surfaces based on multi-view images. Previous methods based on neural volume rendering mostly train a fully implicit model with MLPs, which typically require hours of training for a single scene. Recent efforts explore the explicit volumetric representation to accelerate the optimization via memorizing significant information with learnable voxel grids. However, existing voxel-based methods often struggle in reconstructing fine-grained geometry, even when combined with an SDF-based volume rendering scheme. We reveal that this is because 1) the voxel grids tend to break the color-geometry dependency that facilitates fine-geometry learning, and 2) the under-constrained voxel grids lack spatial coherence and are vulnerable to local minima. In this work, we present Voxurf, a voxel-based surface reconstruction approach that is both efficient and accurate. Voxurf addresses the aforementioned issues via several key designs, including 1) a two-stage training procedure that attains a coherent coarse shape and recovers fine details successively, 2) a dual color network that maintains color-geometry dependency, and 3) a hierarchical geometry feature to encourage information propagation across voxels. Extensive experiments show that Voxurf achieves high efficiency and high quality at the same time. On the DTU benchmark, Voxurf achieves higher reconstruction quality with a 20x training speedup compared to previous fully implicit methods. Our code is publicly available at https://github.com/wutong16/Voxurf/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                455. <h><a href=https://openreview.net/pdf?id=BT4N_v7CLrk>FunkNN: Neural Interpolation for Functional Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>AmirEhsan Khorashadizadeh, Anadi Chaman, Valentin Debarnot, Ivan Dokmanić</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=BT4N_v7CLrk>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/BT4N_v7CLrk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Can we build continuous generative models which generalize across scales, can be evaluated at any coordinate, admit calculation of exact derivatives, and are conceptually simple? Existing MLP-based architectures generate worse samples than the grid-based generators with favorable convolutional inductive biases. Models that focus on generating images at different scales do better, but employ complex architectures not designed for continuous evaluation of images and derivatives.
We take a signal-processing perspective and treat continuous signal generation as interpolation from samples. Indeed, correctly sampled discrete images contain all information about the low spatial frequencies. The question is then how to extrapolate the spectrum in a data-driven way while meeting the above design criteria. Our answer is FunkNN---a novel convolutional network which learns how to reconstruct continuous images at arbitrary coordinates and can be applied to any image dataset. Combined with a discrete generative model it becomes a functional generator which can act as a prior in continuous ill-posed inverse problems. We show that FunkNN generates high-quality continuous images and exhibits strong out-of-distribution performance thanks to its patch-based design. We further showcase its performance in several stylized inverse problems with exact spatial derivatives.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                456. <h><a href=https://openreview.net/pdf?id=6doXHqwMayf>Excess Risk of Two-Layer ReLU Neural Networks in Teacher-Student Settings and its Superiority to Kernel Methods</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shunta Akiyama, Taiji Suzuki</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6doXHqwMayf>7.0 - [8, 8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6doXHqwMayf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While deep learning has outperformed other methods for various tasks, theoretical frameworks that explain its reason have not been fully established. We investigate the excess risk of two-layer ReLU neural networks in a teacher-student regression model, in which a student network learns an unknown teacher network through its outputs. Especially, we consider the student network that has the same width as the teacher network and is trained in two phases: first by noisy gradient descent and then by the vanilla gradient descent. Our result shows that the student network provably reaches a near-global optimal solution and outperforms any kernel methods estimator (more generally, linear estimators), including neural tangent kernel approach, random feature model, and other kernel methods, in a sense of the minimax optimal rate. The key concept inducing this superiority is the non-convexity of the neural network models. Even though the loss landscape is highly non-convex, the student network adaptively learns the teacher neurons.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                457. <h><a href=https://openreview.net/pdf?id=LQIjzPdDt3q>The Role of Coverage in Online Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, Sham M. Kakade</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LQIjzPdDt3q>7.0 - [8, 5, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper shows surprising connections between online and offline learnability, in particular, how coverage in offline RL enables exploration in online RL.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LQIjzPdDt3q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Coverage conditions---which assert that the data logging distribution adequately covers the state space---play a fundamental role in determining the sample complexity of offline reinforcement learning. While such conditions might seem irrelevant to online reinforcement learning at first glance, we establish a new connection by showing---somewhat surprisingly---that the mere existence of a data distribution with good coverage can enable sample-efficient online RL. Concretely, we show that coverability---that is, existence of a data distribution that satisfies a ubiquitous coverage condition called concentrability---can be viewed as a structural property of the underlying MDP, and can be exploited by standard algorithms for sample-efficient exploration, even when the agent does not know said distribution. We complement this result by proving that several weaker notions of coverage, despite being sufficient for offline RL, are insufficient for online RL. We also show that existing complexity measures for online RL, including Bellman rank and Bellman-Eluder dimension, fail to optimally capture coverability, and propose a new complexity measure, the self-normalized coefficient, to provide a unification.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                458. <h><a href=https://openreview.net/pdf?id=nN_nBVKAhhD>Minimalistic Unsupervised Representation Learning with the Sparse Manifold Transform</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yubei Chen, Zeyu Yun, Yi Ma, Bruno Olshausen, Yann LeCun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nN_nBVKAhhD>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We build a &#34;white-box&#34; unsupervised learning model with two parsimonious principles: sparsity and low-rankness, the model can be viewed as the simplest form of VICReg.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nN_nBVKAhhD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We describe a minimalistic and interpretable method for unsupervised representation learning that does not require data augmentation, hyperparameter tuning, or other engineering designs, but nonetheless achieves performance close to the state-of-the-art (SOTA) SSL methods. Our approach leverages the sparse manifold transform, which unifies sparse coding, manifold learning, and slow feature analysis. With a one-layer deterministic (one training epoch) sparse manifold transform, it is possible to achieve $99.3\%$ KNN top-1 accuracy on MNIST, $81.1\%$ KNN top-1 accuracy on CIFAR-10, and $53.2\%$ on CIFAR-100. With simple gray-scale augmentation, the model achieves $83.2\%$ KNN top-1 accuracy on CIFAR-10 and $57\%$ on CIFAR-100. These results significantly close the gap between simplistic ``white-box&#39;&#39; methods and SOTA methods. We also provide visualization to illustrate how an unsupervised representation transform is formed. The proposed method is closely connected to latent-embedding self-supervised methods and can be treated as the simplest form of VICReg. Though a small performance gap remains between our simple constructive model and SOTA methods, the evidence points to this as a promising direction for achieving a principled and white-box approach to unsupervised representation learning, which has potential to significantly improve learning efficiency.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                459. <h><a href=https://openreview.net/pdf?id=yVqC6gCNf4d>Simple initialization and parametrization of sinusoidal networks via their kernel bandwidth</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Filipe de Avila Belbute-Peres, J Zico Kolter</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yVqC6gCNf4d>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We perform a theoretical analysis of a simplified sinusoidal network and use this to propose an informed initialization scheme.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yVqC6gCNf4d.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks with sinusoidal activations have been proposed as an alternative to networks with traditional activation functions. Despite their promise, particularly for learning implicit models, their training behavior is not yet fully understood, leading to a number of empirical design choices that are not well justified. In this work, we first propose a simplified version of such sinusoidal neural networks, which allows both for easier practical implementation and simpler theoretical analysis. We then analyze the behavior of these networks from the neural tangent kernel perspective and demonstrate that their kernel approximates a low-pass filter with an adjustable bandwidth. Finally, we utilize these insights to inform the sinusoidal network initialization, optimizing their performance for each of a series of tasks, including learning implicit models and solving differential equations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                460. <h><a href=https://openreview.net/pdf?id=DEGjDDV22pI>Dichotomy of Control: Separating What You Can Control from What You Cannot</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sherry Yang, Dale Schuurmans, Pieter Abbeel, Ofir Nachum</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DEGjDDV22pI>7.0 - [6, 8, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose dichotomy of control (DoC) for supervised learning in stochastic environments by separating things within a policy&#39;s control (actions) from those outside of a policy’s control (env stochasticity) through a mutual information constraint.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DEGjDDV22pI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Future- or return-conditioned supervised learning is an emerging paradigm for offline reinforcement learning (RL), in which the future outcome (i.e., return) associated with a sequence of actions in an offline dataset is used as input to a policy trained to imitate those same actions. While return-conditioning is at the heart of popular algorithms such as decision transformer (DT), these methods tend to perform poorly in highly stochastic environments, where an occasional high return associated with a sequence of actions may be due more to the randomness of the environment than to the actions themselves. Such situations can lead to a learned policy that is inconsistent with its conditioning inputs; i.e., using the policy – while conditioned on a specific desired return – to act in the environment can lead to a distribution of real returns that is wildly different than desired. In this work, we propose the dichotomy of control (DoC), a future-conditioned supervised learning framework that separates mechanisms within a policy’s control (actions) from those outside of a policy’s control (environment stochasticity). We achieve this by conditioning the policy on a latent variable representation of the future and designing a mutual information constraint that removes any future information from the latent variable that is only due to randomness of the environment. Theoretically, we show that DoC yields policies that are consistent with their conditioning inputs, ensuring that conditioning a learned policy on a desired high-return future outcome will correctly induce high-return behavior. Empirically, we show that DoC is able to achieve significantly better performance than DT on environments with highly stochastic rewards (e.g., Bandit) and transitions (e.g., FrozenLake).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                461. <h><a href=https://openreview.net/pdf?id=PsIk0kO3hKd>Spatio-temporal point processes with deep non-stationary kernels</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zheng Dong, Xiuyuan Cheng, Yao Xie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PsIk0kO3hKd>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Deep non-stationary kernel for spatio-temporal point process data modeling with low-rank structure, and a barrier method for constraint MLE optimization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PsIk0kO3hKd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Point process data are becoming ubiquitous in modern applications, such as social networks, health care, and finance. Despite the powerful expressiveness of the popular recurrent neural network (RNN) models for point process data, they may not successfully capture sophisticated non-stationary dependencies in the data due to their recurrent structures. Another popular type of deep model for point process data is based on representing the influence kernel (rather than the intensity function) by neural networks. We take the latter approach and develop a new deep non-stationary influence kernel that can model non-stationary spatio-temporal point processes. The main idea is to approximate the influence kernel with a novel and general low-rank decomposition, enabling efficient representation through deep neural networks and computational efficiency and better performance. We also take a new approach to maintain the non-negativity constraint of the conditional intensity by introducing a log-barrier penalty. We demonstrate our proposed method&#39;s good performance and computational efficiency compared with the state-of-the-art on simulated and real data. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                462. <h><a href=https://openreview.net/pdf?id=UJTgQBc91_>Progressive Prompts: Continual Learning for Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, Amjad Almahairi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UJTgQBc91_>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UJTgQBc91_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce Progressive Prompts – a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement &gt;20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                463. <h><a href=https://openreview.net/pdf?id=I29Kt0RwChs>Robust Algorithms on Adaptive Inputs from Bounded Adversaries</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yeshwanth Cherapanamjeri, Sandeep Silwal, David Woodruff, Fred Zhang, Qiuyi Zhang, Samson Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=I29Kt0RwChs>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We give algorithms robust to adaptive input from adversaries with bounded capabilities and a general framework for achieving it.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/I29Kt0RwChs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study dynamic algorithms robust to adaptive input generated from sources with bounded capabilities, such as sparsity or limited interaction. For example, we consider robust linear algebraic algorithms when the updates to the input are sparse but given by an adversary with access to a query oracle. We also study robust algorithms in the standard centralized setting, where an adversary queries an algorithm in an adaptive manner, but the number of interactions between the adversary and the algorithm is bounded. We first recall a unified framework of (Hassidim et al., 2020; Beimel et al., 2022; Attias et al., 2023) for answering $Q$ adaptive queries that incurs $\widetilde{\mathcal{O}}(\sqrt{Q})$ overhead in space, which is roughly a quadratic improvement over the na\&#34;{i}ve implementation, and only incurs a logarithmic overhead in query time. Although the general framework has diverse applications in machine learning and data science, such as adaptive distance estimation, kernel density estimation, linear regression, range queries, point queries,  and serves as a preliminary benchmark, we demonstrate even better algorithmic improvements for (1) reducing the pre-processing time for adaptive distance estimation and (2) permitting an unlimited number of adaptive queries for kernel density estimation. Finally, we complement our theoretical results with additional empirical evaluations. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                464. <h><a href=https://openreview.net/pdf?id=FvevdI0aA_h>Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zonghan Yang, Xiaoyuan Yi, Peng Li, Yang Liu, Xing Xie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FvevdI0aA_h>7.0 - [5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an inference-time unified detoxifying and debiasing framework, which achieves better balance among effectiveness, computation cost and generation quality.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FvevdI0aA_h.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based on a parameter-efficient tuning schema without any training data. This leads to minimal generation quality loss and improved rectification performance with acceptable computational cost. Experimental results demonstrate that compared to several strong baselines, UDDIA achieves debiasing and detoxifying simultaneously and better balances efficiency and effectiveness, taking a further step towards practical ethical NLG.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                465. <h><a href=https://openreview.net/pdf?id=cp5PvcI6w8_>TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Noah Hollmann, Samuel Müller, Katharina Eggensperger, Frank Hutter</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cp5PvcI6w8_>7.0 - [8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present TabPFN, a trained Transformer that learned to solve small tabular data classification problems at SOTA level in less than a second by training on synthetic data generated by integrating principles from causal reasoning and simplicity. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cp5PvcI6w8_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods.
TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass.
TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior.
This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures.
On the $18$ datasets in the OpenML-CC18 suite that contain up to 1000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to $230\times$ speedup.
This increases to a $5\,700\times$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML.
We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                466. <h><a href=https://openreview.net/pdf?id=gpmL0D4VjN4>Fundamental limits on the robustness of image classifiers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zheng Dai, David Gifford</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gpmL0D4VjN4>7.0 - [8, 8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Image classifiers are fundamentally sensitive to small perturbations in their inputs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gpmL0D4VjN4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We prove that image classifiers are fundamentally sensitive to small perturbations in their inputs. Specifically, we show that given some image space of $n$-by-$n$ images, all but a tiny fraction of images in any image class induced over that space can be moved outside that class by adding some perturbation whose $p$-norm is $O(n^{1/\max{(p,1)}})$, as long as that image class takes up at most half of the image space. We then show that $O(n^{1/\max{(p,1)}})$ is asymptotically optimal. Finally, we show that an increase in the bit depth of the image space leads to a loss in robustness. We supplement our results with a discussion of their implications for vision systems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                467. <h><a href=https://openreview.net/pdf?id=DjzBCrMBJ_p>Spectral Augmentation for Self-Supervised Learning on Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lu Lin, Jinghui Chen, Hongning Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DjzBCrMBJ_p>7.0 - [8, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel spectral augmentation method which uses graph spectrum to capture structural properties and guide topology augmentations for graph self-supervised learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DjzBCrMBJ_p.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph contrastive learning (GCL), as an emerging self-supervised learning technique on graphs, aims to learn representations via instance discrimination. Its performance heavily relies on graph augmentation to reflect invariant patterns that are robust to small perturbations; yet it still remains unclear about what graph invariance GCL should capture. Recent studies mainly perform topology augmentations in a uniformly random manner in the spatial domain, ignoring its influence on the intrinsic structural properties embedded in the spectral domain. In this work, we aim to find a principled way for topology augmentations by exploring the invariance of graphs from the spectral perspective. We develop spectral augmentation which guides topology augmentations by maximizing the spectral change. Extensive experiments on both graph and node classification tasks demonstrate the effectiveness of our method in self-supervised representation learning. The proposed method also brings promising generalization capability in transfer learning, and is equipped with intriguing robustness property under adversarial attacks. Our study sheds light on a general principle for graph topology augmentation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                468. <h><a href=https://openreview.net/pdf?id=sP1fo2K9DFG>Is Conditional Generative Modeling all you need for Decision Making?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, Pulkit Agrawal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sP1fo2K9DFG>7.0 - [6, 8, 8, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Framing (offline) sequential decision making as conditional diffusion generative modeling
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sP1fo2K9DFG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional generative model, we avoid the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional generative models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                469. <h><a href=https://openreview.net/pdf?id=gSHyqBijPFO>TEMPERA: Test-Time Prompt Editing via Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E. Gonzalez</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gSHyqBijPFO>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gSHyqBijPFO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Careful prompt design is critical to the use of large language models in zero-shot or few-shot learning.  As a consequence, there is a growing interest in automated methods to design optimal prompts. In this work, we propose Test-time Prompt Editing using Reinforcement learning (TEMPERA). In contrast to prior prompt generation methods, TEMPERA can efficiently leverage prior knowledge, is adaptive to different queries and provides an interpretable prompt for every query. To achieve this, we design a novel action space that allows flexible editing of the initial prompts covering a wide set of commonly-used components like instructions, few-shot exemplars, and verbalizers. The proposed method achieves significant gains compared with recent SoTA approaches like prompt tuning, AutoPrompt, and RLPrompt, across a variety of tasks including sentiment analysis, topic classification, natural language inference, and reading comprehension. Our method achieves 5.33x on average improvement in sample efficiency when compared to the traditional fine-tuning methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                470. <h><a href=https://openreview.net/pdf?id=gLPkzWjdhBN>Learning Iterative Neural Optimizers for Image Steganography</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiangyu Chen, Varsha Kishore, Kilian Q Weinberger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gLPkzWjdhBN>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gLPkzWjdhBN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Image steganography is the process of concealing secret information in images through imperceptible changes. 
Recent work has formulated this task as a classic constrained optimization problem. In this paper, we argue that image steganography is inherently performed on the (elusive) manifold of natural images, and propose an iterative neural network trained to perform the optimization steps. In contrast to classical optimization methods like L-BFGS or projected gradient descent, we train the neural network to also stay close to the manifold of natural images throughout the optimization. We show that our learned neural optimization is faster and more reliable than classical optimization approaches. In comparison to previous state-of-the-art encoder-decoder based steganography methods, it reduces the recovery error rate by multiple orders of magnitude and achieves zero error up to 3 bits per pixel (bpp) without the need for error-correcting codes. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                471. <h><a href=https://openreview.net/pdf?id=bHW9njOSON>ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hee Suk Yoon, Joshua Tian Jin Tee, Eunseop Yoon, Sunjae Yoon, Gwangsu Kim, Yingzhen Li, Chang D. Yoo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bHW9njOSON>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a tuning-free calibration obejctive loss Expected Squared Difference (ESD), where we view the calibration error from the perspective of the squared difference between two expectations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bHW9njOSON.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model calibration in various batch size settings without the need for internal hyperparameter tuning, (2) ESD yields the best-calibrated results compared with previous approaches, and (3) ESD drastically improves the computational costs required for calibration during training due to the absence of internal hyperparameter. The code is publicly accessible at https://github.com/hee-suk-yoon/ESD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                472. <h><a href=https://openreview.net/pdf?id=ThXqBsRI-cY>Provable Defense Against Geometric Transformations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rem Yang, Jacob Laurel, Sasa Misailovic, Gagandeep Singh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ThXqBsRI-cY>7.0 - [8, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a training framework and verifier for deterministic certified robustness against geometric transformations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ThXqBsRI-cY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Geometric image transformations that arise in the real world, such as scaling and rotation, have been shown to easily deceive deep neural networks (DNNs). Hence, training DNNs to be certifiably robust to these perturbations is critical. However, no prior work has been able to incorporate the objective of deterministic certified robustness against geometric transformations into the training procedure, as existing verifiers are exceedingly slow. To address these challenges, we propose the first provable defense for deterministic certified geometric robustness. Our framework leverages a novel GPU-optimized verifier that can certify images between 60$\times$ to 42,600$\times$ faster than existing geometric robustness verifiers, and thus unlike existing works, is fast enough for use in training. Across multiple datasets, our results show that networks trained via our framework consistently achieve state-of-the-art deterministic certified geometric robustness and clean accuracy. Furthermore, for the first time, we verify the geometric robustness of a neural network for the challenging, real-world setting of autonomous driving.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                473. <h><a href=https://openreview.net/pdf?id=kPPVmUF6bM_>Augmentation with Projection: Towards an Effective and Efficient Data Augmentation Paradigm for Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziqi Wang, Yuexin Wu, Frederick Liu, Daogao Liu, Le Hou, Hongkun Yu, Jing Li, Heng Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kPPVmUF6bM_>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed an effective and efficient data augmentation paradigm for knowledge distillation
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kPPVmUF6bM_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge distillation is one of the primary methods of transferring knowledge from large to small models. However, it requires massive task-specific data, which may not be plausible in many real-world applications. Data augmentation methods such as representation interpolation, token replacement, or augmentation with models are applied to tackle this problem. However, these data augmentation methods either potentially cause shifts in decision boundaries (representation interpolation), are not expressive enough (token replacement), or introduce too much computational overhead (augmentation with models). To this end, we propose AugPro (Augmentation with Projection), an effective and efficient data augmentation method for distillation. Our method builds on top of representation interpolation augmentation methods to maintain the diversity of expressions and converts the augmented data to tokens to avoid shifting decision boundaries. It uses simple operations that come with little computational overhead. The results on multiple GLUE tasks show that our methods can improve distillation performance by a large margin at a low time cost.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                474. <h><a href=https://openreview.net/pdf?id=WZH7099tgfM>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H. Chi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WZH7099tgfM>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel prompting strategy, least-to-most prompting, that enables large language models to achieve easy-to-hard generalization
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WZH7099tgfM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split)  with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting.  This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                475. <h><a href=https://openreview.net/pdf?id=SjzFVSJUt8S>Replay Memory as An Empirical MDP: Combining Conservative Estimation with Experience Replay</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongming Zhang, Chenjun Xiao, Han Wang, Jun Jin, bo xu, Martin Müller</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SjzFVSJUt8S>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SjzFVSJUt8S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Experience replay, which stores transitions in a replay memory for repeated use, plays an important role of improving sample efficiency in reinforcement learning. Existing techniques such as reweighted sampling, episodic learning and reverse sweep update further process the information in the replay memory to make experience replay more efficient. In this work, we further exploit the information in the replay memory by treating it as an empirical \emph{Replay Memory MDP (RM-MDP)}. By solving it with dynamic programming, we learn a conservative value estimate that \emph{only} considers transitions observed in the replay memory. Both value and policy regularizers based on this conservative estimate are developed and integrated with model-free learning algorithms. We design the metric \textit{memory density} to measure the quality of RM-MDP. Our empirical studies quantitatively find a strong correlation between performance improvement and memory density. Our method combines \emph{Conservative Estimation with Experience Replay (CEER)}, improving sample efficiency by a large margin, especially when the memory density is high. Even when the memory density is low, such a conservative estimate can still help to avoid suicidal actions and thereby improve performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                476. <h><a href=https://openreview.net/pdf?id=PldynS56bN>Contextual Convolutional Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuxian Liang, Xu Shen, Tongliang Liu, Xian-Sheng Hua</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PldynS56bN>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we propose to augment potential category memberships as contextual priors in the convolution for contextualized representation learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PldynS56bN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents a new Convolutional Neural Network, named Contextual Convolutional Network, that capably serves as a general-purpose backbone for visual recognition. Most existing convolutional backbones follow the representation-to-classification paradigm, where representations of the input are firstly generated by category-agnostic convolutional operations, and then fed into classifiers for specific perceptual tasks (e.g., classification and segmentation). In this paper, we deviate from this classic paradigm and propose to augment potential category memberships as contextual priors in the convolution for contextualized representation learning. Specifically, top-k likely classes from the preceding stage are encoded as a contextual prior vector. Based on this vector and the preceding features, offsets for spatial sampling locations and kernel weights are generated to modulate the convolution operations. The new convolutions can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation without additional supervision. The qualities of Contextual Convolutional Networks make it compatible with a broad range of vision tasks and boost the state-of-the-art architecture ConvNeXt-Tiny by 1.8% on top-1 accuracy of ImageNet classification. The superiority of the proposed model reveals the potential of contextualized representation learning for vision tasks. Code is available at: \url{https://github.com/liang4sx/contextual_cnn}.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                477. <h><a href=https://openreview.net/pdf?id=KRLUvxh8uaX>When and Why Vision-Language Models Behave like Bags-Of-Words, and What to Do About It?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KRLUvxh8uaX>7.0 - [8, 8, 6, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KRLUvxh8uaX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode the compositional relationships between objects and attributes. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information. ARO consists of \emph{Visual Genome Attribution}, to test the understanding of objects&#39; properties; \emph{Visual Genome Relation}, to test for relational understanding; and \emph{COCO-Order \&amp; Flickr30k-Order}, to test for order sensitivity in VLMs. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We present the settings where  state-of-the-art VLMs behave like bags-of-words---i.e. when they have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large scale datasets with rich compositional structure in the images and captions. Yet, training on these datasets has not been enough to address the lack of compositional understanding, and evaluating on these datasets has failed to surface this deficiency. To understand why these limitations emerge and are not represented in the standard tests, we zoom into the evaluation and training procedures. We demonstrate that it is possible to perform well on image-text retrieval over existing datasets without using the composition and order information. This further motivates the value of using ARO to benchmark VLMs. Given that contrastive pretraining optimizes for retrieval on large datasets with similar shortcuts, we hypothesize that this can explain why the models do not need to learn to represent compositional information. This finding suggests a natural solution: composition-aware hard negative mining. We show that a simple-to-implement modification of contrastive learning significantly improves the performance on tasks requiring understanding of order and compositionality. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                478. <h><a href=https://openreview.net/pdf?id=8pOVAeo8ie>LPT: Long-tailed Prompt Tuning  for Image Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bowen Dong, Pan Zhou, Shuicheng Yan, Wangmeng Zuo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8pOVAeo8ie>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8pOVAeo8ie.jpg loading="lazy">
                </div>
                <div class="abstract">
                    For long-tailed classification tasks, most works often pretrain a big model on a large-scale (unlabeled) dataset, and then fine-tune the whole pretrained  model for  adapting to long-tailed data. Though promising, fine-tuning the whole pretrained model tends to suffer from high cost in computation and deployment of different models for different tasks, as well as weakened generalization capability for overfitting to certain features of long-tailed data. To alleviate these issues, we propose an effective Long-tailed Prompt Tuning (LPT) method for long-tailed classification tasks. LPT introduces several trainable prompts into a frozen pretrained model to adapt it to long-tailed data. For better effectiveness, we divide prompts into two groups: 1) a shared prompt for the whole long-tailed dataset to learn general features and to adapt a pretrained model into the target long-tailed domain; and 2) group-specific prompts to gather group-specific features for the samples which have similar features and also to empower the pretrained model with fine-grained discrimination ability. Then we design a two-phase training paradigm to learn these prompts. In the first phase, we train the shared prompt via conventional supervised prompt tuning to adapt a pretrained model to the desired long-tailed domain. In the second phase, we use the learnt shared prompt as query to select a small best matched set for a group of similar samples from the group-specific prompt set to dig the common features of these similar samples, and then optimize these prompts with a dual sampling strategy and the asymmetric Gaussian Clouded Logit loss. By only fine-tuning a few prompts while fixing the pretrained model, LPT can reduce training cost and deployment cost by storing a few prompts, and enjoys a strong generalization ability of the pretrained model. Experiments show that on various long-tailed benchmarks, with only $\sim$1.1\% extra trainable parameters, LPT achieves comparable or higher performance than previous whole model fine-tuning methods, and is more robust to domain-shift.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                479. <h><a href=https://openreview.net/pdf?id=z9SIj-IM7tn>Competitive Physics Informed Networks </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qi Zeng, Yash Kothari, Spencer H Bryngelson, Florian Tobias Schaefer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=z9SIj-IM7tn>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce competitive physics informed networks where two neural networks solve a partial differential equation by playing a zero-sum game.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/z9SIj-IM7tn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks can be trained to solve partial differential equations (PDEs) by using the PDE residual as the loss function. This strategy is called &#34;physics-informed neural networks&#34; (PINNs), but it currently cannot produce high-accuracy solutions, typically attaining about $0.1\%$ relative error. We present an adversarial approach that overcomes this limitation, which we call competitive PINNs (CPINNs). CPINNs train a discriminator that is rewarded for predicting mistakes the PINN makes. The discriminator and PINN participate in a zero-sum game with the exact PDE solution as an optimal strategy. This approach avoids squaring the large condition numbers of PDE discretizations, which is the likely reason for failures of previous attempts to decrease PINN errors even on benign problems. Numerical experiments on a Poisson problem show that CPINNs achieve errors four orders of magnitude smaller than the best-performing PINN. We observe relative errors on the order of single-precision accuracy, consistently decreasing with each epoch. To the authors&#39; knowledge, this is the first time this level of accuracy and convergence behavior has been achieved. Additional experiments on the nonlinear Schr{\&#34;o}dinger, Burgers&#39;, and Allen--Cahn equation show that the benefits of CPINNs are not limited to linear problems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                480. <h><a href=https://openreview.net/pdf?id=B73niNjbPs>Continuous PDE Dynamics Forecasting with Implicit Neural Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, patrick gallinari</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=B73niNjbPs>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a continuous-time, continuous-space data-driven PDE forecasting model with extensive spatiotemporal extrapolation capabilities including generalization to unseen sparse meshes and resolutions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/B73niNjbPs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Effective data-driven PDE forecasting methods often rely on fixed spatial and / or temporal discretizations. This raises limitations in real-world applications like weather prediction where flexible extrapolation at arbitrary spatiotemporal locations is required. We address this problem by introducing a new data-driven approach, DINo, that models a PDE&#39;s flow with continuous-time dynamics of spatially continuous functions. This is achieved by embedding spatial observations independently of their discretization via Implicit Neural Representations in a small latent space temporally driven by a learned ODE. This separate and flexible treatment of time and space makes DINo the first data-driven model to combine the following advantages. It extrapolates at arbitrary spatial and temporal locations; it can learn from sparse irregular grids or manifolds; at test time, it generalizes to new grids or resolutions. DINo outperforms alternative neural PDE forecasters in a variety of challenging generalization scenarios on representative PDE systems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                481. <h><a href=https://openreview.net/pdf?id=oJpVVGXu9i>Share Your Representation Only: Guaranteed Improvement of the Privacy-Utility Tradeoff in Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zebang Shen, Jiayuan Ye, Anmin Kang, Hamed Hassani, Reza Shokri</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oJpVVGXu9i>7.0 - [8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oJpVVGXu9i.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Repeated parameter sharing in federated learning causes significant information leakage about private data, thus defeating its main purpose: data privacy.  Mitigating the risk of this information leakage, using state of the art differentially private algorithms, also does not come for free.  Randomized mechanisms can prevent convergence of models on learning even the useful representation functions, especially if there is more disagreement between local models on the classification functions (due to data heterogeneity). In this paper, we consider a representation federated learning objective that encourages various parties to collaboratively refine the consensus part of the model, with differential privacy guarantees, while separately allowing sufficient freedom for local personalization (without releasing it).  We prove that in the linear representation setting, while the objective is non-convex, our proposed new algorithm \DPFEDREP\ converges to a ball centered around the \emph{global optimal} solution at a linear rate, and the radius of the ball is proportional to the reciprocal of the privacy budget.  With this novel utility analysis, we improve the SOTA utility-privacy trade-off for this problem by a factor of $\sqrt{d}$, where $d$ is the input dimension.  We empirically evaluate our method with the image classification task on CIFAR10, CIFAR100, and EMNIST, and observe a significant performance improvement over the prior work under the same small privacy budget. The code can be found in this link, https://github.com/shenzebang/CENTAUR-Privacy-Federated-Representation-Learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                482. <h><a href=https://openreview.net/pdf?id=vsMyHUq_C1c>A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marc Anton Finzi, Andres Potapczynski, Matthew Choptuik, Andrew Gordon Wilson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vsMyHUq_C1c>7.0 - [8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop Neural-IVP, a new method for solving initial value PDEs with Neural Networks that is both stable and scalable.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vsMyHUq_C1c.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs applicability to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of model parameters, they are restricted to small neural networks, significantly limiting their ability to represent intricate PDE initial conditions and solutions. Building on these insights, we develop Neural-IVP, an ODE based IVP solver which prevents the network from getting ill-conditioned and runs in time linear in the number of parameters, enabling us to evolve the dynamics of challenging PDEs with neural networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                483. <h><a href=https://openreview.net/pdf?id=6u7mf9s2A9>Interpretable Geometric Deep Learning via Learnable Randomness Injection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Siqi Miao, Yunan Luo, Mia Liu, Pan Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6u7mf9s2A9>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6u7mf9s2A9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Point cloud data is ubiquitous in scientific fields. Recently, geometric deep learning (GDL) has been widely applied to solve prediction tasks with such data. However, GDL models are often complicated and hardly interpretable, which poses concerns to scientists who are to deploy these models in scientific analysis and experiments. This work proposes a general mechanism, learnable randomness injection (LRI), which allows building inherently interpretable models based on general GDL backbones. LRI-induced models, once trained, can detect the points in the point cloud data that carry information indicative of the prediction label. We also propose four datasets from real scientific applications that cover the domains of high-energy physics and biochemistry to evaluate the LRI mechanism. Compared with previous post-hoc interpretation methods, the points detected by LRI align much better and stabler with the ground-truth patterns that have actual scientific meanings. LRI is grounded by the information bottleneck principle, and thus LRI-induced models are also more robust to distribution shifts between training and test scenarios. Our code and datasets are available at https://github.com/Graph-COM/LRI.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                484. <h><a href=https://openreview.net/pdf?id=rvsbw2YthH_>The Trade-off between Universality and Label Efficiency of Representations from Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, Somesh Jha</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rvsbw2YthH_>7.0 - [6, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We focus on contrastive learning and systematically study a trade-off between label efficiency and universality both empirically and theoretically.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rvsbw2YthH_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pre-training representations (a.k.a. foundation models) has recently become a prevalent learning paradigm, where one first pre-trains a representation using large-scale unlabeled data, and then learns simple predictors on top of the representation using small labeled data from the downstream tasks. There are two key desiderata for the representation: label efficiency (the ability to learn an accurate classifier on top of the representation with a small amount of labeled data) and universality (usefulness across a wide range of downstream tasks). In this paper, we focus on one of the most popular instantiations of this paradigm: contrastive learning with linear probing, i.e., learning a linear predictor on the representation pre-trained by contrastive learning. We show that there exists a trade-off between the two desiderata so that one may not be able to achieve both simultaneously. 
Specifically, we provide analysis using a theoretical data model and show that,  while more diverse pre-training data result in more diverse features for different tasks (improving universality), it puts less emphasis on task-specific features, giving rise to larger sample complexity for down-stream supervised tasks, and thus worse prediction performance. Guided by this analysis, we propose a contrastive regularization method to improve the trade-off. We validate our analysis and method empirically with systematic experiments using real-world datasets and foundation models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                485. <h><a href=https://openreview.net/pdf?id=PFbzoWZyZRX>Bridging the Gap between ANNs and SNNs by Calibrating Offset Spikes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zecheng Hao, Jianhao Ding, Tong Bu, Tiejun Huang, Zhaofei Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PFbzoWZyZRX>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A calibration method based on shifting initial membrane potential is proposed for ANN-SNN conversion to reach the same level of performance as BPTT.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PFbzoWZyZRX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Spiking Neural Networks (SNNs) have attracted great attention due to their distinctive characteristics of low power consumption and temporal information processing. ANN-SNN conversion, as the most commonly used training method for applying SNNs, can ensure that converted SNNs achieve comparable performance to ANNs on large-scale datasets. However, the performance degrades severely under low quantities of time-steps, which hampers the practical applications of SNNs to neuromorphic chips. 
In this paper, instead of evaluating different conversion errors and then eliminating these errors, we define an offset spike to measure the degree of deviation between actual and desired SNN firing rates. We perform a detailed analysis of offset spike and note that the firing of one additional (or one less) spike is the main cause of conversion errors. Based on this, we propose an optimization strategy based on shifting the initial membrane potential and we theoretically prove the corresponding optimal shifting distance for calibrating the spike. In addition, we also note that our method has a unique iterative property that enables further reduction of conversion errors. The experimental results show that our proposed method achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet datasets. For example, we reach a top-1 accuracy of 67.12% on ImageNet when using 6 time-steps. To the best of our knowledge, this is the first time an ANN-SNN conversion has been shown to simultaneously achieve high accuracy and ultralow latency on complex datasets. Code is available at https://github.com/hzc1208/ANN2SNN_COS.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                486. <h><a href=https://openreview.net/pdf?id=oGDKSt9JrZi>Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jesse Farebrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan, Ross Goroshin, Pablo Samuel Castro, Marc G Bellemare</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oGDKSt9JrZi>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oGDKSt9JrZi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well-understood; in practice, how-ever, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent’s network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan &amp; Maggioni (2007)’s proto-value functions to deep reinforcement learning – accordingly, we call the resulting object proto-value networks. Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment’s reward function.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                487. <h><a href=https://openreview.net/pdf?id=KdwnGErdT6>Calibrating the Rigged Lottery: Making All Tickets Reliable</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bowen Lei, Ruqi Zhang, Dongkuan Xu, Bani Mallick</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KdwnGErdT6>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KdwnGErdT6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Although sparse training has been successfully used in various deep learning tasks to save memory and reduce inference time, the reliability of the produced sparse models remains unexplored. Previous research has shown that deep neural networks tend to be over-confident, and we find that sparse training exacerbates this problem. Therefore, calibrating the sparse models is crucial for reliable prediction and decision making. In this paper, we propose a new sparse training method to produce sparse models with improved confidence calibration. In contrast to previous research that uses only one mask to control the sparse topology, our method utilizes two masks, including a deterministic mask and a random mask. The former efficiently searches and activates important weights by exploiting the magnitude of weights and gradients. While the latter brings better exploration and finds more appropriate weight values by random updates. Theoretically, we prove our method can be viewed as a hierarchical variational approximation of a probabilistic deep Gaussian process. Extensive experiments on multiple datasets, model architectures, and sparsities show that our method can reduce ECE values by up to 47.8\% and simultaneously maintain or even improve accuracy with only a slight increase in computational and storage burden.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                488. <h><a href=https://openreview.net/pdf?id=M2unceRvqhh>Learning with Logical Constraints but without Shortcut Satisfaction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, Jian L\&#34;{u}</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=M2unceRvqhh>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/M2unceRvqhh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent studies have started to explore the integration of logical knowledge into deep learning via encoding logical constraints as an additional loss function. However, existing approaches tend to vacuously satisfy logical constraints through shortcuts, failing to fully exploit the knowledge. In this paper, we present a new framework for learning with logical constraints. Specifically, we address the shortcut satisfaction issue by introducing dual variables for logical connectives, encoding how the constraint is satisfied. We further propose a variational framework where the encoded logical constraint is expressed as a distributional loss that is compatible with the model&#39;s original training loss. The theoretical analysis shows that the proposed approach bears some nice properties, and the experimental evaluations demonstrate its superior performance in both model generalizability and constraint satisfaction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                489. <h><a href=https://openreview.net/pdf?id=TN9gQ4x0Ep3>Don’t fear the unlabelled: safe semi-supervised learning via debiasing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hugo Schmutz, Olivier HUMBERT, Pierre-Alexandre Mattei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TN9gQ4x0Ep3>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a slight modification of most common semi-supervised learning methods to make them safe by debiasing their risk estimate. In particular, we apply it successfully to Fixmatch.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TN9gQ4x0Ep3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Semi-supervised learning (SSL) provides an effective means of leveraging unlabelled data to improve a model’s performance. Even though the domain has received a considerable amount of attention in the past years, most methods present the common drawback of lacking theoretical guarantees. Our starting point is to notice that the estimate of the risk that most discriminative SSL methods minimise is biased, even asymptotically. This bias impedes the use of standard statistical learning theory and can hurt empirical performance. We propose a simple way of removing the bias. Our debiasing approach is straightforward to implement and applicable to most deep SSL methods.  We provide simple theoretical guarantees on the trustworthiness of these modified methods, without having to rely on the strong assumptions on the data distribution that SSL theory usually requires. In particular, we provide generalisation error bounds for the proposed methods. We evaluate debiased versions of different existing SSL methods, such as the Pseudo-label method and Fixmatch, and show that debiasing can compete with classic deep SSL techniques in various settings by providing better calibrated models. Additionally, we provide a theoretical explanation of the intuition of the popular SSL methods.  An implementation of a debiased version of Fixmatch is available at
https://github.com/HugoSchmutz/DeFixmatch
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                490. <h><a href=https://openreview.net/pdf?id=6orC5MvgPBK>Thalamus: a brain-inspired algorithm for biologically-plausible continual learning and disentangled representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ali Hummos</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6orC5MvgPBK>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A brain-inspired algorithm that alternates optimizing in weight space with optimizing the latent embedding space in the same neural network leading to open-ended discovery of tasks and disentangled learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6orC5MvgPBK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Animals thrive in a constantly changing environment and leverage the temporal structure to learn well-factorized causal representations. In contrast, traditional neural networks suffer from forgetting in changing environments and many methods have been proposed to limit forgetting with different trade-offs. Inspired by the brain thalamocortical circuit, we introduce a simple algorithm that uses optimization at inference time to generate internal representations of the current task dynamically. The algorithm alternates between updating the model weights and a latent task embedding, allowing the agent to parse the stream of temporal experience into discrete events and organize learning about them. On a continual learning benchmark, it achieves competitive end average accuracy by mitigating forgetting, but importantly, the interaction between the weights dynamics and the latent dynamics organizes knowledge into flexible structures with a cognitive interface to control them. Tasks later in the sequence can be solved through knowledge transfer as they become reachable within the well-factorized latent space. The algorithm meets many of the desiderata of an ideal continually learning agent in open-ended environments, and its simplicity suggests fundamental computations in circuits with abundant feedback control loops such as the thalamocortical circuits in the brain
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                491. <h><a href=https://openreview.net/pdf?id=h9O0wsmL-cT>Regression with Label Differential Privacy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Badih Ghazi, Pritish Kamath, Ravi Kumar, Ethan Leeman, Pasin Manurangsi, Avinash Varadarajan, Chiyuan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=h9O0wsmL-cT>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a new label differentially private algorithm for training regression models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/h9O0wsmL-cT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the task of training regression models with the guarantee of _label_ differential privacy (DP). Based on a global prior distribution of label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a &#34;randomized response on bins&#34;, and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                492. <h><a href=https://openreview.net/pdf?id=JLg5aHHv7j>(Certified!!) Adversarial Robustness for Free!</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, J Zico Kolter</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JLg5aHHv7j>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Using an off-the-shelf diffusion model as a denoiser gives state-of-the-art certified adversarial robustness.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JLg5aHHv7j.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper we show how to achieve state-of-the-art certified adversarial robustness to 2-norm bounded perturbations by relying exclusively on off-the-shelf pretrained models. To do so, we instantiate the denoised smoothing approach of Salman et al. by combining a pretrained denoising diffusion probabilistic model and a standard high-accuracy classifier. This allows us to certify 71% accuracy on ImageNet under adversarial perturbations constrained to be within a 2-norm of 0.5, an improvement of 14 percentage points over the prior certified SoTA using any approach, or an improvement of 30 percentage points over denoised smoothing. We obtain these results using only pretrained diffusion models and image classifiers, without requiring any fine tuning or retraining of model parameters.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                493. <h><a href=https://openreview.net/pdf?id=AatUEvC-Wjv>Hyper-Decision Transformer for Efficient Online Policy Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mengdi Xu, Yuchen Lu, Yikang Shen, Shun Zhang, Ding Zhao, Chuang Gan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AatUEvC-Wjv>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose Hyper-Decision Transformer (HDT), a transformer-based model which generalizes to novel unseen tasks maintaining strong data and parameter efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AatUEvC-Wjv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Decision Transformers (DT) have demonstrated strong performances in offline reinforcement learning settings, but quickly adapting to unseen novel tasks remains challenging. To address this challenge, we propose a new framework, called Hyper-Decision Transformer (HDT), that can generalize to novel tasks from a handful of demonstrations in a data- and parameter-efficient manner. To achieve such a goal, we propose to augment the base DT with an adaptation module, whose parameters are initialized by a hyper-network. When encountering unseen tasks, the hyper-network takes a handful of demonstrations as inputs and initializes the adaptation module accordingly. This initialization enables HDT to efficiently adapt to novel tasks by only fine-tuning the adaptation module. We validate HDT&#39;s generalization capability on object manipulation tasks. We find that with a single expert demonstration and fine-tuning only 0.5% of DT parameters, HDT adapts faster to unseen tasks than fine-tuning the whole DT model. Finally, we explore a more challenging setting where expert actions are not available, and we show that HDT outperforms state-of-the-art baselines in terms of task success rates by a large margin. Demos are available on our project page: https://sites.google.com/view/hdtforiclr2023/home.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                494. <h><a href=https://openreview.net/pdf?id=20GtJ6hIaPA>Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang, He Wang, Li Yi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=20GtJ6hIaPA>7.0 - [6, 6, 6, 10] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/20GtJ6hIaPA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Category-level articulated object pose estimation aims to estimate a hierarchy of articulation-aware object poses of an unseen articulated object from a known category. To reduce the heavy annotations needed for supervised learning methods, we present a novel self-supervised strategy that solves this problem without any human labels. Our key idea is to factorize canonical shapes and articulated object poses from input articulated shapes through part-level equivariant shape analysis. Specifically, we first introduce the concept of part-level SE(3) equivariance and devise a network to learn features of such property. Then, through a carefully designed fine-grained pose-shape disentanglement strategy, we expect that canonical spaces to support pose estimation could be induced automatically. Thus, we could further predict articulated object poses as per-part rigid transformations describing how parts transform from their canonical part spaces to the camera space. Extensive experiments demonstrate the effectiveness of our method on both complete and partial point clouds from synthetic and real articulated object datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                495. <h><a href=https://openreview.net/pdf?id=TDf-XFAwc79>Meta Knowledge Condensation for Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ping Liu, Xin Yu, Joey Tianyi Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TDf-XFAwc79>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TDf-XFAwc79.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing federated learning paradigms usually extensively exchange distributed models, rather than original data, at a central solver to achieve a more powerful model. However, this would incur severe communication burden between a server and multiple clients especially when data distributions are heterogeneous. As a result, current federated learning methods often require plenty of communication rounds in training. Unlike existing paradigms, we introduce an alternative perspective to significantly decrease the federate learning communication cost without leaking original data. In this work, we first present a meta knowledge representation method that extracts meta knowledge from distributed clients.  The extracted meta knowledge encodes essential information that can be used to improve the current model. As the training progresses, the contributions of the same training samples to a federated model should also vary. Thus, we introduce a dynamic weight assignment mechanism that enables informative samples to contribute adaptively to the current model update. Then, informative meta knowledge from all active clients is sent to the server for model update. Training model on the combined meta knowledge that is regarded as a condense form of original data can significantly mitigate the heterogeneity issues. Moreover, to further ameliorate data heterogeneity, we also exchange meta knowledge among clients as conditional initialisation for meta knowledge extraction. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method. Remarkably, our method outperforms the state-of-the-art by a large margin (from $74.07\%$ to $92.95\%$) on MNIST with a restricted communication budget (\textit{i.e.}, 10 rounds).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                496. <h><a href=https://openreview.net/pdf?id=NAQvF08TcyG>An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, Daniel Cohen-or</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NAQvF08TcyG>7.0 - [8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present the task of personalized text-to-image generation, and introduce an inversion-based method that allows us to synthesize novel scenes of user-provided visual concepts, guided by natural language instructions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NAQvF08TcyG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Text-to-image models offer unprecedented freedom to guide creation through natural language. Yet, it is unclear how such freedom can be exercised to generate images of specific unique concepts, modify their appearance, or compose them in new roles and novel scenes.
In other words, we ask: how can we use language-guided models to turn *our* cat into a painting, or imagine a new product based on *our* favorite toy? 
Here we present a simple approach that allows such creative freedom. Using only $3$-$5$ images of a user-provided concept, like an object or a style, we learn to represent it through new ``words&#34; in the embedding space of a frozen text-to-image model.
These ``words&#34; can be composed into natural language sentences, guiding *personalized* creation in an intuitive way.
Notably, we find evidence that a *single* word embedding is sufficient for capturing unique and varied concepts. 
We compare our approach to a wide range of baselines, and demonstrate that it can more faithfully portray the concepts across a range of applications and tasks. Our code, data and new words will be available.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                497. <h><a href=https://openreview.net/pdf?id=bhUPJnS2g0X>Ask Me Anything: A simple strategy for prompting language models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Christopher Re</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bhUPJnS2g0X>7.0 - [8, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a prompting strategy based on aggregating the predictions of multiple prompts, which enables a 6B parameter model to exceed the few-shot performance of GPT3-175B on 15/20 popular benchmarks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bhUPJnS2g0X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly crafted &#34;perfect prompt&#34; for a task. To mitigate the high degree of effort, we instead ask whether collecting multiple decent, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed method, Ask Me Anything (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation (&#34;Who went to the park?&#34;) tend to outperform those that restrict the model outputs (&#34;John went to the park. True or False?&#34;). AMA recursively uses the LLM to transform task inputs to the effective QA format. AM generates multiple questions per input and applies these prompts to collect several noisy &#34;votes&#34; for the input&#39;s true label. We find the prompts have varying accuracies and dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance lift of 10.2\% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B  on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                498. <h><a href=https://openreview.net/pdf?id=dNqxZgyjcYA>Conservative Bayesian Model-Based Value Expansion for Offline Policy Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jihwan Jeong, Xiaoyu Wang, Michael Gimelfarb, Hyunwoo Kim, Baher abdulhai, Scott Sanner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dNqxZgyjcYA>7.0 - [8, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dNqxZgyjcYA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning (RL) addresses the problem of learning a performant policy from a fixed batch of data collected by following some behavior policy. Model-based approaches are particularly appealing in the offline setting since they can extract more learning signals from the logged dataset by learning a model of the environment. However, the performance of existing model-based approaches falls short of model-free counterparts, due to the compounding of estimation errors in the learned model. Driven by this observation, we argue that it is critical for a model-based method to understand when to trust the model and when to rely on model-free estimates, and how to act conservatively w.r.t. both. To this end, we derive an elegant and simple methodology called conservative Bayesian model-based value expansion for offline policy optimization (CBOP), that trades off model-free and model-based estimates during the policy evaluation step according to their epistemic uncertainties, and facilitates conservatism by taking a lower bound on the Bayesian posterior value estimate. On the standard D4RL continuous control tasks, we find that our method significantly outperforms previous model-based approaches: e.g., MOPO by $116.4$%, MOReL by $23.2$% and COMBO by $23.7$%. Further, CBOP achieves state-of-the-art performance on $11$ out of $18$ benchmark datasets while doing on par on the remaining datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                499. <h><a href=https://openreview.net/pdf?id=T2Ncx_PN2K>In-Situ Text-Only Adaptation of Speech Models with Low-Overhead Speech Imputations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ashish Mittal, Sunita Sarawagi, Preethi Jyothi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=T2Ncx_PN2K>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A lightweight text-only adaptation technique for end-to-end speech recognition that is both fast and accurate.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/T2Ncx_PN2K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Fast and accurate adaptation of automatic speech recognition (ASR) systems using only text data in the target domain is a problem of long-standing practical relevance. Text-only adaptation was easy in traditional cascaded ASR systems with completely decoupled acoustic and language models. Recently, the RNNTransducer (RNN-T) has emerged as a default ASR model because of its high accuracy, low latency, and capability of supporting streaming input. However text-only adaptation of the RNN-T model is significantly more challenging due to its tight integration of acoustic and language models and end-to-end training. Existing recent approaches for text-only adaptation of RNN-Ts, either entail significant modification to the network or introduce high latency during decoding. We propose a new approach (TOLSTOI) that imputes speech representations internal to a baseline RNN-T, starting from text-only inputs, and performs in-situ adaptation that results in higher adaptation accuracy without any runtime overheads during decoding. Our imputation model is a function of the labeled data and trained parameters of the ASR model, and that we show, is more effective in controlling catastrophic forgetting compared to existing methods. We establish the effectiveness of TOLSTOI using three target domains and two ASR models of varying complexity. We yield up to 35% relative reduction in word error rate with text-only adaptation while forgetting the least compared to existing adaptation approaches. Our method is easy to implement and can be harnessed on existing RNN-T models without requiring ASR model training from scratch.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                500. <h><a href=https://openreview.net/pdf?id=swEskiem99>Feature selection and low test error in shallow low-rotation ReLU networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matus Telgarsky</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=swEskiem99>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work establishes low test error of gradient methods on two-layer ReLU networks with standard initialization, in three regimes where key sets of weights rotate little, making use of margins as the core analysis technique.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/swEskiem99.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work establishes low test error of gradient flow (GF) and stochastic gradient descent (SGD) on two-layer ReLU networks with standard initialization scale, in three regimes where key sets of weights rotate little (either naturally due to GF and SGD, or due to an artificial constraint), and making use of margins as the core analysis technique. The first regime is near initialization, specifically until the weights have moved by $\mathcal{O}(\sqrt m)$, where $m$ denotes the network width, which is in sharp contrast to the $\mathcal{O}(1)$ weight motion allowed by the Neural Tangent Kernel (NTK); here it is shown that GF and SGD only need a network width and number of samples inversely proportional to the NTK margin, and moreover that GF attains at least the NTK margin itself and in particular escapes bad KKT points of the margin objective, whereas prior work could only establish nondecreasing but arbitrarily small margins. The second regime is the Neural Collapse (NC) setting, where data lies in well-separated groups, and the sample complexity scales with the number of groups; here the contribution over prior work is an analysis of the entire GF trajectory from initialization. Lastly, if the inner layer weights are constrained to change in norm only and can not rotate, then GF with large widths achieves globally maximal margins, and its sample complexity scales with their inverse; this is in contrast to prior work, which required infinite width and a tricky dual convergence assumption.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                501. <h><a href=https://openreview.net/pdf?id=KyoVpYvWWnK>Efficient Certified Training and Robustness Verification of Neural ODEs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mustafa Zeqiri, Mark Niklas Mueller, Marc Fischer, Martin Vechev</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KyoVpYvWWnK>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We enable certified training and scalable robustness verification of neural ODEs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KyoVpYvWWnK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Ordinary Differential Equations (NODEs) are a novel neural architecture, built around initial value problems with learned dynamics which are solved during inference. Thought to be inherently more robust against adversarial perturbations, they were recently shown to be vulnerable to strong adversarial attacks, highlighting the need for formal guarantees.  However, despite significant progress in robustness verification for standard feed-forward architectures, the verification of high dimensional NODEs remains an open problem. In this work we address this challenge and propose GAINS, an analysis framework for NODEs combining three key ideas: (i) a novel class of ODE solvers, based on variable but discrete time steps, (ii) an efficient graph representation of solver trajectories, and (iii) a novel abstraction algorithm operating on this graph representation. Together, these advances enable the efficient analysis and certified training of high-dimensional NODEs, by reducing the runtime from an intractable $\mathcal{O}(\exp(d)+\exp(T))$ to $\mathcal{O}(d+T^2\log^2T)$ in the dimensionality $d$ and integration time $T$.  In an extensive evaluation on computer vision (MNIST and Fashion-MNIST) and time-series forecasting (Physio-Net) problems, we demonstrate the effectiveness of both our certified training and verification methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                502. <h><a href=https://openreview.net/pdf?id=PhkWyijGi5b>Choreographer: Learning and Adapting Skills in Imagination</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, Sai Rajeswar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PhkWyijGi5b>7.0 - [6, 8, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Choreographer: a model-based agent that discovers and learns unsupervised skills in latent imagination, and it&#39;s able to efficiently coordinate and adapt the skills to solve downstream tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PhkWyijGi5b.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised skill learning aims to learn a rich repertoire of behaviors without external supervision, providing artificial agents with the ability to control and influence the environment. However, without appropriate knowledge and exploration, skills may provide control only over a restricted area of the environment, limiting their applicability. Furthermore, it is unclear how to leverage the learned skill behaviors for adapting to downstream tasks in a data-efficient manner. We present Choreographer, a model-based agent that exploits its world model to learn and adapt skills in imagination. Our method decouples the exploration and skill learning processes, being able to discover skills in the latent state space of the model. During adaptation, the agent uses a meta-controller to evaluate and adapt the learned skills efficiently by deploying them in parallel in imagination. Choreographer is able to learn skills both from offline data, and by collecting data simultaneously with an exploration policy. The skills can be used to effectively adapt to downstream tasks, as we show in the URL benchmark, where we outperform previous approaches from both pixels and states inputs. The skills also explore the environment thoroughly, finding sparse rewards more frequently, as shown in goal-reaching tasks from the DMC Suite and Meta-World. 
Project website: https://skillchoreographer.github.io/
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                503. <h><a href=https://openreview.net/pdf?id=2RwXVje1rAh>Exploring Active 3D Object Detection from a Generalization Perspective</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yadan Luo, Zhuoxiao Chen, Zijian Wang, Xin Yu, Zi Huang, Mahsa Baktashmotlagh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2RwXVje1rAh>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2RwXVje1rAh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    To alleviate the high annotation cost in LiDAR-based 3D object detection, active learning is a promising solution that learns to select only a small portion of unlabeled data to annotate, without compromising model performance. Our empirical study, however, suggests that mainstream uncertainty-based and diversity-based active learning policies are not effective when applied in the 3D detection task, as they fail to balance the trade-off between point cloud informativeness and box-level annotation costs. To overcome this limitation, we jointly investigate three novel criteria in our framework CRB for point cloud acquisition - label conciseness, feature representativeness and 
geometric balance, which hierarchically filters out the point clouds of redundant 3D bounding box labels, latent features and geometric characteristics (e.g., point cloud density) from the unlabeled sample pool and greedily selects informative ones with fewer objects to annotate. Our theoretical analysis demonstrates that the proposed criteria aligns the marginal distributions of the selected subset and the prior distributions of the unseen test set, and minimizes the upper bound of the generalization error. To validate the effectiveness and applicability of CRB, we conduct extensive experiments on the two benchmark 3D object detection datasets of KITTI and Waymo and examine both one-stage (i.e., Second) and two-stage 3D detector (i.e., PV-RCNN). Experiments evidence that the proposed approach outperforms existing active learning strategies and achieves fully supervised performance requiring $1\%$ and $8\%$ annotations of bounding boxes and point clouds, respectively. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                504. <h><a href=https://openreview.net/pdf?id=8FroynZv4C>Representation Learning for Low-rank General-sum Markov Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chengzhuo Ni, Yuda Song, Xuezhou Zhang, Zihan Ding, Chi Jin, Mengdi Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8FroynZv4C>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a general representation learning framework for multi-player general-sum Markov games.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8FroynZv4C.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study multi-agent general-sum Markov games with nonlinear function approximation. We focus on low-rank Markov games whose transition matrix admits a hidden low-rank structure on top of an unknown non-linear representation. The goal is to design an algorithm that (1) finds an $\varepsilon$-equilibrium policy sample efficiently without prior knowledge of the environment or the representation, and (2) permits a deep-learning friendly implementation. We leverage representation learning and present a model-based and a model-free approach to construct an effective representation from collected data. For both approaches, the algorithm achieves a sample complexity of poly$(H,d,A,1/\varepsilon)$, where $H$ is the game horizon, $d$ is the dimension of the feature vector, $A$ is the size of the joint action space and $\varepsilon$ is the optimality gap. When the number of players is large, the above sample complexity can scale exponentially with the number of players in the worst case. To address this challenge, we consider Markov Games with a factorized transition structure and present an algorithm that escapes such exponential scaling. To our best knowledge, this is the first sample-efficient algorithm for multi-agent general-sum Markov games that incorporates (non-linear) function approximation. We accompany our theoretical result with a neural network-based implementation of our algorithm and evaluate it against the widely used deep RL baseline, DQN with fictitious play.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                505. <h><a href=https://openreview.net/pdf?id=WHlt5tLz12T>LiftedCL: Lifting Contrastive Learning for Human-Centric Perception</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziwei Chen, Qiang Li, Xiaofeng Wang, Wankou Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WHlt5tLz12T>7.0 - [8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present LiftedCL for self-supervised learning, which improves contrastive learning by leveraging 3D human structure information to learn 3D-aware human-centric representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WHlt5tLz12T.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human-centric perception targets for understanding human body pose, shape and segmentation. Pre-training the model on large-scale datasets and fine-tuning it on specific tasks has become a well-established paradigm in human-centric perception. Recently, self-supervised learning methods have re-investigated contrastive learning to achieve superior performance on various downstream tasks. When handling human-centric perception, there still remains untapped potential since 3D human structure information is neglected during the task-agnostic pre-training. In this paper, we propose the Lifting Contrastive Learning (LiftedCL) to obtain 3D-aware human-centric representations which absorb 3D human structure information. In particular, to induce the learning process, a set of 3D skeletons is randomly sampled by resorting to 3D human kinematic prior. With this set of generic 3D samples, 3D human structure information can be learned into 3D-aware representations through adversarial learning. Empirical results demonstrate that LiftedCL outperforms state-of-the-art self-supervised methods on four human-centric downstream tasks, including 2D and 3D human pose estimation (0.4% mAP and 1.8 mm MPJPE improvement on COCO 2D pose estimation and Human3.6M 3D pose estimation), human shape recovery and human parsing.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                506. <h><a href=https://openreview.net/pdf?id=mWRngkvIki3>Masked Distillation with Receptive Tokens</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tao Huang, Yuan Zhang, Shan You, Fei Wang, Chen Qian, Jian Cao, Chang Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mWRngkvIki3>7.0 - [8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mWRngkvIki3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Distilling from the feature maps can be fairly effective for dense prediction tasks since both the feature discriminability and localization information can be well transferred. However, not every pixel contributes equally to the performance, and a good student should learn from what really matters to the teacher. In this paper, we introduce a learnable embedding dubbed receptive token to locate the pixels of interests (PoIs) in the feature map, with a distillation mask generated via pixel-wise attention. Then the masked distillation will be performed via the pixel-wise reconstruction. In this way, a distillation mask refers to a pattern of pixel dependencies. We thus adopt multiple receptive tokens to investigate more sophisticated and informative pixel dependencies within feature maps to enhance the distillation. To obtain a group of masks, the receptive tokens are learned via the regular task loss but with teacher fixed, and we also leverage a Dice loss to enrich the diversity of obtained masks. Our method dubbed MasKD is simple and practical, and needs no priors of ground-truth labels, which can apply to various dense prediction tasks.  Experiments show that our MasKD can achieve state-of-the-art performance consistently on object detection and semantic segmentation benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                507. <h><a href=https://openreview.net/pdf?id=WGApODQvwRg>Understanding the Covariance Structure of Convolutional Filters</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Asher Trockman, Devin Willmott, J Zico Kolter</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WGApODQvwRg>7.0 - [8, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> If you initialize depthwise convolutional filters from the right multivariate Gaussian distribution, they work so well that you may not even have to train them; we provide such Gaussians in closed-form.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WGApODQvwRg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural network weights are typically initialized at random from univariate distributions, controlling just the variance of individual weights even in highly-structured operations like convolutions. Recent ViT-inspired convolutional networks such as ConvMixer and ConvNeXt use large-kernel depthwise convolutions whose learned filters have notable structure; this presents an opportunity to study their empirical covariances. In this work, we first observe that such learned filters have highly-structured covariance matrices, and moreover, we find that covariances calculated from small networks may be used to effectively initialize a variety of larger networks of different depths, widths, patch sizes, and kernel sizes, indicating a degree of model-independence to the covariance structure. Motivated by these findings, we then propose a learning-free multivariate initialization scheme for convolutional filters using a simple, closed-form construction of their covariance. Models using our initialization outperform those using traditional univariate initializations, and typically meet or exceed the performance of those initialized from the covariances of learned filters; in some cases, this improvement can be achieved without training the depthwise convolutional filters at all. Our code is available at https://github.com/locuslab/convcov.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                508. <h><a href=https://openreview.net/pdf?id=i_1rbq8yFWC>Rhino: Deep Causal Temporal Relationship Learning with History-dependent Noise</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenbo Gong, Joel Jennings, Cheng Zhang, Nick Pawlowski</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=i_1rbq8yFWC>7.0 - [6, 6, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a causal discovery method for time series, which combines deep learning and variational inference to model instantaneous effect and history-dependent noise with structure identifiability guarantee.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/i_1rbq8yFWC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Discovering causal relationships between different variables from time series data has been a long-standing challenge for many domains. For example, in stock markets, the announcement of acquisitions from leading companies may have immediate effects on stock prices and increase the uncertainty of the future market due to this past action. To discover causal relations in such case, the model needs to consider non-linear relations between variables, instantaneous effect and the change of noise distribution due to past actions. We name the latter as history-dependent noise. However, previous works do not offer a solution addressing all these problems together. In this paper, we propose a structural equation model, called Rhino, which combines vector auto-regression, deep learning and variational inference to model non-linear relationships with instantaneous effects while allowing the noise distribution to be modulated by history observations. Theoretically, we prove the structural identifiability of Rhino. Our empirical results from extensive synthetic experiments and two real-world benchmarks demonstrate better discovery performance compared to relevant baselines, with ablation studies revealing its robustness under model misspecification.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                509. <h><a href=https://openreview.net/pdf?id=xfqDe72zh41>Actionable Neural Representations: Grid Cells from Minimal Constraints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Will Dorrell, Peter E. Latham, Timothy E. J. Behrens, James C. R. Whittington</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xfqDe72zh41>7.0 - [8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study a novel definition of an optimal representation of structured spaces, and show that it can be used to derive the brain&#39;s grid cells and their perturbations normatively.  
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xfqDe72zh41.jpg loading="lazy">
                </div>
                <div class="abstract">
                    To afford flexible behaviour, the brain must build internal representations that mirror the structure of variables in the external world. For example, 2D space obeys rules: the same set of actions combine in the same way everywhere (step north, then south, and you won&#39;t have moved, wherever you start). We suggest the brain must represent this consistent meaning of actions across space, as it allows you to find new short-cuts and navigate in unfamiliar settings. We term this representation an  `actionable representation&#39;. We formulate actionable representations using group and representation theory, and show that, when combined with biological and functional constraints - non-negative firing, bounded neural activity, and precise coding - multiple modules of hexagonal grid cells are the optimal representation of 2D space. We support this claim with intuition, analytic justification, and simulations. Our analytic results normatively explain a set of surprising grid cell phenomena, and make testable predictions for future experiments. Lastly, we highlight the generality of our approach beyond just understanding 2D space. Our work characterises a new principle for understanding and designing flexible internal representations: they should be actionable, allowing animals and machines to predict the consequences of their actions, rather than just encode.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                510. <h><a href=https://openreview.net/pdf?id=hLbeJ6jObDD>Collaborative Pure Exploration in Kernel Bandit</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yihan Du, Wei Chen, Yuko Kuroki, Longbo Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hLbeJ6jObDD>7.0 - [6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hLbeJ6jObDD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we propose a novel Collaborative Pure Exploration in Kernel Bandit model (CoPE-KB), where multiple agents collaborate to complete different but related tasks with limited communication. Our model generalizes prior CoPE formulation with the single-task and classic MAB setting to allow multiple tasks and general reward structures. We propose a novel communication scheme with an efficient kernelized estimator, and design optimal algorithms CoKernelFC and CoKernelFB for CoPE-KB with fixed-confidence  and fixed-budget objectives, respectively. Nearly matching upper and lower bounds in both sampling and communication complexity are established to demonstrate the optimality of our algorithms. Our theoretical results explicitly quantify how task similarities influence learning speedup, and only depend on the effective dimension of feature space. Our novel techniques including an efficient kernelized estimator and linear structured instance transformation, which overcome the communication difficulty in high-dimensional feature space and derive communication round lower bounds, can be of independent interests. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                511. <h><a href=https://openreview.net/pdf?id=z289SIQOQna>Plateau in Monotonic Linear Interpolation --- A &#34;Biased&#34; View of Loss Landscape for Deep Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiang Wang, Annie N. Wang, Mo Zhou, Rong Ge</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=z289SIQOQna>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We explain the long plateau in the loss and error curves along the linear interpolation from the initialization to the minimum. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/z289SIQOQna.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Monotonic linear interpolation (MLI) --- on the line connecting a random initialization with the minimizer it converges to, the loss and accuracy are monotonic --- is a phenomenon that is commonly observed in the training of neural networks. Such a  phenomenon may seem to suggest that optimization of neural networks is easy. In this paper, we show that the MLI property is not necessarily related to the hardness of optimization problems, and empirical observations on MLI for deep neural networks depend heavily on the biases. In particular, we show that interpolating both weights and biases linearly leads to very different influences on the final output, and when different classes have different last-layer biases on a deep network, there will be a long plateau in both the loss and accuracy interpolation (which existing theory of MLI cannot explain). We also show how the last-layer biases for different classes can be different even on a perfectly balanced dataset using a simple model. Empirically we demonstrate that similar intuitions hold on practical networks and realistic datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                512. <h><a href=https://openreview.net/pdf?id=LNpMtk15AS4>Boosting Causal Discovery via Adaptive Sample Reweighting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>An Zhang, Fangfu Liu, Wenchang Ma, Zhibo Cai, Xiang Wang, Tat-Seng Chua</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LNpMtk15AS4>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Automatically learn the adaptive weights for each observation to boost score-based causal discovery performance. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LNpMtk15AS4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Under stringent model type and variable distribution assumptions, score-based causal discovery methods learn the directed acyclic graph (DAG) from observational data by evaluating candidate graphs over an averaged score function. Despite the great success in low-dimensional linear systems, it has been observed that these approaches overly exploits easier-to-fit samples, thus inevitably learning spurious edges. Worse still, the common homogeneity assumption of most causal discovery methods can be easily violated due to the widespread existence of heterogeneous data in the real world, resulting in performance vulnerability when noise distributions vary. We propose a simple yet effective model-agnostic framework to boost causal discovery performance by dynamically learning the adaptive weights for the Reweighted Score function, ReScore for short, where the learned weights tailors quantitatively to the important degree of each samples. Intuitively, we leverage the bilevel optimization scheme to alternatively train a standard DAG learner first, then upweight the samples that the DAG learner fails to fit well and downweight the samples that the DAG learner easily extracts the causation information from. Extensive experiments on both synthetic and real-world datasets are carried out to validate the effectiveness of ReScore. We observe consistent and significant boosts in structure learning performance. We further visualize that ReScore concurrently mitigates the influence of spurious edges and generalizes to heterogeneous data. Finally, we perform theoretical analysis to guarantee the structure identifiability and the weight adaptive properties of ReScore. Our codes are available at https://github.com/anzhang314/ReScore.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                513. <h><a href=https://openreview.net/pdf?id=ZPHE4fht19t>Neural Systematic Binder</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gautam Singh, Yeongbin Kim, Sungjin Ahn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZPHE4fht19t>7.0 - [6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel object-centric representation called block-slots, which unlike the conventional slots, provides within-slot disentanglement via vector-formed factor representations. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZPHE4fht19t.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The key to high-level cognition is believed to be the ability to systematically manipulate and compose knowledge pieces. While token-like structured knowledge representations are naturally provided in text, it is elusive how to obtain them for unstructured modalities such as scene images. In this paper, we propose a neural mechanism called Neural Systematic Binder or SysBinder for constructing a novel structured representation called Block-Slot Representation. In Block-Slot Representation, object-centric representations known as slots are constructed by composing a set of independent factor representations called blocks, to facilitate systematic generalization. SysBinder obtains this structure in an unsupervised way by alternatingly applying two different binding principles: spatial binding for spatial modularity across the full scene and factor binding for factor modularity within an object. SysBinder is a simple, deterministic, and general-purpose layer that can be applied as a drop-in module in any arbitrary neural network and on any modality.  In experiments, we find that SysBinder provides significantly better factor disentanglement within the slots than the conventional object-centric methods, including, for the first time, in visually complex scene images such as CLEVR-Tex. Furthermore, we demonstrate factor-level systematicity in controlled scene generation by decoding unseen factor combinations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                514. <h><a href=https://openreview.net/pdf?id=hNyJBk3CwR>Is Model Ensemble Necessary? Model-based RL via a Single Model with Lipschitz Regularized Value Function</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruijie Zheng, Xiyao Wang, Huazhe Xu, Furong Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hNyJBk3CwR>7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hNyJBk3CwR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Probabilistic dynamics model ensemble is widely used in existing model-based reinforcement learning methods as it outperforms a single dynamics model in both asymptotic performance and sample efficiency. In this paper, we provide both practical and theoretical insights on the empirical success of the probabilistic dynamics model ensemble through the lens of Lipschitz continuity. We find that, for a value function, the stronger the Lipschitz condition is, the smaller the gap between the true dynamics- and learned dynamics-induced Bellman operators is, thus enabling the converged value function to be closer to the optimal value function. Hence, we hypothesize that the key functionality of the probabilistic dynamics model ensemble is to regularize the Lipschitz condition of the value function using generated samples. To validate this hypothesis, we devise two practical robust training mechanisms through computing the adversarial noise and regularizing the value network’s spectral norm to directly regularize the Lipschitz condition of the value functions. Empirical results show that combined with our mechanisms, model-based RL algorithms with a single dynamics model outperform those with ensemble of the probabilistic dynamics models. These findings not only support the theoretical insight, but also provide a practical solution for developing computationally efficient model-based RL algorithms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                515. <h><a href=https://openreview.net/pdf?id=TGJSPbRpJX->What Makes Convolutional Models Great on Long Sequence Modeling?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, Debadeepta Dey</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TGJSPbRpJX->7.0 - [6, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed a simple Strucured Global Convolution Kernel for long-range dependencies.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TGJSPbRpJX-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependencies efficiently. Attention overcomes this problem by aggregating global information based on the pair-wise attention score but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. With Fast Fourier Transform, S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes that combine the wisdom from several prior works. As a result, S4 is less intuitive and hard to use for researchers with limited prior knowledge. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses the previous SoTA on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                516. <h><a href=https://openreview.net/pdf?id=urF_CBK5XC0>Generative Augmented Flow Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ling Pan, Dinghuai Zhang, Aaron Courville, Longbo Huang, Yoshua Bengio</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=urF_CBK5XC0>7.0 - [8, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel GFlowNet learning framework to incorporate intermediate rewards represented by intrinsic motivation to improve exploration.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/urF_CBK5XC0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The Generative Flow Network is a probabilistic framework where an agent learns a stochastic policy for object generation, such that the probability of generating an object is proportional to a given reward function. Its effectiveness has been shown in discovering high-quality and diverse solutions, compared to reward-maximizing reinforcement learning-based methods. Nonetheless, GFlowNets only learn from rewards of the terminal states, which can limit its applicability. Indeed, intermediate rewards play a critical role in learning, for example from intrinsic motivation to provide intermediate feedback even in particularly challenging sparse reward tasks. Inspired by this, we propose Generative Augmented Flow Networks (GAFlowNets), a novel learning framework to incorporate intermediate rewards into GFlowNets. We specify intermediate rewards by intrinsic motivation to tackle the exploration problem in sparse reward environments. GAFlowNets can leverage edge-based and state-based intrinsic rewards in a joint way to improve exploration. Based on extensive experiments on the GridWorld task, we demonstrate the effectiveness and efficiency of GAFlowNet in terms of convergence, performance, and diversity of solutions. We further show that GAFlowNet is scalable to a more complex and large-scale molecule generation domain, where it achieves consistent and significant performance improvement.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                517. <h><a href=https://openreview.net/pdf?id=8aHzds2uUyB>Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8aHzds2uUyB>7.0 - [8, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide an open-source framework, benchmark, and novel algorithm to train large language models to better align to automated measures of human preferences.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8aHzds2uUyB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?

To help answer this, we first introduce an open-source modular library, $RL4LMs$ (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the $GRUE$ (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, $NLPO$ (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                518. <h><a href=https://openreview.net/pdf?id=WzGdBqcBicl>Understanding and Adopting Rational Behavior by Bellman Score Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kuno Kim, Stefano Ermon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WzGdBqcBicl>6.86 - [6, 8, 8, 5, 8, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We estimate the Bellman score in order to solve IRL, reward transfer, and counterfactual prediction problems
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WzGdBqcBicl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We are interested in solving a class of problems that seek to understand and adopt rational behavior from demonstrations. We may broadly classify these problems into four categories of reward identification, counterfactual analysis, behavior imitation, and behavior transfer. In this work, we make a key observation that knowing how changes in the underlying rewards affect the optimal behavior allows one to solve a variety of aforementioned problems. To a local approximation, this quantity is precisely captured by what we term the Bellman score, i.e gradient of log probabilities of the optimal policy with respect to the reward. We introduce the Bellman score operator which provably converges to the gradient of the infinite-horizon optimal Q-values with respect to the reward which can then be used to directly estimate the score. Guided by our theory, we derive a practical score-learning algorithm which can be used for score estimation in high-dimensional state-actions spaces. We show that score-learning can be used to reliably identify rewards, perform counterfactual predictions, achieve state-of-the-art behavior imitation, and transfer policies across environments. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                519. <h><a href=https://openreview.net/pdf?id=-CwPopPJda>TaskPrompter: Spatial-Channel Multi-Task Prompting for Dense Scene Understanding</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hanrong Ye, Dan Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-CwPopPJda>6.8 - [8, 6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel multi-task prompting framework to concurrently learn task-specific and task-generic representations as well as cross-task interaction along spatial and channel dimensions based on transformer for multiple dense predictions tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-CwPopPJda.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning effective representations simultaneously from multiple tasks in a unified network framework is a fundamental paradigm for multi-task dense visual scene understanding. This requires joint modeling (i) task-generic and (ii) task-specific representations, and (iii) cross-task representation interactions. Existing works typically model these three perspectives with separately designed structures, using shared network modules for task-generic learning, different modules for task-specific learning, and establishing connections among these components for cross-task interactions. It is barely explored in the literature to model these three perspectives in each network layer in an end-to-end manner, which can not only minimize the effort of carefully designing empirical structures for the three multi-task representation learning objectives, but also greatly improve the representation learning capability of the multi-task network since all the model capacity will be used to optimize the three objectives together. In this paper, we propose TaskPrompter, a novel spatial-channel multi-task prompting transformer framework to achieve this target. Specifically, we design a set of spatial-channel task prompts and learn their spatial- and channel interactions with the shared image tokens in each transformer layer with attention mechanism, as aggregating spatial and channel information is critical for dense prediction tasks. Each task prompt learns task-specific representation for one task, while all the prompts can jointly contribute to the learning of the shared image token representations, and the interactions between different task prompts model the cross-task relationship. To decode dense predictions for multiple tasks with the learned spatial-channel task prompts from transformer, we accordingly design a dense task prompt decoding mechanism, which queries the shared image tokens using task prompts to obtain spatial- and channel-wise task-specific representations. Extensive experiments on two challenging multi-task dense scene understanding benchmarks (i.e. NYUD-V2 and PASCAL-Context) show the superiority of the proposed framework and TaskPrompter establishes significant state-of-the-art performances on multi-task dense predictions. Codes and models are made publicly available at https://github.com/prismformore/Multi-Task-Transformer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                520. <h><a href=https://openreview.net/pdf?id=kj6oK_Hj40>Self-Distillation for Further Pre-training of Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Seanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kj6oK_Hj40>6.8 - [8, 6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose self-distillation in further pre-training to improve effectiveness of adaptation of pre-trained model to target tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kj6oK_Hj40.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pre-training a large transformer model on a massive amount of unlabeled data and fine-tuning it on labeled datasets for diverse downstream tasks has proven to be a successful strategy, for a variety of vision and natural language processing tasks. However, direct fine-tuning of the pre-trained model may be suboptimal if there exist large discrepancies across data domains for pre-training and fine-tuning. To tackle this issue, several previous studies have proposed further pre-training strategies, where we continue to pre-train the model on the target unlabeled dataset before fine-tuning. However, all of them solely focus on language models and we empirically find that a Vision Transformer is vulnerable to overfitting as we continue to pretrain the model on target unlabeled data. In order to tackle this limitation, we propose self-distillation as a regularization for a further pre-training stage. Specifically, we first further pre-train the initial pre-trained model on the target unlabeled data and then consider it as a teacher for self-distillation. Then we take the same initial pre-trained model as a student and enforce its hidden representations to be close to those of the teacher while optimizing the student with a masked auto-encoding objective. We empirically validate the efficacy of self-distillation on a variety of benchmark datasets for image and text classification tasks. Experimentally, we show that our proposed method outperforms all the relevant baselines. Theoretically, we analyze the proposed method with a simplified model to understand how self-distillation for further pre-training can potentially help improve the performance of the downstream tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                521. <h><a href=https://openreview.net/pdf?id=QPtMRyk5rb>Contrastive Audio-Visual Masked Autoencoder</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuan Gong, Andrew Rouditchenko, Alexander H. Liu, David Harwath, Leonid Karlinsky, Hilde Kuehne, James R. Glass</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QPtMRyk5rb>6.8 - [8, 6, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the Contrastive Audio-Visual Masked Auto-Encoder that combines contrastive learning and masked data modeling, two major self-supervised learning frameworks, to learn a joint and coordinated audio-visual representation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QPtMRyk5rb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we first extend the recent Masked Auto-Encoder (MAE) model from a single modality to audio-visual multi-modalities. Subsequently, we propose the Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE) by combining contrastive learning and masked data modeling, two major self-supervised learning frameworks, to learn a joint and coordinated audio-visual representation.
Our experiments show that the contrastive audio-visual correspondence learning objective not only enables the model to perform audio-visual retrieval tasks, but also helps the model learn a better joint representation. As a result, our fully self-supervised pretrained CAV-MAE achieves a new SOTA accuracy of 65.9% on VGGSound, and is comparable with the previous best supervised pretrained model on AudioSet in the audio-visual event classification task. Code and pretrained models are at https://github.com/yuangongnd/cav-mae.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                522. <h><a href=https://openreview.net/pdf?id=G1H4NSATlr>RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sangwoo Mo, Jong-Chyi Su, Chih-Yao Ma, Mido Assran, Ishan Misra, Licheng Yu, Sean Bell</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=G1H4NSATlr>6.8 - [5, 8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a robust semi-supervised learning method for uncurated data derived from a novel probabilistic view of learned representations
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/G1H4NSATlr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Semi-supervised learning aims to train a model using limited labels. State-of-the-art semi-supervised methods for image classification such as PAWS rely on self-supervised representations learned with large-scale unlabeled but curated data. However, PAWS is often less effective when using real-world unlabeled data that is uncurated, e.g., contains out-of-class data. We propose RoPAWS, a robust extension of PAWS that can work with real-world unlabeled data. We first reinterpret PAWS as a generative classifier that models densities using kernel density estimation. From this probabilistic perspective, we calibrate its prediction based on the densities of labeled and unlabeled data, which leads to a simple closed-form solution from the Bayes&#39; rule. We demonstrate that RoPAWS significantly improves PAWS for uncurated Semi-iNat by +5.3% and curated ImageNet by +0.4%.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                523. <h><a href=https://openreview.net/pdf?id=9aokcgBVIj1>FiT: Parameter Efficient Few-shot Transfer Learning for Personalized and Federated Image Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aliaksandra Shysheya, John F Bronskill, Massimiliano Patacchiola, Sebastian Nowozin, Richard E Turner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9aokcgBVIj1>6.8 - [8, 6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose FiT, a parameter efficient few-shot image classification system that uses a Naive Bayes head, FiLM layers that modulate a pretrained backbone, and an episodic fine-tuning protocol that achieves SOTA on the VTAB-1k benchmark.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9aokcgBVIj1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern deep learning systems are increasingly deployed in situations such as personalization and federated learning where it is necessary to support i) learning on small amounts of data, and ii) communication efficient distributed training protocols. In this work, we develop FiLM Transfer (FiT) which fulfills these requirements in the image classification setting by combining ideas from transfer learning (fixed pretrained backbones and fine-tuned FiLM adapter layers) and meta-learning (automatically configured Naive Bayes classifiers and episodic training) to yield parameter efficient models with superior classification accuracy at low-shot. The resulting parameter efficiency is key for enabling few-shot learning, inexpensive model updates for personalization, and communication efficient federated learning. We experiment with FiT on a wide range of downstream datasets and show that it achieves better classification accuracy than the leading Big Transfer (BiT) algorithm at low-shot and achieves state-of-the art accuracy on the challenging VTAB-1k benchmark, with fewer than 1% of the updateable parameters. Finally, we demonstrate the parameter efficiency and superior accuracy of FiT in distributed low-shot applications including model personalization and federated learning where model update size is an important performance metric.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                524. <h><a href=https://openreview.net/pdf?id=8duT3mi_5n>GReTo: Remedying dynamic graph topology-task discordance via target homophily</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhengyang Zhou, qihe huang, Gengyu Lin, Kuo Yang, LEI BAI, Yang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8duT3mi_5n>6.8 - [6, 6, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper revisits how node-wise relation modeling to facilitate regressions on dynamic graphs, from a new perspective of target-homophily. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8duT3mi_5n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dynamic graphs are ubiquitous across disciplines where observations usually change over time. Regressions on dynamic graphs often contribute to diverse critical tasks, such as climate early-warning and traffic controlling. Existing homophily Graph Neural Networks (GNNs) adopt physical connections or feature similarity as adjacent matrix to perform node-level aggregations. However, on dynamic graphs with diverse node-wise relations, exploiting a pre-defined fixed topology for message passing inevitably leads to the aggregations of target-deviated neighbors. We designate such phenomenon as the topology-task discordance, which naturally challenges the homophily assumption. In this work, we revisit node-wise relationships and explore novel homophily measurements on dynamic graphs with both signs and distances, capturing multiple node-level spatial relations and temporal evolutions. We discover that advancing homophily aggregations to signed target-oriented message passing can effectively resolve the discordance and promote aggregation capacity. Therefore, a GReTo is proposed, which performs signed message passing in immediate neighborhood, and exploits both local environments and target awareness to realize high-order message propagation. Empirically, our solution achieves significant improvements against best baselines, notably improving 24.79% on KnowAir and 3.60% on Metr-LA. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                525. <h><a href=https://openreview.net/pdf?id=HVoJCRLByVk>Can Neural Networks Learn Implicit Logic from Physical Reasoning?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aaron Traylor, Roman Feiman, Ellie Pavlick</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HVoJCRLByVk>6.8 - [8, 8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HVoJCRLByVk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the success of neural network models in a range of domains, it remains an open question whether they can learn to represent abstract logical operators such as negation and disjunction. We test the hypothesis that neural networks without inherent inductive biases for logical reasoning can acquire an implicit representation of negation and disjunction. Here, implicit refers to limited, domain-specific forms of these operators, and work in psychology suggests these operators may be a precursor (developmentally and evolutionarily) to the type of abstract, domain-general logic that is characteristic of adult humans. To test neural networks, we adapt a test designed to diagnose the presence of negation and disjunction in animals and pre-verbal children, which requires inferring the location of a hidden object using constraints of the physical environment as well as implicit logic: if a ball is hidden in A or B, and shown not to be in A, can the subject infer that it is in B? Our results show that, despite the neural networks learning to track objects behind occlusion, they are unable to generalize to a task that requires implicit logic. We further show that models are unable to generalize to the test task even when they are trained directly on a logically identical (though visually dissimilar) task. However, experiments using transfer learning reveal that the models do recognize structural similarity between tasks which invoke the same logical reasoning pattern, suggesting that some desirable abstractions are learned, even if they are not yet sufficient to pass established tests of logical reasoning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                526. <h><a href=https://openreview.net/pdf?id=xjxUjHa_Wpa>VA-DepthNet: A Variational Approach to Single Image Depth Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ce Liu, Suryansh Kumar, Shuhang Gu, Radu Timofte, Luc Van Gool</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xjxUjHa_Wpa>6.75 - [8, 8, 6, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xjxUjHa_Wpa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce VA-DepthNet, a simple, effective, and accurate deep neural network approach for the single-image depth prediction (SIDP) problem. The proposed approach advocates using classical first-order variational constraints for this problem. While state-of-the-art deep neural network methods for SIDP learn the scene depth from images in a supervised setting, they often overlook the invaluable invariances and priors in the rigid scene space, such as the regularity of the scene. The paper&#39;s main contribution is to reveal the benefit of classical and well-founded variational constraints in the neural network design for the SIDP task. It is shown that imposing first-order variational constraints in the scene space together with popular encoder-decoder-based network architecture design provides excellent results for the supervised SIDP task. The imposed first-order variational constraint makes the network aware of the depth gradient in the scene space, i.e., regularity. The paper demonstrates the usefulness of the proposed approach via extensive evaluation and ablation analysis over several benchmark datasets, such as KITTI, NYU Depth V2, and SUN RGB-D. The VA-DepthNet at test time shows considerable improvements in depth prediction accuracy compared to the prior art and is accurate also at high-frequency regions in the scene space.  At the time of writing this paper, our method---labeled as VA-DepthNet, when tested on the KITTI depth-prediction evaluation set benchmarks, shows state-of-the-art results, and is the top-performing published approach.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                527. <h><a href=https://openreview.net/pdf?id=X5ZMzRYqUjB>Humanly Certifying Superhuman Classifiers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qiongkai Xu, Christian Walder, Chenchen Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=X5ZMzRYqUjB>6.75 - [8, 6, 8, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A theory for estimating the performance of a classifier by comparing with human annotators, even when the humans are inferior to the classifier.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/X5ZMzRYqUjB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper addresses a key question in current machine learning research: if we believe that a model&#39;s predictions might be better than those given by human experts, how can we (humans) verify these beliefs? In some cases, this ``superhuman&#39;&#39; performance is readily demonstrated; for example by defeating top-tier human players in traditional two player games. On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance. Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations. In reality, human annotators are subjective and can make mistakes. Evaluating the performance with respect to a genuine oracle is more objective and reliable, even when querying the oracle is more expensive or sometimes impossible. In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is $\textit{unobserved}$. We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference. Our analysis provides an executable recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification. We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles. Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our mild assumptions a number of models from recent years have already achieved superhuman performance with high probability---suggesting that our new oracle based performance evaluation metrics are overdue as an alternative to the widely used accuracy metrics that are naively based on imperfect human annotations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                528. <h><a href=https://openreview.net/pdf?id=to3qCB3tOh9>Protein Representation Learning by Geometric Structure Pretraining</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zuobai Zhang, Minghao Xu, Arian Rokkum Jamasb, Vijil Chenthamarakshan, Aurelie Lozano, Payel Das, Jian Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=to3qCB3tOh9>6.75 - [6, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this work, we propose a versatile protein structure encoder GearNet, a superior protein structure pre-trainining algorithm Multiview Contrast and a suite of protein structure pre-training baselines.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/to3qCB3tOh9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein function or structure. Existing approaches usually pretrain protein language models on a large number of unlabeled amino acid sequences and then finetune the models with some labeled data in downstream tasks. Despite the effectiveness of sequence-based approaches, the power of pretraining on known protein structures, which are available in smaller numbers only, has not been explored for protein property prediction, though protein structures are known to be determinants of protein function. In this paper, we propose to pretrain protein representations according to their 3D structures. We first present a simple yet effective encoder to learn the geometric features of a protein. We pretrain the protein graph encoder by leveraging multiview contrastive learning and different self-prediction tasks. Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data. Our implementation is available at https://github.com/DeepGraphLearning/GearNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                529. <h><a href=https://openreview.net/pdf?id=cbpRzMy-UZH>Effective Self-supervised Pre-training on Low-compute Networks without Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fuwen Tan, Fatemeh Sadat Saleh, Brais Martinez</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cbpRzMy-UZH>6.75 - [8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cbpRzMy-UZH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the impressive progress of self-supervised learning (SSL), its applicability to low-compute networks has received limited attention. Reported performance has trailed behind standard supervised pre-training by a large margin, barring self-supervised learning from making an impact on models that are deployed on device. Most prior works attribute this poor performance to the capacity bottleneck of the low-compute networks and opt to bypass the problem through the use of knowledge distillation (KD). In this work, we revisit SSL for efficient neural networks, taking a closer at what are the detrimental factors causing the practical limitations, and whether they are intrinsic to the self-supervised low-compute setting. We find that, contrary to accepted knowledge, there is no intrinsic architectural bottleneck, we diagnose that the performance bottleneck is related to the model complexity vs regularization strength trade-off. In particular, we start by empirically observing that the use of local views can have a dramatic impact on the effectiveness of the SSL methods. This hints at view sampling being one of the performance bottlenecks for SSL on low-capacity networks. We hypothesize that the view sampling strategy for large neural networks, which requires matching views in very diverse spatial scales and contexts, is too demanding for low-capacity architectures. We systematize the design of the view sampling mechanism, leading to a new training methodology that consistently improves the performance across different SSL methods (e.g. MoCo-v2, SwAV or DINO), different low-size networks (convolution-based networks, e.g. MobileNetV2, ResNet18, ResNet34 and vision transformer, e.g. ViT-Ti), and different tasks (linear probe, object detection, instance segmentation and semi-supervised learning). Our best models establish new state-of-the-art for SSL methods on low-compute networks despite not using a KD loss term. Code is publicly available at github.com/saic-fi/SSLight.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                530. <h><a href=https://openreview.net/pdf?id=sbWVtxq8-zE>Can discrete information extraction prompts generalize across language models?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nathanaël Carraz Rakotonirina, Roberto Dessi, Fabio Petroni, Sebastian Riedel, Marco Baroni</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sbWVtxq8-zE>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that automatically generated prompts can be learned on a language model and used to retrieve information from another. We further provide some preliminary insights on the nature of these &#34;universal prompts&#34;.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sbWVtxq8-zE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study whether automatically-induced prompts that effectively extract information from a language model can also be used, out-of-the-box, to probe other language models for the same information. After confirming that discrete prompts induced with the AutoPrompt algorithm outperform manual and semi-manual prompts on the slot-filling task, we demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another. We introduce a way to induce prompts by mixing language models at training time that results in prompts that generalize well across models. We conduct an extensive analysis of the induced prompts, finding that the more general prompts include a larger proportion of existing English words and have a less order-dependent and more uniform distribution of information across their component tokens. Our work provides preliminary evidence that it&#39;s possible to generate discrete prompts that can be induced once and used with a number of different models, and gives insights on the properties characterizing such prompts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                531. <h><a href=https://openreview.net/pdf?id=o3Q4m8jg4BR>LS-IQ: Implicit Reward Regularization for Inverse Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Firas Al-Hafez, Davide Tateo, Oleg Arenz, Guoping Zhao, Jan Peters</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=o3Q4m8jg4BR>6.75 - [8, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel perspective on implicit L2 reward regularization for inverse reinforcement learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/o3Q4m8jg4BR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent methods for imitation learning directly learn a $Q$-function using an implicit reward formulation rather than an explicit reward function. However, these methods generally require implicit reward regularization to improve stability and often mistreat absorbing states. Previous works show that a squared norm regularization on the implicit reward function is effective, but do not provide a theoretical analysis of the resulting properties of the algorithms. In this work, we show that using this regularizer under a mixture distribution of the policy and the expert provides a particularly illuminating perspective: the original objective can be understood as squared Bellman error minimization, and the corresponding optimization problem minimizes a bounded $\chi^2$-Divergence between the expert and the mixture distribution. This perspective allows us to address instabilities and properly treat absorbing states. We show that our method, Least Squares Inverse Q-Learning (LS-IQ), outperforms state-of-the-art algorithms, particularly in environments with absorbing states. Finally, we propose to use an inverse dynamics model to learn from observations only. Using this approach, we retain performance in settings where no expert actions are available.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                532. <h><a href=https://openreview.net/pdf?id=aCuFa-RRqtI>Label Propagation with Weak Supervision</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rattana Pukdee, Dylan Sam, Pradeep Kumar Ravikumar, Nina Balcan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aCuFa-RRqtI>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Theoretical analysis of label propagation with prior information and connection to weak supervision
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aCuFa-RRqtI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu &amp; Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                533. <h><a href=https://openreview.net/pdf?id=gmL46YMpu2J>Promptagator: Few-shot Dense Retrieval From 8 Examples</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, Ming-Wei Chang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gmL46YMpu2J>6.75 - [8, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gmL46YMpu2J.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other retrieval tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval problems, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To address this, we introduce Prompt-based Query Generation forRetrieval (Promptagator): for each task, we feed the few-shot examples to a large language model (LLM) and prompt it to behave as a task-specific query generator. Using this, we can synthetically generate a large number of relevant queries for any document, yielding abundant data for training task-specific retrievers --- with no reliance on traditional resources such as Natural Questions (Kwiatkowskiet al., 2019) or MS MARCO (Nguyen et al., 2016). Surprisingly, Promptagator with only 8 annotated examples enables efficient dual encoder retrievers to outperform computationally more expensive models trained on MS MARCO such as ColBERT v2 (Santhanam et al., 2022) by more than 1.2 points nDCG@10 on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 points nDCG@10 improvement. Our studies show that synthetic query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                534. <h><a href=https://openreview.net/pdf?id=3aBuJEza5sq>Test-Time Robust Personalization for Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Liangze Jiang, Tao Lin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3aBuJEza5sq>6.75 - [8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We identify the pitfalls of existing personalized federated learning methods during deployment and propose a novel test-time personalization solution.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3aBuJEza5sq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated Learning (FL) is a machine learning paradigm where many clients collaboratively learn a shared global model with decentralized training data. Personalization on FL models additionally adapts the global model to different clients, achieving promising results on consistent local training &amp; test distributions. However, for real-world personalized FL applications, it is crucial to go one step further: robustifying FL models under the evolving local test set during deployment, where various types of distribution shifts can arise. In this work, we identify the pitfalls of existing works under test-time distribution shifts and propose Federated Test-time Head Ensemble plus tuning (FedTHE+), which personalizes FL models with robustness to various test-time distribution shifts. We illustrate the advancement of FedTHE+ (and its degraded computationally efficient variant FedTHE) over strong competitors, for training various neural architectures (CNN, ResNet, and Transformer) on CIFAR10 and ImageNet and evaluating on diverse test distributions. Along with this, we build a benchmark for assessing the performance and robustness of personalized FL methods during deployment. Code: \url{https://github.com/LINs-lab/FedTHE}.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                535. <h><a href=https://openreview.net/pdf?id=s130rTE3U_X>Understanding the Role of Nonlinearity in Training Dynamics of Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuandong Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=s130rTE3U_X>6.75 - [8, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We analyze the role played by the nonlinearity in the training dynamics nonlinear 2-layer network for contrastive learning. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/s130rTE3U_X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While the empirical success of self-supervised learning (SSL) heavily relies on the usage of deep nonlinear models, existing theoretical works on SSL understanding still focus on linear ones. In this paper, we study the role of nonlinearity in the training dynamics of contrastive learning (CL) on one and two-layer nonlinear networks with homogeneous activation $h(x) = h&#39;(x)x$. We have two major theoretical discoveries. First, the presence of nonlinearity can lead to many local optima even in 1-layer setting, each corresponding to certain patterns from the data distribution, while with linear activation, only one major pattern can be learned. This suggests that models with lots of parameters can be regarded as a \emph{brute-force} way to find these local optima induced by nonlinearity. Second, in the 2-layer case, linear activation is proven not capable of learning specialized weights into diverse patterns, demonstrating the importance of nonlinearity. In addition, for 2-layer setting, we also discover \emph{global modulation}: those local patterns discriminative from the perspective of global-level patterns are prioritized to learn, further characterizing the learning process. Simulation verifies our theoretical findings.  
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                536. <h><a href=https://openreview.net/pdf?id=UT-_SVOyD1H>Contextual bandits with concave rewards, and an application to fair ranking</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Virginie Do, Elvis Dohmatob, Matteo Pirotta, Alessandro Lazaric, Nicolas Usunier</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UT-_SVOyD1H>6.75 - [8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show a reduction of concave multi-reward contextual bandits to classical single-reward bandits, and apply this reduction to a fair ranking problem.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UT-_SVOyD1H.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider Contextual Bandits with Concave Rewards (CBCR), a multi-objective bandit problem where the desired trade-off between the rewards is defined by a known concave objective function, and the reward vector depends on an observed stochastic context. We present the first algorithm with provably vanishing regret for CBCR without restrictions on the policy space, whereas prior works were restricted to finite policy spaces or tabular representations. Our solution is based on a geometric interpretation of CBCR algorithms as optimization algorithms over the convex set of expected rewards spanned by all stochastic policies. Building on Frank-Wolfe analyses in constrained convex optimization, we derive a novel reduction from the CBCR regret to the regret of a \emph{scalar-reward} bandit problem. We illustrate how to apply the reduction off-the-shelf to obtain algorithms for CBCR with both linear and general reward functions, in the case of non-combinatorial actions. Motivated by fairness in recommendation, we describe a special case of CBCR with rankings and fairness-aware objectives, leading to the first algorithm with regret guarantees for contextual combinatorial bandits with fairness of exposure.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                537. <h><a href=https://openreview.net/pdf?id=BYWWwSY2G5s>Score-based Continuous-time Discrete Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, Hanjun Dai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=BYWWwSY2G5s>6.75 - [5, 10, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> a generalized discrete score matching for learning continuous-time diffusion in categorical spaces, with new parameterization and novel analytical sampling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/BYWWwSY2G5s.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Score-based modeling through stochastic differential equations (SDEs) has provided a new perspective on diffusion models, and demonstrated superior performance on continuous data. However, the gradient of the log-likelihood function, \ie, the score function, is not properly defined for discrete spaces. This makes it non-trivial to adapt SDE with score functions to categorical data. In this paper, we extend diffusion models to discrete variables by introducing a stochastic jump process where the reverse process denoises via a continuous-time Markov chain. This formulation admits an analytical simulation during backward sampling. To learn the reverse process, we extend score matching to general categorical data, and show that an unbiased estimator can be obtained via simple matching of the conditional marginal distributions. We demonstrate the effectiveness of the proposed method on a set of synthetic and real-world music and image benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                538. <h><a href=https://openreview.net/pdf?id=7D5EECbOaf9>Moderate Coreset: A Universal Method of Data Selection for Real-world Data-efficient Deep Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaobo Xia, Jiale Liu, Jun Yu, Xu Shen, Bo Han, Tongliang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7D5EECbOaf9>6.75 - [8, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7D5EECbOaf9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning methods nowadays rely on massive data, resulting in substantial costs of data storage and model training. Data selection is a useful tool to alleviate such costs, where a coreset of massive data is extracted to practically perform on par with full data. Based on carefully-designed score criteria, existing methods first count the score of each data point and then select the data points whose scores lie in a certain range to construct a coreset. These methods work well in their respective preconceived scenarios but are not robust to the change of scenarios, since the optimal range of scores varies as the scenario changes. The issue limits the application of these methods, because realistic scenarios often mismatch preconceived ones, and it is inconvenient or unfeasible to tune the criteria and methods accordingly. In this paper, to address the issue, a concept of the moderate coreset is discussed. Specifically, given any score criterion of data selection, different scenarios prefer data points with scores in different intervals. As the score median is a proxy of the score distribution in statistics, the data points with scores close to the score median can be seen as a proxy of full data and generalize different scenarios, which are used to construct the moderate coreset. As a proof-of-concept, a universal method that inherits the moderate coreset and uses the distance of a data point to its class center as the score criterion, is proposed to meet complex realistic scenarios. Extensive experiments confirm the advance of our method over prior state-of-the-art methods, leading to a strong baseline for future research. The implementation is available at https://github.com/tmllab/Moderate-DS.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                539. <h><a href=https://openreview.net/pdf?id=KGV-GBh8fb>Not All Tasks Are Born Equal: Understanding Zero-Shot Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jing Zhou, Zongyu Lin, Yanan Zheng, Jian Li, Zhilin Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KGV-GBh8fb>6.75 - [8, 5, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KGV-GBh8fb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent work has achieved remarkable zero-shot performance with multi-task prompted pretraining, but little has been understood. For the first time, we show that training on a small number of key tasks beats using all the training tasks, while removing these key tasks substantially hurts performance. We also find that these key tasks are mostly question answering (QA) tasks. These novel findings combined deepen our understanding about zero-shot generalization—training on certain tasks such as QA encodes general knowledge transferable to a wide range of tasks. In addition, to automate this procedure, we devise a method that (1) identifies key training tasks without observing the test tasks by examining the pairwise generalization results and (2) resamples training tasks for better data distribution. Empirically, our approach achieves improved results across various model scales and tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                540. <h><a href=https://openreview.net/pdf?id=k4fevFqSQcX>SAM as an Optimal Relaxation of Bayes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thomas Möllenhoff, Mohammad Emtiyaz Khan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k4fevFqSQcX>6.75 - [6, 5, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that SAM can be seen as a relaxation of Bayes, by using Fenchel conjugates.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k4fevFqSQcX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sharpness-aware minimization (SAM) and related adversarial deep-learning methods can drastically improve generalization, but their underlying mechanisms are not yet fully understood. Here, we establish SAM as a relaxation of the Bayes objective where the expected negative-loss is replaced by the optimal convex lower bound, obtained by using the so-called Fenchel biconjugate. The connection enables a new Adam-like extension of SAM to automatically obtain reasonable uncertainty estimates, while sometimes also improving its accuracy. By connecting adversarial and Bayesian methods, our work opens a new path to robustness.  
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                541. <h><a href=https://openreview.net/pdf?id=nI2HmVA0hvt>Unsupervised visualization of image datasets using contrastive learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Niklas Böhm, Philipp Berens, Dmitry Kobak</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nI2HmVA0hvt>6.75 - [6, 5, 10, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nI2HmVA0hvt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Visualization methods based on the nearest neighbor graph, such as t-SNE or UMAP, are widely used for visualizing high-dimensional data. Yet, these approaches only produce meaningful results if the nearest neighbors themselves are meaningful. For images represented in pixel space this is not the case, as distances in pixel space are often not capturing our sense of similarity and therefore neighbors are not semantically close. This problem can be circumvented by self-supervised approaches based on contrastive learning, such as SimCLR, relying on data augmentation to generate implicit neighbors, but these methods do not produce two-dimensional embeddings suitable for visualization. Here, we present a new method, called t-SimCNE, for unsupervised visualization of image data. T-SimCNE combines ideas from contrastive learning and neighbor embeddings, and trains a parametric mapping from the high-dimensional pixel space into two dimensions. We show that the resulting 2D embeddings achieve classification accuracy comparable to the state-of-the-art high-dimensional SimCLR representations, thus faithfully capturing semantic relationships. Using t-SimCNE, we obtain informative visualizations of the CIFAR-10 and CIFAR-100 datasets, showing rich cluster structure and highlighting artifacts and outliers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                542. <h><a href=https://openreview.net/pdf?id=KfptQCEKVW4>Automating Nearest Neighbor Search Configuration with Constrained Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Philip Sun, Ruiqi Guo, Sanjiv Kumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KfptQCEKVW4>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KfptQCEKVW4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The approximate nearest neighbor (ANN) search problem is fundamental to efficiently serving many real-world machine learning applications. A number of techniques have been developed for ANN search that are efficient, accurate, and scalable. However, such techniques typically have a number of parameters that affect the speed-recall tradeoff, and exhibit poor performance when such parameters aren&#39;t properly set. Tuning these parameters has traditionally been a manual process, demanding in-depth knowledge of the underlying search algorithm. This is becoming an increasingly unrealistic demand as ANN search grows in popularity. To tackle this obstacle to ANN adoption, this work proposes a constrained optimization-based approach to tuning quantization-based ANN algorithms. Our technique takes just a desired search cost or recall as input, and then generates tunings that, empirically, are very close to the speed-recall Pareto frontier and give leading performance on standard benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                543. <h><a href=https://openreview.net/pdf?id=Rq13idF0F73>Molecule Generation For Target Protein Binding with Structural Motifs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>ZAIXI ZHANG, Yaosen Min, Shuxin Zheng, Qi Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Rq13idF0F73>6.75 - [8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Rq13idF0F73.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Designing ligand molecules that bind to specific protein binding sites is a fundamental problem in structure-based drug design. Although deep generative models and geometric deep learning have made great progress in drug design, existing works either sample in the 2D graph space or fail to generate valid molecules with realistic substructures. To tackle these problems, we propose a Fragment-based LigAnd Generation framework (FLAG), to generate 3D molecules with valid and realistic substructures fragment-by-fragment. In FLAG, a motif vocabulary is constructed by extracting common molecular fragments (i.e., motif) in the dataset. At each generation step, a 3D graph neural network is first employed to encode the intermediate context information. Then, our model selects the focal motif, predicts the next motif type, and attaches the new motif. The bond lengths/angles can be quickly and accurately determined by cheminformatics tools. Finally, the molecular geometry is further adjusted according to the predicted rotation angle and the structure refinement. Our model not only achieves competitive performances on conventional metrics such as binding affinity, QED, and SA, but also outperforms baselines by a large margin in generating molecules with realistic substructures.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                544. <h><a href=https://openreview.net/pdf?id=hJqGbUpDGV>On the Sensitivity of Reward Inference to Misspecified Human Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Joey Hong, Kush Bhatia, Anca Dragan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hJqGbUpDGV>6.75 - [8, 3, 8, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We investigate the impact of assuming wrong human models on reward learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hJqGbUpDGV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inferring reward functions from human behavior is at the center of value alignment – aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases in behavior that lead to arbitrarily large errors in the inferred reward. However, and arguably more importantly, we are also able to identify reasonable assumptions under which the reward inference error can be bounded linearly in the error in the human model. Finally, we verify our theoretical insights in discrete and continuous control tasks with simulated and human data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                545. <h><a href=https://openreview.net/pdf?id=PbfgkZ2HdbE>Learning Controllable Adaptive Simulation for Multi-resolution Physics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tailin Wu, Takashi Maruyama, Qingqing Zhao, Gordon Wetzstein, Jure Leskovec</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PbfgkZ2HdbE>6.75 - [8, 8, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a method jointly learns the surrogate model and dynamically selects appropriate spatial resolutions that devote more compute to the highly dynamic regions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PbfgkZ2HdbE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Simulating the time evolution of physical systems is pivotal in many scientific and engineering problems. An open challenge in simulating such systems is their multi-resolution dynamics: a small fraction of the system is extremely dynamic, and requires very fine-grained resolution, while a majority of the system is changing slowly and can be modeled by coarser spatial scales. Typical learning-based surrogate models use a uniform spatial scale, which needs to resolve to the finest required scale and can waste a huge compute to achieve required accuracy. In this work, we introduce Learning controllable Adaptive simulation for Multi-resolution Physics (LAMP) as the first full deep learning-based surrogate model that jointly learns the evolution model and optimizes appropriate spatial resolutions that devote more compute to the highly dynamic regions. LAMP consists of a Graph Neural Network (GNN) for learning the forward evolution, and a GNN-based actor-critic for learning the policy of spatial refinement and coarsening. We introduce learning techniques that optimizes LAMP with weighted sum of error and computational cost as objective, allowing LAMP to adapt to varying relative importance of error vs. computation tradeoff at inference time. We evaluate our method in a 1D benchmark of nonlinear PDEs and a challenging 2D mesh-based simulation. We demonstrate that our LAMP outperforms state-of-the-art deep learning surrogate models, and can adaptively trade-off computation to improve long-term prediction error: it achieves an average of 33.7% error reduction for 1D nonlinear PDEs, and outperforms  MeshGraphNets + classical Adaptive Mesh Refinement (AMR) in 2D mesh-based simulations. Project website with data and code can be found at: http://snap.stanford.edu/lamp.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                546. <h><a href=https://openreview.net/pdf?id=EXnIyMVTL8s>Towards Understanding and Mitigating Dimensional Collapse in Heterogeneous Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yujun Shi, Jian Liang, Wenqing Zhang, Vincent Tan, Song Bai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EXnIyMVTL8s>6.75 - [5, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show data heterogeneity in federated learning causes dimensional collapse for trained models, and propose FedDecorr to mitigate such problem.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EXnIyMVTL8s.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning aims to train models collaboratively across different clients without sharing data for privacy considerations. However, one major challenge for this learning paradigm is the data heterogeneity problem, which refers to the discrepancies between the local data distributions among various clients. To tackle this problem, we first study how data heterogeneity affects the representations of the globally aggregated models. Interestingly, we find that heterogeneous data results in the global model suffering from severe dimensional collapse, in which representations tend to reside in a lower-dimensional space instead of the ambient space. Moreover, we observe a similar phenomenon on models locally trained on each client and deduce that the dimensional collapse on the global model is inherited from local models. In addition, we theoretically analyze the gradient flow dynamics to shed light on how data heterogeneity result in dimensional collapse for local models. To remedy this problem caused by the data heterogeneity, we propose FedDecorr, a novel method that can effectively mitigate dimensional collapse in federated learning. Specifically, FedDecorr applies a regularization term during local training that encourages different dimensions of representations to be uncorrelated. FedDecorr, which is implementation-friendly and computationally-efficient, yields consistent improvements over baselines on standard benchmark datasets. Code: https://github.com/bytedance/FedDecorr.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                547. <h><a href=https://openreview.net/pdf?id=woa783QMul>Fairness-aware Contrastive Learning with Partially Annotated Sensitive Attributes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fengda Zhang, Kun Kuang, Long Chen, Yuxuan Liu, Chao Wu, Jun Xiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=woa783QMul>6.75 - [6, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Proposing a new problem of fair unsupervised representation learning with limited annotated sensitive attributes and a fairness-aware contrastive learning framework.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/woa783QMul.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning high-quality representation is important and essential for visual recognition. Unfortunately, traditional representation learning suffers from fairness issues since the model may learn information of sensitive attributes. Recently, a series of studies have been proposed to improve fairness by explicitly decorrelating target labels and sensitive attributes. Most of these methods, however, rely on the assumption that fully annotated labels on target variable and sensitive attributes are available, which is unrealistic due to the expensive annotation cost. In this paper, we investigate a novel and practical problem of Fair Unsupervised Representation Learning with Partially annotated Sensitive labels (FURL-PS). FURL-PS has two key challenges: 1) how to make full use of the samples that are not annotated with sensitive attributes; 2) how to eliminate bias in the dataset without target labels. To address these challenges, we propose a general Fairness-aware Contrastive Learning (FairCL) framework consisting of two stages. Firstly, we generate contrastive sample pairs, which share the same visual information apart from sensitive attributes, for each instance in the original dataset. In this way, we construct a balanced and unbiased dataset. Then, we execute fair contrastive learning by closing the distance between representations of contrastive sample pairs. Besides, we also propose an unsupervised way to balance the utility and fairness of learned representations by feature reweighting. Extensive experimental results illustrate the effectiveness of our method in terms of fairness and utility, even with very limited sensitive attributes and serious data bias.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                548. <h><a href=https://openreview.net/pdf?id=g4OTKRKfS7R>Liquid Structural State-Space Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=g4OTKRKfS7R>6.75 - [8, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use the recently proposed parametrization and memorization techniques for training state-space models in a linearized version of liquid neural networks, and achieve SOTA on sequence modeling tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/g4OTKRKfS7R.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on an extensive series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structured SSM, such as S4, is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structured state-space model, dubbed Liquid-S4, improves generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32\% on the Long-Range Arena benchmark. On the full raw Speech Command recognition dataset, Liquid-S4 achieves 96.78\% accuracy with a 30\% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4&#39;s kernel structure that takes into account the similarities of the input sequence samples during training and inference.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                549. <h><a href=https://openreview.net/pdf?id=sPCKNl5qDps>Ollivier-Ricci Curvature for Hypergraphs: A Unified Framework</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Corinna Coupette, Sebastian Dalleiger, Bastian Rieck</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sPCKNl5qDps>6.75 - [8, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a flexible framework for Ollivier-Ricci curvature on hypergraphs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sPCKNl5qDps.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Bridging geometry and topology, curvature is a powerful and expressive invariant. While the utility of curvature has been theoretically and empirically confirmed in the context of manifolds and graphs, its generalization to the emerging domain of hypergraphs has remained largely unexplored. On graphs, the Ollivier-Ricci curvature measures differences between random walks via Wasserstein distances, thus grounding a geometric concept in ideas from probability theory and optimal transport. We develop Orchid, a flexible framework generalizing Ollivier-Ricci curvature to hypergraphs, and prove that the resulting curvatures have favorable theoretical properties. Through extensive experiments on synthetic and real-world hypergraphs from different domains, we demonstrate that Orchid curvatures are both scalable and useful to perform a variety of hypergraph tasks in practice.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                550. <h><a href=https://openreview.net/pdf?id=gJW8hSGBys8>Compositional Semantic Parsing with Large Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gJW8hSGBys8>6.75 - [8, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Using an extension of least-to-most prompting we demonstrate strong performance on two benchmarks for compositional generalization, CFQ and COGS, and achieve state of the art on CFQ while using only 1% of the training data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gJW8hSGBys8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Humans can reason compositionally when presented with new tasks.  Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                551. <h><a href=https://openreview.net/pdf?id=zClyiZ5V6sL>TiAda: A Time-scale Adaptive Algorithm for Nonconvex Minimax Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiang Li, Junchi YANG, Niao He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zClyiZ5V6sL>6.75 - [6, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zClyiZ5V6sL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Adaptive gradient methods have shown their ability to adjust the stepsizes on the fly in a parameter-agnostic manner, and empirically achieve faster convergence for solving minimization problems. When it comes to nonconvex minimax optimization, however, current convergence analyses of gradient descent ascent (GDA) combined with adaptive stepsizes require careful tuning of hyper-parameters and the knowledge of problem-dependent parameters. Such a discrepancy arises from the primal-dual nature of minimax problems and the necessity of delicate time-scale separation between the primal and dual updates in attaining convergence. In this work, we propose a single-loop adaptive GDA algorithm called TiAda for nonconvex minimax optimization that automatically adapts to the time-scale separation. Our algorithm is fully parameter-agnostic and can achieve near-optimal complexities simultaneously in deterministic and stochastic settings of nonconvex-strongly-concave minimax problems. The effectiveness of the proposed method is further justified numerically for a number of machine learning applications.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                552. <h><a href=https://openreview.net/pdf?id=3ULaIHxn9u7>Seeing Differently, Acting Similarly: Heterogeneously Observable Imitation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xin-Qiang Cai, Yao-Xiang Ding, Zixuan Chen, Yuan Jiang, Masashi Sugiyama, Zhi-Hua Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3ULaIHxn9u7>6.75 - [10, 8, 3, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3ULaIHxn9u7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In many real-world imitation learning tasks, the demonstrator and the learner have to act under different observation spaces. This situation brings significant obstacles to existing imitation learning approaches, since most of them learn policies under homogeneous observation spaces. On the other hand, previous studies under different observation spaces have strong assumptions that these two observation spaces coexist during the entire learning process. However, in reality, the observation coexistence will be limited due to the high cost of acquiring expert observations. In this work, we study this challenging problem with limited observation coexistence under heterogeneous observations: Heterogeneously Observable Imitation Learning (HOIL). We identify two underlying issues in HOIL: the dynamics mismatch and the support mismatch, and further propose the Importance Weighting with REjection (IWRE) algorithm based on importance weighting and learning with rejection to solve HOIL problems. Experimental results show that IWRE can solve various HOIL tasks, including the challenging tasks of transforming the vision-based demonstrations to random access memory (RAM)-based policies in the Atari domain, even with limited visual observations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                553. <h><a href=https://openreview.net/pdf?id=c7rM7F7jQjN>From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi Shafiullah, Lerrel Pinto</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=c7rM7F7jQjN>6.75 - [8, 8, 5, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train a transformer-based model on uncurated play data, which can produce targeted real-world robot policies by conditioning on future observations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/c7rM7F7jQjN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While large-scale sequence modelling from offline data has led to impressive performance gains in natural language generation and image generation, directly translating such ideas to robotics has been challenging. One critical reason for this is that uncurated robot demonstration data, i.e. play data, collected from non-expert human demonstrators are often noisy, diverse, and distributionally multi-modal. This makes extracting useful, task-centric behaviors from such data a difficult generative modelling problem. In this work, we present Conditional Behavior Transformers (C-BeT), a method that combines the multi-modal generation ability of Behavior Transformer with future-conditioned goal specification. On a suite of simulated benchmark tasks, we find that C-BeT improves upon prior state-of-the-art work in learning from play data by an average of 45.7%. Further, we demonstrate for the first time that useful task-centric behaviors can be learned on a real-world robot purely from play data without any task labels or reward information. Robot videos are best viewed on our project website: play-to-policy.github.io
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                554. <h><a href=https://openreview.net/pdf?id=9x3CO0ZU9LR>Sample Complexity of Nonparametric Off-Policy Evaluation on Low-Dimensional Manifolds using Deep Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiang Ji, Minshuo Chen, Mengdi Wang, Tuo Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9x3CO0ZU9LR>6.75 - [6, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9x3CO0ZU9LR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the off-policy evaluation problem of reinforcement learning using deep convolutional neural networks. We analyze the deep fitted Q-evaluation method for estimating the expected cumulative reward of a target policy, when the data are generated from an unknown behavior policy. We show that, by choosing network size appropriately, one can leverage any low-dimensional manifold structure in the Markov decision process and obtain a sample-efficient estimator without suffering from the curse of high data ambient dimensionality. Specifically, we establish a sharp error bound for fitted Q-evaluation, which depends on the intrinsic dimension of the state-action space, the smoothness of Bellman operator, and a function class-restricted $\chi^2$-divergence. It is noteworthy that the restricted $\chi^2$-divergence measures the behavior and target policies&#39; {\it mismatch in the function space}, which can be small even if the two policies are not close to each other in their tabular forms. We also develop a novel approximation result for convolutional neural networks in Q-function estimation. Numerical experiments are provided to support our theoretical analysis.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                555. <h><a href=https://openreview.net/pdf?id=JknGeelZJpHP>Sparse Distributed Memory is a Continual Learner</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Trenton Bricken, Xander Davies, Deepak Singh, Dmitry Krotov, Gabriel Kreiman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JknGeelZJpHP>6.75 - [8, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Improving Sparse Distributed Memory via additional neurobiology results in a deep learning model with strong, organic continual learning and insights into sparse models more broadly.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JknGeelZJpHP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Continual learning is a problem for artificial neural networks that their biological counterparts are adept at solving. Building on work using Sparse Distributed Memory (SDM) to connect a core neural circuit with the powerful Transformer model, we create a modified Multi-Layered Perceptron (MLP) that is a strong continual learner. We find that every component of our MLP variant translated from biology is necessary for continual learning. Our solution is also free from any memory replay or task information, and introduces novel methods to train sparse networks that may be broadly applicable.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                556. <h><a href=https://openreview.net/pdf?id=jQj-_rLVXsj>DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jQj-_rLVXsj>6.75 - [8, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose DiffuSeq: a diffusion model designed for sequence-to-sequence text generation tasks
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jQj-_rLVXsj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing DiffuSeq: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find DiffuSeq achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of DiffuSeq is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between DiffuSeq and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at https://github.com/Shark-NLP/DiffuSeq
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                557. <h><a href=https://openreview.net/pdf?id=1PL1NIMMrw>Self-Consistency Improves Chain of Thought Reasoning in Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1PL1NIMMrw>6.75 - [10, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new decoding strategy, self-consistency, that greatly improves chain-of-thought prompting
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1PL1NIMMrw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Chain-of-thought prompting combined with pretrained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out all possible reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer.  Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                558. <h><a href=https://openreview.net/pdf?id=qxRscesArBZ>Robust Graph Dictionary Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weijie Liu, Jiahao Xie, Chao Zhang, Makoto Yamada, Nenggan Zheng, Hui Qian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qxRscesArBZ>6.75 - [8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a robust graph dictionary learning method based on a novel robust variant of GWD.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qxRscesArBZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Traditional Dictionary Learning (DL) aims to approximate data vectors as sparse linear combinations of basis elements (atoms) and is widely used in machine learning, computer vision, and signal processing. To extend DL to graphs, Vincent-Cuaz et al. 2021 propose a method, called GDL, which describes the topology of each graph with a pairwise relation matrix (PRM) and compares PRMs via the Gromov-Wasserstein Discrepancy (GWD). However, the lack of robustness often excludes GDL from a variety of real-world applications since GWD is sensitive to the structural noise in graphs. This paper proposes an improved graph dictionary learning algorithm based on a robust Gromov-Wasserstein discrepancy (RGWD) which has theoretically sound properties and an efficient numerical scheme. Based on such a discrepancy, our dictionary learning algorithm can learn atoms from noisy graph data. Experimental results demonstrate that our algorithm achieves good performance on both simulated and real-world datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                559. <h><a href=https://openreview.net/pdf?id=Tl8OmiibP99>Improving Differentiable Neural Architecture Search by Encouraging Transferability</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Parth Sheth, Pengtao Xie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Tl8OmiibP99>6.75 - [6, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Tl8OmiibP99.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Differentiable neural architecture search  methods are increasingly  popular due to their computational efficiency. However, these methods have unsatisfactory generalizability and stability. Their searched    architectures  are often  degenerate with a dominant number of skip connections and perform unsatisfactorily on test data. Existing methods for solving this problem   have  a variety of limitations, such as cannot prevent the happening of architecture degeneration, being excessively restrictive in setting the number of skip connections,  etc. To address these limitations, we propose a new approach for improving the generalizability and stability of differentiable NAS, by  developing a transferability-encouraging tri-level optimization  framework  which improves the  architecture  of a main model  by encouraging  good transferability to an auxiliary model. Our framework  involves three stages performed end-to-end: 1) train network weights of  a main model; 2) transfer knowledge from the  main  model  to an auxiliary model; 3) optimize the architecture of the main model by maximizing its  transferability to the  auxiliary model. We propose a new knowledge transfer approach based on matching quadruple relative similarities.  Experiments on several datasets demonstrate the effectiveness of our method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                560. <h><a href=https://openreview.net/pdf?id=PmP_sf3JkrH>Real-Time Image Demoir$\acute{e}$ing on Mobile Devices</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuxin Zhang, Mingbao Lin, Xunchao Li, Han Liu, Guozhi Wang, Fei Chao, Ren Shuai, Yafei Wen, Xiaoxin Chen, Rongrong Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PmP_sf3JkrH>6.75 - [8, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents a dynamic demoireing acceleration method towards a real-time image demoireing on mobile devices.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PmP_sf3JkrH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Moir$\acute{e}$ patterns appear frequently when taking photos of digital screens, drastically degrading the image quality. Despite the advance of CNNs in image demoir$\acute{e}$ing, existing networks are with heavy design, causing massive computation burden for mobile devices. In this paper, we launch the first study on accelerating demoir$\acute{e}$ing networks and propose a dynamic demoir$\acute{e}$ing acceleration method (DDA) towards a real-time deployment on mobile devices. Our stimulus stems from a simple-yet-universal fact that moir${\&#39;e}$ patterns often unbalancedly distribute across an image. Consequently, excessive computation is wasted upon non-moir$\acute{e}$ areas. Therefore, we reallocate computation costs in proportion to the complexity of image patches. In order to achieve this aim, we measure the complexity of an image patch by a novel moir$\acute{e}$ prior that considers both colorfulness and frequency information of moir$\acute{e}$ patterns. Then, we restore higher-complex image patches using larger networks and the lower-complex ones are assigned with smaller networks to relieve the computation burden. At last, we train all networks in a parameter-shared supernet paradigm to avoid additional parameter burden. Extensive experiments on several benchmarks demonstrate the efficacy of our DDA. In addition, the acceleration evaluated on the VIVO X80 Pro smartphone equipped with the chip of Snapdragon 8 Gen 1 also shows that our method can drastically reduce the inference time, leading to a real-time image demoir$\acute{e}$ing on mobile devices. Source codes and models are released at https://github.com/zyxxmu/DDA.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                561. <h><a href=https://openreview.net/pdf?id=UVAmFAtC5ye>TranSpeech: Speech-to-Speech Translation With Bilateral Perturbation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rongjie Huang, Jinglin Liu, Huadai Liu, Yi Ren, Lichao Zhang, Jinzheng He, Zhou Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UVAmFAtC5ye>6.75 - [8, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose TranSpeech, a speech-to-speech translation model with bilateral perturbation to address multimodality and parallel decoding to reduce inference latency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UVAmFAtC5ye.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Direct speech-to-speech translation (S2ST) with discrete units leverages recent progress in speech representation learning. Specifically, a sequence of discrete representations derived in a self-supervised manner are predicted from the model and passed to a vocoder for speech reconstruction, while still facing the following challenges: 1) Acoustic multimodality: the discrete units derived from speech with same content could be indeterministic due to the acoustic property (e.g., rhythm, pitch, and energy), which causes deterioration of translation accuracy; 2) high latency: current S2ST systems utilize autoregressive models which predict each unit conditioned on the sequence previously generated, failing to take full advantage of parallelism. In this work, we propose TranSpeech, a speech-to-speech translation model with bilateral perturbation. To alleviate the acoustic multimodal problem, we propose bilateral perturbation (BiP), which consists of the style normalization and information enhancement stages, to learn only the linguistic information from speech samples and generate more deterministic representations. With reduced multimodality, we step forward and become the first to establish a non-autoregressive S2ST technique, which repeatedly masks and predicts unit choices and produces high-accuracy results in just a few cycles. Experimental results on three language pairs demonstrate that BiP yields an improvement of 2.9 BLEU on average compared with a baseline textless S2ST model. Moreover, our parallel decoding shows a significant reduction of inference latency, enabling speedup up to 21.4x than autoregressive technique. Audio samples are available at https://TranSpeech.github.io
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                562. <h><a href=https://openreview.net/pdf?id=NO0ThzteQdI>NERDS: A General Framework to Train Camera Denoisers from Raw-RGB Noisy Image Pairs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Heewon Kim, Kyoung Mu Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NO0ThzteQdI>6.75 - [8, 8, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NO0ThzteQdI.jpg loading="lazy">
                </div>
                <div class="abstract">
                      We aim to train accurate denoising networks for smartphone/digital cameras from single noisy images. Downscaling is commonly used as a practical denoiser for low-resolution images. Based on this processing, we found that the pixel variance of the natural images is more robust to downscaling than the pixel variance of the camera noises. Intuitively, downscaling easily removes high-frequency noises than natural textures. To utilize this property, we can adopt noisy/clean image synthesis at low-resolution to train camera denoisers. On this basis, we propose a new solution pipeline -- NERDS that estimates camera noises and synthesizes noisy-clean image pairs from only noisy images.  In particular, it first models the noise in raw-sensor images as a Poisson-Gaussian distribution, then estimates the noise parameters using the difference of pixel variances by downscaling. We formulate the noise estimation as a gradient-descent-based optimization problem through a reparametrization trick. We further introduce a new Image Signal Processor (ISP) estimation method that enables denoiser training in a human-readable RGB space by transforming the synthetic raw images to the style of a given RGB noisy image. The noise and ISP estimations utilize rich augmentation to synthesize image pairs for denoiser training. Experiments show that our NERDS can accurately train CNN-based denoisers (e.g., DnCNN, ResNet-style network) outperforming previous noise-synthesis-based and self-supervision-based denoisers in real datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                563. <h><a href=https://openreview.net/pdf?id=kPLzOfPfA2l>Warping the Space: Weight Space Rotation for Class-Incremental Few-Shot Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Do-Yeon Kim, Dong-Jun Han, Jun Seo, Jaekyun Moon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kPLzOfPfA2l>6.75 - [8, 6, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper introduced a concept of weight space rotation which makes changes to parameter space itself for solving incremental few-shot learning problem.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kPLzOfPfA2l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Class-incremental few-shot learning, where new sets of classes are provided sequentially with only a few training samples, presents a great challenge due to catastrophic forgetting of old knowledge and overfitting caused by lack of data. During finetuning on new classes, the performance on previous classes deteriorates quickly even when only a small fraction of parameters are updated, since the previous knowledge is broadly associated with most of the model parameters in the original parameter space. In this paper, we introduce WaRP, the \textit{weight space rotation process}, which transforms the original parameter space into a new space so that we can push most of the previous knowledge compactly into only a few important parameters. By properly identifying and freezing these key parameters in the new weight space, we can finetune the remaining parameters without affecting the knowledge of previous classes. As a result, WaRP provides an additional room for the model to effectively learn new classes in future incremental sessions. Experimental results confirm the effectiveness of our solution and show the improved performance over the state-of-the-art methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                564. <h><a href=https://openreview.net/pdf?id=O5rKg7IRQIO>Guarded Policy Optimization with Imperfect Online Demonstrations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhenghai Xue, Zhenghao Peng, Quanyi Li, Zhihan Liu, Bolei Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=O5rKg7IRQIO>6.75 - [8, 5, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Introducing a new policy optimization method exploiting imperfect online demonstrations from a guardian policy.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/O5rKg7IRQIO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The Teacher-Student Framework (TSF) is a reinforcement learning setting where a teacher agent guards the training of a student agent by intervening and providing online demonstrations. Assuming optimal, the teacher policy has the perfect timing and capability to intervene in the learning process of the student agent, providing safety guarantee and exploration guidance. Nevertheless, in many real-world settings it is expensive or even impossible to obtain a well-performing teacher policy. In this work, we relax the assumption of a well-performing teacher and develop a new method that can incorporate arbitrary teacher policies with modest or inferior performance. We instantiate an Off-Policy Reinforcement Learning algorithm, termed Teacher-Student Shared Control (TS2C), which incorporates teacher intervention based on trajectory-based value estimation. Theoretical analysis validates that the proposed TS2C algorithm attains efficient exploration and substantial safety guarantee without being affected by the teacher&#39;s own performance. Experiments on various continuous control tasks show that our method can exploit teacher policies at different performance levels while maintaining a low training cost. Moreover, the student policy surpasses the imperfect teacher policy in terms of higher accumulated reward in held-out testing environments. Code is available at https://metadriverse.github.io/TS2C.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                565. <h><a href=https://openreview.net/pdf?id=mnVf1W6ipGm>Unveiling the sampling density in non-uniform geometric graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Raffaele Paolino, Aleksandar Bojchevski, Stephan Günnemann, Gitta Kutyniok, Ron Levie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mnVf1W6ipGm>6.75 - [6, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce geometric graphs with hubs, an effective model for real-world graphs, and retrieve the sampling density by which those graphs are sampled from continuous latent spaces, to achieve various tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mnVf1W6ipGm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A powerful framework for studying graphs is to consider them as geometric graphs: nodes are randomly sampled from an underlying metric space, and any pair of nodes is connected if their distance is less than a specified neighborhood radius. Currently, the literature mostly focuses on uniform sampling and constant neighborhood radius. However, real-world graphs are likely to be better represented by a model in which the sampling density and the neighborhood radius can both vary over the latent space. For instance, in a social network communities can be modeled as densely sampled areas, and hubs as nodes with larger neighborhood radius. In this work, we first perform a rigorous mathematical analysis of this (more general) class of models, including derivations of the resulting graph shift operators. The key insight is that graph shift operators should be corrected in order to avoid potential distortions introduced by the non-uniform sampling. Then, we develop methods to estimate the unknown sampling density in a self-supervised fashion.  Finally, we present exemplary applications in which the learnt density is used to 1) correct the graph shift operator and improve performance on a variety of tasks, 2) improve pooling, and 3) extract knowledge from networks. Our experimental findings support our theory and provide strong evidence for our model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                566. <h><a href=https://openreview.net/pdf?id=6BHlZgyPOZY>Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Felix Chalumeau, Raphael Boige, Bryan Lim, Valentin Macé, Maxime Allard, Arthur Flajolet, Antoine Cully, Thomas PIERROT</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6BHlZgyPOZY>6.75 - [8, 8, 6, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6BHlZgyPOZY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep Reinforcement Learning (RL) has emerged as a powerful paradigm for training neural policies to solve complex control tasks. However, these policies tend to be overfit to the exact specifications of the task and environment they were trained on, and thus do not perform well when conditions deviate slightly or when composed hierarchically to solve even more complex tasks. Recent work has shown that training a mixture of policies, as opposed to a single one, that are driven to explore different regions of the state-action space can address this shortcoming by generating a diverse set of behaviors, referred to as skills, that can be collectively used to great effect in adaptation tasks or for hierarchical planning. This is typically realized by including a diversity term - often derived from information theory - in the objective function optimized by RL. However these approaches often require careful hyperparameter tuning to be effective. In this work, we demonstrate that less widely-used neuroevolution methods, specifically Quality Diversity (QD), are a competitive alternative to information-theory-augmented RL for skill discovery. Through an extensive empirical evaluation comparing eight state-of-the-art algorithms (four flagship algorithms from each line of work) on the basis of (i) metrics directly evaluating the skills&#39; diversity, (ii) the skills&#39; performance on adaptation tasks, and (iii) the skills&#39; performance when used as primitives for hierarchical planning; QD methods are found to provide equal, and sometimes improved, performance whilst being less sensitive to hyperparameters and more scalable. As no single method is found to provide near-optimal performance across all environments, there is a rich scope for further research which we support by proposing future directions and providing optimized open-source implementations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                567. <h><a href=https://openreview.net/pdf?id=sbS10BCtc7>Gromov-Wasserstein Autoencoders</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nao Nakagawa, Ren Togo, Takahiro Ogawa, Miki Haseyama</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sbS10BCtc7>6.75 - [8, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> GWAEs, our novel generative models, learn representations based on meta-priors by directly fitting their latent space into the data space.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sbS10BCtc7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Variational Autoencoder (VAE)-based generative models offer flexible representation learning by incorporating meta-priors, general premises considered beneficial for downstream tasks. However, the incorporated meta-priors often involve ad-hoc model deviations from the original likelihood architecture, causing undesirable changes in their training. In this paper, we propose a novel representation learning method, Gromov-Wasserstein Autoencoders (GWAE), which directly matches the latent and data distributions using the variational autoencoding scheme. Instead of likelihood-based objectives, GWAE models minimize the Gromov-Wasserstein (GW) metric between the trainable prior and given data distributions. The GW metric measures the distance structure-oriented discrepancy between distributions even with different dimensionalities, which provides a direct measure between the latent and data spaces. By restricting the prior family, we can introduce meta-priors into the latent space without changing their objective. The empirical comparisons with VAE-based models show that GWAE models work in two prominent meta-priors, disentanglement and clustering, with their GW objective unchanged.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                568. <h><a href=https://openreview.net/pdf?id=1C_kSW1-k0>STREET: A MULTI-TASK STRUCTURED REASONING AND EXPLANATION BENCHMARK</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, zhiheng huang, William Yang Wang, George Karypis, Bing Xiang, Dan Roth</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1C_kSW1-k0>6.75 - [6, 8, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1C_kSW1-k0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                569. <h><a href=https://openreview.net/pdf?id=zoz7Ze4STUL>Energy-based Out-of-Distribution Detection for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qitian Wu, Yiting Chen, Chenxiao Yang, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zoz7Ze4STUL>6.75 - [6, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an energy-based model as a provably effective OOD discriminator from a GNN classifier trained in semi-supervised learning on graphs
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zoz7Ze4STUL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Representation learning on semi-structured data, e.g., graphs, has become a central problem in deep learning community as relational structures are pervasive in real situations and induce data inter-dependence that hinders trivial adaptation of existing approaches in other domains where the inputs are assumed to be i.i.d. sampled. However, current models in this regime mostly focus on improving testing performance of in-distribution data and largely ignores the potential risk w.r.t. out-of-distribution (OOD) testing samples that may cause negative outcome if the model is overconfident in prediction on them. In this paper, we identify a provably effective OOD discriminator based on an energy function directly extracted from a graph neural network trained with standard supervised classification loss. This paves a way for a simple and efficient OOD detection model for GNN-based semi-supervised learning on graphs, which we call GNN-Safe. It also has nice theoretical properties that guarantee an overall distinguishable margin between the detection scores for in-distribution and OOD samples, which, more critically, can be further strengthened by a non-learning-based structured propagation scheme. Extensive experiments over five real-world datasets validate the practical efficacy of the proposed model for detecting various OOD instances that are inter-connected in a graph with up to 17.0% improvement on average AUROC over competitive peer models and without sacrificing in-distribution testing accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                570. <h><a href=https://openreview.net/pdf?id=RecZ9nB9Q4>Sparse Mixture-of-Experts are Domain Generalizable Learners</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, Ziwei Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RecZ9nB9Q4>6.75 - [6, 8, 5, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We theoretically investigate the impact of backbone architecture on DG. We propose a novel SOTA model Generalizable Mixture-of-Experts (GMoE) for DG.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RecZ9nB9Q4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human visual perception can easily generalize to out-of-distributed visual data, which is far beyond the capability of modern machine learning models. Domain generalization (DG) aims to close this gap, with existing DG methods mainly focusing on the loss function design. In this paper, we propose to explore an orthogonal direction, i.e., the design of the backbone architecture. It is motivated by an empirical finding that transformer-based models trained with empirical risk minimization (ERM) outperform CNN-based models employing state-of-the-art (SOTA) DG algorithms on multiple DG datasets. We develop a formal framework to characterize a network&#39;s robustness to distribution shifts by studying its architecture&#39;s alignment with the correlations in the dataset. This analysis guides us to propose a novel DG model built upon vision transformers, namely \emph{Generalizable Mixture-of-Experts (GMoE)}. Extensive experiments on DomainBed demonstrate that GMoE trained with ERM outperforms SOTA DG baselines by a large margin. Moreover, GMoE is complementary to existing DG methods and its performance is substantially improved when trained with DG algorithms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                571. <h><a href=https://openreview.net/pdf?id=fWWFv--P0xP>On the Importance and Applicability of Pre-Training for Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hong-You Chen, Cheng-Hao Tu, Ziwei Li, Han Wei Shen, Wei-Lun Chao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fWWFv--P0xP>6.75 - [8, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fWWFv--P0xP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pre-training is prevalent in nowadays deep learning to improve the learned model&#39;s performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients&#39; data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients&#39; data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We conclude our paper with an attempt to understand the effect of pre-training on FL. We found that pre-training enables the learned global models under different clients&#39; data conditions to converge to the same loss basin, and makes global aggregation in FL more stable. Nevertheless, pre-training seems to not alleviate local model drifting, a fundamental problem in FL under non-IID data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                572. <h><a href=https://openreview.net/pdf?id=vCJ9-Ri-6xU>Momentum Stiefel Optimizer, with Applications to Suitably-Orthogonal Attention, and Optimal Transport</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lingkai Kong, Yuqing Wang, Molei Tao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vCJ9-Ri-6xU>6.75 - [10, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes accurate and efficient optimizers on Stiefel manifold based on a new variational principle and its careful discretization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vCJ9-Ri-6xU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The problem of optimization on Stiefel manifold, i.e., minimizing functions of (not necessarily square) matrices that satisfy orthogonality constraints, has been extensively studied. Yet, a new approach is proposed based on, for the first time, an interplay between thoughtfully designed continuous and discrete dynamics. It leads to a gradient-based optimizer with intrinsically added momentum. This method exactly preserves the manifold structure but does not require additional operation to keep momentum in the changing (co)tangent space, and thus has low computational cost and pleasant accuracy. Its generalization to adaptive learning rates is also demonstrated. Notable performances are observed in practical tasks. For instance, we found that placing orthogonal constraints on attention heads of trained-from-scratch Vision Transformer (Dosovitskiy et al., 2020) could markedly improve its performance, when our optimizer is used, and it is better that each head is made orthogonal within itself but not necessarily to other heads. This optimizer also makes the useful notion of Projection Robust Wasserstein Distance (Paty and Cuturi, 2019; Lin et al., 2020) for high-dim. optimal transport even more effective.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                573. <h><a href=https://openreview.net/pdf?id=rSUCajhLsQ>Easy Differentially Private Linear Regression</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kareem Amin, Matthew Joseph, Mónica Ribero, Sergei Vassilvitskii</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rSUCajhLsQ>6.75 - [5, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A practical algorithm for differentially private linear regression which does not require data bounds or parameter tuning but is competitive with methods that do.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rSUCajhLsQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Linear regression is a fundamental tool for statistical analysis. This has motivated the development of linear regression methods that also satisfy differential privacy and thus guarantee that the learned model reveals little about any one data point used to construct it. However, existing differentially private solutions assume that the end user can easily specify good data bounds and hyperparameters. Both present significant practical obstacles. In this paper, we study an algorithm which uses the exponential mechanism to select a model with high Tukey depth from a collection of non-private regression models. Given $n$ samples of $d$-dimensional data used to train $m$ models, we construct an efficient analogue using an approximate Tukey depth that runs in time $O(d^2n + dm\log(m))$. We find that this algorithm obtains strong empirical performance in the data-rich setting with no data bounds or hyperparameter selection required.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                574. <h><a href=https://openreview.net/pdf?id=CsKwavjr7A>Visual Recognition with Deep Nearest Centroids</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenguan Wang, Cheng Han, Tianfei Zhou, Dongfang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CsKwavjr7A>6.75 - [5, 8, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CsKwavjr7A.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We devise deep nearest centroids (DNC), a conceptually elegant yet surprisingly effective network for large-scale visual recognition, by revisiting Nearest Centroids, one of the most classic and simple classifiers. Current deep models learn the classifier in a fully parametric manner, ignoring the latent data structure and lacking simplicity and explainability. DNC instead conducts nonparametric, case-based reasoning; it utilizes sub-centroids of training samples to describe class distributions and clearly explains the classification as the proximity of test data and the class sub-centroids in the feature space. Due to the distance-based nature, the network output dimensionality is flexible, and all the learnable parameters are only for data embedding. That means all the knowledge learnt for ImageNet classification can be completely transferred for pixel recognition learning, under the ‘pre-training and fine-tuning’ paradigm. Apart from its nested simplicity and intuitive decision-making mechanism, DNC can even possess ad-hoc explainability when the sub-centroids are selected as actual training images that humans can view and inspect. Compared with parametric counterparts, DNC performs better on image classification (CIFAR-10, ImageNet) and greatly boots pixel recognition (ADE20K, Cityscapes), with improved transparency and fewer learnable parameters, using various network architectures (ResNet, Swin) and segmentation models (FCN, DeepLabV3, Swin). We feel this work brings fundamental insights into related fields. Our code is available at https://github.com/ChengHan111/DNC.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                575. <h><a href=https://openreview.net/pdf?id=raU07GpP0P>Improving Deep Regression with Ordinal Entropy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shihao Zhang, Linlin Yang, Michael Bi Mi, Xiaoxu Zheng, Angela Yao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=raU07GpP0P>6.75 - [8, 3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We observe that many regression problems are preferably formulated as classification tasks, and we provide a theoretical analysis to explain this phenomenon then we propose an ordinal entropy loss to improve the performance of regression.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/raU07GpP0P.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In computer vision, it is often observed that formulating regression problems as a classification task yields better performance. We investigate this curious phenomenon and provide a derivation to show that classification, with the cross-entropy loss, outperforms regression with a mean squared error loss in its ability to learn high-entropy feature representations. Based on the analysis, we propose an ordinal entropy loss to encourage higher-entropy feature spaces while maintaining ordinal relationships to improve the performance of regression tasks. Experiments on synthetic and real-world regression tasks demonstrate the importance and benefits of increasing entropy for regression.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                576. <h><a href=https://openreview.net/pdf?id=zWy7dqOcel>Sampling with Mollified Interaction Energy Descent</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lingxiao Li, qiang liu, Anna Korba, Mikhail Yurochkin, Justin Solomon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zWy7dqOcel>6.75 - [5, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Unconstrained and constrained sampling by minimizing a new class of mollified interaction energies.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zWy7dqOcel.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sampling from a target measure whose density is only known up to a normalization constant is a fundamental problem in computational statistics and machine learning. In this paper, we present a new optimization-based method for sampling called mollified interaction energy descent (MIED). MIED minimizes a new class of energies on probability measures called mollified interaction energies (MIEs). These energies rely on mollifier functions---smooth approximations of the Dirac delta originated from PDE theory. We show that as the mollifier approaches the Dirac delta, the MIE converges to the chi-square divergence with respect to the target measure and the gradient flow of the MIE agrees with that of the chi-square divergence. Optimizing this energy with proper discretization yields a practical first-order particle-based algorithm for sampling in both unconstrained and constrained domains. We show experimentally that for unconstrained sampling problems our algorithm performs on par with existing particle-based algorithms like SVGD, while for constrained sampling problems our method readily incorporates constrained optimization techniques to handle more flexible constraints with strong performance compared to alternatives.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                577. <h><a href=https://openreview.net/pdf?id=PLUXnnxUdr4>Graph Contrastive Learning for Skeleton-based Action Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaohu Huang, Hao Zhou, Jian Wang, Haocheng Feng, Junyu Han, Errui Ding, Jingdong Wang, Xinggang Wang, Wenyu Liu, Bin Feng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PLUXnnxUdr4>6.75 - [8, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> For GCN-based methods in skeleton-based action recognition, this work extends the graph learning from using intra-sequence local context to exploring cross-sequence global context.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PLUXnnxUdr4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the field of skeleton-based action recognition, current top-performing graph convolutional networks (GCNs) exploit intra-sequence context to construct adaptive graphs for feature aggregation. However, we argue that such context is still $\textit{local}$ since the rich cross-sequence relations have not been explicitly investigated. In this paper, we propose a graph contrastive learning framework for skeleton-based action recognition ($\textit{SkeletonGCL}$) to explore the $\textit{global}$ context across all sequences. In specific, SkeletonGCL associates graph learning across sequences by enforcing graphs to be class-discriminative, i.e., intra-class compact and inter-class dispersed, which improves the GCN capacity to distinguish various action patterns. Besides, two memory banks are designed to enrich cross-sequence context from two complementary levels, i.e., instance and semantic levels, enabling graph contrastive learning in multiple context scales. Consequently, SkeletonGCL establishes a new training paradigm, and it can be seamlessly incorporated into current GCNs. Without loss of generality, we combine SkeletonGCL with three GCNs (2S-ACGN, CTR-GCN, and InfoGCN), and achieve consistent improvements on NTU60, NTU120, and NW-UCLA benchmarks. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                578. <h><a href=https://openreview.net/pdf?id=frE4fUwz_h>Spikformer: When Spiking Neural Network Meets Transformer </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng YAN, Yonghong Tian, Li Yuan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=frE4fUwz_h>6.75 - [6, 3, 10, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/frE4fUwz_h.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax.  Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models. Code is avaiable at https://github.com/ZK-Zhou/spikformer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                579. <h><a href=https://openreview.net/pdf?id=SRIQZTh0IK>Analogy-Forming Transformers for Few-Shot 3D Parsing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nikolaos Gkanatsios, Mayank Singh, Zhaoyuan Fang, Shubham Tulsiani, Katerina Fragkiadaki</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SRIQZTh0IK>6.75 - [6, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SRIQZTh0IK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present Analogical Networks, a model that segments 3D object scenes with analogical reasoning: instead of mapping a scene to part segments directly, our model first retrieves related scenes from memory and their corresponding part structures, and then predicts analogous part structures in the input object 3D point cloud, via an end-to-end learnable modulation mechanism. By conditioning on more than one retrieved memories, compositions of structures are predicted, that mix and match parts across the retrieved memories. One-shot, few-shot or many-shot learning are treated uniformly in Analogical Networks, by conditioning on the appropriate set of memories, whether taken from a single, few or many memory exemplars, and inferring analogous parses. We show Analogical Networks are competitive with state-of-the-art 3D segmentation transformer in many-shot settings and outperform them and existing paradigms of meta-learning and few-shot learning in few-shot scenarios. Our model successfully parses instances of novel object categories simply by expanding its memory, without any weight updates.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                580. <h><a href=https://openreview.net/pdf?id=m_GDIItaI3o>Continual Pre-training of Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, Bing Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=m_GDIItaI3o>6.75 - [8, 8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a continual post-training method based on soft-masking to learn a sequence of unlabeled domain corpora to adapt a language model to improve the end-task performances in these domains.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/m_GDIItaI3o.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                581. <h><a href=https://openreview.net/pdf?id=tkwP32nsEq>Variance-Aware Sparse Linear Bandits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yan Dai, Ruosong Wang, Simon Shaolei Du</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tkwP32nsEq>6.75 - [8, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tkwP32nsEq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    It is well-known that for sparse linear bandits, when ignoring the dependency on sparsity which is much smaller than the ambient dimension, the worst-case minimax regret is $\widetilde{\Theta}\left(\sqrt{dT}\right)$ where $d$ is the ambient dimension and $T$ is the number of rounds. On the other hand, in the benign setting where there is no noise and the action set is the unit sphere, one can use divide-and-conquer to achieve $\widetilde{\mathcal O}(1)$ regret, which is (nearly) independent of $d$ and $T$. In this paper, we present the first variance-aware regret guarantee for sparse linear bandits:  $\widetilde{\mathcal O}\left(\sqrt{d\sum_{t=1}^T \sigma_t^2} + 1\right)$, where $\sigma_t^2$ is the variance of the noise at the $t$-th round. This bound naturally interpolates the regret bounds for the worst-case constant-variance regime (i.e., $\sigma_t \equiv \Omega(1)$) and the benign deterministic regimes (i.e., $\sigma_t \equiv 0$). To achieve this variance-aware regret guarantee, we develop a general framework that converts any variance-aware linear bandit algorithm to a variance-aware algorithm for sparse linear bandits in a &#34;black-box&#34; manner. Specifically, we take two recent algorithms as black boxes to illustrate that the claimed bounds indeed hold, where the first algorithm can handle unknown-variance cases and the second one is more efficient.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                582. <h><a href=https://openreview.net/pdf?id=3Y5Uhf5KgGK>No Reason for No Supervision: Improved Generalization in Supervised Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mert Bülent Sarıyıldız, Yannis Kalantidis, Karteek Alahari, Diane Larlus</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3Y5Uhf5KgGK>6.75 - [8, 6, 5, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3Y5Uhf5KgGK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the problem of training a deep neural network on a given classification task, e.g., ImageNet-1K (IN1K), so that it excels at both the training task as well as at other (future) transfer tasks. These two seemingly contradictory properties impose a trade-off between improving the model’s generalization and maintaining its performance on the original task. Models trained with self-supervised learning tend to generalize better than their supervised counterparts for transfer learning; yet, they still lag behind supervised models on IN1K. In this paper, we propose a supervised learning setup that leverages the best of both worlds. We extensively analyze supervised training using multi-scale crops for data augmentation and an expendable projector head, and reveal that the design of the projector allows us to control the trade-off between performance on the training task and transferability. We further replace the last layer of class weights with class prototypes computed on the fly using a memory bank and derive two models: t-ReX that achieves a new state of the art for transfer learning and outperforms top methods such as DINO and PAWS on IN1K, and t-ReX* that matches the highly optimized RSB-A1 model on IN1K while performing better on transfer tasks.
Code and pretrained models: https://europe.naverlabs.com/t-rex
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                583. <h><a href=https://openreview.net/pdf?id=P3PJokAqGW>Learning with Stochastic Orders</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Carles Domingo-Enrich, Yair Schiff, Youssef Mroueh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=P3PJokAqGW>6.75 - [8, 5, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose and study discrepancies and distances between probability measures that arise from the convex or Choquet order, which capture dominance constraints and are useful in applications like image generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/P3PJokAqGW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning high-dimensional distributions is often done with explicit likelihood modeling or implicit modeling via minimizing integral probability metrics (IPMs). In this paper, we expand this learning paradigm to stochastic orders, namely, the convex or Choquet order between probability measures. Towards this end, exploiting the relation between convex orders and optimal transport, we introduce the Choquet-Toland distance between probability measures, that can be used as a drop-in replacement for IPMs. We also introduce the Variational Dominance Criterion (VDC) to learn probability measures with dominance constraints, that encode the desired stochastic order between the learned measure and a known baseline. We analyze both quantities and show that they suffer from the curse of dimensionality and propose surrogates via input convex maxout networks (ICMNs), that enjoy parametric rates. We provide a min-max framework for learning with stochastic orders and validate it experimentally on synthetic and high-dimensional image generation, with promising results. Finally, our ICMNs class of convex functions and its derived Rademacher Complexity are of independent interest beyond their application in convex orders. Code to reproduce experimental results is available at https://github.com/yair-schiff/stochastic-orders-ICMN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                584. <h><a href=https://openreview.net/pdf?id=6ve2CkeQe5S>MEDFAIR: Benchmarking Fairness for Medical Imaging</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yongshuo Zong, Yongxin Yang, Timothy Hospedales</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6ve2CkeQe5S>6.75 - [8, 8, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a fairness benchmark for medical imaging and find that the state-of-the-art bias mitigation algorithm does not significantly outperform ERM.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6ve2CkeQe5S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A multitude of work has shown that machine learning-based medical diagnosis systems can be biased against certain subgroups of people. This has motivated a growing number of bias mitigation algorithms that aim to address fairness issues in machine learning. However, it is difficult to compare their effectiveness in medical imaging for two reasons. First, there is little consensus on the criteria to assess fairness. Second, existing bias mitigation algorithms are developed under different settings, e.g., datasets, model selection strategies, backbones, and fairness metrics, making a direct comparison and evaluation based on existing results impossible. In this work, we introduce MEDFAIR, a framework to benchmark the fairness of machine learning models for medical imaging. MEDFAIR covers eleven algorithms from various categories, ten datasets from different imaging modalities, and three model selection criteria. Through extensive experiments, we find that the under-studied issue of model selection criterion can have a significant impact on fairness outcomes; while in contrast, state-of-the-art bias mitigation algorithms do not significantly improve fairness outcomes over empirical risk minimization (ERM) in both in-distribution and out-of-distribution settings. We evaluate fairness from various perspectives and make recommendations for different medical application scenarios that require different ethical principles. Our framework provides a reproducible and easy-to-use entry point for the development and evaluation of future bias mitigation algorithms in deep learning. Code is available at https://github.com/ys-zong/MEDFAIR.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                585. <h><a href=https://openreview.net/pdf?id=CZmHHj9MgkP>Guiding Energy-based Models via Contrastive Latent Variables</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hankook Lee, Jongheon Jeong, Sejun Park, Jinwoo Shin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CZmHHj9MgkP>6.75 - [8, 5, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple yet effective framework for improving energy-based models (EBMs) via contrastive representation learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CZmHHj9MgkP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    An energy-based model (EBM) is a popular generative framework that offers both explicit density and architectural flexibility, but training them is difficult since it is often unstable and time-consuming. In recent years, various training techniques have been developed, e.g., better divergence measures or stabilization in MCMC sampling, but there often exists a large gap between EBMs and other generative frameworks like GANs in terms of generation quality. In this paper, we propose a novel and effective framework for improving EBMs via contrastive representation learning (CRL). To be specific, we consider representations learned by contrastive methods as the true underlying latent variable. This contrastive latent variable could guide EBMs to understand the data structure better, so it can improve and accelerate EBM training significantly. To enable the joint training of EBM and CRL, we also design a new class of latent-variable EBMs for learning the joint density of data and the contrastive latent variable. Our experimental results demonstrate that our scheme achieves lower FID scores, compared to prior-art EBM methods (e.g., additionally using variational autoencoders or diffusion techniques), even with significantly faster and more memory-efficient training. We also show conditional and compositional generation abilities of our latent-variable EBMs as their additional benefits, even without explicit conditional training. The code is available at https://github.com/hankook/CLEL.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                586. <h><a href=https://openreview.net/pdf?id=zgVDqw9ZUES>Adaptive Optimization in the $\infty$-Width Limit</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Etai Littwin, Greg Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zgVDqw9ZUES>6.75 - [8, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We derive the infinite width limits of neural networks trained with adaptive optimizers
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zgVDqw9ZUES.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent works have developed detailed understanding of large neural networks&#39; behaviors via their infinite-width limits, e.g., the neural tangent kernel (NTK) and the feature learning ($\mu$) limits. These theories were developed for stochastic gradient descent. Yet, in practice, all large NN are trained using Adam or other adaptive gradient optimizers (AGO), which are not covered by such previous works. Here, we close this gap via the Tensor Programs framework. Specifically, for deep MLPs, we derive the NTK and $\mu$ parametrizations as well as their infinite-width limits. We find 1) The NTK limit of AGO, in contrast to that of SGD, now depends nonlinearly on the loss derivative but nevertheless still fails to learn features; 2) this is fixed by the $\mu$ limit of AGO (as in the case of SGD). To obtain these results, we extend the Tensor Programs language with a new instruction that allows one to express the gradient processing done by AGOs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                587. <h><a href=https://openreview.net/pdf?id=3KWnuT-R1bh>Conditional Positional Encodings for Vision Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Chunhua Shen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3KWnuT-R1bh>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A conditional positional encoding scheme for vision transformers
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3KWnuT-R1bh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a conditional positional encoding (CPE) scheme for vision Transformers. Unlike previous fixed or learnable positional encodings that are predefined and independent of input tokens, CPE is dynamically generated and conditioned on the local neighborhood of the input tokens. As a result, CPE can easily generalize to the input sequences that are longer than what the model has ever seen during the training. Besides, CPE can keep the desired translation equivalence in vision tasks, resulting in improved performance. We implement CPE with a simple Position Encoding Generator (PEG) to get seamlessly incorporated into the current Transformer framework. Built on PEG, we present Conditional Position encoding Vision Transformer (CPVT). We demonstrate that CPVT has visually similar attention maps compared to those with learned positional encodings and delivers outperforming results.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                588. <h><a href=https://openreview.net/pdf?id=klK17OQ3KB>How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, Christopher Re</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=klK17OQ3KB>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a new theoretical interpretation of S4 and generalize it to other basis functions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/klK17OQ3KB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4&#39;s ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular *time-varying* dynamical system, and the use of this matrix as a *time-invariant* SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4&#39;s performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                589. <h><a href=https://openreview.net/pdf?id=8tYRqb05pVn>Linearly Mapping from Image to Text Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8tYRqb05pVn>6.75 - [6, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Language models (LMs) can &#39;understand&#39; images through a single tuned linear layer between a frozen image encoder and the LM input, showcasing the similarities in their conceptual representation spaces.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8tYRqb05pVn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The extent to which text-only language models (LMs)  learn to represent the physical, non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to ``understand&#39;&#39; visual inputs when the models&#39; parameters are updated on image captioning tasks. We test a stronger hypothesis: that the conceptual representations learned by text-only models are functionally equivalent (up to a linear transformation) to those learned by models trained on vision tasks. Specifically, we show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs.\ elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                590. <h><a href=https://openreview.net/pdf?id=S8-A2FXnIh>Learning to Compose Soft Prompts for Compositional Zero-Shot Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nihal V. Nayak, Peilin Yu, Stephen Bach</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=S8-A2FXnIh>6.75 - [5, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/S8-A2FXnIh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce compositional soft prompting (CSP), a parameter-efficient learning technique to improve the zero-shot compositionality of large-scale pretrained vision-language models (VLMs) like CLIP. We develop CSP for compositional zero-shot learning, the task of predicting unseen attribute-object compositions (e.g., old cat and young tiger). VLMs have a flexible text encoder that can represent arbitrary classes as natural language prompts but they often underperform task-specific architectures on the compositional zero-shot benchmark datasets. CSP treats the attributes and objects that define classes as learnable tokens of vocabulary. During training, the vocabulary is tuned to recognize classes that compose tokens in multiple ways (e.g., old cat and white cat). At test time, we recompose the learned attribute-object vocabulary in new combinations to recognize novel classes. We show that CSP outperforms the CLIP on benchmark datasets by an average of 10.9 percentage points on AUC. CSP also outperforms CoOp, a soft prompting method that fine-tunes the prefix context tokens, by an average of 5.8 percentage points on AUC. We perform additional experiments to show that CSP improves generalization to higher-order attribute-attribute-object compositions (e.g., old white cat) and combinations of pretrained attributes and fine-tuned objects. The code is available at https://github.com/BatsResearch/csp.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                591. <h><a href=https://openreview.net/pdf?id=cEygmQNOeI>Language Models are Realistic Tabular Data Generators</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, Gjergji Kasneci</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cEygmQNOeI>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The GReaT approach utilizes the capabilities of large language models to synthesize realistic tabular data. A challenging set of experiments validates the proposed method’s efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cEygmQNOeI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Tabular data is among the oldest and most ubiquitous forms of data. However, the generation of synthetic samples with the original data’s characteristics remains a significant challenge for tabular data. While many generative models from the computer vision domain, such as variational autoencoders or generative adversarial networks, have been adapted for tabular data generation, less research has been directed towards recent transformer-based large language models (LLMs), which are also generative in nature. To this end, we propose GReaT (Generation of Realistic Tabular data), which exploits an auto-regressive generative LLM to sample synthetic and yet highly realistic tabular data. Furthermore, GReaT can model tabular data distributions by conditioning on any subset of features; the remaining features are sampled without additional overhead. We demonstrate the effectiveness of the proposed approach in a series of experiments that quantify the validity and quality of the produced data samples from multiple angles. We find that GReaT maintains state-of-the-art performance across numerous real-world and synthetic data sets with heterogeneous feature types coming in various sizes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                592. <h><a href=https://openreview.net/pdf?id=HDxgaKk956l>Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huangjie Zheng, Pengcheng He, Weizhu Chen, Mingyuan Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HDxgaKk956l>6.75 - [6, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose truncated diffusion probabilistic models, which models an implicit prior to truncate the diffusion chain and requires significantly fewer reverse steps to generate high-quality samples.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HDxgaKk956l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Employing a forward diffusion chain to gradually map the data to a  noise distribution, diffusion-based generative models learn how to generate the data by inferring a reverse diffusion chain. However, this approach is slow and costly because it needs many forward and reverse steps. We propose a faster and cheaper approach that adds noise not until the data become pure random noise, but until they reach a hidden noisy data distribution that we can confidently learn. Then, we use fewer reverse steps to generate data by starting from this hidden distribution that is made similar to the noisy data. We reveal that the proposed model can be cast as an adversarial auto-encoder empowered by both the diffusion process and a learnable implicit prior. Experimental results show even with a significantly smaller number of reverse diffusion steps, the proposed truncated diffusion probabilistic models can provide consistent improvements over the non-truncated ones in terms of performance in both unconditional and text-guided image generations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                593. <h><a href=https://openreview.net/pdf?id=gmwDKo-4cY>Composing Ensembles of Pre-trained Models via Iterative Consensus</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Igor Mordatch</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gmwDKo-4cY>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a unified framework for composing pre-trained models for a variety of zero-shot multimodal tasks through iterative consensus.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gmwDKo-4cY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as &#34;generators&#34; or &#34;scorers&#34; and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                594. <h><a href=https://openreview.net/pdf?id=GRZtigJljLY>Scalable Batch-Mode Deep Bayesian Active Learning via Equivalence Class Annealing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Renyu Zhang, Aly A Khan, Robert L. Grossman, Yuxin Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GRZtigJljLY>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new scalable batch-mode active learning algorithm
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GRZtigJljLY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Active learning has demonstrated data efficiency in many fields. Existing active learning algorithms, especially in the context of batch-mode deep Bayesian active models, rely heavily on the quality of uncertainty estimations of the model, and are often challenging to scale to large batches. In this paper, we propose Batch-BALanCe, a scalable batch-mode active learning algorithm, which combines insights from decision-theoretic active learning, combinatorial information measure, and diversity sampling. At its core, Batch-BALanCe relies on a novel decision-theoretic acquisition function that facilitates differentiation among different equivalence classes. Intuitively, each equivalence class consists of hypotheses (e.g., posterior samples of deep neural networks) with similar predictions, and Batch-BALanCe adaptively adjusts the size of the equivalence classes as learning progresses. To scale up the computation of queries to large batches, we further propose an efficient batch-mode acquisition procedure, which aims to maximize a novel combinatorial information measure defined through the acquisition function. We show that our algorithm can effectively handle realistic multi-class classification tasks, and achieves compelling performance on several benchmark datasets for active learning under both low- and large-batch regimes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                595. <h><a href=https://openreview.net/pdf?id=4oXTQ6m_ws8>The Role of ImageNet Classes in Fréchet Inception Distance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tuomas Kynkäänniemi, Tero Karras, Miika Aittala, Timo Aila, Jaakko Lehtinen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4oXTQ6m_ws8>6.75 - [8, 5, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We elucidate why using ImageNet pre-trained Inception features in FID can cause discrepancies with human judgement.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4oXTQ6m_ws8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Fréchet Inception Distance (FID) is the primary metric for ranking models in data-driven generative modeling. While remarkably successful, the metric is known to sometimes disagree with human judgement. We investigate a root cause of these discrepancies, and visualize what FID &#34;looks at&#34; in generated images. We show that the feature space that FID is (typically) computed in is so close to the ImageNet classifications that aligning the histograms of Top-$N$ classifications between sets of generated and real images can reduce FID substantially — without actually improving the quality of results. Thus, we conclude that FID is prone to intentional or accidental distortions. As a practical example of an accidental distortion, we discuss a case where an ImageNet pre-trained FastGAN achieves a FID comparable to StyleGAN2, while being worse in terms of human evaluation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                596. <h><a href=https://openreview.net/pdf?id=FCnohuR6AnM>Dataless Knowledge Fusion by Merging Weights of Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, Pengxiang Cheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FCnohuR6AnM>6.75 - [5, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the problem of merging individual models built on different training data sets and propose a novel merging algorithm.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FCnohuR6AnM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a data-less knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                597. <h><a href=https://openreview.net/pdf?id=plKu2GByCNW>Vision Transformer Adapter for Dense Predictions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, Yu Qiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=plKu2GByCNW>6.75 - [8, 8, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/plKu2GByCNW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work investigates a simple yet powerful dense prediction task adapter for Vision Transformer (ViT). Unlike recently advanced variants that incorporate vision-specific inductive biases into their architectures, the plain ViT suffers inferior performance on dense predictions due to weak prior assumptions. To address this issue, we propose the ViT-Adapter, which allows plain ViT to achieve comparable performance to vision-specific transformers. Specifically, the backbone in our framework is a plain ViT that can learn powerful representations from large-scale multi-modal data. When transferring to downstream tasks, a pre-training-free adapter is used to introduce the image-related inductive biases into the model, making it suitable for these tasks. We verify ViT-Adapter on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation. Notably, without using extra detection data, our ViT-Adapter-L yields state-of-the-art 60.9 box AP and 53.0 mask AP on COCO test-dev. We hope that the ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. Code and models will be released at https://github.com/czczup/ViT-Adapter.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                598. <h><a href=https://openreview.net/pdf?id=9DZKk85Z4zA>Gradient-Guided Importance Sampling for Learning Binary Energy-Based Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Meng Liu, Haoran Liu, Shuiwang Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9DZKk85Z4zA>6.75 - [8, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9DZKk85Z4zA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning energy-based models (EBMs) is known to be difficult especially on discrete data where gradient-based learning strategies cannot be applied directly. Although ratio matching is a sound method to learn discrete EBMs, it suffers from expensive computation and excessive memory requirements, thereby resulting in difficulties in learning EBMs on high-dimensional data. Motivated by these limitations, in this study, we propose ratio matching with gradient-guided importance sampling (RMwGGIS). Particularly, we use the gradient of the energy function w.r.t. the discrete data space to approximately construct the provably optimal proposal distribution, which is subsequently used by importance sampling to efficiently estimate the original ratio matching objective. We perform experiments on density modeling over synthetic discrete data, graph generation, and training Ising models to evaluate our proposed method. The experimental results demonstrate that our method can significantly alleviate the limitations of ratio matching, perform more effectively in practice, and scale to high-dimensional problems. Our implementation is available at https://github.com/divelab/RMwGGIS.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                599. <h><a href=https://openreview.net/pdf?id=JLLTtEdh1ZY>Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Florent Delgrange, Ann Nowe, Guillermo Perez</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JLLTtEdh1ZY>6.75 - [8, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JLLTtEdh1ZY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the distilled policy, for which the formal guarantees apply. Our approach yields bisimulation guarantees while learning the distilled policy, allowing concrete optimization of the abstraction and representation model quality. Our experiments show that, besides distilling policies up to 10 times faster, the latent model quality is indeed better in general. Moreover, we present experiments from a simple time-to-failure verification algorithm on the latent space. The fact that our approach enables such simple verification techniques highlights its applicability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                600. <h><a href=https://openreview.net/pdf?id=HE_75XY5Ljh>StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuechen Yu, Yulin Li, Chengquan Zhang, Xiaoqiang Zhang, Zengyuan Guo, Xiameng Qin, Kun Yao, Junyu Han, Errui Ding, Jingdong Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HE_75XY5Ljh>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HE_75XY5Ljh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we present StrucTexTv2, an effective document image pre-training framework, by performing masked visual-textual prediction. It consists of two self-supervised pre-training tasks: masked image modeling and masked language modeling, based on text region-level image masking. The proposed method randomly masks some image regions according to the bounding box coordinates of text words. The objectives of our pre-training tasks are reconstructing the pixels of masked image regions and the corresponding masked tokens simultaneously. Hence the pre-trained encoder can capture more textual semantics in comparison to the masked image modeling that usually predicts the masked image patches. Compared to the masked multi-modal modeling methods for document image understanding that rely on both the image and text modalities, StrucTexTv2 models image-only input and potentially deals with more application scenarios free from OCR pre-processing. Extensive experiments on mainstream benchmarks of document image understanding demonstrate the effectiveness of StrucTexTv2. It achieves competitive or even new state-of-the-art performance in various downstream tasks such as image classification, layout analysis, table structure recognition, document OCR, and information extraction under the end-to-end scenario.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                601. <h><a href=https://openreview.net/pdf?id=tLScKVhcCR>FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Takashi Matsubara, Takaharu Yaguchi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tLScKVhcCR>6.75 - [8, 3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Real-world dynamical systems have invariant quantities such as energy, momenta, and mass. Even without prior knowledge, the proposed neural network finds and preserves such quantities from data by leveraging projection and discrete gradient methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tLScKVhcCR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many real-world dynamical systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time. The discovery and understanding of first integrals are fundamental and important topics both in the natural sciences and in industrial applications. First integrals arise from the conservation laws of system energy, momentum, and mass, and from constraints on states; these are typically related to specific geometric structures of the governing equations. Existing neural networks designed to ensure such first integrals have shown excellent accuracy in modeling from data. However, these models incorporate the underlying structures, and in most situations where neural networks learn unknown systems, these structures are also unknown. This limitation needs to be overcome for scientific discovery and modeling of unknown systems. To this end, we propose first integral-preserving neural differential equation (FINDE). By leveraging the projection method and the discrete gradient method, FINDE finds and preserves first integrals from data, even in the absence of prior knowledge about underlying structures. Experimental results demonstrate that FINDE can predict future states of target systems much longer and find various quantities consistent with well-known first integrals in a unified manner.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                602. <h><a href=https://openreview.net/pdf?id=lIu-ixf-Tzf>Learning topology-preserving data representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ilya Trofimov, Daniil Cherniavskii, Eduard Tulchinskii, Nikita Balabin, Evgeny Burnaev, Serguei Barannikov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lIu-ixf-Tzf>6.75 - [3, 8, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a method for learning topology-preserving data representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lIu-ixf-Tzf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a method for learning topology-preserving data representations (dimensionality reduction). The method aims to provide topological similarity between the data manifold and its latent representation via enforcing the similarity in topological features (clusters,  loops, 2D voids, etc.) and their localization. 
The core of the method is the minimization of the Representation Topology Divergence (RTD) between original high-dimensional data and low-dimensional representation in latent space. RTD minimization provides closeness in topological features with strong theoretical guarantees. 
We develop a scheme for RTD differentiation and apply it as a loss term for the autoencoder. The proposed method &#34;RTD-AE&#34; better preserves the global structure and topology of the data manifold than state-of-the-art competitors as measured by linear correlation, triplet distance ranking accuracy, and Wasserstein distance between persistence barcodes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                603. <h><a href=https://openreview.net/pdf?id=gfPUokHsW->Learning without Prejudices: Continual Unbiased Learning via Benign and Malignant Forgetting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Myeongho Jeon, Hyoje Lee, Yedarm Seong, Myungjoo Kang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gfPUokHsW->6.75 - [6, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel method, coined Learning without Prejudices, that encourages benign forgetting and regularizes malignant forgetting for continual unbiased learning. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gfPUokHsW-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Although machine learning algorithms have achieved state-of-the-art status in image classification, recent studies have substantiated that the ability of the models to learn several tasks in sequence, termed continual learning (CL), often suffers from abrupt degradation of performance from previous tasks.  A large body of CL frameworks has been devoted to alleviating this issue. However, we observe that forgetting phenomena in CL are not always unfavorable, especially when there is bias (spurious correlation) in training data. We term such type of forgetting benign forgetting, and categorize detrimental forgetting as malignant forgetting. Based on this finding, our objective in this study is twofold:  (a) to discourage malignant forgetting by generating previous representations, and (b) encourage benign forgetting by employing contrastive learning in conjunction with feature-level augmentation. Extensive evaluations of biased experimental setups demonstrate that our proposed method, Learning without Prejudices, is effective for continual unbiased learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                604. <h><a href=https://openreview.net/pdf?id=hY6M0JHl3uL>Linear Connectivity Reveals Generalization Strategies</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, João Sedoc, Naomi Saphra</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hY6M0JHl3uL>6.75 - [6, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Basins on the in-domain test loss surface predict generalization strategies for NLI, paraphrase, and CoLA tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hY6M0JHl3uL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the mode connectivity literature, it is widely accepted that there are common circumstances in which two neural networks, trained similarly on the same data, will maintain loss when interpolated in the weight space. In particular, transfer learning is presumed to ensure the necessary conditions for linear mode connectivity across training runs. In contrast to existing results from image classification, we find that among text classifiers (trained on MNLI, QQP, and CoLA), some pairs of finetuned models have large barriers of increasing loss on the linear paths between them. On each task, we find distinct clusters of models which are linearly connected on the test loss surface, but are disconnected from models outside the cluster---models that occupy separate basins on the surface. By measuring performance on specially-crafted diagnostic datasets, we find that these clusters correspond to different generalization strategies. For example, on MNLI, one cluster behaves like a bag of words model under domain shift, while another cluster uses syntactic heuristics. Our work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions in standard finetuning settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                605. <h><a href=https://openreview.net/pdf?id=CrfhZAsJDsZ>Nonlinear Reconstruction for Operator Learning of PDEs with Discontinuities</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samuel Lanthaler, Roberto Molinaro, Patrik Hadorn, Siddhartha Mishra</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CrfhZAsJDsZ>6.75 - [8, 8, 3, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Operator learning based on non-linear reconstruction (FNOs, shift-DeepONets) outperform methods based on linear reconstruction (DeepONets, PCA-Net) for PDEs with discontinuities.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CrfhZAsJDsZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Discontinuous solutions arise in a large class of hyperbolic and advection-dominated PDEs. This paper investigates, both theoretically and empirically, the operator learning of PDEs with discontinuous solutions. We rigorously prove, in terms of lower approximation bounds, that methods which entail a linear reconstruction step (e.g. DeepONets or PCA-Nets) fail to efficiently approximate the solution operator of such PDEs. In contrast, we show that certain methods employing a non-linear reconstruction mechanism can overcome these fundamental lower bounds and approximate the underlying operator efficiently. The latter class includes Fourier Neural Operators and a novel extension of DeepONets termed shift-DeepONets. Our theoretical findings are confirmed by empirical results for advection equations, inviscid Burgers’ equation and the compressible Euler equations of gas dynamics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                606. <h><a href=https://openreview.net/pdf?id=a4COps0uokg>User-Interactive Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Phillip Swazinna, Steffen Udluft, Thomas Runkler</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=a4COps0uokg>6.75 - [10, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Offline RL policies need to be adaptvie after training so that a user can alter its behavior to its needs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/a4COps0uokg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning algorithms still lack trust in practice due to the risk that the learned policy performs worse than the original policy that generated the dataset or behaves in an unexpected way that is unfamiliar to the user. At the same time, offline RL algorithms are not able to tune their most important hyperparameter - the proximity of the learned policy to the original policy. We propose an algorithm that allows the user to tune this hyperparameter at runtime, thereby addressing both of the above mentioned issues simultaneously. This allows users to start with the original behavior and grant successively greater deviation, as well as stopping at any time when the policy deteriorates or the behavior is too far from the familiar one.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                607. <h><a href=https://openreview.net/pdf?id=T5nUQDrM4u>Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, Neil Houlsby</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=T5nUQDrM4u>6.75 - [6, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We create sparsely activated Mixture-of-Experts models from pre-existing dense models, showing significant performance improvements and computational savings in doing so.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/T5nUQDrM4u.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training large, deep neural networks to convergence can be prohibitively expensive. As a result, often only a small selection of popular, dense models are reused across different contexts and tasks. Increasingly, sparsely activated models, which seek to decouple model size from computation costs, are becoming an attractive alternative to dense models. Although more efficient in terms of quality and computation cost, sparse models remain data-hungry and costly to train from scratch in the large scale regime. In this work, we propose sparse upcycling -- a simple way to reuse sunk training costs by initializing a sparsely activated Mixture-of-Experts model from a dense checkpoint. We show that sparsely upcycled T5 Base, Large, and XL language models and Vision Transformer Base and Large models, respectively, significantly outperform their dense counterparts on SuperGLUE and ImageNet, using only ~50% of the initial dense pretraining sunk cost. The upcycled models also outperform sparse models trained from scratch on 100% of the initial dense pretraining computation budget.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                608. <h><a href=https://openreview.net/pdf?id=jREF4bkfi_S>Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gregory Schwartzman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jREF4bkfi_S>6.75 - [10, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jREF4bkfi_S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We answer the question: &#34;Does \emph{local} progress (on batches) imply \emph{global} progress (on the entire dataset) for mini-batch $k$-means?&#34;. Specifically, we consider mini-batch $k$-means which terminates only when the improvement in the quality of the clustering on the sampled batch is below some threshold.

Although at first glance it appears that this algorithm might execute forever, we answer the above question in the affirmative and show that if the batch is of size $\tilde{\Omega}((d/\epsilon)^2)$, it must terminate within $O(d/\epsilon)$ iterations with high probability, where $d$ is the dimension of the input, and $\epsilon$ is a threshold parameter for termination. This is true \emph{regardless} of how the centers are initialized. When the algorithm is initialized with the $k$-means++ initialization scheme, it achieves an approximation ratio of $O(\log k)$ (the same as the full-batch version). 

Finally, we show the applicability of our results to the mini-batch $k$-means algorithm implemented in the scikit-learn (sklearn) python library. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                609. <h><a href=https://openreview.net/pdf?id=p7hvOJ6Gq0i>DensePure: Understanding Diffusion Models for Adversarial Robustness</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chaowei Xiao, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Weili Nie, Mingyan Liu, Anima Anandkumar, Bo Li, Dawn Song</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p7hvOJ6Gq0i>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We theoretically analyze the fundamental properties of diffusion models to understand why and how it enhances certified robustness. Inspired by the analysis, we propose a new method to improve the certified robustness of the clearn classifier
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p7hvOJ6Gq0i.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models have been recently employed to  improve certified robustness through the process of denoising.  However, the theoretical understanding of why diffusion models are able to improve the certified robustness is still lacking, preventing from further improvement.  In this study, we close this gap by analyzing the fundamental properties of diffusion models and  establishing the conditions under which they can enhance certified robustness. This deeper understanding allows us to propose a new method   DensePure,  designed to improve the certified robustness of a pretrained model (i.e. classifier).   Given an (adversarial) input, DensePure consists of multiple runs of denoising via the reverse process of the diffusion model (with different random seeds) to get multiple reversed samples, which are then passed through the classifier, followed by majority voting of inferred labels to make the final prediction.  This design of using multiple runs of denoising is informed by our theoretical analysis of the conditional distribution of the reversed  sample. Specifically, when the data density of a clean sample is high, its conditional density under the reverse process in a diffusion model is also high;  thus sampling from the latter conditional distribution can purify the adversarial example and return the corresponding clean sample with a high probability. By using the highest density point in the conditional distribution as the reversed sample, we identify the robust region of a given instance under the diffusion model&#39;s reverse process. We show that this robust region is a union of multiple convex sets, and is potentially much larger than the robust regions identified in previous works. In practice, DensePure can approximate the label of the high density region in the conditional distribution so that it can enhance certified robustness. We conduct extensive experiments to demonstrate the effectiveness of DensePure by evaluating its certified robustness  given a standard model via randomized smoothing. We show that DensePure is consistently better than existing methods on ImageNet, with 7% improvement on average. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                610. <h><a href=https://openreview.net/pdf?id=U5XOGxAgccS>Solving Continuous Control via Q-learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tim Seyde, Peter Werner, Wilko Schwarting, Igor Gilitschenski, Martin Riedmiller, Daniela Rus, Markus Wulfmeier</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=U5XOGxAgccS>6.75 - [6, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Decoupling action dimensions during optimization and exploration for DQN in combination with bang-bang action discretization achieves state-of-the-art performance on a variety of continuous control tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/U5XOGxAgccS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While there has been substantial success for solving continuous control with actor-critic methods, simpler critic-only methods such as Q-learning find limited application in the associated high-dimensional action spaces. However, most actor-critic methods come at the cost of added complexity: heuristics for stabilisation, compute requirements and wider hyperparameter search spaces. We show that a simple modification of deep Q-learning largely alleviates these issues. By combining bang-bang action discretization with value decomposition, framing single-agent control as cooperative multi-agent reinforcement learning (MARL), this simple critic-only approach matches performance of state-of-the-art continuous actor-critic methods when learning from features or pixels. We extend classical bandit examples from cooperative MARL to provide intuition for how decoupled critics leverage state information to coordinate joint optimization, and demonstrate surprisingly strong performance across a variety of continuous control tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                611. <h><a href=https://openreview.net/pdf?id=wCFB37bzud4>Bidirectional Language Models Are Also Few-shot Learners</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ajay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, Chris Callison-Burch</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wCFB37bzud4>6.75 - [8, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present Sequential Autoregressive Prompting, a technique that enables prompting of bidirectional models demonstrating prompt-based learning is an emergent property of a broader class of language models, rather than of only unidirectional models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wCFB37bzud4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models such as GPT-3 (Brown et al., 2020) can perform arbitrary tasks without undergoing fine-tuning after being prompted with only a few labeled examples. An arbitrary task can be reformulated as a natural language prompt, and a language model can be asked to generate the completion, indirectly performing the task in a paradigm known as prompt-based learning. To date, emergent prompt-based learning capabilities have mainly been demonstrated for unidirectional language models. However, bidirectional language models pre-trained on denoising objectives such as masked language modeling produce stronger learned representations for transfer learning. This motivates the possibility of prompting bidirectional models, but their pre-training objectives have made them largely incompatible with the existing prompting paradigm. We present SAP (Sequential Autoregressive Prompting), a technique that enables the prompting of bidirectional models. Utilizing the machine translation task as a case study, we prompt the bidirectional mT5 model (Xue et al., 2021) with SAP and demonstrate its few-shot and zero-shot translations outperform the few-shot translations of unidirectional models like GPT-3 and XGLM (Lin et al., 2021), despite mT5&#39;s approximately 50% fewer parameters. We further show SAP is effective on question answering and summarization. For the first time, our results demonstrate prompt-based learning is an emergent property of a broader class of language models, rather than only unidirectional models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                612. <h><a href=https://openreview.net/pdf?id=ktrw68Cmu9c>CodeT:  Code Generation with Generated Tests</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ktrw68Cmu9c>6.75 - [8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ktrw68Cmu9c.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                613. <h><a href=https://openreview.net/pdf?id=itZ6ggvMnzS>Causal Representation Learning for Instantaneous and Temporal Effects in Interactive Systems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M Asano, Taco Cohen, Efstratios Gavves</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=itZ6ggvMnzS>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A causal representation learning method that can identify causal variables with instantaneous effects and their graph from temporal sequences with interventions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/itZ6ggvMnzS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Causal representation learning is the task of identifying the underlying causal variables and their relations from high-dimensional observations, such as images. Recent work has shown that one can reconstruct the causal variables from temporal sequences of observations under the assumption that there are no instantaneous causal relations between them. In practical applications, however, our measurement or frame rate might be slower than many of the causal effects. This effectively creates ``instantaneous&#39;&#39; effects and invalidates previous identifiability results. To address this issue, we propose iCITRIS, a causal representation learning method that allows for instantaneous effects in intervened temporal sequences when intervention targets can be observed, e.g., as actions of an agent. iCITRIS identifies the potentially multidimensional causal variables from temporal observations, while simultaneously using a differentiable causal discovery method to learn their causal graph. In experiments on three datasets of interactive systems, iCITRIS accurately identifies the causal variables and their causal graph.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                614. <h><a href=https://openreview.net/pdf?id=EBC60mxBwyw>How gradient estimator variance and bias impact learning in neural networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Arna Ghosh, Yuhan Helena Liu, Guillaume Lajoie, Konrad Kording, Blake Aaron Richards</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EBC60mxBwyw>6.75 - [6, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We characterize the impact of variance and bias in gradient estimates on learning and generalization and study how network architecture properties modulate these effects.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EBC60mxBwyw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There is growing interest in understanding how real brains may approximate gradients and how gradients can be used to train neuromorphic chips. However, neither real brains nor neuromorphic chips can perfectly follow the loss gradient, so parameter updates would necessarily use gradient estimators that have some variance and/or bias. Therefore, there is a need to understand better how variance and bias in gradient estimators impact learning dependent on network and task properties. Here, we show that variance and bias can impair learning on the training data, but some degree of variance and bias in a gradient estimator can be beneficial for generalization. We find that the ideal amount of variance and bias in a gradient estimator are dependent on several properties of the network and task: the size and activity sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. As such, whether considering biologically-plausible learning algorithms or algorithms for training neuromorphic chips, researchers can analyze these properties to determine whether their approximation to gradient descent will be effective for learning given their network and task properties.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                615. <h><a href=https://openreview.net/pdf?id=_hHYaKu0jcj>Robust Explanation Constraints for Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthew Robert Wicker, Juyeon Heo, Luca Costabello, Adrian Weller</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_hHYaKu0jcj>6.75 - [8, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a method for guaranteeing adversarial robustness of explanations that are based on the input gradient of a neural network.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_hHYaKu0jcj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Post-hoc explanation methods are used with the intent of providing insights about neural networks and are sometimes said to help engender trust in their outputs. However, popular explanations methods have been found to be fragile to minor perturbations of input features or model parameters. Relying on constraint relaxation techniques from non-convex optimization, we develop a method that upper-bounds the largest change an adversary can make to a gradient-based explanation via bounded manipulation of either the input features or model parameters. By propagating a compact input or parameter set as symbolic intervals through the forwards and backwards computations of the neural network we can formally certify the robustness of gradient-based explanations. Our bounds are differentiable, hence we can incorporate provable explanation robustness into neural network training. Empirically, our method surpasses the robustness provided by previous heuristic approaches. We find that our training method is the only method able to learn neural networks with certificates of explanation robustness across all six datasets tested.  
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                616. <h><a href=https://openreview.net/pdf?id=KzkLAE49H9b>Training language models to summarize narratives improves brain alignment</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Khai Loong Aw, Mariya Toneva</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KzkLAE49H9b>6.75 - [8, 5, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that training language models for deeper narrative understanding (characters, emotions, relationships) results in richer representations that have improved alignment to human brain activity.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KzkLAE49H9b.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Building systems that achieve a deeper understanding of language is one of the central goals of natural language processing (NLP). Towards this goal, recent works have begun to train language models on narrative datasets which require extracting the most critical information by integrating across long contexts. However, it is still an open question whether these models are learning a deeper understanding of the text, or if the models are simply learning a heuristic to complete the task. This work investigates this further by turning to the one language processing system that truly understands complex language: the human brain. We show that training language models for deeper narrative understanding results in richer representations that have improved alignment to human brain activity. We further find that the improvements in brain alignment are larger for character names than for other discourse features, which indicates that these models are learning important narrative elements. Taken together, these results suggest that this type of training can indeed lead to deeper language understanding. These findings have consequences both for cognitive neuroscience by revealing some of the significant factors behind brain-NLP alignment, and for NLP by highlighting that understanding of long-range context can be improved beyond language modeling.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                617. <h><a href=https://openreview.net/pdf?id=Tuk3Pqaizx>Sampling-free Inference for Ab-Initio Potential Energy Surface Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nicholas Gao, Stephan Günnemann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Tuk3Pqaizx>6.75 - [5, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We improve neural wave function methods by avoid numerical integration at inference time and introducing restricted neural wave functions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Tuk3Pqaizx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, it has been shown that neural networks not only approximate the ground-state wave functions of a single molecular system well but can also generalize to multiple geometries. While such generalization significantly speeds up training, each energy evaluation still requires Monte Carlo integration which limits the evaluation to a few geometries. In this work, we address the inference shortcomings by proposing the Potential learning from ab-initio Networks (PlaNet) framework, in which we simultaneously train a surrogate model in addition to the neural wave function. At inference time, the surrogate avoids expensive Monte-Carlo integration by directly estimating the energy, accelerating the process from hours to milliseconds. In this way, we can accurately model high-resolution multi-dimensional energy surfaces for larger systems that previously were unobtainable via neural wave functions. Finally, we explore an additional inductive bias by introducing physically-motivated restricted neural wave function models. We implement such a function with several additional improvements in the new PESNet++ model. In our experimental evaluation, PlaNet accelerates inference by 7 orders of magnitude for larger molecules like ethanol while preserving accuracy. Compared to previous energy surface networks, PESNet++ reduces energy errors by up to 74%.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                618. <h><a href=https://openreview.net/pdf?id=3mlITJRYYbs>Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>David Lipshutz, Cengiz Pehlevan, Dmitri Chklovskii</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3mlITJRYYbs>6.75 - [8, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that adding interneurons to a recurrent neural network for statistical whitening accelerates the learning dynamics
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3mlITJRYYbs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Early sensory systems in the brain rapidly adapt to fluctuating input statistics, which requires recurrent communication between neurons. Mechanistically, such recurrent communication is often indirect and mediated by local interneurons. In this work, we explore the computational benefits of mediating recurrent communication via interneurons compared with direct recurrent connections. To this end, we consider two mathematically tractable recurrent neural networks that statistically whiten their inputs --- one with direct recurrent connections and the other with interneurons that mediate recurrent communication. By analyzing the corresponding continuous synaptic dynamics and numerically simulating the networks, we show that the network with interneurons is more robust to initialization than the network with direct recurrent connections in the sense that the convergence time for the synaptic dynamics in the network with interneurons (resp. direct recurrent connections) scales logarithmically (resp. linearly) with the spectrum of their initialization. Our results suggest that interneurons are computationally useful for rapid adaptation to changing input statistics. Interestingly, the network with interneurons is an overparameterized solution of the whitening objective for the network with direct recurrent connections, so our results can be viewed as a recurrent neural network analogue of the implicit acceleration phenomenon observed in overparameterized feedforward linear networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                619. <h><a href=https://openreview.net/pdf?id=53FyUAdP7d>Discovering Generalizable Multi-agent Coordination Skills from Multi-task Offline Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fuxiang Zhang, Chengxing Jia, Yi-Chen Li, Lei Yuan, Yang Yu, Zongzhang Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=53FyUAdP7d>6.75 - [8, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel multi-agent reinforcement learning algorithm to discover coordination skills from multi-task offline data and realize multi-task generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/53FyUAdP7d.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Cooperative multi-agent reinforcement learning (MARL) faces the challenge of adapting to multiple tasks with varying agents and targets. Previous multi-task MARL approaches require costly interactions to simultaneously learn or fine-tune policies in different tasks. However, the situation that an agent should generalize to multiple tasks with only offline data from limited tasks is more in line with the needs of real-world applications. Since offline multi-task data contains a variety of behaviors, an effective data-driven approach is to extract informative latent variables that can represent universal skills for realizing coordination across tasks. In this paper, we propose a novel Offline MARL algorithm to Discover coordInation Skills (ODIS) from multi-task data. ODIS first extracts task-invariant coordination skills from offline multi-task data and learns to delineate different agent behaviors with the discovered coordination skills. Then we train a coordination policy to choose optimal coordination skills with the centralized training and decentralized execution paradigm. We further demonstrate that the discovered coordination skills can assign effective coordinative behaviors, thus significantly enhancing generalization to unseen tasks. Empirical results in cooperative MARL benchmarks, including the StarCraft multi-agent challenge, show that ODIS obtains superior performance in a wide range of tasks only with offline data from limited sources.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                620. <h><a href=https://openreview.net/pdf?id=OnM3R47KIiU>Visual Imitation Learning with Patch Rewards</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minghuan Liu, Tairan He, Weinan Zhang, Shuicheng YAN, Zhongwen Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OnM3R47KIiU>6.75 - [6, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We leverage to learn patch reward and present PatchAIL, an intuitive and principled learning framework for efficient visual imitation learning. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OnM3R47KIiU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Visual imitation learning enables reinforcement learning agents to learn to behave from expert visual demonstrations such as videos or image sequences, without explicit, well-defined rewards. 
Previous reseaches either adopt supervised learning techniques or induce simple and coarse scalar rewards from pixels, neglecting the dense information contained in the image demonstrations.
In this work, we propose to measure the expertise of various local regions of image samples, or called patches, and recover multi-dimensional patch rewards accordingly. 
Patch reward is a more precise rewarding characterization that serves as fine-grained expertise measurement and visual explainability tool.
Specifically, we present Adversarial Imitation Learning with Patch Rewards (PatchAIL), which employs a patch-based discriminator to measure the expertise of different local parts from given images and provide patch rewards.
The patch-based knowledge is also used to regularize the aggregated reward and stabilize the training.
We evaluate our method on the standard pixel-based benchmark DeepMind Control Suite. 
The experiment results have demonstrated that PatchAIL outperforms baseline methods and provides valuable interpretations for visual demonstrations.  
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                621. <h><a href=https://openreview.net/pdf?id=XXTyv1zD9zD>Packed Ensembles for efficient uncertainty estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Olivier Laurent, Adrien Lafage, Enzo Tartaglione, Geoffrey Daniel, Jean-marc Martinez, Andrei Bursuc, Gianni Franchi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XXTyv1zD9zD>6.67 - [6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Packed-Ensembles leverage the width of DNNs and grouped convolutions to train subnetworks in parallel and form an efficient ensemble.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XXTyv1zD9zD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our code available at https://github.com/ENSTA-U2IS/torch-uncertainty.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                622. <h><a href=https://openreview.net/pdf?id=eSQh8rG8Oa>Greedy Actor-Critic: A New Conditional Cross-Entropy Method for Policy Improvement</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samuel Neumann, Sungsu Lim, Ajin George Joseph, Yangchen Pan, Adam White, Martha White</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eSQh8rG8Oa>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an alternative update for the actor in actor-critic algorithms that does not rely on entropy-regularization
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eSQh8rG8Oa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many policy gradient methods are variants of Actor-Critic (AC), where a value function (critic) is learned to facilitate updating the parameterized policy (actor). The update to the actor involves a log-likelihood update weighted by the action-values, with the addition of entropy regularization for soft variants. In this work, we explore an alternative update for the actor, based on an extension of the cross entropy method (CEM) to condition on inputs (states). The idea is to start with a broader policy and slowly concentrate around maximal actions, using a maximum likelihood update towards actions in the top percentile per state. The speed of this concentration is controlled by a proposal policy, that concentrates at a slower rate than the actor. We first provide a policy improvement result in an idealized setting, and then prove that our conditional CEM (CCEM) strategy tracks a CEM update per state, even with changing action-values. We empirically show that our Greedy AC algorithm, that uses CCEM for the actor update, performs better than Soft Actor-Critic and is much less sensitive to entropy-regularization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                623. <h><a href=https://openreview.net/pdf?id=_8mS2NE-HXN>SYNC: SAFETY-AWARE NEURAL CONTROL FOR STABILIZING STOCHASTIC DELAY-DIFFERENTIAL EQUATIONS</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jingdong Zhang, Qunxi Zhu, Wei Yang, Wei Lin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_8mS2NE-HXN>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new class of neural control polices for stabilizing stochastic delay-differential equations with safety guarantee, named as SYNC and including both deterministic and stochastic control outperforming the existing methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_8mS2NE-HXN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Stabilization of the systems described by \textit{stochastic delay}-differential equations (SDDEs) under preset conditions is a challenging task in the control community. Here, to achieve this task, we leverage neural networks to learn control policies using the information of the controlled systems in some prescribed regions.  Specifically, two learned control policies, i.e., the neural deterministic controller (NDC) and the neural stochastic controller (NSC), work effectively in the learning procedures that rely on, respectively, the well-known LaSalle-type theorem and the newly-established theorem for guaranteeing the stochastic stability in SDDEs. We theoretically investigate the performance of the proposed controllers in terms of convergence time and energy cost.  More practically and significantly, we improve our learned control policies through considering the situation where the controlled trajectories only evolve in some specific safety set. {\color{black}  The practical validity of such control policies restricted in safety set is attributed to the theory that we further develop for safety and stability guarantees in SDDEs using the stochastic control barrier function and the spatial discretization}. We call this control as SYNC (\textbf{S}afet\textbf{Y}-aware \textbf{N}eural \textbf{C}ontrol).   The efficacy of all the articulated control policies, including the SYNC, is demonstrated systematically by using representative control problems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                624. <h><a href=https://openreview.net/pdf?id=gU5sJ6ZggcX>REPAIR: REnormalizing Permuted Activations for Interpolation Repair</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, Behnam Neyshabur</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gU5sJ6ZggcX>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper we empirically investigate the conjecture from Entezari et al. 2021 which states that if permutation invariance is taken into account, then there should be no  barrier in the linear interpolation between SGD solutions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gU5sJ6ZggcX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper we empirically investigate the conjecture from Entezari et al. (2021) which states that if permutation invariance is taken into account, then there should be no loss barrier to the linear interpolation between SGD solutions. We conduct our investigation using standard computer vision architectures trained on CIFAR-10 and ImageNet.  First, we observe a general phenomenon in which interpolated deep networks suffer a collapse in the variance of their activations. We demonstrate that an appropriate rescaling of the pre-activations of the interpolated networks ameliorates this problem and significantly reduces the barrier. Second, by combining this with an algorithm for finding permutations based on maximizing correlations between the activations of matched neurons, we are able to reduce the interpolation barrier for a standard ResNet18 trained on CIFAR-10 to 1.5% absolute test error. We explore the interaction between our method and the choice of normalization layer, and demonstrate its robustness across a variety of architectures and training sets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                625. <h><a href=https://openreview.net/pdf?id=cWmtUcsYC3V>Mind the Pool: Convolutional Neural Networks Can Overfit Input Size</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bilal Alsallakh, David Yan, Narine Kokhlikyan, Vivek Miglani, Orion Reblitz-Richardson, Pamela Bhattacharya</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cWmtUcsYC3V>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Standard pooling arithmetic can cause CNNs to overfit the input size used during; an adjustment improves generalization to arbitrary sizes and robustness to translation shifts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cWmtUcsYC3V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We demonstrate how convolutional neural networks can overfit the input size: The accuracy drops significantly when using certain sizes, compared with favorable ones. This issue is inherent to pooling arithmetic, with standard downsampling layers playing a major role in favoring certain input sizes and skewing the weights accordingly. We present a solution to this problem by depriving these layers from the arithmetic cues they use to overfit the input size. Through various examples, we show how our proposed spatially-balanced pooling improves the generalization of the network to arbitrary input sizes and its robustness to translational shifts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                626. <h><a href=https://openreview.net/pdf?id=y81ppNf_vg>AutoTransfer: AutoML with Knowledge Transfer - An Application to Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaidi Cao, Jiaxuan You, Jiaju Liu, Jure Leskovec</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=y81ppNf_vg>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose AutoTransfer, an AutoML solution that improves search efficiency by transferring the known architectural design knowledge to the novel task of interest.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/y81ppNf_vg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    AutoML has demonstrated remarkable success in finding an effective neural architecture for a given machine learning task defined by a specific dataset and an evaluation metric. However, most present AutoML techniques consider each task independently from scratch, which requires exploring many architectures, leading to high computational cost. Here we propose AutoTransfer, an AutoML solution that improves search efficiency by transferring the prior architectural design knowledge to the novel task of interest. Our key innovation includes a task-model bank that captures the model performance over a diverse set of GNN architectures and tasks, and a computationally efficient task embedding that can accurately measure the similarity among different tasks. Based on the task-model bank and the task embeddings, we estimate the design priors of desirable models of the novel task, by aggregating a similarity-weighted sum of the top-K design distributions on tasks that are similar to the task of interest. The computed design priors can be used with any AutoML search algorithm. We evaluate AutoTransfer on six datasets in the graph machine learning domain. Experiments demonstrate that (i) our proposed task embedding can be computed efficiently, and that tasks with similar embeddings have similar best-performing architectures; (ii) AutoTransfer significantly improves search efficiency with the transferred design priors, reducing the number of explored architectures by an order of magnitude. Finally, we release GNN-Bank-101, a large-scale dataset of detailed GNN training information of 120,000 task-model combinations to facilitate and inspire future research.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                627. <h><a href=https://openreview.net/pdf?id=CIoSZ_HKHS7>AIM: Adapting Image Models for Efficient Video Action Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Taojiannan Yang, Yi Zhu, Yusheng Xie, Aston Zhang, Chen Chen, Mu Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CIoSZ_HKHS7>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new method to adapt frozen image pre-trained model for efficient video action recognition
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CIoSZ_HKHS7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent vision transformer based video models mostly follow the ``image pre-training then finetuning&#34; paradigm and have achieved great success on multiple video benchmarks. However, fully finetuning such a video model could be computationally expensive and unnecessary, given the pre-trained image transformer models have demonstrated exceptional transferability. In this work, we propose a novel method to Adapt pre-trained Image Models (AIM) for efficient video understanding. By freezing the pre-trained image model and adding a few lightweight Adapters, we introduce spatial adaptation, temporal adaptation and joint adaptation to gradually equip an image model with spatiotemporal reasoning capability. We show that our proposed AIM can achieve competitive or even better performance than prior arts with substantially fewer tunable parameters on four video action recognition benchmarks. Thanks to its simplicity, our method is also generally applicable to different image pre-trained models, which has the potential to leverage more powerful image foundation models in the future. The project webpage is https://adapt-image-models.github.io/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                628. <h><a href=https://openreview.net/pdf?id=DWn1TEb2fK>Treeformer: Dense Gradient Trees for Efficient Attention Computation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, Prateek Jain</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DWn1TEb2fK>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Efficient Decision Tree based attention computation to reduce FLOPs for self-attention
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DWn1TEb2fK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are &#34;dense&#34;. To optimize such challenging discrete layers, we propose a two-level bootstrapped training method. Using extensive experiments on standard NLP benchmarks, especially for long-sequences, we demonstrate that our Treeformer architecture can be almost as accurate as baseline Transformer while using 30x lesser FLOPs in the attention layer. Compared to Linformer, the accuracy can be as much as 12% higher while using similar FLOPs in the attention layer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                629. <h><a href=https://openreview.net/pdf?id=g7U9jD_2CUr>EVA3D: Compositional 3D Human Generation from 2D Image Collections</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fangzhou Hong, Zhaoxi Chen, Yushi LAN, Liang Pan, Ziwei Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=g7U9jD_2CUr>6.67 - [6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose EVA3D, a high-quality unconditional 3D human generative model learned from 2D image collections.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/g7U9jD_2CUr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inverse graphics aims to recover 3D models from 2D observations. Utilizing differentiable rendering, recent 3D-aware generative models have shown impressive results of rigid object generation using 2D images. However, it remains challenging to generate articulated objects, like human bodies, due to their complexity and diversity in poses and appearances. In this work, we propose, EVA3D, an unconditional 3D human generative model learned from 2D image collections only. EVA3D can sample 3D humans with detailed geometry and render high-quality images (up to 512x256) without bells and whistles (e.g. super resolution). At the core of EVA3D is a compositional human NeRF representation, which divides the human body into local parts. Each part is represented by an individual volume. This compositional representation enables 1) inherent human priors, 2) adaptive allocation of network parameters, 3) efficient training and rendering. Moreover, to accommodate for the characteristics of sparse 2D human image collections (e.g. imbalanced pose distribution), we propose a pose-guided sampling strategy for better GAN learning. Extensive experiments validate that EVA3D achieves state-of-the-art 3D human generation performance regarding both geometry and texture quality. Notably, EVA3D demonstrates great potential and scalability to &#34;inverse-graphics&#34; diverse human bodies with a clean framework.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                630. <h><a href=https://openreview.net/pdf?id=eR2dG8yjnQ>Using Language to Extend to Unseen Domains</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor Darrell, Joseph E. Gonzalez, Aditi Raghunathan, Anna Rohrbach</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eR2dG8yjnQ>6.67 - [6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Transforming multimodal embeddings with language improves accuracy on an unseen domain. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eR2dG8yjnQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    It is expensive to collect training data for every possible domain that a vision model may encounter when deployed. We instead consider how simply $\textit{verbalizing}$ the training domain (e.g.``photos of birds&#39;&#39;) as well as domains we want to extend to but do not have data for (e.g.``paintings of birds&#39;&#39;) can improve robustness. Using a multimodal model with a joint image and language embedding space, our method $\textit{LADS}$ learns a transformation of the image embeddings from the source domain to each target domain, while preserving task relevant information. Without using any images from the target domain, we show that over the $\textit{extended}$ domain containing both source and target, $\textit{LADS}$ outperforms standard fine-tuning and ensemble approaches over a suite of 4 benchmarks targeting domain adaptation and dataset bias.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                631. <h><a href=https://openreview.net/pdf?id=0WVNuEnqVu>Deep Reinforcement Learning for Cost-Effective Medical Diagnosis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zheng Yu, Yikuan Li, Joseph Chahn Kim, Kaixuan Huang, Yuan Luo, Mengdi Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0WVNuEnqVu>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our proposed RL-based approach is able to reduce up to 85% testing cost while having the state-of-art diagnosis accuracy in three real-world medical diagnostics tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0WVNuEnqVu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dynamic diagnosis is desirable when medical tests are costly or time-consuming. In this work, we use reinforcement learning (RL) to find a dynamic policy that selects lab test panels sequentially based on previous observations, ensuring accurate testing at a low cost. Clinical diagnostic data are often highly imbalanced; therefore, we aim to maximize the F1 score instead of the error rate. However, optimizing the non-concave $F_1$ score is not a classic RL problem, thus invalidating standard RL methods. To remedy this issue, we develop a reward shaping approach, leveraging properties of the $F_1$ score and duality of policy optimization, to provably find the set of all Pareto-optimal policies for budget-constrained $F_1$ score maximization. To handle the combinatorially complex state space, we propose a Semi-Model-based Deep Diagnosis Policy Optimization (SM-DDPO) framework that is compatible with end-to-end training and online learning. SM-DDPO is tested on diverse clinical tasks: ferritin abnormality detection, sepsis mortality prediction, and acute kidney injury diagnosis. Experiments with real-world data validate that SM-DDPO trains efficiently and identify all Pareto-front solutions. Across all tasks, SM-DDPO is able to achieve state-of-the-art diagnosis accuracy (in some cases higher than conventional methods) with up to $85\%$ reduction in testing cost. Core codes are available at https://github.com/Zheng321/Deep-Reinforcement-Learning-for-Cost-Effective-Medical-Diagnosis.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                632. <h><a href=https://openreview.net/pdf?id=oMsN9TYwJ0j>PiFold: Toward effective and efficient protein inverse folding</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhangyang Gao, Cheng Tan, Stan Z. Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oMsN9TYwJ0j>6.67 - [6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oMsN9TYwJ0j.jpg loading="lazy">
                </div>
                <div class="abstract">
                    How can we design protein sequences folding into the desired structures effectively and efficiently? AI methods for structure-based protein design have attracted increasing attention in recent years; however, few methods can simultaneously improve the accuracy and efficiency due to the lack of expressive features and autoregressive sequence decoder. To address these issues, we propose PiFold, which contains a novel residue featurizer and PiGNN layers to generate protein sequences in a one-shot way with improved recovery. Experiments show that PiFold could achieve 51.66\% recovery on CATH 4.2, while the inference speed is 70 times faster than the autoregressive competitors. In addition, PiFold achieves 58.72\% and 60.42\% recovery scores on TS50 and TS500, respectively. We conduct comprehensive ablation studies to reveal the role of different types of protein features and model designs, inspiring further simplification and improvement. The PyTorch code is available at \href{https://github.com/A4Bio/PiFold}{GitHub}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                633. <h><a href=https://openreview.net/pdf?id=bzaPGEllsjE>A view of mini-batch SGD via generating functions: conditions of convergence, phase transitions,  benefit from negative momenta.</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Maksim Velikanov, Denis Kuznedelev, Dmitry Yarotsky</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bzaPGEllsjE>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We have developed an analytic framework for analysis of mini-batch SGD dynamics via generating functions using a novel Spectrally Expressible approximation. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bzaPGEllsjE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Mini-batch SGD with momentum is a fundamental algorithm for learning large predictive models. In this paper we develop a new analytic framework to analyze noise-averaged properties of mini-batch SGD for linear models at constant learning rates, momenta and sizes of batches. Our key idea is to consider the dynamics of the second moments of model parameters for a special family of &#34;Spectrally Expressible&#34; approximations. This allows to obtain an explicit expression for the generating function of the sequence of loss values. By analyzing this generating function, we find, in particular, that 1) the SGD dynamics exhibits several convergent and divergent regimes depending on the spectral distributions of the problem; 2) the convergent regimes admit explicit stability conditions, and explicit loss asymptotics in the case of power-law spectral distributions; 3) the optimal convergence rate can be achieved at negative momenta. We verify our theoretical predictions by extensive experiments with MNIST and synthetic problems, and find a good quantitative agreement.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                634. <h><a href=https://openreview.net/pdf?id=Ph5cJSfD2XN>Unbiased Supervised Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Carlo Alberto Barbano, Benoit Dufumier, Enzo Tartaglione, Marco Grangetto, Pietro Gori</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Ph5cJSfD2XN>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce FairKL, a debiasing regularization technique along with a metric learning theoretical framework and a novel formulation of the supervised contrastive loss, ϵ-SupInfoNCE
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Ph5cJSfD2XN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many datasets are biased, namely they contain easy-to-learn features that are highly correlated with the target class only in the dataset but not in the true underlying distribution of the data. For this reason, learning unbiased models from biased data has become a very relevant research topic in the last years. In this work, we tackle the problem of learning representations that are robust to biases. We first present a margin-based theoretical framework that allows us to clarify why recent contrastive losses (InfoNCE, SupCon, etc.) can fail when dealing with biased data. Based on that, we derive a novel formulation of the supervised contrastive loss ($\epsilon$-SupInfoNCE), providing more accurate control of the minimal distance between positive and negative samples. 
Furthermore, thanks to our theoretical framework, we also propose FairKL, a new debiasing regularization loss, that works well even with extremely biased data. We validate the proposed losses on standard vision datasets including CIFAR10, CIFAR100, and ImageNet, and we assess the debiasing capability of FairKL with $\epsilon$-SupInfoNCE, reaching state-of-the-art performance on a number of biased datasets, including real instances of biases &#34;in the wild&#34;.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                635. <h><a href=https://openreview.net/pdf?id=cXMHQD-xQas>Learning Probabilistic Topological Representations Using Discrete Morse Theory</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaoling Hu, Dimitris Samaras, Chao Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cXMHQD-xQas>6.67 - [6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use discrete Morse theory and persistent homology to construct an one-parameter family of structures as the topological/structural representation space to perform inference tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cXMHQD-xQas.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Accurate delineation of fine-scale structures is a very important yet challenging problem. Existing methods use topological information as an additional training loss, but are ultimately making pixel-wise predictions. In this paper, we propose a novel deep learning based method to learn topological/structural. We use discrete Morse theory and persistent homology to construct a one-parameter family of structures as the topological/structural representation space. Furthermore, we learn a probabilistic model that can perform inference tasks in such a topological/structural representation space. Our method generates true structures rather than pixel-maps, leading to better topological integrity in automatic segmentation tasks. It also facilitates semi-automatic interactive annotation/proofreading via the sampling of structures and structure-aware uncertainty.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                636. <h><a href=https://openreview.net/pdf?id=HehQobsr0S>Text Summarization with Oracle Expectation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yumo Xu, Mirella Lapata</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HehQobsr0S>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HehQobsr0S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Extractive summarization produces summaries by identifying and concatenating the most important sentences in a document. Since most summarization datasets do not come with gold labels indicating whether document sentences are summary-worthy, different labeling algorithms have been proposed to extrapolate oracle extracts for model training. In this work, we identify two flaws with the widely used greedy labeling approach: it delivers suboptimal and deterministic oracles. To alleviate both issues, we propose a simple yet effective labeling algorithm that creates soft, expectation-based sentence labels. We define a new learning objective for extractive summarization which incorporates learning signals from multiple oracle summaries and prove it is equivalent to estimating the oracle expectation for each document sentence. Without any architectural modifications, the proposed labeling scheme achieves superior performance on a variety of summarization benchmarks across domains and languages, in both supervised and zero-shot settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                637. <h><a href=https://openreview.net/pdf?id=uhLAcrAZ9cJ>Efficient Federated Domain Translation </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyu Zhou, Sheikh Shams Azam, Christopher Brinton, David I. Inouye</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uhLAcrAZ9cJ>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uhLAcrAZ9cJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A central theme in federated learning (FL) is the fact that client data distributions are often not independent and identically distributed (IID), which has strong implications on the training process. While most existing FL algorithms focus on the conventional non-IID setting of class imbalance or missing classes across clients, in practice, the distribution differences could be more complex, e.g., changes in class conditional (domain) distributions. In this paper, we consider this complex case in FL wherein each client has access to only one domain distribution. For tasks such as domain generalization, most existing learning algorithms require access to data from multiple clients (i.e., from multiple domains) during training, which is prohibitive in FL. To address this challenge, we propose a federated domain translation method that generates pseudodata for each client which could be useful for multiple downstream learning tasks. We empirically demonstrate that our translation model is more resource-efficient (in terms of both communication and computation) and easier to train in an FL setting than standard domain translation methods. Furthermore, we demonstrate that the learned translation model enables use of state-of-the-art domain generalization methods in a federated setting, which enhances accuracy and robustness to increases in the synchronization period compared to existing methodology.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                638. <h><a href=https://openreview.net/pdf?id=wZ2SVhOTzBX>That Label&#39;s got Style: Handling Label Style Bias for Uncertain Image Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kilian Zepf, Eike Petersen, Jes Frellsen, Aasa Feragen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wZ2SVhOTzBX>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a method to reduce bias caused by differing label style for uncertain image segmentation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wZ2SVhOTzBX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Segmentation uncertainty models predict a distribution over plausible segmentations for a given input, which they learn from the annotator variation in the training set. However, in practice these annotations can differ systematically in the way they are generated, for example through the use of different labeling tools. This results in datasets that contain both data variability and differing label styles. In this paper, we demonstrate that applying state-of-the-art segmentation uncertainty models on such datasets can lead to model bias caused by the different label styles. We present an updated modelling objective conditioning on labeling style for aleatoric uncertainty estimation, and modify two state-of-the-art-architectures for segmentation uncertainty accordingly. We show with extensive experiments that this method reduces label style bias, while improving segmentation performance, increasing the applicability of segmentation uncertainty models in the wild. We curate two datasets, with annotations in different label styles, which we will make publicly available along with our code upon publication.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                639. <h><a href=https://openreview.net/pdf?id=FtOxgKe_Zg2>Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, Minjoon Seo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FtOxgKe_Zg2>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce Flipped Learning, a meta-training method that computes the likelihood of the task instruction given input instance and label.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FtOxgKe_Zg2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as FLIPPED, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized FLIPPED outperforms zero-shot T0-11B (Sanh et al, 2021) and even a 16 times larger 3-shot GPT-3 (175B) (Brown et al, 2020) on average by 8.4% and 9.7% points, respectively. FLIPPED gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% average F1 score. This indicates that the strong task generalization of FLIPPED comes from improved generalization to novel labels. We release our code at github.com/seonghyeonye/Flipped-Learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                640. <h><a href=https://openreview.net/pdf?id=J4mJjotSauh>Fooling SHAP with Stealthily Biased Sampling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>gabriel laberge, Ulrich Aïvodji, Satoshi Hara, Mario Marchand, Foutse Khomh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=J4mJjotSauh>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that Shapley-based explanation techniques commonly used in ML can be manipulated to show false compliance (e.g., during an algorithmic fairness audit) and that this type of attack can be hard to detect.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/J4mJjotSauh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    SHAP explanations aim at identifying which features contribute the most to the difference in model prediction at a specific input versus 
a background distribution. Recent studies have shown that they can be manipulated by malicious adversaries to produce arbitrary desired 
explanations. However, existing attacks focus solely on altering the black-box model itself. In this paper, we propose a complementary family 
of attacks that leave the model intact and manipulate SHAP explanations using stealthily biased sampling of the data points used to approximate expectations w.r.t the background distribution. In the context of fairness audit, we show that our attack can reduce the importance of a sensitive feature when explaining the difference in outcomes between groups while remaining undetected. More precisely, experiments performed on real-world datasets showed that our attack could yield up to a 90\% relative decrease in amplitude of the sensitive feature attribution. These results highlight the manipulability of SHAP explanations and encourage auditors to treat them with skepticism.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                641. <h><a href=https://openreview.net/pdf?id=1w_Amtk67X>Constraining Representations Yields Models That Know What They Don&#39;t Know</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Joao Monteiro, Pau Rodriguez, Pierre-Andre Noel, Issam H. Laradji, David Vazquez</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1w_Amtk67X>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a model class able to provide confidence scores indicating how likely it is that it is making an erroneous prediction.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1w_Amtk67X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A well-known failure mode of neural networks is that they may confidently return erroneous predictions. Such unsafe behaviour is particularly frequent when the use case slightly differs from the training context, and/or in the presence of an adversary. This work presents a novel direction to address these issues in a broad, general manner: imposing class-aware constraints on a model&#39;s internal activation patterns. Specifically, we assign to each class a unique, fixed, randomly-generated binary vector - hereafter called class code - and train the model so that its cross-depths activation patterns predict the appropriate class code according to the input sample&#39;s class. The resulting predictors are dubbed total activation classifiers (TAC), and TACs may either be trained from scratch, or used with negligible cost as a thin add-on on top of a frozen, pre-trained neural network. The distance between a TAC&#39;s activation pattern and the closest valid code acts as an additional confidence score, besides the default unTAC&#39;ed prediction head&#39;s. In the add-on case, the original neural network&#39;s inference head is completely unaffected (so its accuracy remains the same) but we now have the option to use TAC&#39;s own confidence and prediction when determining which course of action to take in an hypothetical production workflow. In particular, we show that TAC strictly improves the value derived from models allowed to reject/defer. We provide further empirical evidence that TAC works well on multiple types of architectures and data modalities and that it is at least as good as state-of-the-art alternative confidence scores derived from existing models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                642. <h><a href=https://openreview.net/pdf?id=QsCSLPP55Ku>Effective passive membership inference attacks in federated learning against overparameterized models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiacheng Li, Ninghui Li, Bruno Ribeiro</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QsCSLPP55Ku>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The observation that gradients of large overparameterized neural networks that generalize well behave like high-dimensional independent isotropic random vectors, leads to a new class of passive membership inference attacks in federated learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QsCSLPP55Ku.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work considers the challenge of performing membership inference attacks in a federated learning setting ---for image classification--- where an adversary can only observe the communication between the central node and a single client (a passive white-box attack). Passive attacks are one of the hardest-to-detect attacks, since they can be performed without modifying how the behavior of the central server or its clients, and assumes *no access to private data instances*. The key insight of our method is empirically observing that, near parameters that generalize well in test, the gradient of large overparameterized neural network models statistically behave like high-dimensional independent isotropic random vectors.  Using this insight, we devise two attacks that are often little impacted by existing and proposed defenses. Finally, we validated the hypothesis that our attack depends on the overparametrization by showing that increasing the level of overparametrization (without changing the neural network architecture) positively correlates with our attack effectiveness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                643. <h><a href=https://openreview.net/pdf?id=6kxApT2r2i>The Augmented Image Prior: Distilling 1000 Classes by Extrapolating from a Single Image</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuki M Asano, Aaqib Saeed</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6kxApT2r2i>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that it is possible to extrapolate to semantic classes such as those of ImageNet or Kinetics using just a single datum plus heavy augmentations as visual inputs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6kxApT2r2i.jpg loading="lazy">
                </div>
                <div class="abstract">
                    What can neural networks learn about the visual world when provided with only a single image as input? While any image obviously cannot contain the multitudes of all existing objects, scenes and lighting conditions -- within the space of all  $256^{3\cdot224\cdot224}$ possible $224$-sized square images, it might still provide a strong prior for natural images. To analyze this ``augmented image prior&#39;&#39;  hypothesis, we develop a simple framework for training neural networks from scratch using a single image and augmentations using knowledge distillation from a supervised pretrained teacher. With this, we find the answer to the above question to be: `surprisingly, a lot&#39;. In quantitative terms, we find accuracies of $94\%$/$74\%$ on CIFAR-10/100, $69$\% on ImageNet, and by extending this method to video and audio, $51\%$ on Kinetics-400 and $84$\% on SpeechCommands. In extensive analyses spanning 13 datasets, we disentangle the effect of augmentations, choice of data and network architectures and also provide qualitative evaluations that include lucid ``panda neurons&#39;&#39; in networks that have never even seen one. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                644. <h><a href=https://openreview.net/pdf?id=ULzyv9M1j5>Transformer-based model for symbolic regression via joint supervised learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenqiang Li, Weijun Li, Linjun Sun, Min Wu, Lina Yu, Jingyi Liu, Yanjie Li, Songsong Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ULzyv9M1j5>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ULzyv9M1j5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Symbolic regression (SR) is an important technique for discovering hidden mathematical expressions from observed data. Transformer-based approaches have been widely used for machine translation due to their high performance, and are recently highly expected to be used for SR. They input the data points, then output the expression skeleton, and finally optimize the coefficients. However, recent transformer-based methods for SR focus more attention on large scale training data and ignore the ill-posed problem: the lack of sufficient supervision, i.e., expressions that may be completely different have the same supervision because of their same skeleton, which makes it challenging to deal with data that may be from the same expression skeleton but with different coefficients. Therefore, we present a transformer-based model for SR with the ability to alleviate this problem. Specifically, we leverage a feature extractor based on pure residual MLP networks to obtain more information about data points. Furthermore, the core idea is that we propose a joint learning mechanism combining supervised contrastive learning, which makes features of data points from expressions with the same skeleton more similar so as to effectively alleviates the ill-posed problem. The benchmark results show that the proposed method is up to 25% higher with respect to the recovery rate of skeletons than typical transformer-based methods. Moreover, our method outperforms state-of-the-art SR methods based on reinforcement learning and genetic programming in terms of the coefficient of determination ($R^2$).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                645. <h><a href=https://openreview.net/pdf?id=hCmjBJeGXcu>Enhancing Meta Learning via Multi-Objective Soft Improvement Functions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Runsheng Yu, Weiyu Chen, Xinrun Wang, James Kwok</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hCmjBJeGXcu>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hCmjBJeGXcu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Meta-learning tries to leverage information from similar learning tasks. In the commonly-used bilevel optimization formulation, the shared parameter is learned in the outer loop by minimizing the average loss over all tasks. However, the converged solution may be comprised in that it only focuses on optimizing on a small subset of tasks. To alleviate this problem, we consider meta-learning as a multi-objective optimization (MOO) problem, in which each task is an objective. However, existing MOO solvers need to access all the objectives’ gradients in each iteration, and cannot scale to the huge number of tasks in typical meta-learning settings. To alleviate this problem, we propose a scalable gradient-based solver with the use of mini-batch. We provide theoretical guarantees on the Pareto optimality or Pareto stationarity of the converged solution. Empirical studies on various machine learning settings demonstrate that the proposed method is efficient, and achieves better performance than the baselines, particularly on improving the performance of the poorly-performing tasks and thus alleviating the compromising phenomenon.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                646. <h><a href=https://openreview.net/pdf?id=RlPmWBiyp6w>GAIN: On the Generalization of Instructional Action Understanding</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junlong Li, Guangyi Chen, Yansong Tang, Jinan Bao, Kun Zhang, Jie Zhou, Jiwen Lu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RlPmWBiyp6w>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RlPmWBiyp6w.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the great success achieved in instructional action understanding by deep learning and mountainous data, deploying trained models to the unseen environment still remains a great challenge, since it requires strong generalizability of models from in-distribution training data to out-of-distribution (OOD) data. In this paper, we introduce a benchmark, named GAIN, to analyze the GeneralizAbility of INstructional action understanding models. In GAIN, we reassemble steps of existing instructional video training datasets to construct the OOD tasks and then collect the corresponding videos. We evaluate the generalizability of models trained on in-distribution datasets with the performance on OOD videos and observe a significant performance drop. We further propose a simple yet effective approach, which cuts off the excessive contextual dependency of action steps by performing causal inference, to provide a potential direction for enhancing the OOD generalizability. In the experiments, we show that this simple approach can improve several baselines on both instructional action segmentation and detection tasks. We expect the introduction of the GAIN dataset will promote future in-depth research on the generalization of instructional video understanding.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                647. <h><a href=https://openreview.net/pdf?id=7C9aRX2nBf2>Sequential Latent Variable Models for Few-Shot High-Dimensional Time-Series Forecasting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiajun Jiang, Ryan Missel, Zhiyuan Li, Linwei Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7C9aRX2nBf2>6.67 - [8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present the very first step toward few-shot high-dimensional sequence forecasting by a Bayesian meta-learning model that learns the process of learning latent dynamics that changes with the small number of observations that are available.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7C9aRX2nBf2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern applications increasingly require learning and forecasting latent dynamics from high-dimensional time-series. Compared to univariate time-series forecasting, this adds a new challenge of reasoning about the latent dynamics of an unobserved abstract state. Sequential latent variable models (LVMs) present an attractive solution, although existing works either struggle with long-term forecasting or have difficulty learning across diverse dynamics. In this paper, we first present a conceptual framework of sequential LVMs to unify existing works, contrast their fundamental limitations, and identify an intuitive solution to long-term forecasting for diverse dynamics via meta-learning. We then present the first framework of few-shot forecasting for high-dimensional time-series: instead of learning a single dynamic function, we leverage data of diverse dynamics and learn to adapt latent dynamic functions to few-shot support series. This is realized via Bayesian meta-learning underpinned by: 1) a latent dynamic function conditioned on knowledge derived from few-shot support series, and 2) a meta-model that learns to extract such dynamic-specific knowledge via feed-forward embedding of support set. We compared the presented framework with a comprehensive set of baseline models trained 1) globally on the large meta-training set with diverse dynamics, and 2) individually on single dynamics, both with and without fine-tuning to k-shot support series used by the meta-models. We demonstrated that the presented framework is agnostic to the latent dynamic function of choice and, at meta-test time, is able to forecast for new dynamics given variable-shot of support series.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                648. <h><a href=https://openreview.net/pdf?id=92gvk82DE->Large Language Models are Human-Level Prompt Engineers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=92gvk82DE->6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an algorithm for automatic instruction generation and selection for large language models with human level performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/92gvk82DE-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the &#34;program,&#34; optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 21/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                649. <h><a href=https://openreview.net/pdf?id=UMERaIHMwB3>Learning to Jointly Share and Prune Weights for Grounding Based Vision and Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shangqian Gao, Burak Uzkent, Yilin Shen, Heng Huang, Hongxia Jin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UMERaIHMwB3>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UMERaIHMwB3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transformers have seen growing interest in processing different modalities,  including language and image data. As a result, we can process vision and language data using transformers that are architecturally similar. Leveraging this feature of transformers, we propose weight sharing across two transformer backbones and within the same transformer backbone and pruning across two backbones in a unified framework. More specifically, we investigate weight sharing and pruning for two components of the transformers: (1) Multi-Head Attention (MSA) and (2) Feed-Forward Network (FFN) layers. To jointly perform weight sharing and pruning, we propose to use a regularization term to align model weights and the desired structure during the multimodal pre-training step. The structure vectors of sharing and pruning are generated by using a hypernetwork, which can capture complex interactions between pruning and sharing across layers and modalities. We train the hypernetwork and model weights iteratively so that the learned structure evolves along with model weights. After minimizing the proposed objective in the pre-training step, we perform weight sharing and pruning and fine-tune the compressed model on downstream tasks. Finally, we perform experiments on vision and language tasks, including Referring Expression Comprehension (REC), Visual Question Answering (VQA), and Object Detection using the state-of-the-art grounding based models: MDETR and GLIP. Our experiments show that we can compress these models by $35-40\%$ by sharing and pruning MSA and FFN weights without almost any loss in accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                650. <h><a href=https://openreview.net/pdf?id=YPChvOgRXRA>Backstepping Temporal Difference Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Han-Dong Lim, Donghwan Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YPChvOgRXRA>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper develops a new unifying view to design off-policy temporal difference learning algorithms.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YPChvOgRXRA.jpg loading="lazy">
                </div>
                <div class="abstract">
                     Off-policy learning ability is an important feature of reinforcement learning (RL) for practical applications. However, even one of the most elementary RL algorithms, temporal-difference (TD) learning, is known to suffer form divergence issue when the off-policy scheme is used together with linear function approximation. To overcome the divergent behavior, several off-policy TD learning algorithms have been developed until now. In this work, we provide a unified view of such algorithms from a purely control-theoretic perspective. Our method relies on the backstepping technique, which is widely used in nonlinear control theory.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                651. <h><a href=https://openreview.net/pdf?id=6iEoTr-jeB7>Learning Continuous Normalizing Flows For Faster Convergence To Target Distribution via Ascent Regularizations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuangshuang Chen, Sihao Ding, Yiannis Karayiannidis, Mårten Björkman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6iEoTr-jeB7>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6iEoTr-jeB7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Normalizing flows (NFs) have been shown to be advantageous in modeling complex distributions and improving sampling efficiency for unbiased sampling. In this work, we propose a new class of continuous NFs, ascent continuous normalizing flows (ACNFs), that makes a base distribution  converge faster to a target distribution. As solving such a flow is non-trivial and barely possible, we propose a practical implementation to learn flexibly parametric ACNFs via ascent regularization and apply it in two learning cases: maximum likelihood learning for density estimation and minimizing reverse KL divergence for unbiased sampling and variational inference. The learned ACNFs demonstrate faster convergence towards the target distributions, therefore, achieving better density estimations, unbiased sampling and variational approximation at lower computational costs. Furthermore, the flows show to stabilize themselves to mitigate performance deterioration and are less sensitive to the choice of training flow length $T$.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                652. <h><a href=https://openreview.net/pdf?id=3OaBBATwsvP>Generative Modeling Helps Weak Supervision (and Vice Versa)</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Benedikt Boecking, Nicholas Roberts, Willie Neiswanger, Stefano Ermon, Frederic Sala, Artur Dubrawski</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3OaBBATwsvP>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3OaBBATwsvP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervised synthetic images and pseudolabels. Additionally, its learned latent variables can be inspected qualitatively. The model outperforms baseline weak supervision label models on a number of multiclass image classification datasets, improves the quality of generated images, and further improves end-model performance through data augmentation with synthetic samples.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                653. <h><a href=https://openreview.net/pdf?id=b_CQDy9vrD1>ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, Hao Su</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b_CQDy9vrD1>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b_CQDy9vrD1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generalizable manipulation skills, which can be composed to tackle long-horizon and complex daily chores, are one of the cornerstones of Embodied AI. However, existing benchmarks, mostly composed of a suite of simulatable environments, are insufficient to push cutting-edge research works because they lack object-level topological and geometric variations, are not based on fully dynamic simulation, or are short of native support for multiple types of manipulation tasks. To this end, we present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. It implements a render server infrastructure to allow sharing rendering resources across all environments, thereby significantly reducing memory usage. We open-source all codes of our benchmark (simulator, environments, and baselines) and host an online challenge open to interdisciplinary researchers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                654. <h><a href=https://openreview.net/pdf?id=Z63RvyAZ2Vh>UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinhao Jiang, Kun Zhou, Xin Zhao, Ji-Rong Wen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Z63RvyAZ2Vh>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Z63RvyAZ2Vh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG).
To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately.
Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. 
In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying  retrieval and reasoning in both model architecture and parameter learning.
For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs.
For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies.
Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. 
Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task.
Our codes and data are publicly available at~\url{https://github.com/RUCAIBox/UniKGQA}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                655. <h><a href=https://openreview.net/pdf?id=zJXg_Wmob03>Progressive Voronoi Diagram Subdivision Enables Accurate Data-free Class-Incremental Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chunwei Ma, Zhanghexuan Ji, Ziyun Huang, Yan Shen, Mingchen Gao, Jinhui Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zJXg_Wmob03>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that progressive Voronoi Diagram is a powerful model for Class-incremental Learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zJXg_Wmob03.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Data-free Class-incremental Learning (CIL) is a challenging problem because rehearsing data from previous phases is strictly prohibited, causing catastrophic forgetting of Deep Neural Networks (DNNs). In this paper, we present \emph{iVoro}, a novel framework derived from computational geometry. We found Voronoi Diagram (VD), a classical model for space subdivision, is especially powerful for solving the CIL problem, because VD itself can be constructed favorably in an incremental manner -- the newly added sites (classes) will only affect the proximate classes, making the non-contiguous classes hardly forgettable. Furthermore, we bridge DNN and VD using Power Diagram Reduction, and show that the VD structure can be progressively refined along the phases using a divide-and-conquer algorithm. Moreover, our VD construction is not restricted to the deep feature space, but is also applicable to multiple intermediate feature spaces, promoting VD to be multilayer VD that efficiently captures multi-grained features from DNN. Importantly, \emph{iVoro} is also capable of handling uncertainty-aware test-time Voronoi cell assignment and has exhibited high correlations between geometric uncertainty and predictive accuracy (up to ${\sim}0.9$). Putting everything together, \emph{iVoro} achieves up to $25.26\%$, $37.09\%$, and $33.21\%$ improvements on CIFAR-100, TinyImageNet, and ImageNet-Subset, respectively, compared to the state-of-the-art non-exemplar CIL approaches. In conclusion, \emph{iVoro} enables highly accurate, privacy-preserving, and geometrically interpretable CIL that is particularly useful when cross-phase data sharing is forbidden, e.g. in medical applications.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                656. <h><a href=https://openreview.net/pdf?id=UaAD-Nu86WX>DiGress: Discrete Denoising diffusion for graph generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, Pascal Frossard</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UaAD-Nu86WX>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. It is state-of-the-art on both abstract and molecular datasets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UaAD-Nu86WX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes.
Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories.
A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks.
We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features.
A procedure for conditioning the generation on graph-level features is also proposed.
DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. 
It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                657. <h><a href=https://openreview.net/pdf?id=EVrz7UM-ZDm>Volumetric Optimal Transportation by Fast Fourier Transform</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Na Lei, DONGSHENG An, Min Zhang, Xiaoyin Xu, David Gu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EVrz7UM-ZDm>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Optimal transport, Monge-Amp\`ere equation, Elliptic PDE, Fast Fourier transform
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EVrz7UM-ZDm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The optimal transportation map finds the most economical way to transport one probability measure to another, and it has been applied in a broad range of applications in machine learning and computer vision. By the Brenier theory, computing the optimal transport map is equivalent to solving a Monge-Amp\`ere equation, which is highly non-linear. Therefore, the computation of optimal transportation maps is intrinsically challenging.

In this work, we propose a novel and powerful method, the FFT-OT (fast Fourier transform-optimal transport), to compute the 3-dimensional OT problems. The method is based on several key ideas: first, the Monge-Amp\`ere equation is linearized to a sequence of linear elliptic PDEs with spacial and temporal variant coefficients; second, the obliqueness property of optimal transportation maps is reformulated as a Neumann boundary condition; and third, the variant coefficient elliptic PDEs are approximated by constant coefficient elliptic PDEs and solved by FFT on GPUs. We also prove that the algorithm converges linearly, namely the approximation error decreases exponentially fast. Experimental results show that the FFT-OT algorithm is more than a hundred times faster than the conventional methods based on the convex geometry. Furthermore, the method can be directly applied for sampling from complex 3D density functions in machine learning and magnifying the volumetric data in medical imaging. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                658. <h><a href=https://openreview.net/pdf?id=sdQGxouELX>MMVAE+: Enhancing the Generative Quality of Multimodal VAEs without Compromises</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Emanuele Palumbo, Imant Daunhawer, Julia E Vogt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sdQGxouELX>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sdQGxouELX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multimodal VAEs have recently gained attention as efficient models for weakly-supervised generative learning with multiple modalities. However, all existing variants of multimodal VAEs are affected by a non-trivial trade-off between generative quality and generative coherence. In particular mixture-based models achieve good coherence only at the expense of sample diversity and a resulting lack of generative quality. We present a novel variant of the mixture-of-experts multimodal variational autoencoder that improves its generative quality, while maintaining high semantic coherence. We model shared and modality-specific information in separate latent subspaces, proposing an objective that overcomes certain dependencies on hyperparameters that arise for existing approaches with the same latent space structure. Compared to these existing approaches, we show increased robustness with respect to changes in the design of the latent space, in terms of the capacity allocated to modality-specific subspaces. We show that our model achieves both good generative coherence and high generative quality in challenging experiments, including more complex multimodal datasets than those used in previous works.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                659. <h><a href=https://openreview.net/pdf?id=Zob4P9bRNcK>Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, Feng Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Zob4P9bRNcK>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Zob4P9bRNcK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Cutting planes (cuts) are important for solving mixed-integer linear programs (MILPs), which formulate a wide range of important real-world applications. Cut selection---which aims to select a proper subset of the candidate cuts to improve the efficiency of solving MILPs---heavily depends on (P1) which cuts should be preferred, and (P2) how many cuts should be selected. Although many modern MILP solvers tackle (P1)-(P2) by manually designed heuristics, machine learning offers a promising approach to learn more effective heuristics from MILPs collected from specific applications. However, many existing learning-based methods focus on learning which cuts should be preferred, neglecting the importance of learning the number of cuts that should be selected. Moreover, we observe from extensive empirical results that (P3) what order of selected cuts should be preferred has a significant impact on the efficiency of solving MILPs as well. To address this challenge, we propose a novel hierarchical sequence model (HEM) to learn cut selection policies via reinforcement learning. Specifically, HEM consists of a two-level model: (1) a higher-level model to learn the number of cuts that should be selected, (2) and a lower-level model---that formulates the cut selection task as a sequence to sequence learning problem---to learn policies selecting an ordered subset with the size determined by the higher-level model. To the best of our knowledge, HEM is the first method that can tackle (P1)-(P3) in cut selection simultaneously from a data-driven perspective. Experiments show that HEM significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs, including MIPLIB 2017. Moreover, experiments demonstrate that HEM well generalizes to MILPs that are significantly larger than those seen during training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                660. <h><a href=https://openreview.net/pdf?id=QTXKTXJKIh>Achieving Near-Optimal Individual Regret &amp; Low Communications in Multi-Agent Bandits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xuchuang Wang, Lin Yang, Yu-Zhen Janice Chen, Xutong Liu, Mohammad Hajiesmaili, Don Towsley, John C.S. Lui</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QTXKTXJKIh>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A near-optimal algorithm for both individual and group regrets and only requiring O(\log (\log T)) communication times
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QTXKTXJKIh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Cooperative multi-agent multi-armed bandits (CM2AB) study how distributed agents cooperatively play the same multi-armed bandit game. Most existing CM2AB works focused on maximizing the group performance of all agents---the accumulation of all agents&#39; individual performance (i.e., individual reward). However, in many applications, the performance of the system is more sensitive to the ``bad&#39;&#39; agent---the agent with the worst individual performance. For example, in a drone swarm, a ``bad&#39;&#39; agent may crash into other drones and severely degrade the system performance. In that case, the key of the learning algorithm design is to coordinate computational and communicational resources among agents so to optimize the individual learning performance of the ``bad&#39;&#39; agent. In CM2AB, maximizing the group performance is equivalent to minimizing the group regret of all agents, and minimizing the individual performance can be measured by minimizing the maximum (worst) individual regret among agents. Minimizing the maximum individual regret was largely ignored in prior literature, and currently, there is little work on how to minimize this objective with a low communication overhead. In this paper, we propose a near-optimal algorithm on both individual and group regrets, in addition,  we also propose a novel communication module in the algorithm, which only needs \(O(\log (\log T))\) communication times where \(T\) is the number of decision rounds. We also conduct simulations to illustrate the advantage of our algorithm by comparing it to other known baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                661. <h><a href=https://openreview.net/pdf?id=gm0VZ-h-hPy>Proposal-Contrastive Pretraining for Object Detection from Fewer Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Quentin Bouniot, Romaric Audigier, Angelique Loesch, Amaury Habrard</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gm0VZ-h-hPy>6.67 - [6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present Proposal Selection Contrast (ProSeCo), a novel unsupervised overall pretraining approach for Object Detection that leverages the large number of object proposals generated by transformer-based detectors using an improved contrastive loss.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gm0VZ-h-hPy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The use of pretrained deep neural networks represents an attractive way to achieve strong results with few data available. When specialized in dense problems such as object detection, learning local rather than global information in images has proven to be more efficient. However, for unsupervised pretraining, the popular contrastive learning requires a large batch size and, therefore, a lot of resources. To address this problem, we are interested in transformer-based object detectors that have recently gained traction in the community with good performance and with the particularity of generating many diverse object proposals. 
    In this work, we present Proposal Selection Contrast (ProSeCo), a novel unsupervised overall pretraining approach that leverages this property. ProSeCo uses the large number of object proposals generated by the detector for contrastive learning, which allows the use of a smaller batch size, combined with object-level features to learn local information in the images. To improve the effectiveness of the contrastive loss, we introduce the object location information in the selection of positive examples to take into account multiple overlapping object proposals. When reusing pretrained backbone, we advocate for consistency in learning local information between the backbone and the detection head. 
    We show that our method outperforms state of the art in unsupervised pretraining for object detection on standard and novel benchmarks in learning with fewer data. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                662. <h><a href=https://openreview.net/pdf?id=JHW30A4DXtO>Learning to Generate Columns with Application to Vertex Coloring</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuan Sun, Andreas T Ernst, Xiaodong Li, Jake Weiner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JHW30A4DXtO>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JHW30A4DXtO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a new column generation approach based on Machine Learning (ML) for solving combinatorial optimization problems. The aim of our method is to generate high-quality columns that belong to an optimal integer solution, in contrast to the traditional approach that aims at solving linear programming relaxations. To achieve this aim, we design novel features to characterize a column, and develop an effective ML model to predict whether a column belongs to an optimal integer solution. We then use the ML model as a filter to select high-quality columns generated from a sampling method and use the selected columns to construct an integer solution.  Our method is computationally fast compared to the traditional methods that generate columns by repeatedly solving a pricing problem. We demonstrate the efficacy of our method on the vertex coloring problem, by empirically showing that the columns selected by our ML model are significantly better, in terms of the integer solution that can be constructed from them, than those selected randomly or based only on their reduced cost. Further, we show that the columns generated by our method can be used as a warm start to boost the performance of a column generation-based heuristic.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                663. <h><a href=https://openreview.net/pdf?id=bBpT6dEjeRG>Adversarial Attacks on Adversarial Bandits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuzhe Ma, Zhijin Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bBpT6dEjeRG>6.67 - [6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bBpT6dEjeRG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study a security threat to adversarial multi-armed bandit, in which an attacker perturbs the loss or reward signal to control the behavior of the victim bandit player. We show that the attacker is able to mislead any no-regret adversarial bandit algorithm into selecting a suboptimal target action in every but sublinear (T−o(T )) number of rounds, while incurring only sublinear (o(T)) cumulative attack cost. This result implies critical security concern in real-world bandit-based systems, e.g., in online recommendation, an attacker might be able to hijack the recommender system and promote a desired product. Our proposed attack algorithms require knowledge of only the regret rate, thus are agnostic to the concrete bandit algorithm employed by the victim player. We also derived a theoretical lower bound on the cumulative attack cost that any victim-agnostic attack algorithm must incur. The lower bound matches the upper bound achieved by our attack, which shows that our attack is asymptotically optimal.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                664. <h><a href=https://openreview.net/pdf?id=n-UHRIdPju>Revisiting Populations in multi-agent Communication</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Paul Michel, Mathieu Rita, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n-UHRIdPju>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n-UHRIdPju.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite evidence from cognitive sciences that larger groups of speakers tend to develop more structured languages in human communication, scaling up to populations has failed to yield significant benefits in emergent multi-agent communication. In this paper we advocate for an alternate population-level training paradigm for referential games based on the idea of &#34;partitioning&#34; the agents into sender-receiver pairs and limiting co-adaptation across pairs. We show that this results in optimizing a different objective at the population level, where agents maximize (1) their respective &#34;internal&#34; communication accuracy and (2) some measure of alignment between agents. In experiments, we find that this leads to the emergence of languages that are significantly more compositional. Moreover, when agents are trained in populations that are not fully connected (ie. not all agent pairs interact at training time), this approach reduces multi-linguality and improves zero-shot communication with new agents (ie. agents are able to communicate successfully with other agents outside their training partners).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                665. <h><a href=https://openreview.net/pdf?id=zt53IDUR1U>MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, Yifei Xiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zt53IDUR1U>6.67 - [6, 8, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> New modeling perspective, new forecasting framework, linear complexity and best performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zt53IDUR1U.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, Transformer-based methods have achieved surprising performance in the field of long-term series forecasting, but the attention mechanism for computing global correlations entails high complexity. And they do not allow for targeted modeling of local features as CNN structures do. To solve the above problems, we propose to combine local features and global correlations to capture the overall view of time series (e.g., fluctuations, trends). To fully exploit the underlying information in the time series, a multi-scale branch structure is adopted to model different potential patterns separately. Each pattern is extracted with down-sampled convolution and isometric convolution for local features and global correlations, respectively. In addition to being more effective, our proposed method, termed as Multi-scale Isometric Convolution Network (MICN), is more efficient with linear complexity about the sequence length with suitable convolution kernels. Our experiments on six benchmark datasets show that compared with state-of-the-art methods, MICN yields 17.2% and 21.6% relative improvements for multivariate and univariate time series, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                666. <h><a href=https://openreview.net/pdf?id=k7p_YAO7yE>MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, Chang Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k7p_YAO7yE>6.67 - [8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a structured end-to-end framework for efficient online vectorized HD map construction.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k7p_YAO7yE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    High-definition (HD) map provides abundant and precise environmental information of the driving scene, serving as a fundamental and indispensable component for planning in autonomous driving system. We present MapTR, a structured end-to-end Transformer for efficient online vectorized HD map construction. We propose a unified permutation-equivalent modeling approach, i.e., modeling map element as a point set with a group of equivalent permutations, which accurately describes the shape of map element and stabilizes the learning process. We design a hierarchical query embedding scheme to flexibly encode structured map information and perform hierarchical bipartite matching for map element learning. MapTR achieves the best performance and efficiency with only camera input among existing vectorized map construction approaches on nuScenes dataset. In particular, MapTR-nano runs at real-time inference speed ($25.1$ FPS) on RTX 3090, $8\times$ faster than the existing state-of-the-art camera-based method while achieving $5.0$ higher mAP. Even compared with the existing state-of-the-art multi-modality method, MapTR-nano achieves $0.7$ higher mAP and $8\times$ faster inference speed, and MapTR-tiny achieves $13.5$ higher mAP and $3\times$ faster inference speed. Abundant qualitative results show that MapTR maintains stable and robust map construction quality in complex and various driving scenes. MapTR is of great application value in autonomous driving. Code and more demos are available at https://github.com/hustvl/MapTR.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                667. <h><a href=https://openreview.net/pdf?id=ALDM5SN2r7M>Robust Active Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Cenk Baykal, Khoa Trinh, Fotis Iliopoulos, Gaurav Menghani, Erik Vee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ALDM5SN2r7M>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new way of actively soft-labeling points in semi-supervised knowledge distillation to teach the student model in an efficient and robust way
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ALDM5SN2r7M.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Distilling knowledge from a large teacher model to a lightweight one is a widely successful approach for generating compact, powerful models in the semi-supervised learning setting where a limited amount of labeled data is available. In large-scale applications, however, the teacher tends to provide a large number of incorrect soft-labels that impairs student performance. The sheer size of the teacher additionally constrains the number of soft-labels that can be queried due to prohibitive computational and/or financial costs. The difficulty in achieving simultaneous \emph{efficiency} (i.e., minimizing soft-label queries) and \emph{robustness} (i.e., avoiding student inaccuracies due to incorrect labels) hurts the widespread application of knowledge distillation to many modern tasks. In this paper, we present a parameter-free approach with provable guarantees to query the soft-labels of points that are simultaneously informative and correctly labeled by the teacher. At the core of our work lies a game-theoretic formulation that explicitly considers the inherent trade-off between the informativeness and correctness of input instances. We establish bounds on the expected performance of our approach that hold even in worst-case distillation instances. We present empirical evaluations on popular benchmarks that demonstrate the improved distillation performance enabled by our work relative to that of state-of-the-art active learning and active distillation methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                668. <h><a href=https://openreview.net/pdf?id=p8coElqiSDw>Neural Architecture Design and Robustness: A Dataset</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Steffen Jung, Jovita Lukasik, Margret Keuper</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p8coElqiSDw>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p8coElqiSDw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning models have proven to be successful in a wide range of machine learning tasks. Yet, they are often highly sensitive to perturbations on the input data which can lead to incorrect decisions with high confidence, hampering their deployment for practical use-cases. Thus, finding architectures that are (more) robust against perturbations has received much attention in recent years. Just like the search for well-performing architectures in terms of clean accuracy, this usually involves a tedious trial-and-error process with one additional challenge: the evaluation of a network&#39;s robustness is significantly more expensive than its evaluation for clean accuracy. Thus, the aim of this paper is to facilitate better streamlined research on architectural design choices with respect to their impact on robustness as well as, for example, the evaluation of surrogate measures for robustness. We therefore borrow one of the most commonly considered search spaces for neural architecture search for image classification, NAS-Bench-201, which contains a manageable size of 6466 non-isomorphic network designs. We evaluate all these networks on a range of common adversarial attacks and corruption types and introduce a database on neural architecture design and robustness evaluations. We further present three exemplary use cases of this dataset, in which we (i) benchmark robustness measurements based on Jacobian and Hessian matrices for their robustness predictability, (ii) perform neural architecture search on robust accuracies, and (iii) provide an initial analysis of how architectural design choices affect robustness. We find that carefully crafting the topology of a network can have substantial impact on its robustness, where networks with the same parameter count range in mean adversarial robust accuracy from 20%-41%. Code and data is available at http://robustness.vision/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                669. <h><a href=https://openreview.net/pdf?id=-HHJZlRpGb>Learning Domain-Agnostic Representation for Disease Diagnosis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Churan Wang, Jing  Li, Xinwei Sun, Fandong Zhang, Yizhou Yu, Yizhou Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-HHJZlRpGb>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a disentanglement model in medical imaging diagnosis, in order to achieve robustness to multi centers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-HHJZlRpGb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In clinical environments, image-based diagnosis is desired to achieve robustness on multi-center samples. Toward this goal, a natural way is to capture only clinically disease-related features. However, such disease-related features are often entangled with center-effect, disabling robust transferring to unseen centers/domains. To disentangle disease-related features, we first leverage structural causal modeling to explicitly model disease-related and center-effects that are provable to be disentangled from each other. Guided by this, we propose a novel Domain Agnostic Representation Model (DarMo) based on variational Auto-Encoder. To facilitate disentanglement, we design domain-agnostic and domain-aware encoders to respectively capture disease-related features and varied center-effects by incorporating a domain-aware batch normalization layer. Besides, we constrain the disease-related features to well predict the disease label as well as clinical attributes, by leveraging Graph Convolutional Network (GCN) into our decoder. The effectiveness and utility of our method are demonstrated by the superior performance over others on both public datasets and inhouse datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                670. <h><a href=https://openreview.net/pdf?id=mhnHqRqcjYU>DFPC: Data flow driven pruning of coupled channels without data.</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tanay Narshana, Chaitanya Murti, Chiranjib Bhattacharyya</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mhnHqRqcjYU>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel data-free algorithm to accelerate neural networks via pruning coupled channels.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mhnHqRqcjYU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern, multi-branched neural network architectures often possess complex interconnections between layers, which we call coupled channels (CCs). Structured pruning of CCs in these multi-branch networks is an under-researched problem, as most existing works are typically designed for pruning single-branch models like VGG-nets. While these methods yield accurate subnetworks, the improvements in inference times when applied to multi-branch networks are comparatively modest, as these methods do not prune CCs, which we observe contribute significantly to inference time. For instance, layers with CCs as input or output take more than 66% of the inference time in ResNet-50. Moreover, pruning in the data-free regime, where data is not used for pruning, is gaining traction owing to privacy concerns and computational costs associated with fine-tuning. Motivated by this, we study the problem of pruning CCs in the data-free regime. To facilitate the development of algorithms to prune CCs, we define Data Flow Couplings (DFCs) to enumerate the layers that constitute coupled connections and the associated transformation. Additionally, saliencies for pruning CCs cannot be gauged in isolation, as there may be discrepancies among the layerwise importance of CCs using conventional scoring strategies. This necessitates finding grouped saliencies to gauge the importance of all corresponding coupled elements in a network. We thus propose the Backwards Graph-based Saliency Computation (BGSC) algorithm, a data-free method that computes saliencies by estimating an upper bound to the reconstruction error of intermediate layers; we call this pruning strategy Data Flow driven Pruning of Coupled channels (DFPC). Finally, we show the efficacy of DFPC for models trained on standard datasets. Since we pruned coupled channels, we achieve up to 1.66x improvements in inference time for ResNet-101 trained on CIFAR-10 with a 5% accuracy drop without fine-tuning. With access to the ImageNet training set, we achieve significant improvements over the data-free method and see an improvement of at least 47.1% in speedup for a 2.3% accuracy drop for ResNet-50 against our baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                671. <h><a href=https://openreview.net/pdf?id=TXPN6MtdSE4>Learning Sparse and Low-Rank Priors for Image Recovery via Iterative Reweighted Least Squares Minimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Stamatios Lefkimmiatis, Iaroslav Sergeevich Koshelev</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TXPN6MtdSE4>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TXPN6MtdSE4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work we introduce a novel optimization algorithm for image recovery under learned sparse and low-rank constraints, which are parameterized with weighted extensions of the $\ell_p^p$-vector and $\mathcal{S}_p^p$ Schatten-matrix quasi-norms for $0\!&lt;p\!\le1$, respectively. Our proposed algorithm generalizes the Iteratively Reweighted Least Squares (IRLS) method, used for signal recovery under $\ell_1$ and nuclear-norm constrained minimization. Further, we interpret our overall minimization approach as a recurrent network that we then employ to deal with inverse low-level computer vision problems. Thanks to the convergence guarantees that our IRLS strategy offers, we are able to train the derived reconstruction networks using a memory-efficient implicit back-propagation scheme, which does not pose any restrictions on their effective depth. To assess our networks&#39; performance, we compare them against other existing reconstruction methods on several inverse problems, namely image deblurring, super-resolution, demosaicking and sparse recovery. Our reconstruction results are shown to be very competitive and in many cases outperform those of existing unrolled networks, whose number of parameters is orders of magnitude higher than that of our learned models. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                672. <h><a href=https://openreview.net/pdf?id=n1bLgxHW6jW>Zeroth-Order Optimization with Trajectory-Informed Derivative Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yao Shu, Zhongxiang Dai, Weicong Sng, Arun Verma, Patrick Jaillet, Bryan Kian Hsiang Low</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n1bLgxHW6jW>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n1bLgxHW6jW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Zeroth-order (ZO) optimization, in which the derivative is unavailable, has recently succeeded in many important machine learning applications. Existing algorithms rely on finite difference (FD) methods for derivative estimation and gradient descent (GD)-based approaches for optimization. However, these algorithms suffer from query inefficiency because many additional function queries are required for derivative estimation in their every GD update, which typically hinders their deployment in real-world applications where every function query is expensive. To this end, we propose a trajectory-informed derivative estimation method which only employs the optimization trajectory (i.e., the history of function queries during optimization) and hence can eliminate the need for additional function queries to estimate a derivative. Moreover, based on our derivative estimation, we propose the technique of dynamic virtual updates, which allows us to reliably perform multiple steps of GD updates without reapplying derivative estimation. Based on these two contributions, we introduce the zeroth-order optimization with trajectory-informed derivative estimation (ZoRD) algorithm for query-efficient ZO optimization. We theoretically demonstrate that our trajectory-informed derivative estimation and our ZoRD algorithm improve over existing approaches, which is then supported by our real-world experiments such as black-box adversarial attack, non-differentiable metric optimization, and derivative-free reinforcement learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                673. <h><a href=https://openreview.net/pdf?id=yTbNYYcopd>Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben-Yaacov, Daniel Soudry</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yTbNYYcopd>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method to quantize all training matrix multiplication in 4 bit with standard formats
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yTbNYYcopd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Quantization of the weights and activations is one of the main methods to reduce the computational footprint of Deep Neural Networks (DNNs) training. Current methods enable 4-bit quantization of the forward phase. However, this constitutes only a third of the training process. Reducing the computational footprint of the entire training process requires the quantization of the neural gradients, i.e., the loss gradients with respect to the outputs of intermediate neural layers. 

Previous works separately showed that accurate 4-bit quantization of the neural gradients needs to (1) be unbiased and (2) have a log scale. However, no previous work aimed to combine both ideas, as we do in this work. Specifically, we examine the importance of having unbiased quantization in quantized neural network training, where to maintain it, and how to combine it with logarithmic quantization. Based on this, we suggest a $\textit{logarithmic unbiased quantization}$ (LUQ) method to quantize all both the forward and backward phase to 4-bit, achieving state-of-the-art results in 4-bit training without overhead. For example, in ResNet50 on ImageNet, we achieved a degradation of 1.1 %. We further improve this to degradation of only 0.32 % after three epochs of high precision fine-tunining, combined with a variance reduction method---where both these methods add overhead comparable to previously suggested methods. 
A reference implementation is supplied in the supplementary material.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                674. <h><a href=https://openreview.net/pdf?id=hQ4K9Bf4G2B>Behavior Prior Representation learning for Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongyu Zang, Xin Li, Jie Yu, Chen Liu, Riashat Islam, Remi Tachet des Combes, Romain Laroche</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hQ4K9Bf4G2B>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a state representation learning method with a surprisingly simple, easy-to-integrate objective based on behavior cloning of the dataset
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hQ4K9Bf4G2B.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning (RL) struggles in environments with rich and noisy inputs, where the agent only has access to a fixed dataset without environment interactions. Past works have proposed common workarounds based on the pre-training of state representations, followed by policy training. In this work, we introduce a simple, yet effective approach for learning state representations. Our method, Behavior Prior Representation (BPR), learns state representations with an easy-to-integrate objective based on behavior cloning of the dataset: we first learn a state representation by mimicking actions from the dataset, and then train a policy on top of the fixed representation, using any off-the-shelf Offline RL algorithm. Theoretically, we prove that BPR carries out performance guarantees when integrated into algorithms that have either policy improvement guarantees (conservative algorithms) or produce lower bounds of the policy values (pessimistic algorithms). Empirically, we show that BPR combined with existing state-of-the-art Offline RL algorithms leads to significant improvements across several offline control benchmarks. The code is available at \url{https://github.com/bit1029public/offline_bpr}
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                675. <h><a href=https://openreview.net/pdf?id=tQG-o3SeipT>On the Perils of Cascading Robust Classifiers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ravi Mangal, Zifan Wang, Chi Zhang, Klas Leino, Corina Pasareanu, Matt Fredrikson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tQG-o3SeipT>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our work reveals a critical pitfall of cascading certifiably robust models by showing that the seemingly beneficial strategy of cascading can actually hurt the robustness of the resulting ensemble.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tQG-o3SeipT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Ensembling certifiably robust neural networks is a promising approach for improving the \emph{certified robust accuracy} of neural models. 
Black-box ensembles that assume only query-access to the constituent models (and their robustness certifiers) during prediction are particularly attractive due to their modular structure. Cascading ensembles are a popular instance of black-box ensembles that appear to improve certified robust accuracies in practice. However, we show that the robustness certifier used by a cascading ensemble is unsound. That is, when a cascading ensemble is certified as locally robust at an input $x$ (with respect to $\epsilon$), there can be inputs $x&#39;$ in the $\epsilon$-ball centered at $x$, such that the cascade&#39;s prediction at $x&#39;$ is different from $x$ and thus the ensemble is not locally robust. Our theoretical findings are accompanied by empirical results that further demonstrate this unsoundness. We present a new attack against cascading ensembles and show that: (1) there exists an adversarial input for up to 88\% of the samples where the ensemble claims to be certifiably robust and accurate; and (2) the accuracy of a cascading ensemble under our attack is as low as 11\% when it claims to be certifiably robust and accurate on 97\% of the test set. Our work reveals a critical pitfall of cascading certifiably robust models by showing that the seemingly beneficial strategy of cascading can actually hurt the robustness of the resulting ensemble. Our code is available at https://github.com/TristaChi/ensembleKW.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                676. <h><a href=https://openreview.net/pdf?id=i8L9qoeZOS>A Theory of Dynamic Benchmarks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ali Shirali, Rediet Abebe, Moritz Hardt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=i8L9qoeZOS>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a formal model of dynamic benchmarks illuminating their benefits and limitations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/i8L9qoeZOS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dynamic benchmarks interweave model fitting and data collection in an attempt to mitigate the limitations of static benchmarks. In contrast to an extensive theoretical and empirical study of the static setting, the dynamic counterpart lags behind due to limited empirical studies and no apparent theoretical foundation to date. Responding to this deficit, we initiate a theoretical study of dynamic benchmarking. We examine two realizations, one capturing current practice and the other modeling more complex settings. In the first model, where data collection and model fitting alternate sequentially, we prove that model performance improves initially but can stall after only three rounds. Label noise arising from, for instance, annotator disagreement leads to even stronger negative results. Our second model generalizes the first to the case where data collection and model fitting have a hierarchical dependency structure. We show that this design guarantees strictly more progress than the first, albeit at a significant increase in complexity. We support our theoretical analysis by simulating dynamic benchmarks on two popular datasets. These results illuminate the benefits and practical limitations of dynamic benchmarking, providing both a theoretical foundation and a causal explanation for observed bottlenecks in empirical work.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                677. <h><a href=https://openreview.net/pdf?id=K9RHxPpjn2>Active Image Indexing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pierre Fernandez, Matthijs Douze, Herve Jegou, Teddy Furon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=K9RHxPpjn2>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In the context of image tracing, instead of watermarking an image with an ID, we slightly modify it to make its representation more indexing-friendly, which makes plain content-based indexing much more robust (62% → 100% accuracy for some settings).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/K9RHxPpjn2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Image copy detection and retrieval from large databases leverage two components. First, a neural network maps an image to a vector representation, that is relatively robust to various transformations of the image. Second, an efficient but approximate similarity search algorithm trades scalability (size and speed) against quality of the search, thereby introducing a source of error. 
This paper improves the robustness of image copy detection with active indexing, that optimizes the interplay of these two components.  We reduce the quantization loss of a given image representation by making imperceptible changes to the image before its release. The loss is back-propagated through the deep neural network back to the image, under perceptual constraints. These modifications make the image more retrievable. 
Our experiments show that the retrieval and copy detection of activated images is significantly improved. For instance, activation improves by $+40\%$ the Recall1@1 on various image transformations, and for several popular indexing structures based on product quantization and locality sensitivity hashing.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                678. <h><a href=https://openreview.net/pdf?id=0qSOodKmJaN>Calibrating Sequence likelihood Improves Conditional Language Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, Peter J Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0qSOodKmJaN>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A proposed sequence likelihood calibration stage improves fine-tuned conditional language models, leading to new state-of-the-art results in abstractive summarization, question generation, abstractive question answering and data-to-text.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0qSOodKmJaN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Conditional language models are predominantly trained with maximum likelihood estimation (MLE), giving probability mass to sparsely observed target sequences. While MLE trained models assign high probability to plausible sequences given the context, the model probabilities often do not accurately rank-order generated sequences by quality.  This has been empirically observed in beam search decoding as output quality degrading with large beam sizes,  and decoding strategies benefiting from heuristics such as length normalization and repetition-blocking. In this work, we introduce sequence likelihood calibration (SLiC) where the likelihood of model generated sequences are calibrated to better align with reference sequences in the model’s latent space.  With SLiC, decoding heuristics become unnecessary and decoding candidates’ quality significantly improves regardless of the decoding method. Furthermore, SLiC shows no sign of diminishing returns with model scale, and presents alternative ways to improve quality with limited training and inference budgets. With SLiC, we exceed or match SOTA results on a wide range of generation tasks spanning abstractive summarization, question generation, abstractive question answering and data-to-text generation, even with modest-sized models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                679. <h><a href=https://openreview.net/pdf?id=V7CYzdruWdm>Bias Propagation in Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongyan Chang, Reza Shokri</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=V7CYzdruWdm>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/V7CYzdruWdm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                680. <h><a href=https://openreview.net/pdf?id=sWOsRj4nT1n>Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guangji Bai, Chen Ling, Liang Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sWOsRj4nT1n>6.67 - [6, 8, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel framework is proposed to dynamically model how neural networks evolve across domains for characterizing the distribution drift across time in temporal domain generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sWOsRj4nT1n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Temporal domain generalization is a promising yet extremely challenging area where the goal is to learn models under temporally changing data distributions and generalize to unseen data distributions following the trends of the change. The advancement of this area is challenged by: 1) characterizing data distribution drift and its impacts on models, 2) expressiveness in tracking the model dynamics, and 3) theoretical guarantee on the performance. To address them, we propose a Temporal Domain Generalization with Drift-Aware Dynamic Neural Network (DRAIN) framework. Specifically, we formulate the problem into a Bayesian framework that jointly models the relation between data and model dynamics. We then build a recurrent graph generation scenario to characterize the dynamic graph-structured neural networks learned across different time points. It captures the temporal drift of model parameters and data distributions and can predict models in the future without the presence of future data. In addition, we explore theoretical guarantees of the model performance under the challenging temporal DG setting and provide theoretical analysis, including uncertainty and generalization error. Finally, extensive experiments on several real-world benchmarks with temporal drift demonstrate the proposed method’s effectiveness and efficiency.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                681. <h><a href=https://openreview.net/pdf?id=-M0TNnyWFT5>Task-Aware Information Routing from Common Representation Space in Lifelong Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Prashant Shivaram Bhat, Bahram Zonooz, Elahe Arani</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-M0TNnyWFT5>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A continual learning method that entails task-attention modules to capture task-specific information from the common representation space
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-M0TNnyWFT5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Intelligent systems deployed in the real world suffer from catastrophic forgetting when exposed to a sequence of tasks. Humans, on the other hand, acquire, consolidate, and transfer knowledge between tasks that rarely interfere with the consolidated knowledge.  Accompanied by self-regulated neurogenesis, continual learning in the brain is governed by the rich set of neurophysiological processes that harbor different types of knowledge which are then integrated by the conscious processing. Thus, inspired by Global Workspace Theory of conscious information access in the brain, we propose TAMiL, a continual learning method that entails task-attention modules to capture task-specific information from the common representation space. We employ simple, undercomplete autoencoders to create a communication bottleneck between the common representation space and the global workspace, allowing only the task-relevant information to the global workspace, thereby greatly reducing task interference. Experimental results show that our method outperforms state-of-the-art rehearsal-based and dynamic sparse approaches and bridges the gap between fixed capacity and parameter isolation approaches while being scalable. We also show that our method effectively mitigates catastrophic forgetting while being well-calibrated with reduced task-recency bias.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                682. <h><a href=https://openreview.net/pdf?id=7ynoX1ojPMt>OTOv2: Automatic, Generic, User-Friendly</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianyi Chen, Luming Liang, Tianyu DING, Zhihui Zhu, Ilya Zharkov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7ynoX1ojPMt>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7ynoX1ojPMt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The existing model compression methods via structured pruning typically require complicated multi-stage procedures. Each individual stage necessitates numerous engineering efforts and domain-knowledge from the end-users which prevent their wider applications onto broader scenarios. We propose the second generation of Only-Train-Once (OTOv2), which first automatically trains and compresses a general DNN only once from scratch to produce a more compact model with competitive performance without fine-tuning. OTOv2 is automatic and pluggable into various deep learning applications, and requires almost minimal engineering efforts from the users. Methodologically, OTOv2 proposes two major improvements: (i) Autonomy: automatically exploits the dependency of general DNNs, partitions the trainable variables into Zero-Invariant Groups (ZIGs), and constructs the compressed model; and (ii) Dual Half-Space Projected Gradient (DHSPG): a novel optimizer to more reliably solve structured-sparsity problems. Numerically, we demonstrate the generality and autonomy of OTOv2 on a variety of model architectures such as VGG, ResNet, CARN, ConvNeXt, DenseNet and StackedUnets, the majority of which cannot be handled by other methods without extensive handcrafting efforts. Together with benchmark datasets including CIFAR10/100, DIV2K, Fashion-MNIST, SVNH and ImageNet, its effectiveness is validated by performing competitively or even better than the state-of-the-arts. The source code is available at https://github.com/tianyic/only_train_once.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                683. <h><a href=https://openreview.net/pdf?id=YlGsTZODyjz>The Tilted Variational Autoencoder: Improving Out-of-Distribution Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Griffin Floto, Stefan Kremer, Mihai Nica</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YlGsTZODyjz>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A generalization of the Gaussian distribution improves the performance of out-of-distribution detection with variational autoencoders.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YlGsTZODyjz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A problem with using the Gaussian distribution as a prior for the variational autoencoder (VAE) is that the set on which Gaussians have high probability density is small as the latent dimension increases. This is an issue because VAEs try to attain both a high likelihood with respect to a prior distribution and at the same time, separation between points for better reconstruction. Therefore, a small volume in the high-density region of the prior is problematic because it restricts the separation of latent points.  To ameliorate this, we propose a simple generalization of the Gaussian distribution, called the tilted Gaussian, which has a maximum probability density occurring on a sphere instead of a single point. The tilted Gaussian has exponentially more volume in high-density regions than the standard Gaussian as a function of the distribution dimension. We empirically demonstrate that this simple change in the prior distribution improves VAE performance on the task of detecting unsupervised out-of-distribution (OOD) samples. We also introduce a new OOD testing procedure, called the Will-It-Move test, where the tilted Gaussian achieves remarkable OOD performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                684. <h><a href=https://openreview.net/pdf?id=lLp-C5nTdJG>Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>David Bieber, Rishab Goel, Dan Zheng, Hugo Larochelle, Daniel Tarlow</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lLp-C5nTdJG>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> For statically predicting runtime errors, the IPA-GNN scales to complex programs, models exception handling, and executes resource descriptions; it performs well and surprisingly localizes errors despite training without location supervision.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lLp-C5nTdJG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The execution behavior of a program often depends on external resources, such as program inputs or file contents, and so the program cannot be run in isolation. Nevertheless, software developers benefit from fast iteration loops where automated tools identify errors as early as possible, even before programs can be compiled and run. This presents an interesting machine learning challenge: can we predict runtime errors in a &#34;static&#34; setting, where program execution is not possible? Here, we introduce a competitive programming dataset and task for predicting runtime errors, which we show is difficult for generic models like Transformers. We approach this task by developing an interpreter-inspired architecture with an inductive bias towards mimicking program executions, which models exception handling and &#34;learns to execute&#34; descriptions of external resources. Surprisingly, we show that the model can also predict the locations of errors, despite being trained only on labels indicating error presence or absence and kind. In total, we present a practical and difficult-yet-approachable challenge problem related to learning program execution behavior and we demonstrate promising new capabilities of interpreter-inspired machine learning models for code.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                685. <h><a href=https://openreview.net/pdf?id=eKllxpLOOm>Combating Exacerbated Heterogeneity for Robust Models in Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianing Zhu, Jiangchao Yao, Tongliang Liu, quanming yao, Jianliang Xu, Bo Han</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eKllxpLOOm>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eKllxpLOOm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Privacy and security concerns in real-world applications have led to the development of adversarially robust federated models. However, the straightforward combination between adversarial training and federated learning in one framework can lead to the undesired robustness deterioration. We discover that the attribution behind this phenomenon is that the generated adversarial data could exacerbate the data heterogeneity among local clients, making the wrapped federated learning perform poorly. To deal with this problem, we propose a novel framework called Slack Federated Adversarial Training (SFAT), assigning the client-wise slack during aggregation to combat the intensified heterogeneity. Theoretically, we analyze the convergence of the proposed method to properly relax the objective when combining federated learning and adversarial training. Experimentally, we verify the rationality and effectiveness of SFAT on various benchmarked and real-world datasets with different adversarial training and federated optimization methods. The code is publicly available at: https://github.com/ZFancy/SFAT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                686. <h><a href=https://openreview.net/pdf?id=ju_Uqw384Oq>TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, Mingsheng Long</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ju_Uqw384Oq>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Based on the multi-periodicity, we analyze the intraperiod- and interperiod-variations in 2D space and propose the TimesNet as a task-general model, which achieves consistent state-of-the-art in five mainstream time series analysis tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ju_Uqw384Oq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Time series analysis is of immense importance in extensive applications, such as weather forecasting, anomaly detection, and action recognition. This paper focuses on temporal variation modeling, which is the common key problem of extensive analysis tasks. Previous methods attempt to accomplish this directly from the 1D time series, which is extremely challenging due to the intricate temporal patterns. Based on the observation of multi-periodicity in time series, we ravel out the complex temporal variations into the multiple intraperiod- and interperiod-variations. To tackle the limitations of 1D time series in representation capability, we extend the analysis of temporal variations into the 2D space by transforming the 1D time series into a set of 2D tensors based on multiple periods. This transformation can embed the intraperiod- and interperiod-variations into the columns and rows of the 2D tensors respectively, making the 2D-variations to be easily modeled by 2D kernels. Technically, we propose the TimesNet with TimesBlock as a task-general backbone for time series analysis. TimesBlock can discover the multi-periodicity adaptively and extract the complex temporal variations from transformed 2D tensors by a parameter-efficient inception block. Our proposed TimesNet achieves consistent state-of-the-art in five mainstream time series analysis tasks, including short- and long-term forecasting, imputation, classification, and anomaly detection. Code is available at this repository: https://github.com/thuml/TimesNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                687. <h><a href=https://openreview.net/pdf?id=ULnHxczCBaE>On the Convergence of AdaGrad(Norm) on $\mathbb{R}^d$: Beyond Convexity, Non-Asymptotic Rate and Acceleration</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zijian Liu, Ta Duy Nguyen, Alina Ene, Huy Nguyen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ULnHxczCBaE>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> New techniques to prove the convergence rate of AdaGrad and new accelerated adaptive algorithms without bounded domain assumption beyond standard convex and smooth functions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ULnHxczCBaE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing analysis of AdaGrad and other adaptive methods for smooth convex optimization is typically for functions with bounded domain diameter. In unconstrained problems, previous works guarantee an asymptotic convergence rate without an explicit constant factor that holds true for the entire function class. Furthermore, in the stochastic setting, only a modified version of AdaGrad, different from the one commonly used in practice, in which the latest gradient is not used to update the stepsize, has been analyzed. Our paper aims at bridging these gaps and developing a deeper understanding of AdaGrad and its variants in the standard setting of smooth convex functions as well as the more general setting of quasar convex functions. First, we demonstrate new techniques to explicitly bound the convergence rate of the vanilla AdaGrad for unconstrained problems in both deterministic and stochastic settings. Second, we propose a variant of AdaGrad for which we can show the convergence of the last iterate, instead of the average iterate. Finally, we give new accelerated adaptive algorithms and their convergence guarantee in the deterministic setting with explicit dependency on the problem parameters, improving upon the asymptotic rate shown in previous works. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                688. <h><a href=https://openreview.net/pdf?id=2nocgE1m0A>KnowDA: All-in-One Knowledge Mixture Model for Data Augmentation in Low-Resource NLP</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yufei Wang, Jiayi Zheng, Can Xu, Xiubo Geng, Tao Shen, Chongyang Tao, Daxin Jiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2nocgE1m0A>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a Knowledge Mixture Data Augmentation Model (KnowDA) that is trained with diverse NLP task knowledge. KnowDA could generate additional synthetic data to improve model performance in various low-resource NLP tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2nocgE1m0A.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper focuses on data augmentation for low-resource NLP tasks where the training set is limited. The existing solutions either leverage task-independent heuristic rules (e.g., Synonym Replacement) or fine-tune general-purpose pre-trained language models (e.g., GPT2) using the limited training instances to produce new synthetic data. Consequently, they have trivial task-specific knowledge and are limited to yielding low-quality synthetic data. To combat this issue, we propose Knowledge Mixture Data Augmentation Model (KnowDA), a Seq2Seq language model pretrained on a mixture of diverse NLP tasks under a novel framework of Knowledge Mixture Training (KoMT). The goal of KoMT is to condense diverse NLP task-specific knowledge into the single KnowDA model
(i.e., all-in-one). The resulting KnowDA could utilize these knowledge to quickly grasp the inherent synthesis law of the target task through limited training instances. Specifically, KoMT reformulates input examples from various heterogeneous NLP tasks into a unified text-to-text format and employs denoising training objectives in different granularity to learn to reconstruct partial or complete samples. To the best of our knowledge, we are the first to attempt to apply 100+ NLP multi-task training for data augmentation. Extensive experiments show that i) the synthetic data produced by KnowDA successfully improves the performance of the strong pre-trained language
models (i.e., Bert, ALBert and Deberta) by a large margin on the low-resource NLP benchmark FewGLUE, CoNLL’03 and WikiAnn; ii) KnowDA successful transfer the task knowledge to NLP tasks whose types are seen and unseen in KoMT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                689. <h><a href=https://openreview.net/pdf?id=fhcu4FBLciL>Efficient Model Updates for Approximate Unlearning of Graph-Structured Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eli Chien, Chao Pan, Olgica Milenkovic</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fhcu4FBLciL>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the approximate unlearning problem for graph-structured data with theoretical guarantees.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fhcu4FBLciL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With the adoption of recent laws ensuring the ``right to be forgotten&#39;&#39;, the problem of machine unlearning has become of significant importance. This is particularly the case for graph-structured data, and learning tools specialized for such data, including graph neural networks (GNNs). This work introduces the first known approach for \emph{approximate graph unlearning} with provable theoretical guarantees. The challenges in addressing the problem are two-fold. First, there exist multiple different types of unlearning requests that need to be considered, including node feature, edge and node unlearning. Second, to establish provable performance guarantees, one needs to carefully evaluate the process of feature mixing during propagation. We focus on analyzing Simple Graph Convolutions (SGC) and their generalized PageRank (GPR) extensions, thereby laying the theoretical foundations for unlearning GNNs. Empirical evaluations of six benchmark datasets demonstrate excellent performance/complexity/privacy trade-offs of our approach compared to complete retraining and general methods that do not leverage graph information. For example, unlearning $200$ out of $1208$ training nodes of the Cora dataset only leads to a $0.1\%$ loss in test accuracy, but offers a $4$-fold speed-up compared to complete retraining with a $(\epsilon,\delta)=(1,10^{-4})$ ``privacy cost&#39;&#39;. We also exhibit a $12\%$ increase in test accuracy for the same dataset when compared to unlearning methods that do not leverage graph information, with comparable time complexity and the same privacy guarantee.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                690. <h><a href=https://openreview.net/pdf?id=O8Vc52xFSUR>Quasi-optimal Reinforcement Learning with Continuous Actions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuhan Li, Wenzhuo Zhou, Ruoqing Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=O8Vc52xFSUR>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The paper proposes a novel learning algorithm for reliable continuous action allocations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/O8Vc52xFSUR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many real-world applications of reinforcement learning (RL) require making decisions in continuous action environments. In particular, determining the optimal dose level plays a vital role in developing medical treatment regimes. One challenge in adapting existing RL algorithms to medical applications, however, is that the popular infinite support stochastic policies, e.g., Gaussian policy, may assign riskily high dosages and harm patients seriously. Hence, it is important to induce a policy class whose support only contains near-optimal actions, and shrink the action-searching area for effectiveness and reliability. To achieve this, we develop a novel quasi-optimal learning algorithm, which can be easily optimized in off-policy settings with guaranteed convergence under general function approximations. Theoretically, we analyze the consistency, sample complexity, adaptability, and convergence of the proposed algorithm. We evaluate our algorithm with comprehensive simulated experiments and a dose suggestion real application to Ohio Type 1 diabetes dataset.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                691. <h><a href=https://openreview.net/pdf?id=COZDy0WYGg>Hungry Hungry Hippos: Towards Language Modeling with State Space Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, Christopher Re</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=COZDy0WYGg>6.67 - [6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the expressivity gap between state space models (SSMs) and attention on language modeling and reduce the hardware barrier between SSMs and attention.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/COZDy0WYGg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                692. <h><a href=https://openreview.net/pdf?id=9OmCr1q54Z>AE-FLOW: Autoencoders with Normalizing Flows  for  Medical Images Anomaly Detection </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuzhong Zhao, Qiaoqiao Ding, Xiaoqun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9OmCr1q54Z>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a normalizing flow based autoencoder for medical anomaly detection and it outperformed the other approaches by a large margin.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9OmCr1q54Z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Anomaly detection from medical images is an important task for clinical screening and diagnosis. In general, a large dataset of normal images are available while only few abnormal images can be collected in clinical practice. By mimicking the diagnosis process of radiologists, we attempt to tackle this problem by learning a tractable distribution of normal images and identify anomalies by differentiating the original image and the reconstructed normal image. More specifically, we propose a normalizing flow-based autoencoder for an efficient and tractable representation of normal medical images. The anomaly score consists of the likelihood originated from the normalizing  flow  and the reconstruction error of the autoencoder, which allows to identify the abnormality and provide an interpretability at both image and pixel levels. Experimental evaluation on two  medical images datasets showed that the proposed model outperformed the other approaches by a large margin, which validated the  effectiveness and robustness of the proposed method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                693. <h><a href=https://openreview.net/pdf?id=gwcQajoXNF>Computing all Optimal Partial Transports</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Abhijeet Phatak, Sharath Raghvendra, CHITTARANJAN TRIPATHY, Kaiyi Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gwcQajoXNF>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gwcQajoXNF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the classical version of the optimal partial transport problem. Let $\mu$ (with a mass of $U$) and $\nu$ (with a mass of $S$) be two discrete mass distributions with $S \le U$ and let $n$ be the total number of points in the supports of $\mu$ and $\nu$. For a parameter $\alpha \in [0,S]$, consider the minimum-cost transport plan  $\sigma_\alpha$ that transports a mass of $\alpha$ from $\nu$ to $\mu$. An \emph{OT-profile} captures the behavior of the cost of $\sigma_\alpha$ as $\alpha$ varies from $0$ to $S$. There is only limited work on OT-profile and its mathematical properties (see~\cite{figalli2010optimal}). In this paper, we present a novel framework to analyze the properties of the OT-profile and also present an algorithm to compute it.  When $\mu$ and $\nu$ are discrete mass distributions, we show that the OT-profile is a piecewise-linear non-decreasing convex function. Let $K$ be the combinatorial complexity of this function, i.e., the number of line segments required to represent the OT-profile. Our exact algorithm computes the OT-profile in $\tilde{O}(n^2K)$ time. Given $\delta &gt; 0$, we also show that the algorithm by ~\cite{lahn2019graph} can be used to $\delta$-approximate the OT-profile in $O(n^2/\delta + n/\delta^2)$ time. This approximation is a piecewise-linear function of a combinatorial complexity of $O(1/\delta)$.
An OT-profile is arguably more valuable than the OT-cost itself and can be used within applications. Under a reasonable assumption of outliers, we also show that the first derivative of the OT-profile sees a noticeable rise before any of the mass from outliers is transported. By using this property, we get an improved prediction accuracy for an outlier detection experiment. We also use this property to predict labels and estimate the class priors within PU-Learning experiments. Both these experiments are conducted on real datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                694. <h><a href=https://openreview.net/pdf?id=sIoED-yPK9l>Massively Scaling Heteroscedastic Classifiers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mark Collier, Rodolphe Jenatton, Basil Mustafa, Neil Houlsby, Jesse Berent, Effrosyni Kokiopoulou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sIoED-yPK9l>6.67 - [6, 8, 6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sIoED-yPK9l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Heteroscedastic classifiers, which learn a multivariate Gaussian distribution over prediction logits, have been shown to perform well on image classification problems with hundreds to thousands of classes. However, compared to standard classifiers, they introduce extra parameters that scale linearly with the number of classes. This makes them infeasible to apply to larger-scale problems. In addition heteroscedastic classifiers introduce a critical temperature hyperparameter which must be tuned. We propose HET-XL, a heteroscedastic classifier whose parameter count when compared to a standard classifier scales independently of the number of classes. In our large-scale settings, we show that we can remove the need to tune the temperature hyperparameter, by directly learning it on the training data. On large image classification datasets with up to 4B images and 30k classes our method requires 14X fewer additional parameters, does not require tuning the temperature on a held-out set and performs consistently better than the baseline heteroscedastic classifier. HET-XL improves ImageNet 0-shot classification in a multimodal contrastive learning setup which can be viewed as a 3.5 billion class classification problem.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                695. <h><a href=https://openreview.net/pdf?id=KKBMz-EL4tD>Alternating Differentiation for Optimization Layers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haixiang Sun, Ye Shi, Jingya Wang, Hoang Duong Tuan, H. Vincent Poor, Dacheng Tao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KKBMz-EL4tD>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new implicit differentiation framework (Alt-Diff) that decouples optimization layers in an alternating way to increase the computational speed. We also prove the convergence of Alt-Diff and show the upper bound of truncated error.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KKBMz-EL4tD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The idea of embedding optimization problems into deep neural networks as optimization layers to encode constraints and inductive priors has taken hold in recent years. Most existing methods focus on implicitly differentiating Karush–Kuhn–Tucker (KKT) conditions in a way that requires expensive computations on the Jacobian matrix, which can be slow and memory-intensive. In this paper, we developed a new framework, named Alternating Differentiation (Alt-Diff), that differentiates optimization problems (here, specifically in the form of convex optimization problems with polyhedral constraints) in a fast and recursive way. Alt-Diff decouples the differentiation procedure into a primal update and a dual update in an alternating way. Accordingly, Alt-Diff substantially decreases the dimensions of the Jacobian matrix especially for optimization with large-scale constraints and thus increases the computational speed of implicit differentiation. We show that the gradients obtained by Alt-Diff are consistent with those obtained by differentiating KKT conditions. In addition, we propose to truncate Alt-Diff to further accelerate the computational speed. Under some standard assumptions, we show that the truncation error of gradients is upper bounded by the same order of variables&#39; estimation error. Therefore, Alt-Diff can be truncated to further increase computational speed without sacrificing much accuracy. A series of comprehensive experiments validate the superiority of Alt-Diff. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                696. <h><a href=https://openreview.net/pdf?id=9krnQ-ue9M>Explicitly Minimizing the Blur Error of Variational Autoencoders</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gustav Bredell, Kyriakos Flouris, Krishna Chaitanya, Ertunc Erdil, Ender Konukoglu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9krnQ-ue9M>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new reconstruction term for VAEs that explicitly focuses on minimizing the blur of generated/reconstructed images while still optimizing the ELBO.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9krnQ-ue9M.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Variational autoencoders (VAEs) are powerful generative modelling methods, however they suffer from blurry generated samples and reconstructions compared to the images they have been trained on. Significant research effort has been spent to increase the generative capabilities by creating more flexible models but often flexibility comes at the cost of higher complexity and computational cost. Several works have focused on altering the reconstruction term of the evidence lower bound (ELBO), however, often at the expense of losing the mathematical link to maximizing the likelihood of the samples under the modeled distribution. Here we propose a new formulation of the reconstruction term for the VAE that specifically penalizes the generation of blurry images while at the same time still maximizing the ELBO under the modeled distribution.  
We show the potential of the proposed loss on three different data sets, where it outperforms several recently proposed reconstruction losses for VAEs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                697. <h><a href=https://openreview.net/pdf?id=XVjTT1nw5z>Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xingchao Liu, Chengyue Gong, qiang liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XVjTT1nw5z>6.67 - [6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XVjTT1nw5z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present rectified flow, a simple approach to learning (neural) ordinary differential equation (ODE) models to transport between two empirically observed distributions $\pi_0$ and $\pi_1$, hence providing a unified solution to generative modeling and domain transfer, among various other tasks involving distribution transport. The idea of rectified flow is to learn the ODE to follow the straight paths connecting the points drawn from $\pi_0$ and $\pi_1$ as much as possible. This is  achieved by solving a straightforward nonlinear least squares optimization problem, which can be easily scaled to large models without introducing extra parameters beyond standard supervised learning. The straight paths are the shortest paths between two points, and can be simulated exactly without time discretization and hence yield computationally efficient models. We show that, by learning a rectified flow from data, we effectively turn an arbitrary coupling of $\pi_0$ and $\pi_1$ to a  new deterministic coupling with provably non-increasing convex transport costs. In addition, with a ``reflow&#34; procedure that iteratively learns a new rectified flow from the data bootstrapped from the previous one, we obtain a sequence of flows with increasingly straight paths, which can be simulated accurately with coarse time discretization in the inference phase. In empirical studies, we show that rectified flow performs superbly on image generation, image-to-image translation, and domain adaptation. In particular, on image generation and translation, our method yields nearly straight flows that give high quality results even with \emph{a single Euler discretization step}. Code is available at \url{https://github.com/gnobitab/RectifiedFlow}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                698. <h><a href=https://openreview.net/pdf?id=N4K5ck-BTT>Scaffolding a Student to Instill Knowledge</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anil Kag, Durmus Alp Emre Acar, Aditya Gangrade, Venkatesh Saligrama</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=N4K5ck-BTT>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a novel KD scheme where the teacher scaffolds the student&#39;s prediction on hard-to-learn examples.  It smoothens student&#39;s loss landscape so that the student encounters fewer local minima. As a result it has good generalization properties.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/N4K5ck-BTT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a novel knowledge distillation (KD) method to selectively instill teacher knowledge into a student model motivated by situations where the student&#39;s capacity is significantly smaller than that of the teachers. In vanilla KD, the teacher primarily sets a predictive target for the student to follow, and we posit that this target is overly optimistic due to the student&#39;s lack of capacity. We develop a novel scaffolding scheme where the teacher, in addition to setting a predictive target, also scaffolds the student&#39;s prediction by censoring hard-to-learn examples. Scaffolding utilizes the same information as the teacher&#39;s soft-max predictions as inputs, and in this sense, our proposal can be viewed as a natural variant of vanilla KD. We show on synthetic examples that censoring hard-examples leads to smoothening the student&#39;s loss landscape so that the student encounters fewer local minima. As a result, it has good generalization properties. Against vanilla KD, we achieve improved performance and are comparable to more intrusive techniques that leverage feature matching on benchmark datasets.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                699. <h><a href=https://openreview.net/pdf?id=k8_yVW3Wqln>Systematic Rectification of Language Models via Dead-end Analysis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Meng Cao, Mehdi Fatemi, Jackie CK Cheung, Samira Shabanian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k8_yVW3Wqln>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k8_yVW3Wqln.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. We believe this is important since many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach generates notably better results compared to the base LLMs and other techniques in terms of the overall language and detoxification performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                700. <h><a href=https://openreview.net/pdf?id=BGF9IeDfmlH>Learning to Linearize Deep Neural Networks  for Secure and Efficient Private Inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Tiffany Liu, Peter Anthony Beerel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=BGF9IeDfmlH>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present an automated linearization method to train a DNN with limited ReLU budget for inference in yielding models able to perform significantly better than exiting private inference SOTA both in terms of potentially improved latency and accuracy.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/BGF9IeDfmlH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The large number of ReLU non-linearity operations in existing deep neural networks makes them ill-suited for latency-efficient private inference (PI). Existing techniques to reduce ReLU operations often involve manual effort and sacrifice significant accuracy. In this paper, we first present a novel measure of non-linearity layers’ ReLU sensitivity, enabling mitigation of the time-consuming manual efforts in identifying the same. Based on this sensitivity, we then present SENet, a three-stage training method that for a given ReLU budget, automatically assigns per-layer ReLU counts, decides the ReLU locations for each layer’s activation map, and trains a model with significantly fewer ReLUs to potentially yield latency and communication efficient PI. Experimental evaluations with multiple models on various datasets show SENet’s superior performance both in terms of reduced ReLUs and improved classification accuracy compared to existing alternatives. In particular, SENet can yield models that require up to ∼2× fewer ReLUs while yielding similar accuracy. For a similar ReLU budget SENet can yield models with ∼2.32% improved classification accuracy, evaluated on CIFAR-100.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                701. <h><a href=https://openreview.net/pdf?id=7tJyBmu9iCj>Neural-based classification rule learning for sequential data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marine Collery, Philippe Bonnard, François Fages, Remy Kusters</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7tJyBmu9iCj>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7tJyBmu9iCj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Discovering interpretable patterns for classification of sequential data is of key importance for a variety of fields, ranging from genomics to fraud detection or more generally interpretable decision-making.
In this paper, we propose a novel differentiable fully interpretable method to discover both local and global patterns (i.e. catching a relative or absolute temporal dependency) for rule-based binary classification.
It consists of a convolutional binary neural network with an interpretable neural filter and a training strategy based on dynamically-enforced sparsity.
We demonstrate the validity and usefulness of the approach on synthetic datasets and on an open-source peptides dataset.
Key to this end-to-end differentiable method is that the expressive patterns used in the rules are learned alongside the rules themselves.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                702. <h><a href=https://openreview.net/pdf?id=jsZsEd8VEY>Grounding Graph Network Simulators using Physical Sensor Observations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jonas Linkerhägner, Niklas Freymuth, Paul Maria Scheikl, Franziska Mathis-Ullrich, Gerhard Neumann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jsZsEd8VEY>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We ground Graph Network Simulators with physical sensor information to resolve uncertainties and improve long-term prediction quality.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jsZsEd8VEY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Physical simulations that accurately model reality are crucial for many engineering disciplines such as mechanical engineering and robotic motion planning. In recent years, learned Graph Network Simulators produced accurate mesh-based simulations while requiring only a fraction of the computational cost of traditional simulators. Yet, the resulting predictors are confined to learning from data generated by existing mesh-based simulators and thus cannot include real world sensory information such as point cloud data. As these predictors have to simulate complex physical systems from only an initial state, they exhibit a high error accumulation for long-term predictions. In this work, we integrate sensory information to ground Graph Network Simulators on real world observations. In particular, we predict the mesh state of deformable objects by utilizing point cloud data. The resulting model allows for accurate predictions over longer time horizons, even under uncertainties in the simulation, such as unknown material properties. Since point clouds are usually not available for every time step, especially in online settings, we employ an imputation-based model. The model can make use of such additional information only when provided, and resorts to a standard Graph Network Simulator, otherwise. We experimentally validate our approach on a suite of prediction tasks for mesh-based interactions between soft and rigid bodies. Our method results in utilization of additional point cloud information to accurately predict stable simulations where existing Graph Network Simulators fail.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                703. <h><a href=https://openreview.net/pdf?id=uqg3FhRZaq>On the complexity of nonsmooth automatic differentiation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jerome Bolte, Ryan Boustany, Edouard Pauwels, Béatrice Pesquet-Popescu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uqg3FhRZaq>6.67 - [8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Backpropagation of nonsmooth  gradients is proved to be a fast/cheap process for the vast class of locally Lipschitz semi-algebraic functions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uqg3FhRZaq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Using the notion of conservative gradient, we provide a simple model to estimate the computational costs of the backward and forward modes of algorithmic differentiation for a wide class of nonsmooth programs. The complexity overhead of the backward mode turns out to be independent of the dimension when using programs with locally Lipschitz semi-algebraic or definable elementary functions. This extends considerably the Baur-Strassen&#39;s smooth cheap gradient principle. We illustrate our results by establishing fast backpropagation results of conservative gradients through feedforward neural networks with standard activation and  loss functions. Nonsmooth backpropagation&#39;s cheapness contrasts with concurrent forward approaches, which have, to this day, dimensional-dependent worst case  overhead estimates. We provide further results suggesting the superiority of backward propagation of conservative gradients. Indeed, we relate the complexity of computing a large number of directional derivatives to that of matrix multiplication, and we show that finding two subgradients in the Clarke subdifferential of a function is a NP-hard problem.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                704. <h><a href=https://openreview.net/pdf?id=rzrqh85f4Sc>Towards Addressing Label Skews in One-Shot Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yiqun Diao, Qinbin Li, Bingsheng He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rzrqh85f4Sc>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose FedOV to significantly improve the test accuracy under diverse label skews in one-shot federated learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rzrqh85f4Sc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning (FL) has been a popular research area, where multiple clients collaboratively train a model without sharing their local raw data. Among existing FL solutions, one-shot FL is a promising and challenging direction, where the clients conduct FL training with a single communication round. However, while label skew is a common real-world scenario where some clients may have few or no data of some classes, existing one-shot FL approaches that conduct voting on the local models are not able to produce effective global models. Due to the limited number of classes in each party, the local models misclassify the data from unseen classes into seen classes, which leads to very ineffective global models from voting. To address the label skew issue in one-shot FL, we propose a novel approach named FedOV which generates diverse outliers and introduces them as an additional unknown class in local training to improve the voting performance. Specifically, based on open-set recognition, we propose novel outlier generation approaches by corrupting the original features and further develop adversarial learning to enhance the outliers. Our extensive experiments show that FedOV can significantly improve the test accuracy compared to state-of-the-art approaches in various label skew settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                705. <h><a href=https://openreview.net/pdf?id=4rXMRuoJlai>Mind&#39;s Eye: Grounded Language Model Reasoning through Simulation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M. Dai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4rXMRuoJlai>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a new reasoning paradigm that grounds language model reasoning on simulation results from the advanced physics engine MuJoCo.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4rXMRuoJlai.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world---their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind&#39;s Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind&#39;s MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind&#39;s Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind&#39;s Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind&#39;s Eye through ablation studies.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                706. <h><a href=https://openreview.net/pdf?id=ugA1HX69sf>Understanding Embodied Reference with Touch-Line Transformer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yang Li, Xiaoxue Chen, Hao Zhao, Jiangtao Gong, Guyue Zhou, Federico Rossano, Yixin Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ugA1HX69sf>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> People often refer to objects using both language expression and pointing gesture at the same time. Our model locate these objects (referents) more accurately using the virtual touch line. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ugA1HX69sf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study embodied reference understanding, the task of locating referents using embodied gestural signals and language references. Human studies have revealed that, contrary to popular belief, objects referred to or pointed to do not lie on the elbow-wrist line, but rather on the so-called virtual touch line. Nevertheless, contemporary human pose representations lack the virtual touch line. To tackle this problem, we devise the touch-line Transformer: It takes as input tokenized visual and textual features and simultaneously predicts the referent’s bounding box and a touch-line vector. Leveraging this touch-line prior, we further devise a geometric consistency loss that promotes co-linearity between referents and touch lines. Using the touch line as gestural information dramatically improves model performances: Experiments on the YouRefIt dataset demonstrate that our method yields a +25.0% accuracy improvement under the 0.75 IoU criterion, hence closing 63.6% of the performance difference between models and humans. Furthermore, we computationally validate prior human studies by demonstrating that computational models more accurately locate referents when employing the virtual touch line than when using the elbow-wrist line.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                707. <h><a href=https://openreview.net/pdf?id=sKWlRDzPfd7>MAESTRO: Open-Ended Environment Design for Multi-Agent Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mikayel Samvelyan, Akbir Khan, Michael D Dennis, Minqi Jiang, Jack Parker-Holder, Jakob Nicolaus Foerster, Roberta Raileanu, Tim Rocktäschel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sKWlRDzPfd7>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sKWlRDzPfd7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Open-ended learning methods that automatically generate a curriculum of increasingly challenging tasks serve as a promising avenue toward generally capable reinforcement learning agents. Existing methods adapt curricula independently over either environment parameters (in single-agent settings) or co-player policies (in multi-agent settings). However, the strengths and weaknesses of co-players can manifest themselves differently depending on environmental features. It is thus crucial to consider the dependency between the environment and co-player when shaping a curriculum in multi-agent domains. In this work, we use this insight and extend Unsupervised Environment Design (UED) to multi-agent environments. We then introduce Multi-Agent Environment Design Strategist for Open-Ended Learning (MAESTRO), the first multi-agent UED approach for two-player zero-sum settings. MAESTRO efficiently produces adversarial, joint curricula over both environments and co-players and attains minimax-regret guarantees at Nash equilibrium. Our experiments show that MAESTRO outperforms a number of strong baselines on competitive two-player games, spanning discrete and continuous control settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                708. <h><a href=https://openreview.net/pdf?id=0qmwFNJyxCL>Rethinking the Effect of Data Augmentation in Adversarial Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rundong Luo, Yifei Wang, Yisen Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0qmwFNJyxCL>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We revisit adversarial contrastive training through the lens of data augmentation, and propose an effective adversarial contrastive framework that outperforms vanilla supervised adversarial robustness.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0qmwFNJyxCL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent works have shown that self-supervised learning can achieve remarkable robustness when integrated with adversarial training (AT). However, the robustness gap between supervised AT (sup-AT) and self-supervised AT (self-AT) remains significant. Motivated by this observation, we revisit existing self-AT methods and discover an inherent dilemma that affects self-AT robustness: either strong or weak data augmentations are harmful to self-AT, and a medium strength is insufficient to bridge the gap. To resolve this dilemma, we propose a simple remedy named DYNACL (Dynamic Adversarial Contrastive Learning). In particular, we propose an augmentation schedule that gradually anneals from a strong augmentation to a weak one to benefit from both extreme cases. Besides, we adopt a fast post-processing stage for adapting it to downstream tasks. Through extensive experiments, we show that DYNACL can improve state-of-the-art self-AT robustness by 8.84% under Auto-Attack on the CIFAR-10 dataset, and can even outperform vanilla supervised adversarial training for the first time. Our code is available at \url{https://github.com/PKU-ML/DYNACL}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                709. <h><a href=https://openreview.net/pdf?id=-RwZOVybbj>Risk-Aware Reinforcement Learning with Coherent Risk Measures and Non-linear Function Approximation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thanh Lam, Arun Verma, Bryan Kian Hsiang Low, Patrick Jaillet</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-RwZOVybbj>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a unified framework to analyze the regret of risk-aware RL policy that uses a coherent risk measure in conjunction with non-linear function approximation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-RwZOVybbj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the risk-aware reinforcement learning (RL) problem in the episodic finite-horizon Markov decision process with unknown transition and reward functions. In contrast to the risk-neutral RL problem, we consider minimizing the risk of having low rewards, which arise due to the intrinsic randomness of the MDPs and imperfect knowledge of the model. Our work provides a unified framework to analyze the regret of risk-aware RL policy with coherent risk measures in conjunction with non-linear function approximation, which gives the first sub-linear regret bounds in the setting. Finally, we validate our theoretical results via empirical experiments on synthetic and real-world data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                710. <h><a href=https://openreview.net/pdf?id=My57qBufZWs>Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=My57qBufZWs>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an optimizer, Bort, for training explainable neural networks with boundedness and orthogonality constraints.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/My57qBufZWs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning has revolutionized human society, yet the black-box nature of deep neural networks hinders further application to reliability-demanded industries. In the attempt to unpack them, many works observe or impact internal variables to improve the comprehensibility and invertibility of the black-box models. However, existing methods rely on intuitive assumptions and lack mathematical guarantees. To bridge this gap, we introduce Bort, an optimizer for improving model explainability with boundedness and orthogonality constraints on model parameters, derived from the sufficient conditions of model comprehensibility and invertibility. We perform reconstruction and backtracking on the model representations optimized by Bort and observe a clear improvement in model explainability. Based on Bort, we are able to synthesize explainable adversarial samples without additional parameters and training. Surprisingly, we find Bort constantly improves the classification accuracy of various architectures including ResNet and DeiT on MNIST, CIFAR-10, and ImageNet. Code: https://github.com/zbr17/Bort.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                711. <h><a href=https://openreview.net/pdf?id=PYbe4MoHf32>Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Linfeng Zhao, Huazhe Xu, Lawson L.S. Wong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PYbe4MoHf32>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PYbe4MoHf32.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation and needs to balance forward planner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                712. <h><a href=https://openreview.net/pdf?id=_9k5kTgyHT>Globally Optimal Training of Neural Networks with Threshold Activation Functions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tolga Ergen, Halil Ibrahim Gulluk, Jonathan Lacotte, Mert Pilanci</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_9k5kTgyHT>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that training problem of regularized deep neural networks with threshold activations can be equivalently formulated as a standard convex optimization problem, which parallels the LASSO method.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_9k5kTgyHT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Threshold activation functions are highly preferable in neural networks due to their efficiency in hardware implementations. Moreover, their mode of operation is more interpretable and resembles that of biological neurons. However, traditional gradient based algorithms such as Gradient Descent cannot be used to train the parameters of neural networks with threshold activations since the activation function has zero gradient except at a single non-differentiable point. To this end, we study weight decay regularized training problems of deep neural networks with threshold activations. We first show that regularized deep threshold network training problems can be equivalently formulated as a standard convex optimization problem, which parallels the LASSO method, provided that the last hidden layer width exceeds a certain threshold. We also derive a simplified convex optimization formulation when the dataset can be shattered at a certain layer of the network. We corroborate our theoretical results with various numerical experiments.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                713. <h><a href=https://openreview.net/pdf?id=r9fX833CsuN>Curriculum-based Co-design of Morphology and Control of Voxel-based Soft Robots</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuxing Wang, Shuang Wu, Haobo Fu, QIANG FU, Tiantian Zhang, Yongzhe Chang, Xueqian Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=r9fX833CsuN>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Curriculum-based Co-design of Morphology and Control of Voxel-based Soft Robots
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/r9fX833CsuN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Co-design of morphology and control of a Voxel-based Soft Robot (VSR) is challenging due to the notorious bi-level optimization. In this paper, we present a Curriculum-based Co-design (CuCo) method for learning to design and control VSRs through an easy-to-difficult process. Specifically, we expand the design space from a small size to the target size gradually through a predefined curriculum. At each learning stage of the curriculum, we use reinforcement learning to simultaneously train the design policy and the control policy, which is enabled by incorporating the design process into the environment and using differentiable policy representations. The converged morphology and the learned policies from last stage are inherited and then serve as the starting point for the next stage. In empirical studies, we show that CuCo is more efficient in creating larger robots with better performance by reusing the practical design and control patterns learned within each stage, in comparison to prior approaches that learn from scratch in the space of target size.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                714. <h><a href=https://openreview.net/pdf?id=isiQ5KIXbjj>QuAnt: Quantum Annealing with Learnt Couplings</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marcel Seelbach Benkner, Maximilian Krahn, Edith Tretschk, Zorah Lähner, Michael Moeller, Vladislav Golyanik</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=isiQ5KIXbjj>6.67 - [6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/isiQ5KIXbjj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern quantum annealers can find high-quality solutions to combinatorial optimisation objectives given as quadratic unconstrained binary optimisation (QUBO) problems. Unfortunately, obtaining suitable QUBO forms in computer vision remains challenging and currently requires problem-specific analytical derivations. Moreover, such explicit formulations impose tangible constraints on solution encodings. In stark contrast to prior work, this paper proposes to learn QUBO forms from data through gradient backpropagation instead of deriving them. As a result, the solution encodings can be  chosen flexibly and compactly. Furthermore, our methodology is general and virtually independent of the specifics of the target problem type. We demonstrate the advantages of learnt  QUBOs on the diverse problem types of graph matching, 2D point cloud alignment and 3D rotation estimation. Our results are competitive with the previous quantum state of the art while requiring much fewer logical and physical qubits, enabling our method to scale to larger problems. The code and the new dataset are available at https://4dqv.mpi-inf.mpg.de/QuAnt/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                715. <h><a href=https://openreview.net/pdf?id=MTTPLcwvqTt>The Provable Benefit of Unsupervised Data Sharing for Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao Hu, Yiqin Yang, Qianchuan Zhao, Chongjie Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MTTPLcwvqTt>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a principled way to leverage unlabeled offline RL dataset with guarantees in linear MDPs and it outperforms previous methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MTTPLcwvqTt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-supervised methods have become crucial for advancing deep learning by leveraging data itself to reduce the need for expensive annotations. However, the question of how to conduct self-supervised offline reinforcement learning (RL) in a principled way remains unclear.
In this paper, we address this issue by investigating the theoretical benefits of utilizing reward-free data in linear Markov Decision Processes (MDPs) within a semi-supervised setting. Further, we propose a novel, Provable Data Sharing algorithm (PDS) to utilize such reward-free data for offline RL. PDS uses additional penalties on the reward function learned from labeled data to prevent overestimation, ensuring a conservative algorithm. Our results on various offline RL tasks demonstrate that PDS significantly improves the performance of offline RL algorithms with reward-free data. Overall, our work provides a promising approach to leveraging the benefits of unlabeled data in offline RL while maintaining theoretical guarantees. We believe our findings will contribute to developing more robust self-supervised RL methods.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                716. <h><a href=https://openreview.net/pdf?id=1C6nCCaRe6p>A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Brandon Trabucco, Gunnar A Sigurdsson, Robinson Piramuthu, Gaurav S. Sukhatme, Ruslan Salakhutdinov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1C6nCCaRe6p>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A System For Exploring A Scene, Mapping Objects, and Rearranging Objects To A Visual Goal
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1C6nCCaRe6p.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Physically rearranging objects is an important capability for embodied agents. Visual room rearrangement evaluates an agent&#39;s ability to rearrange objects in a room to a desired goal based solely on visual input. We propose a simple yet effective method for this problem: (1) search for and map which objects need to be rearranged, and (2) rearrange each object until the task is complete. Our approach consists of an off-the-shelf semantic segmentation model, voxel-based semantic map, and semantic search policy to efficiently find objects that need to be rearranged. Our method was the winning submission to the AI2-THOR Rearrangement Challenge in the 2022 Embodied AI Workshop at CVPR 2022, and improves on current state-of-the-art end-to-end reinforcement learning-based methods that learn visual room rearrangement policies from 0.53% correct rearrangement to 16.56%, using only 2.7% as many samples from the environment.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                717. <h><a href=https://openreview.net/pdf?id=D7srTrGhAs>HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bing Yin, Tuo Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=D7srTrGhAs>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel task-agnostic distillation method for Transformer-based language models equipped with iterative pruning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/D7srTrGhAs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge distillation has been shown to be a powerful model compression approach to facilitate the deployment of pre-trained language models in practice. This paper focuses on task-agnostic distillation. It produces a compact pre-trained model that can be easily fine-tuned on various tasks with small computational costs and memory footprints. Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher&#39;s over a massive amount of open-domain training data. Such a large prediction discrepancy often diminishes the benefits of knowledge distillation. To address this challenge, we propose Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning. Specifically, we initialize the student model from the teacher model, and iteratively prune the student&#39;s neurons until the target width is reached. Such an approach maintains a small discrepancy between the teacher&#39;s and student&#39;s predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer. Extensive experiments demonstrate that HomoDistil achieves significant improvements on existing baselines. Our codes will be released.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                718. <h><a href=https://openreview.net/pdf?id=FIrQfNSOoTr>Instance-wise Batch Label Restoration via Gradients in Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kailang Ma, Yu Sun, Jian Cui, Dawei Li, Zhenyu Guan, Jianwei Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FIrQfNSOoTr>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an analytic method to perform instance-wise batch label restoration and enhance the existing gradient inversion attacks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FIrQfNSOoTr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Gradient inversion attacks have posed a serious threat to the privacy of federated learning. The attacks search for the optimal pair of input and label best matching the shared gradients and the search space of the attacks can be reduced by pre-restoring labels. Recently, label restoration technique allows for the extraction of labels from gradients analytically, but even the state-of-the-art remains limited to identify the presence of categories (i.e., the class-wise label restoration). This work considers the more real-world settings, where there are multiple instances of each class in a training batch. An analytic method is proposed to perform instance-wise batch label restoration from only the gradient of the final layer. On the basis of the approximate recovered class-wise embeddings and post-softmax probabilities, we establish linear equations of the gradients, probabilities and labels to derive the Number of Instances (NoI) per class by the Moore-Penrose pseudoinverse algorithm. Our experimental evaluations reach over 99% Label existence Accuracy (LeAcc) and exceed 96% Label number Accuracy (LnAcc) in most cases on three image datasets and four classification models. The two metrics are used to evaluate class-wise and instance-wise label restoration accuracy, respectively. And the recovery is made feasible even with a batch size of 4096 and partially negative activations (e.g., Leaky ReLU and Swish). Furthermore, we demonstrate that our method facilitates the existing gradient inversion attacks by exploiting the recovered labels, with an increase of 6-7 in PSNR on both MNIST and CIFAR100. Our code is
available at https://github.com/BUAA-CST/iLRG.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                719. <h><a href=https://openreview.net/pdf?id=lGz9u1ubUXE>Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lin Guan, Karthik Valmeekam, Subbarao Kambhampati</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lGz9u1ubUXE>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lGz9u1ubUXE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generating complex behaviors that satisfy the preferences of non-expert users is a crucial requirement for AI agents. Interactive reward learning from trajectory comparisons (a.k.a. RLHF) is one way to allow non-expert users to convey complex objectives by expressing preferences over short clips of agent behaviors. Even though this parametric method can encode complex tacit knowledge present in the underlying tasks, it implicitly assumes that the human is unable to provide richer feedback than binary preference labels, leading to intolerably high feedback complexity and poor user experience. While providing a detailed symbolic closed-form specification of the objectives might be tempting, it is not always feasible even for an expert user. However, in most cases, humans are aware of how the agent should change its behavior along meaningful axes to fulfill their underlying purpose, even if they are not able to fully specify task objectives symbolically. Using this as motivation, we introduce the notion of Relative Behavioral Attributes, which allows the users to tweak the agent behavior through symbolic concepts (e.g., increasing the softness or speed of agents&#39; movement). We propose two practical methods that can learn to model any kind of behavioral attributes from ordered behavior clips. We demonstrate the effectiveness of our methods on four tasks with nine different behavioral attributes, showing that once the attributes are learned, end users can produce desirable agent behaviors relatively effortlessly, by providing feedback just around ten times. This is over an order of magnitude less than that required by the popular learning-from-human-preferences baselines. The supplementary video and source code are available at: https://guansuns.github.io/pages/rba.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                720. <h><a href=https://openreview.net/pdf?id=fjh7UGQgOB>Rethinking Graph Lottery Tickets: Graph Sparsity Matters</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bo Hui, Da Yan, Xiaolong Ma, Wei-Shinn Ku</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fjh7UGQgOB>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fjh7UGQgOB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Lottery Ticket Hypothesis (LTH) claims the existence of a winning ticket (i.e., a properly pruned sub-network together with original weight initialization) that can achieve competitive performance to the original dense network. A recent work, called UGS, extended LTH to prune graph neural networks (GNNs) for effectively accelerating GNN inference. UGS simultaneously prunes the graph adjacency matrix and the model weights using the same masking mechanism, but since the roles of the graph adjacency matrix and the weight matrices are very different, we find that their sparsifications lead to different performance characteristics. Specifically, we find that the performance of a sparsified GNN degrades significantly when the graph sparsity goes beyond a certain extent. Therefore, we propose two techniques to improve GNN performance when the graph sparsity is high. First, UGS prunes the adjacency matrix using a loss formulation which, however, does not properly involve all elements of the adjacency matrix; in contrast, we add a new auxiliary loss head to better guide the edge pruning by involving the entire adjacency matrix. Second, by regarding unfavorable graph sparsification as adversarial data perturbations, we formulate the pruning process as a min-max optimization problem to gain the robustness of lottery tickets when the graph sparsity is high. We further investigate the question: Can the ``retrainable&#39;&#39; winning ticket of a GNN be also effective for graph transferring learning? We call it the transferable graph lottery ticket (GLT) hypothesis. Extensive experiments were conducted which demonstrate the superiority of our proposed sparsification method over UGS, and which empirically verified our transferable GLT hypothesis.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                721. <h><a href=https://openreview.net/pdf?id=jBEXnEMdNOL>Fairness and Accuracy under Domain Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thai-Hoang Pham, Xueru Zhang, Ping Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jBEXnEMdNOL>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents (1) theoretical bounds for fairness and accuracy under domain generalization, and (2) a proposed model that can achieve good fairness and accuracy in an unseen target domain through invariant representation learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jBEXnEMdNOL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As machine learning (ML) algorithms are increasingly used in high-stakes applications, concerns have arisen that they may be biased against certain social groups. Although many approaches have been proposed to make ML models fair, they typically rely on the assumption that data distributions in training and deployment are identical. Unfortunately, this is commonly violated in practice and a model that is fair during training may lead to an unexpected outcome during its deployment. Although the problem of designing robust ML models under dataset shifts has been widely studied, most existing works focus only on the transfer of accuracy. In this paper, we study the transfer of both fairness and accuracy under domain generalization where the data at test time may be sampled from never-before-seen domains. We first develop theoretical bounds on the unfairness and expected loss at deployment, and then derive sufficient conditions under which fairness and accuracy can be perfectly transferred via invariant representation learning. Guided by this, we design a learning algorithm such that fair ML models learned with training data still have high fairness and accuracy when deployment environments change. Experiments on real-world data validate the proposed algorithm.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                722. <h><a href=https://openreview.net/pdf?id=0Vv4H4Ch0la>Capturing the Motion of Every Joint: 3D Human Pose and Shape Estimation with Independent Tokens</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sen Yang, Wen Heng, Gang Liu, GUOZHONG LUO, Wankou Yang, Gang YU</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0Vv4H4Ch0la>6.67 - [8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel, effective and robust model with designed independent tokens to estimate 3D human pose and shape from monocular videos
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0Vv4H4Ch0la.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper we present a novel method to estimate 3D human pose and shape from monocular videos. This task requires directly recovering pixel-alignment 3D human pose and body shape from monocular images or videos, which is challenging due to its inherent ambiguity. To improve precision, existing methods highly rely on the initialized mean pose and shape as prior estimates and parameter regression with an iterative error feedback manner. In addition, video-based approaches model the overall change over the image-level features to temporally enhance the single-frame feature, but fail to capture the rotational motion at the joint level, and cannot guarantee local temporal consistency. To address these issues, we propose a novel Transformer-based model with a design of independent tokens. First, we introduce three types of tokens independent of the image feature: \textit{joint rotation tokens, shape token, and camera token}. 
By progressively interacting with image features through Transformer layers, these tokens learn to encode the prior knowledge of human 3D joint rotations, body shape, and position information from large-scale data, and are updated to estimate SMPL parameters conditioned on a given image. Second, benefiting from the proposed token-based representation, we further use a temporal model to focus on capturing the rotational temporal information of each joint, which is empirically conducive to preventing large jitters in local parts. Despite being conceptually simple, the proposed method attains superior performances on the 3DPW and Human3.6M datasets. Using ResNet-50 and Transformer architectures, it obtains 42.0 mm error on the PA-MPJPE metric of the challenging 3DPW, outperforming state-of-the-art counterparts by a large margin. Code will be publicly available\footnote{\url{https://github.com/yangsenius/INT_HMR_Model}}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                723. <h><a href=https://openreview.net/pdf?id=q3F0UBAruO>Towards Effective and Interpretable Human-Agent Collaboration in MOBA Games: A Communication Perspective</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yiming Gao, Feiyu Liu, Liang Wang, Zhenjie Lian, Weixuan Wang, Siqin Li, Xianliang Wang, Xianhan Zeng, Rundong Wang, jiawei wang, QIANG FU, Yang Wei, Lanxiao Huang, Wei Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=q3F0UBAruO>6.67 - [6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an efficient and interpretable Meta-Command Communication-based (MCC) framework for accomplishing effective human-AI collaboration in MOBA games. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/q3F0UBAruO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    MOBA games, e.g., Dota2 and Honor of Kings, have been actively used as the testbed for the recent AI research on games, and various AI systems have been developed at the human level so far. However, these AI systems mainly focus on how to compete with humans, less on exploring how to collaborate with humans. To this end, this paper makes the first attempt to investigate human-agent collaboration in MOBA games. In this paper, we propose to enable humans and agents to collaborate through explicit communication by designing an efficient and interpretable Meta-Command Communication-based framework, dubbed MCC, for accomplishing effective human-agent collaboration in MOBA games. The MCC framework consists of two pivotal modules: 1) an interpretable communication protocol, i.e., the Meta-Command, to bridge the communication gap between humans and agents; 2) a meta-command value estimator, i.e., the Meta-Command Selector, to select a valuable meta-command for each agent to achieve effective human-agent collaboration. Experimental results in Honor of Kings demonstrate that MCC agents can collaborate reasonably well with human teammates and even generalize to collaborate with different levels and numbers of human teammates. Videos are available at https://sites.google.com/view/mcc-demo.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                724. <h><a href=https://openreview.net/pdf?id=uLE3WF3-H_5>Adversarial Diversity in Hanabi</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Brandon Cui, Andrei Lupu, Samuel Sokota, Hengyuan Hu, David J Wu, Jakob Nicolaus Foerster</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uLE3WF3-H_5>6.67 - [6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We produce meaningfully diverse and reasonable joint policies using off-belief learning and adversarial reward shaping.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uLE3WF3-H_5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many Dec-POMDPs admit a qualitatively diverse set of &#39;&#39;reasonable&#39;&#39; joint policies, where reasonableness is indicated by symmetry equivariance, non-sabotaging behaviour and the graceful degradation of performance when paired with ad-hoc partners. Some of the work in diversity literature is concerned with generating these policies. Unfortunately, existing methods fail to produce teams of agents that are simultaneously diverse, high performing, and reasonable. In this work, we propose a novel approach, adversarial diversity (ADVERSITY), which is designed for turn-based Dec-POMDPs with public actions. ADVERSITY relies on off-belief learning to encourage reasonableness and skill, and on &#39;&#39;repulsive&#39;&#39; fictitious transitions to encourage diversity. We use this approach to generate new agents with distinct but reasonable play styles for the card game Hanabi and open-source our agents to be used for future research on (ad-hoc) coordination.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                725. <h><a href=https://openreview.net/pdf?id=EIgLnNx_lC>TDR-CL: Targeted Doubly Robust Collaborative Learning for Debiased Recommendations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoxuan Li, Yan Lyu, Chunyuan Zheng, Peng Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EIgLnNx_lC>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a principled approach that can effectively reduce the bias and variance simultaneously compared to existing DR estimators for debiased recommendations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EIgLnNx_lC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Bias is a common problem inherent in recommender systems, which is entangled with users&#39; preferences and poses a great challenge to unbiased learning. For debiasing tasks, the doubly robust (DR) method and its variants show superior performance due to the double robustness property, that is, DR is unbiased when either imputed errors or learned propensities are accurate.
However, our theoretical analysis reveals that DR usually has a large variance. Meanwhile, DR would suffer unexpectedly large bias and poor generalization caused by inaccurate imputed errors and learned propensities, which usually occur in practice. In this paper, we propose a principled approach that can effectively reduce the bias and variance simultaneously for existing DR approaches when the error  imputation model is misspecified. In addition, we further propose a novel semi-parametric collaborative learning approach that decomposes imputed errors into parametric and nonparametric parts and updates them collaboratively, resulting in more accurate predictions. Both theoretical analysis and experiments demonstrate the superiority of the proposed methods compared with existing debiasing methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                726. <h><a href=https://openreview.net/pdf?id=bLmSMXbqXr>Quality-Similar Diversity via Population Based Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuang Wu, Jian Yao, Haobo Fu, Ye Tian, Chao Qian, Yaodong Yang, QIANG FU, Yang Wei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bLmSMXbqXr>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We formulate the Quality-Similar Diversity (QSD) problem and propose an efficient population-based RL algorithm to optimize the user-defined diversity at multiple quality levels throughout training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bLmSMXbqXr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diversity is a growing research topic in Reinforcement Learning (RL). Previous research on diversity has mainly focused on promoting diversity to encourage exploration and thereby improve quality (the cumulative reward), maximizing diversity subject to quality constraints, or jointly maximizing quality and diversity, known as the quality-diversity problem. In this work, we present the quality-similar diversity problem that features diversity among policies of similar qualities. In contrast to task-agnostic diversity, we focus on task-specific diversity defined by a set of user-specified Behavior Descriptors (BDs). A BD is a scalar function of a trajectory (e.g., the fire action rate for an Atari game), which delivers the type of diversity the user prefers. To derive the gradient of the user-specified diversity with respect to a policy, which is not trivially available, we introduce a set of BD estimators and connect it with the classical policy gradient theorem. Based on the diversity gradient, we develop a population-based RL algorithm to adaptively and efficiently optimize the population diversity at multiple quality levels throughout training. Extensive results on MuJoCo and Atari demonstrate that our algorithm significantly outperforms previous methods in terms of generating user-specified diverse policies across different quality levels.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                727. <h><a href=https://openreview.net/pdf?id=lq62uWRJjiY>Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lq62uWRJjiY>6.67 - [6, 6, 8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propsoe MARVEL to adaptively allocate the parameter budget among weight matrices in correspondence to their importance. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lq62uWRJjiY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Fine-tuning large pre-trained language models on downstream tasks has become an important paradigm in NLP. However, common practice fine-tunes all of the parameters in a pre-trained model, which becomes prohibitive when a large number of downstream tasks are present. Therefore, many fine-tuning methods are proposed to learn incremental updates of pre-trained weights in a parameter efficient way, e.g., low-rank increments. These methods often evenly distribute the budget of incremental updates across all pre-trained weight matrices, and overlook the varying importance of different weight parameters. As a consequence, the fine-tuning performance is suboptimal. To bridge this gap, we propose AdaLoRA, which adaptively allocates the parameter budget among weight matrices according to their importance score. In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition. Such a novel approach allows us to effectively prune the singular values of unimportant updates, which is essentially to reduce their parameter budget but circumvent intensive exact SVD computations. We conduct extensive experiments with several pre-trained models on natural language processing, question answering, and natural language generation to validate the effectiveness of AdaLoRA. Results demonstrate that AdaLoRA manifests notable improvement over baselines, especially in the low budget settings. Our code is publicly available at https://github.com/QingruZhang/AdaLoRA .
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                728. <h><a href=https://openreview.net/pdf?id=5spDgWmpY6x>How Sharpness-Aware Minimization Minimizes Sharpness?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaiyue Wen, Tengyu Ma, Zhiyuan Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5spDgWmpY6x>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> we prove the implicit bias of Sharpness-Aware Minimization (SAM) is minimizing the top eigenvalue of Hessian in the full-batch setting or minimizing the trace of Hessian when batch size is 1.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5spDgWmpY6x.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sharpness-Aware Minimization (SAM) is a highly effective regularization technique for improving the generalization of deep neural networks for various settings. However, the underlying working of SAM remains elusive because of various intriguing approximations in the theoretical characterizations. SAM intends to penalize a notion of sharpness of the model but implements a computationally efficient variant; moreover, a third notion of sharpness was used for proving generalization guarantees. The subtle differences in these notions of sharpness can indeed lead to significantly different empirical results. This paper rigorously nails down the exact sharpness notion that SAM regularizes and clarifies the underlying mechanism. We also show that the two steps of approximations in the original motivation of SAM individually lead to inaccurate local conclusions, but their combination accidentally reveals the correct effect, when full-batch gradients are applied. Furthermore, we also prove that the stochastic version of SAM in fact regularizes the third notion of sharpness mentioned above, which is most likely to be the preferred notion for practical performance. The key mechanism behind this intriguing phenomenon is  the alignment between the gradient and the top eigenvector of Hessian when SAM is applied.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                729. <h><a href=https://openreview.net/pdf?id=fVm3nZMZs9>On Achieving Optimal Adversarial Test Error</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Justin D. Li, Matus Telgarsky</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fVm3nZMZs9>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fVm3nZMZs9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We first elucidate various fundamental properties of optimal adversarial predictors: the structure of optimal adversarial convex predictors in terms of optimal adversarial zero-one predictors, bounds relating the adversarial convex loss to the adversarial zero-one loss, and the fact that continuous predictors can get arbitrarily close to the optimal adversarial error for both convex and zero-one losses. Applying these results along with new Rademacher complexity bounds for adversarial training near initialization, we prove that for general data distributions and perturbation sets, adversarial training on shallow networks with early stopping and an idealized optimal adversary is able to achieve optimal adversarial test error. By contrast, prior theoretical work either considered specialized data distributions or only provided training error guarantees.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                730. <h><a href=https://openreview.net/pdf?id=WsUMeHPo-2>Learnable Graph Convolutional Attention Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Adrián Javaloy, Pablo Sanchez Martin, Amit Levi, Isabel Valera</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WsUMeHPo-2>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a GNN which learns to use, in each layer, an interpolation of a GCN, GAT, and a GAT with convolved features. It outperforms existing methods, is more robust, and removes the need of cross-validating.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WsUMeHPo-2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing Graph Neural Networks (GNNs) compute the message exchange between nodes by either aggregating uniformly (convolving) the features of all the neighbor- ing nodes, or by applying a non-uniform score (attending) to the features. Recent works have shown the strengths and weaknesses of the resulting GNN architectures, respectively, GCNs and GATs. In this work, we aim at exploiting the strengths of both approaches to their full extent. To this end, we first introduce the graph convolutional attention layer (CAT), which relies on convolutions to compute the attention scores. Unfortunately, as in the case of GCNs and GATs, we show that there exists no clear winner between the three—neither theoretically nor in practice—as their performance directly depends on the nature of the data (i.e., of the graph and features). This result brings us to the main contribution of our work, the learnable graph convolutional attention network (L-CAT): a GNN architecture that automatically interpolates between GCN, GAT and CAT in each layer, by adding only two scalar parameters. Our results demonstrate that L-CAT is able to efficiently combine different GNN layers along the network, outperforming competing methods in a wide range of datasets, and resulting in a more robust model that reduces the need of cross-validating.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                731. <h><a href=https://openreview.net/pdf?id=sd90a2ytrt>Semi-Implicit Variational Inference via Score Matching</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Longlin Yu, Cheng Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sd90a2ytrt>6.67 - [6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new semi-implict variational inference method using a score matching training objective
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sd90a2ytrt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Semi-implicit variational inference (SIVI) greatly enriches the expressiveness of variational families by considering implicit variational distributions defined in a hierarchical manner. However, due to the intractable densities of variational distributions, current SIVI approaches often use surrogate evidence lower bounds (ELBOs) or employ expensive inner-loop MCMC runs for unbiased ELBOs for training. In this paper, we propose SIVI-SM, a new method for SIVI based on an alternative training objective via score matching. Leveraging the hierarchical structure of semi-implicit variational families, the score matching objective allows a minimax formulation where the intractable variational densities can be naturally handled with denoising score matching.  We show that SIVI-SM closely matches the accuracy of MCMC and outperforms ELBO-based SIVI methods in a variety of Bayesian inference tasks. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                732. <h><a href=https://openreview.net/pdf?id=2YQrqe4RNv>Edgeformers: Graph-Empowered Transformers for Representation Learning on Textual-Edge Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bowen Jin, Yu Zhang, Yu Meng, Jiawei Han</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2YQrqe4RNv>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2YQrqe4RNv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Edges in many real-world social/information networks are associated with rich text information (e.g., user-user communications or user-product reviews). However, mainstream network representation learning models focus on propagating and aggregating node attributes, lacking specific designs to utilize text semantics on edges. While there exist edge-aware graph neural networks, they directly initialize edge attributes as a feature vector, which cannot fully capture the contextualized text semantics of edges. In this paper, we propose Edgeformers, a framework built upon graph-enhanced Transformers, to perform edge and node representation learning by modeling texts on edges in a contextualized way. Specifically, in edge representation learning, we inject network information into each Transformer layer when encoding edge texts; in node representation learning, we aggregate edge representations through an attention mechanism within each node’s ego-graph. On five public datasets from three different domains, Edgeformers consistently outperform state-of-the-art baselines in edge classification and link prediction, demonstrating the efficacy in learning edge and node representations, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                733. <h><a href=https://openreview.net/pdf?id=p0JSSa1AuV>KwikBucks: Correlation Clustering with Cheap-Weak and Expensive-Strong Signals</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sandeep Silwal, Sara Ahmadian, Andrew Nystrom, Andrew McCallum, Deepak Ramachandran, Seyed Mehran Kazemi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p0JSSa1AuV>6.67 - [8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Inspired by text clustering, we study correlation clustering where similarities must be queried via an expensive model (e.g. a large language model) with additional help from a cheap but noisy model (e.g. an embedding based model).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p0JSSa1AuV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The unprecedented rate at which the sizes of machine learning (ML) models are growing necessitates novel approaches to enable efficient and scalable solutions. We contribute to this line of work by studying a novel version of the Budgeted Correlation Clustering problem (\bcc) where along with a limited number of queries to an expensive oracle for node similarities (e.g. a large ML model), we have unlimited access to a cheaper but less accurate second oracle. Our formulation is inspired by many practical scenarios where coarse approximations of the expensive similarity metric can be efficiently obtained via weaker models. We develop a theoretically motivated algorithm in this setting that leverages the cheap oracle to judiciously query the strong oracle while maintaining high clustering quality. We empirically demonstrate gains in query minimization and clustering metrics on a variety of datasets with diverse strong and cheap oracles. Most notably, we demonstrate a practical application in text clustering based on expensive cross-attention language models by showing that cheaper (but weaker) embedding-based models can be leveraged to substantially reduce the number of inference calls to the former.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                734. <h><a href=https://openreview.net/pdf?id=FRLswckPXQ5>Improved Convergence of Differential Private SGD with Gradient Clipping</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huang Fang, Xiaoyun Li, Chenglin Fan, Ping Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FRLswckPXQ5>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FRLswckPXQ5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Differential private stochastic gradient descent (DP-SGD) with gradient clipping (DP-SGD-GC) is an effective optimization algorithm that can train machine learning models with a privacy guarantee. Despite the popularity of DP-SGD-GC, its convergence in unbounded domain without the Lipschitz continuous assumption is less-understood; existing analysis of DP-SGD-GC either impose additional assumptions or end up with an utility bound that involves an non-vanishing bias term. In this work, for smooth and unconstrained problems, we improve the current analysis and show that DP-SGD-GC can achieve a vanishing utility bound without any bias term. Furthermore, when the noise generated from subsampled gradients is light-tailed, we prove that DP-SGD-GC can achieve nearly the same utility bound as DP-SGD applies to the Lipschitz continuous objectives. As a by-product, we propose a new clipping technique, called value clipping, to mitigate the computational overhead caused by the classic gradient clipping. Experiments on standard benchmark datasets are conducted to support our analysis.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                735. <h><a href=https://openreview.net/pdf?id=2vmGv5wPDBZ>Learning to Estimate Single-View Volumetric Flow Motions without 3D Supervision</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Erik Franz, Barbara Solenthaler, Nils Thuerey</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2vmGv5wPDBZ>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train a network to estimate 3D motions and densities from single view videos of smoke without using 3D ground truth.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2vmGv5wPDBZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We address the challenging problem of jointly inferring the 3D flow and volumetric densities moving in a fluid from a monocular input video with a deep neural network. Despite the complexity of this task, we show that it is possible to train the corresponding networks without requiring any 3D ground truth for training. In the absence of ground truth data we can train our model with observations from real-world capture setups instead of relying on synthetic reconstructions. We make this unsupervised training approach possible by first generating an initial prototype volume which is then moved and transported over time without the need for volumetric supervision. Our approach relies purely on image-based losses, an adversarial discriminator network, and regularization. Our method can estimate long-term sequences in a stable manner, while achieving closely matching targets for inputs such as rising smoke plumes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                736. <h><a href=https://openreview.net/pdf?id=MjsDeTcDEy>What Is Missing in IRM Training and Evaluation? Challenges and Solutions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yihua Zhang, Pranay Sharma, Parikshit Ram, Mingyi Hong, Kush R. Varshney, Sijia Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MjsDeTcDEy>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MjsDeTcDEy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Invariant risk minimization (IRM) has received increasing attention as a way to acquire environment-agnostic data representations and predictions, and also a principled solution for preventing spurious correlations from being learned and improving models’ out-of-distribution generalization. Yet, recent works have found that the optimality of the originally-proposed IRM optimization (IRMV1) may be compromised in practice or could be impossible to achieve in some scenarios. Therefore, a series of advanced IRM algorithms have been developed that show practical improvement over IRMV1. In this work, we revisit these recent IRM advancements and identify and resolve three practical limitations in IRM training and evaluation. First, we find that the effect of batch size during training has been chronically overlooked in previous studies, leaving room for further improvement. We propose small-batch training and highlight the improvements over a set of large-batch optimization techniques. Second, we find that improper selection of evaluation environments could give a false sense of invariance for IRM. To alleviate this effect, we leverage diversified test-time environments to precisely characterize the invariance of IRM when applied in practice. Third, we revisit Ahuja et al. (2020)’s proposal to convert IRM into an ensemble game and identify a limitation when a single invariant predictor is desired instead of an ensemble of individual predictors. We propose a new IRM variant to address this limitation based on a novel viewpoint of ensemble IRM games as consensus-constrained bi-level optimization. Lastly, we conduct extensive experiments (covering 7 existing IRM variants and 7 datasets) to justify the practical significance of revisiting IRM training and evaluation in a principled manner.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                737. <h><a href=https://openreview.net/pdf?id=OOWLRfAI_V_>Quantized Compressed Sensing with Score-Based Generative Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiangming Meng, Yoshiyuki Kabashima</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OOWLRfAI_V_>6.67 - [6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Quantized Compressed Sensing with Score-Based Generative Models
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OOWLRfAI_V_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the general problem of recovering a high-dimensional signal from noisy quantized measurements. Quantization, especially coarse quantization such as 1-bit sign measurements, leads to severe information loss and thus a good prior knowledge of the unknown signal is helpful for accurate recovery. Motivated by the power of score-based generative models (SGM, also known as diffusion models) in capturing the rich structure of natural signals beyond simple sparsity, we propose an unsupervised data-driven approach called quantized compressed sensing with SGM (QCS-SGM), where the prior distribution is modeled by a pre-trained SGM. To perform posterior sampling, an annealed pseudo-likelihood score called ${\textit{noise perturbed pseudo-likelihood score}}$ is introduced and combined with the prior score of SGM. The proposed QCS-SGM applies to an arbitrary number of quantization bits. Experiments on a variety of baseline datasets demonstrate that the proposed QCS-SGM significantly outperforms existing state-of-the-art algorithms by a large margin for both in-distribution and out-of-distribution samples. Moreover, as a posterior sampling method, QCS-SGM can be easily used to obtain confidence intervals or uncertainty estimates of the reconstructed results. $\textit{The code  is available at}$ https://github.com/mengxiangming/QCS-SGM.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                738. <h><a href=https://openreview.net/pdf?id=Q120_4COf-K>Synthetic Data Generation of Many-to-Many Datasets via Random Graph Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kai Xu, Georgi Ganev, Emile Joubert, Rees Davison, Olivier Van Acker, Luke Robinson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Q120_4COf-K>6.67 - [6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We synthesise datasets with many-to-many relationships by first generating the relationships via random graph generation and then generating the data attributes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Q120_4COf-K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Synthetic data generation (SDG) has become a popular approach to release private datasets.
In SDG, a generative model is fitted on the private real data, and samples drawn from the model are released as the protected synthetic data.
While real-world datasets usually consist of multiple tables with potential \emph{many-to-many} relationships (i.e.~\emph{many-to-many datasets}), recent research in SDG mostly focuses on modeling tables \emph{independently} or only considers generating datasets with special cases of many-to-many relationships such as \emph{one-to-many}.
In this paper, we first study challenges of building faithful generative models for many-to-many datasets, identifying limitations of existing methods.
We then present a novel factorization for many-to-many generative models,  which leads to a scalable generation framework by combining recent results from random graph theory and representation learning.
Finally, we extend the framework to establish the notion of $(\epsilon,\delta)$-differential privacy.
Through a real-world dataset, we demonstrate that our method can generate synthetic datasets while preserving information within and across tables better than its closest competitor.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                739. <h><a href=https://openreview.net/pdf?id=f0a_dWEYg-Td>Indiscriminate Poisoning Attacks on Unsupervised Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao He, Kaiwen Zha, Dina Katabi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=f0a_dWEYg-Td>6.67 - [8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/f0a_dWEYg-Td.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Indiscriminate data poisoning attacks are quite effective against supervised learning. However, not much is known about their impact on unsupervised contrastive learning (CL). This paper is the first to consider indiscriminate poisoning attacks of contrastive learning. We propose Contrastive Poisoning (CP), the first effective such attack on CL. We empirically show that Contrastive Poisoning, not only drastically reduces the performance of CL algorithms, but also attacks supervised learning models, making it the most generalizable indiscriminate poisoning attack. We also show that CL algorithms with a momentum encoder are more robust to indiscriminate poisoning, and propose a new countermeasure based on matrix completion. Code is available at: https://github.com/kaiwenzha/contrastive-poisoning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                740. <h><a href=https://openreview.net/pdf?id=s4WVupnJjmX>Explicit Box Detection Unifies End-to-End Multi-Person Pose Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jie Yang, Ailing Zeng, Shilong Liu, Feng Li, Ruimao Zhang, Lei Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=s4WVupnJjmX>6.6 - [5, 8, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/s4WVupnJjmX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents a novel end-to-end framework with Explicit box Detection for multi-person Pose estimation, called ED-Pose, where it unifies the contextual learning between human-level (global) and keypoint-level (local) information. Different from previous one-stage methods, ED-Pose re-considers this task as two explicit box detection processes with a unified representation and regression supervision. First, we introduce a human detection decoder from encoded tokens to extract global features. It can provide a good initialization for the latter keypoint detection, making the training process converge fast. Second, to bring in contextual information near keypoints, we regard pose estimation as a keypoint box detection problem to learn both box positions and contents for each keypoint. A human-to-keypoint detection decoder adopts an interactive learning strategy between human and keypoint features to further enhance global and local feature aggregation. In general, ED-Pose is conceptually simple without post-processing and dense heatmap supervision. It demonstrates its effectiveness and efficiency compared with both two-stage and one-stage methods. Notably, explicit box detection boosts the pose estimation performance by 4.5 AP on COCO and 9.9 AP on CrowdPose. For the first time, as a fully end-to-end framework with a L1 regression loss, ED-Pose surpasses heatmap-based Top-down methods under the same backbone by 1.2 AP on COCO and achieves the state-of-the-art with 76.6 AP on CrowdPose without bells and whistles. Code is available at https://github.com/IDEA-Research/ED-Pose.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                741. <h><a href=https://openreview.net/pdf?id=r0BrY4BiEXO>Decepticons: Corrupted Transformers Breach Privacy in Federated Learning for Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Liam H Fowl, Jonas Geiping, Steven Reich, Yuxin Wen, Wojciech Czaja, Micah Goldblum, Tom Goldstein</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=r0BrY4BiEXO>6.6 - [8, 8, 8, 1, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/r0BrY4BiEXO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Privacy is a central tenet of Federated learning (FL), in which a central server trains models without centralizing user data. However, gradient updates used in FL can leak user information.  While the most industrial uses of FL are for text applications (e.g. keystroke prediction), the majority of attacks on user privacy in FL have focused on simple image classifiers and threat models that assume honest execution of the FL protocol from the server. We propose a novel attack that reveals private user text by deploying malicious parameter vectors, and which succeeds even with mini-batches, multiple users, and long sequences. Unlike previous attacks on FL, the attack exploits characteristics of both the Transformer architecture and the token embedding, separately extracting tokens and positional embeddings to retrieve high-fidelity text. We argue that the threat model of malicious server states is highly relevant from a user-centric perspective, and show that in this scenario, text applications using transformer models are much more vulnerable than previously thought. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                742. <h><a href=https://openreview.net/pdf?id=qco4ekz2Epm>Online Boundary-Free Continual Learning by Scheduled Data Prior</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyunseo Koh, Minhyuk Seo, Jihwan Bang, Hwanjun Song, Deokki Hong, Seulki Park, Jung-Woo Ha, Jonghyun Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qco4ekz2Epm>6.6 - [8, 6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new continual learning setup without explicit task boundary and a method to address it.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qco4ekz2Epm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Typical continual learning setup assumes that the dataset is split into multiple discrete tasks. We argue that it is less realistic as the streamed data would have no notion of task boundary in real-world data. Here, we take a step forward to investigate more realistic online continual learning – learning continuously changing data distribution without explicit task boundary, which we call boundary-free setup. As there is no clear boundary of tasks, it is not obvious when and what information in the past to be preserved as a better remedy for the stability-plasticity dilemma. To this end, we propose a scheduled transfer of previously learned knowledge. We further propose a data-driven balancing between the knowledge in the past and the present in learning objective. Moreover, since it is not straight-forward to use the previously proposed forgetting measure without task boundaries, we further propose a novel forgetting measure based on information theory that can capture forgetting. We empirically evaluate our method on a Gaussian data stream, its periodic extension, which assumes periodic data distribution frequently observed in real-life data, as well as the conventional disjoint task-split. Our method outperforms prior arts by large margins in various setups, using four popular benchmark datasets – CIFAR-10, CIFAR-100, TinyImageNet and ImageNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                743. <h><a href=https://openreview.net/pdf?id=4wZiAXD29TQ>Dataset Pruning: Reducing Training Data by Examining Generalization Influence</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, Ping Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4wZiAXD29TQ>6.6 - [8, 6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4wZiAXD29TQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The great success of deep learning heavily relies on increasingly larger training data, which comes at a price of huge computational and infrastructural costs. This poses crucial questions that, do all training data contribute to model&#39;s performance? How much does each individual training sample or a sub-training-set affect the model&#39;s generalization, and how to construct the smallest subset from the entire training data as a proxy training set without significantly sacrificing the model&#39;s performance? To answer these, we propose dataset pruning, an optimization-based sample selection method that can (1) examine the influence of removing a particular set of training samples on model&#39;s generalization ability with theoretical guarantee, and (2) construct the smallest subset of training data that yields strictly constrained generalization gap. The empirically observed generalization gap of dataset pruning is substantially consistent with our theoretical expectations. Furthermore, the proposed method prunes 40% training examples on the CIFAR-10 dataset, halves the convergence time with only 1.3% test accuracy decrease, which is superior to previous score-based sample selection methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                744. <h><a href=https://openreview.net/pdf?id=kDSmxOspsXQ>Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yinan Huang, Xingang Peng, Jianzhu Ma, Muhan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kDSmxOspsXQ>6.6 - [8, 6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kDSmxOspsXQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node&#39;s representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substructures like ring systems with more than four atoms. To overcome this limitation, we propose I$^2$-GNNs to extend Subgraph MPNNs by assigning different identifiers for the root node and its neighbors in each subgraph. I$^2$-GNNs&#39; discriminative power is shown to be strictly stronger than Subgraph MPNNs and partially stronger than the 3-WL test. More importantly, I$^2$-GNNs are proven capable of counting all 3, 4, 5 and 6-cycles, covering common substructures like benzene rings in organic chemistry, while still keeping linear complexity. To the best of our knowledge, it is the first linear-time GNN model that can count 6-cycles with theoretical guarantees. We validate its counting power in cycle counting tasks and demonstrate its competitive performance in molecular prediction benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                745. <h><a href=https://openreview.net/pdf?id=tmIiMPl4IPa>Factorized Fourier Neural Operators</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alasdair Tran, Alexander Mathews, Lexing Xie, Cheng Soon Ong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tmIiMPl4IPa>6.6 - [8, 6, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An efficient and scalable neural PDE solver using Fourier transform.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tmIiMPl4IPa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose the Factorized Fourier Neural Operator (F-FNO), a learning-based approach for simulating partial differential equations (PDEs). Starting from a recently proposed Fourier representation of flow fields, the F-FNO bridges the performance gap between pure machine learning approaches to that of the best numerical or hybrid solvers. This is achieved with new representations – separable spectral layers and improved residual connections – and a combination of training strategies such as the Markov assumption, Gaussian noise, and cosine learning rate decay. On several challenging benchmark PDEs on regular grids, structured meshes, and point clouds, the F-FNO can scale to deeper networks and outperform both the FNO and the geo-FNO, reducing the error by 83% on the Navier-Stokes problem, 31% on the elasticity problem, 57% on the airfoil flow problem, and 60% on the plastic forging problem. Compared to the state-of-the-art pseudo-spectral method, the F-FNO can take a step size that is an order of magnitude larger in time and achieve an order of magnitude speedup to produce the same solution quality.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                746. <h><a href=https://openreview.net/pdf?id=6qcYDVlVLnK>Mitigating Memorization of Noisy Labels via Regularization between Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao Cheng, Zhaowei Zhu, Xing Sun, Yang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6qcYDVlVLnK>6.6 - [8, 8, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We theoretically show the memorization effect of DNN with resepct to the model capacity and propose a representation-based regularizer to mitigate the memorization effect. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6qcYDVlVLnK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Designing robust loss functions is popular in learning with noisy labels while existing designs did not explicitly consider the overfitting property of deep neural networks (DNNs). As a result, applying these losses may still suffer from overfitting/memorizing noisy labels as training proceeds. In this paper, we first theoretically analyze the memorization effect and show that a lower-capacity model may perform better on noisy datasets. However, it is non-trivial to design a neural network with the best capacity given an arbitrary task. To circumvent this dilemma, instead of changing the model architecture, we decouple DNNs into an encoder followed by a linear classifier and propose to restrict the function space of a DNN by a representation regularizer. Particularly, we require the distance between two self-supervised features to be positively related to the distance between the corresponding two supervised model outputs.  
Our proposed framework is easily extendable and can incorporate many other robust loss functions to further improve performance. Extensive experiments and theoretical analyses support our claims. Code is available at https://github.com/UCSC-REAL/SelfSup_NoisyLabel.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                747. <h><a href=https://openreview.net/pdf?id=XDJwuEYHhme>Towards the Generalization of Contrastive Self-Supervised Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weiran Huang, Mingyang Yi, Xuyang Zhao, Zihao Jiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XDJwuEYHhme>6.6 - [6, 10, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents a theoretical understanding of contrastive learning and provide an upper bound of the downstream classification error.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XDJwuEYHhme.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, self-supervised learning has attracted great attention, since it only requires unlabeled data for model training. Contrastive learning is one popular method for self-supervised learning and has achieved promising empirical performance. However, the theoretical understanding of its generalization ability is still limited. To this end, we define a kind of $(\sigma,\delta)$-measure to mathematically quantify the data augmentation, and then provide an upper bound of the downstream classification error rate based on the measure. It reveals that the generalization ability of contrastive self-supervised learning is related to three key factors: alignment of positive samples, divergence of class centers, and concentration of augmented data. The first two factors are properties of learned representations, while the third one is determined by pre-defined data augmentation. We further investigate two canonical contrastive losses, InfoNCE and cross-correlation, to show how they provably achieve the first two factors. Moreover, we conduct experiments to study the third factor, and observe a strong correlation between downstream performance and the concentration of augmented data.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                748. <h><a href=https://openreview.net/pdf?id=Q-neeWNVv1>Order Matters: Agent-by-agent Policy Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xihuai Wang, Zheng Tian, Ziyu Wan, Ying Wen, Jun Wang, Weinan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Q-neeWNVv1>6.6 - [8, 6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Q-neeWNVv1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While multi-agent trust region algorithms have achieved great success empirically in solving coordination tasks, most of them,  however, suffer from a non-stationarity problem since agents update their policies simultaneously. In contrast, a sequential scheme that updates policies agent-by-agent provides another perspective and shows strong performance. However, sample inefficiency and lack of monotonic improvement guarantees for each agent are still the two significant challenges for the sequential scheme. In this paper, we propose the \textbf{A}gent-by-\textbf{a}gent \textbf{P}olicy \textbf{O}ptimization (A2PO) algorithm to improve the sample efficiency and retain the guarantees of monotonic improvement for each agent during training. We justify the tightness of the monotonic improvement bound compared with other trust region algorithms. From the perspective of sequentially updating agents, we further consider the effect of agent updating order and extend the theory of non-stationarity into the sequential update scheme. To evaluate A2PO, we conduct a comprehensive empirical study on four benchmarks: StarCraftII, Multi-agent MuJoCo, Multi-agent Particle Environment, and Google Research Football full game scenarios. A2PO consistently outperforms strong baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                749. <h><a href=https://openreview.net/pdf?id=06mk-epSwZ>DiffMimic: Efficient Motion Mimicking with Differentiable Physics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiawei Ren, Cunjun Yu, Siwei Chen, Xiao Ma, Liang Pan, Ziwei Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=06mk-epSwZ>6.6 - [6, 6, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Mimic agile skills for physics-based character with differentiable physics simulators.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/06mk-epSwZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Motion mimicking is a foundational task in physics-based character animation. However, most existing motion mimicking methods are built upon reinforcement learning (RL) and suffer from heavy reward engineering, high variance, and slow convergence with hard explorations. Specifically, they usually take tens of hours or even days of training to mimic a simple motion sequence, resulting in poor scalability. In this work, we leverage differentiable physics simulators (DPS) and propose an efficient motion mimicking method dubbed $\textbf{DiffMimic}$. Our key insight is that DPS casts a complex policy learning task to a much simpler state matching problem. In particular, DPS learns a stable policy by analytical gradients with ground-truth physical priors hence leading to significantly faster and stabler convergence than RL-based methods. Moreover, to escape from local optima, we utilize an \textit{Demonstration Replay} mechanism to enable stable gradient backpropagation in a long horizon. Extensive experiments on standard benchmarks show that DiffMimic has a better sample efficiency and time efficiency than existing methods (e.g., DeepMimic). Notably, DiffMimic allows a physically simulated character to learn back-flip after 10 minutes of training and be able to cycle it after 3 hours of training, while DeepMimic requires about a day of training to cycle back-flip. More importantly, we hope DiffMimic can benefit more differentiable animation systems with techniques like differentiable clothes simulation in future research. Our code is available at https://github.com/diffmimic/diffmimic. Qualitative results can be viewed at https://diffmimic-demo-main-g7h0i8.streamlitapp.com.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                750. <h><a href=https://openreview.net/pdf?id=hVrXUps3LFA>Divide to Adapt: Mitigating Confirmation Bias for Domain Adaptation of Black-Box Predictors</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianfei Yang, Xiangyu Peng, Kai Wang, Zheng Zhu, Jiashi Feng, Lihua Xie, Yang You</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hVrXUps3LFA>6.6 - [5, 8, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A black-box model adaptation approach that purifies the pseudo labels for knowledge distillation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hVrXUps3LFA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Domain Adaptation of Black-box Predictors (DABP) aims to learn a model on an unlabeled target domain supervised by a black-box predictor trained on a source domain. It does not require access to both the source-domain data and the predictor parameters, thus addressing the data privacy and portability issues of standard domain adaptation methods. Existing DABP approaches mostly rely on knowledge distillation (KD) from the black-box predictor, i.e., training the model with its noisy target-domain predictions, which however inevitably introduces the confirmation bias accumulated from the prediction noises and leads to degrading performance. To mitigate such bias, we propose a new strategy, \textit{divide-to-adapt}, that purifies cross-domain knowledge distillation by proper domain division. This is inspired by an observation we make for the first time in domain adaptation: the target domain usually contains easy-to-adapt and hard-to-adapt samples that have different levels of domain discrepancy w.r.t. the source domain, and deep models tend to fit easy-to-adapt samples first. Leveraging easy-to-adapt samples with less noise can help KD alleviate the negative effect of prediction noises from black-box predictors. In this sense, the target domain can be divided into an easy-to-adapt subdomain with less noise and a hard-to-adapt subdomain at the early stage of training. Then the adaptation is achieved by semi-supervised learning. We further reduce distribution discrepancy between subdomains and develop weak-strong augmentation strategy to filter the predictor errors progressively. As such, our method is a simple yet effective solution to reduce error accumulation in cross-domain knowledge distillation for DABP. Moreover, we prove that the target error of DABP is bounded by the noise ratio of two subdomains, i.e., the confirmation bias, which provides the theoretical justifications for our method. Extensive experiments demonstrate our method achieves state of the art on all DABP benchmarks, outperforming the existing best approach by 7.0\% on VisDA-17, and is even comparable with the standard domain adaptation methods that use the source-domain data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                751. <h><a href=https://openreview.net/pdf?id=Q3-1vRh3HOA>Dilated convolution with learnable spacings</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ismail Khalfaoui Hassani, Thomas Pellegrini, Timothée Masquelier</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Q3-1vRh3HOA>6.6 - [8, 5, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Dilated convolution with learnable spacings: a new method that improves the accuracy of state-of-the-art CNNs
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Q3-1vRh3HOA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent works indicate that convolutional neural networks (CNN) need large receptive fields (RF) to compete with visual transformers and their attention mechanism. In CNNs, RFs can simply be enlarged by increasing the convolution kernel sizes. Yet the number of trainable parameters, which scales quadratically with the kernel&#39;s size in the 2D case, rapidly becomes prohibitive, and the training is notoriously difficult. This paper presents a new method to increase the RF size without increasing the number of parameters. The dilated convolution (DC) has already been proposed for the same purpose. DC can be seen as a convolution with a kernel that contains only a few non-zero elements placed on a regular grid. Here we present a new version of the DC in which the spacings between the non-zero elements, or equivalently their positions, are no longer fixed but learnable via backpropagation thanks to an interpolation technique. We call this method “Dilated Convolution with Learnable Spacings” (DCLS) and generalize it to the n-dimensional convolution case. However, our main focus here will be on the 2D case. We first tried our approach on ResNet50: we drop-in replaced the standard convolutions with DCLS ones, which increased the accuracy of ImageNet1k classification at iso-parameters, but at the expense of the throughput. Next, we used the recent ConvNeXt state-of-the-art convolutional architecture and drop-in replaced the depthwise convolutions with DCLS ones. This not only increased the accuracy of ImageNet1k classification but also of typical downstream and robustness tasks, again at iso-parameters but this time with negligible cost on throughput, as ConvNeXt uses separable convolutions. Conversely, classic DC led to poor performance with both ResNet50 and ConvNeXt. The code of the method is based on PyTorch and available at: https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                752. <h><a href=https://openreview.net/pdf?id=v8Mi8KU6056>wav2tok: Deep Sequence Tokenizer for Audio Retrieval</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Adhiraj Banerjee, Vipul Arora</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=v8Mi8KU6056>6.6 - [8, 8, 3, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Represent query and target sequences as compressed token sequences for quick retrieval; similarity semantics are learned from sequence pairs
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/v8Mi8KU6056.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Search over audio sequences is a fundamental problem. In this paper, we propose a method to extract concise discrete representations for audio that can be used for efficient retrieval. Our motivation comes from orthography which represents speech of a given language in a concise and distinct discrete form. The proposed method, wav2tok, learns such representations for any kind of audio, speech or non-speech, from pairs of similar audio. wav2tok compresses the query and target sequences into shorter sequences of tokens that are faster to match. The learning method makes use of CTC loss and expectation-maximization algorithm, which are generally used for supervised automatic speech recognition and for learning discrete latent variables, respectively. Experiments show the consistent performance of wav2tok across two audio retrieval tasks: music search (query by humming) and speech search via audio query, outperforming state-of-the-art baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                753. <h><a href=https://openreview.net/pdf?id=BrJATVZDWEH>Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Noam Wies, Yoav Levine, Amnon Shashua</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=BrJATVZDWEH>6.6 - [6, 6, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/BrJATVZDWEH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The field of Natural Language Processing (NLP) has experienced a dramatic leap in capabilities with the recent introduction of huge Language Models (LMs). Despite this success, natural language problems that involve several compounded steps are still practically unlearnable, even by the largest LMs. This complies with experimental failures for end-to-end learning of composite problems that were demonstrated in a variety of domains. An effective mitigation is to introduce intermediate supervision for solving sub-tasks of the compounded problem. Recently, several works have demonstrated high gains by taking a straightforward approach for incorporating intermediate supervision in compounded natural language problems: the sequence-to-sequence LM is fed with an augmented input, in which the decomposed tasks&#39; labels are simply concatenated to the original input. In this paper, we prove a positive learning result that motivates these recent efforts. We show that when concatenating intermediate supervision to the input and training a sequence-to-sequence model on this modified input, unlearnable composite problems can become learnable. We show that this is true for any family of tasks which on the one hand, are unlearnable, and on the other hand, can be decomposed into a polynomial number of simple sub-tasks, each of which depends only on $O(1)$ previous sub-task results. Beyond motivating contemporary empirical efforts for incorporating intermediate supervision in sequence-to-sequence language models, our positive theoretical result is the first of its kind in the landscape of results on the benefits of intermediate supervision for neural-network learning: Until now, all theoretical results on the subject are negative, i.e., show cases where learning is impossible without intermediate supervision, while our result is positive, showing that learning is facilitated in the presence of intermediate supervision.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                754. <h><a href=https://openreview.net/pdf?id=GNjzMAgawq>CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Alignment</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, Jiebo Luo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GNjzMAgawq>6.6 - [8, 8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GNjzMAgawq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pre-trained image-text models, like CLIP, have demonstrated the strong power of vision-language representation learned from a large scale of web-collected image-text data. In light of the well-learned visual features, there are works that transfer image representation to the video domain and achieve good results. However, adapting image-text pre-trained models to video-text pre-training (i.e., post-pretraining) has not demonstrated a significant advantage yet. In this paper, we tackle this challenge by raising and addressing two questions: 1) what are the factors hindering post-pretraining CLIP from improving performance on video-text tasks, and 2) how to mitigate the impact of these factors. Through a series of comparative experiments and analyses, we find that the data scale and domain gap between language sources have large impacts. By these observations, we propose an Omnisource Cross-modal Learning method equipped with a Video Proxy mechanism on the basis of CLIP, namely CLIP-ViP. Extensive results show that our approach improves the performance of CLIP on video-text retrieval by a large margin. Our model achieves state-of-the-art results on a variety of datasets, including MSR-VTT, DiDeMo, LSMDC, and ActivityNet. We release our code and pre-trained CLIP-ViP models at \url{https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                755. <h><a href=https://openreview.net/pdf?id=resApVNcqSB>Weakly-supervised HOI Detection via Prior-guided Bi-level Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bo Wan, Yongfei Liu, Desen Zhou, Tinne Tuytelaars, Xuming He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=resApVNcqSB>6.6 - [8, 6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/resApVNcqSB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human object interaction (HOI) detection plays a crucial role in human-centric scene understanding and serves as a fundamental building block for many vision tasks. One generalizable and scalable strategy for HOI detection is to use weak supervision, learning from image-level annotations only. This is inherently challenging due to ambiguous human-object associations, large search space of detecting HOIs and highly noisy training signal. A promising strategy to address those challenges is to exploit knowledge from large-scale pretrained models (e.g., CLIP), but a direct knowledge distillation strategy does not perform well on the weakly-supervised setting. In contrast, we develop a CLIP-guided HOI representation capable of incorporating the prior knowledge at both image level and HOI instance level, and adopt a self-taught mechanism to prune incorrect human-object associations. Experimental results on HICO-DET and V-COCO
show that our method outperforms the previous works by a sizable margin, showing the efficacy of our HOI representation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                756. <h><a href=https://openreview.net/pdf?id=sC-PmTsiTB>Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Martin Pawelczyk, Teresa Datta, Johan Van den Heuvel, Gjergji Kasneci, Himabindu Lakkaraju</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sC-PmTsiTB>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel framework to generate probabilistically robust algorithmic recourse
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sC-PmTsiTB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As machine learning models are increasingly being employed to make consequential decisions in real-world settings, it becomes critical to ensure that individuals who are adversely impacted (e.g., loan denied) by the predictions of these models are provided with a means for recourse. While several approaches have been proposed to construct recourses for affected individuals, the recourses output by these methods either achieve low costs (i.e., ease-of-implementation) or robustness to small perturbations (i.e., noisy implementations of recourses), but not both due to the inherent trade-offs between the recourse costs and robustness. Furthermore, prior approaches do not provide end users with any agency over navigating the aforementioned trade-offs. In this work, we address the above challenges by proposing the first algorithmic framework which enables users to effectively manage the recourse cost vs. robustness trade-offs. More specifically, our framework Probabilistically ROBust rEcourse (PROBE) lets users choose the probability with which a recourse could get invalidated (recourse invalidation rate) if small changes are made to the recourse i.e., the recourse is implemented somewhat noisily. To this end, we propose a novel objective function which simultaneously minimizes the gap between the achieved (resulting) and desired recourse invalidation rates, minimizes recourse costs, and also ensures that the resulting recourse achieves a positive model prediction. We develop novel theoretical results to characterize the recourse invalidation rates corresponding to any given instance w.r.t. different classes of underlying models (e.g., linear models, tree based models etc.), and leverage these results to efficiently optimize the proposed objective. Experimental evaluation with multiple real world datasets demonstrates the efficacy of the proposed framework.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                757. <h><a href=https://openreview.net/pdf?id=n-hKHMzBgy>Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Margalit Glasgow, Colin Wei, Mary Wootters, Tengyu Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n-hKHMzBgy>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove generalization for max-margin solutions on a 2 Layer NN problem in regimes where uniform convergence bounds provably fail.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n-hKHMzBgy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A major challenge in modern machine learning is theoretically understanding the generalization properties of overparameterized models. Many existing tools rely on uniform convergence  (UC), a property that, when it holds, guarantees that the test loss will be close to the training loss, uniformly over a class of candidate models. Nagarajan and Kolter (2019) show that in certain simple linear and neural-network settings, any uniform convergence bound will be vacuous, leaving open the question of how to prove generalization in settings where UC fails. Our main contribution is proving novel generalization bounds in two such settings, one linear, and one non-linear. We study the linear classification setting of Nagarajan and Kolter (2019), and a quadratic ground truth function learned via a two-layer neural network in the non-linear regime. We prove a new type of margin bound showing that above a certain signal-to-noise threshold, any near-max-margin classifier will achieve almost no test loss in these two settings. Our results show that near-max-margin is important: while any model that achieves at least a $(1 - \epsilon)$-fraction of the max-margin generalizes well, a classifier achieving half of the max-margin may fail terribly. Our analysis provides insight on why memorization can coexist with generalization: we show that in this challenging regime where generalization occurs but UC fails, near-max-margin classifiers simultaneously contain some generalizable components and some overfitting components that memorize the data. The presence of the overfitting components is enough to preclude UC, but the near-extremal margin guarantees that sufficient generalizable components are present.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                758. <h><a href=https://openreview.net/pdf?id=6jfbOWzWTcE>Offline Reinforcement Learning with Differentiable Function Approximation is Provably Efficient</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ming Yin, Mengdi Wang, Yu-Xiang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6jfbOWzWTcE>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6jfbOWzWTcE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning, which aims at optimizing sequential decision-making strategies with historical data, has been extensively applied in real-life applications. State-Of-The-Art algorithms usually leverage powerful function approximators (e.g. neural networks) to alleviate the sample complexity hurdle for better empirical performances. Despite the successes, a more systematic under- standing of the statistical complexity for function approximation remains lacking. Towards bridging the gap, we take a step by considering offline reinforcement learning with differentiable function class approximation (DFA). This function class naturally incorporates a wide range of models with nonlinear/nonconvex structures. We show offline RL with differentiable function approximation is provably efficient by analyzing the pessimistic fitted Q-learning (PFQL) algorithm, and our results provide the theoretical basis for understanding a variety of practical heuristics that rely on Fitted Q-Iteration style design. In addition, we further im- prove our guarantee with a tighter instance-dependent characterization. We hope our work could draw interest in studying reinforcement learning with differentiable function approximation beyond the scope of current research.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                759. <h><a href=https://openreview.net/pdf?id=HWt4BBZjVW>On the Trade-Off between Actionable Explanations and the Right to be Forgotten</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Martin Pawelczyk, Tobias Leemann, Asia Biega, Gjergji Kasneci</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HWt4BBZjVW>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We analyze the tradeoff between actionable explanations and the right to be forgotten, and provide algorithms to find a critical subset of training data points, which, when removed would lead to a maximum invalidation of recourses.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HWt4BBZjVW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As machine learning (ML) models are increasingly being deployed in high-stakes applications, policymakers have suggested tighter data protection regulations (e.g., GDPR, CCPA). One key principle is the “right to be forgotten” which gives users the right to have their data deleted. Another key principle is the right to an actionable explanation, also known as algorithmic recourse, allowing users to reverse unfavorable decisions. To date, it is unknown whether these two principles can be operationalized simultaneously. Therefore, we introduce and study the problem of recourse invalidation in the context of data deletion requests. More specifically, we theoretically and empirically analyze the behavior of popular state-of-the-art algorithms and demonstrate that the recourses generated by these algorithms are likely to be invalidated if a small number of data deletion requests (e.g., 1 or 2) warrant updates of the predictive model. For the setting of differentiable models, we suggest a framework to identify a minimal subset of critical training points which, when removed, maximize the fraction of invalidated recourses.Using our framework, we empirically show that the removal of as little as 2 data instances from the training set can invalidate up to 95 percent of all recourses output by popular state-of-the-art algorithms. Thus, our work raises fundamental questions about the compatibility of ``the right to an actionable explanation&#39;&#39; in the context of the ``right to be forgotten&#39;&#39;, while also providing constructive insights on the determining factors of recourse robustness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                760. <h><a href=https://openreview.net/pdf?id=10R_bcjFwJ>Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shaofeng Zhang, Feng Zhu, Rui Zhao, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=10R_bcjFwJ>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new self-supervised leanring method to learn both spatial-sensitive and global-discriminative information
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/10R_bcjFwJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose ADCLR: \underline{A}ccurate and \underline{D}ense \underline{C}ontrastive \underline{R}epresentation \underline{L}earning, a novel self-supervised learning framework for learning accurate and dense vision representation. To extract spatial-sensitive information, ADCLR introduces query patches for contrasting  in addition with global contrasting. Compared with previous dense contrasting methods, ADCLR mainly enjoys three merits: i) achieving both global-discriminative and spatial-sensitive representation, ii) model-efficient (no extra parameters in addition to the global contrasting baseline), and iii) correspondence-free and thus simpler to implement. Our approach achieves new state-of-the-art performance for contrastive methods. On classification tasks, for ViT-S, ADCLR achieves 78.1\% top-1 accuracy on ImageNet with linear probing, outperforming our baseline (DINO) without our devised techniques as plug-in, by 1.1\%. For ViT-B, ADCLR achieves 79.8\%, 84.0\% accuracy on ImageNet by linear probing and finetune, outperforming DINO by 0.6\%, 0.4\% accuracy. For dense tasks, on MS-COCO, ADCLR achieves significant improvements of 44.3\% AP on object detection, 39.7\% AP on instance segmentation, outperforming previous SOTA method SelfPatch by 2.2\% and 1.2\%, respectively. On ADE20K, ADCLR outperforms SelfPatch by 1.0\% mIoU, 1.2\% mAcc on the segmentation task.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                761. <h><a href=https://openreview.net/pdf?id=NVZvalzCLg>LilNetX: Lightweight Networks with EXtreme Model Compression and Structured Sparsification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sharath Girish, Kamal Gupta, Saurabh Singh, Abhinav Shrivastava</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NVZvalzCLg>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NVZvalzCLg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce LilNetX, an end-to-end trainable technique for neural networks that enables learning models with specified accuracy-rate-computation trade-off. Prior works approach these problems one at a time and often require post-processing or multistage training which become less practical and do not scale very well for large datasets or architectures. Our method constructs a joint training objective that penalizes the self information of network parameters in a latent representation space to encourage small model size while also introducing priors to increase structured sparsity in the parameter space to reduce computation. When compared with existing state-of-the-art model compression methods, we achieve up to 50% smaller model size and 98% model sparsity on ResNet-20 on the CIFAR-10 dataset as well as 37% smaller model size and 71% structured sparsity on ResNet-50 trained on ImageNet while retaining the same accuracy as those methods. We show that the resulting sparsity can improve the inference time of the models by almost 1.8 times the dense ResNet-50 baseline model. Code is available at https://github.com/Sharath-girish/LilNetX.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                762. <h><a href=https://openreview.net/pdf?id=n-bvaLSCC78>EA-HAS-Bench: Energy-aware Hyperparameter and Architecture Search Benchmark</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuguang Dou, XINYANG JIANG, Cai Rong Zhao, Dongsheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n-bvaLSCC78>6.5 - [6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide the first HAS dataset aware of the overall search energy cost
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n-bvaLSCC78.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The energy consumption for training deep learning models is increasing at an alarming rate due to the growth of training data and model scale, resulting in a negative impact on carbon neutrality. Energy consumption is an especially pressing issue for AutoML algorithms because it usually requires repeatedly training large numbers of computationally intensive deep models to search for optimal configurations. This paper takes one of the most essential steps in developing energy-aware (EA) NAS methods, by providing a benchmark that makes EA-NAS research more reproducible and accessible. Specifically, we present the first large-scale energy-aware benchmark that allows studying AutoML methods to achieve better trade-offs between performance and search energy consumption, named EA-HAS-Bench. EA-HAS-Bench provides a large-scale architecture/hyperparameter joint search space, covering diversified configurations related to energy consumption. Furthermore, we propose a novel surrogate model specially designed for large joint search space, which proposes a Bezier curve-based model to predict learning curves with unlimited shape and length. Based on the proposed dataset, we new energy-aware AutoML method that arms existing AutoML algorithms to consider the search energy consumption, and our experiments show that the modified energy-aware AutoML methods achieve a better trade-off between energy consumption and model performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                763. <h><a href=https://openreview.net/pdf?id=10uNUgI5Kl>Reward Design with Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=10uNUgI5Kl>6.5 - [5, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We make reward design easier by using large language models models (like GPT-3) as a proxy for a user&#39;s reward function given that a user provides a few examples (or a description) of the desired behavior.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/10uNUgI5Kl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reward design in reinforcement learning (RL) is challenging since specifying human notions of desired behavior may be difficult via reward functions or require many expert demonstrations. Can we instead cheaply design rewards using a natural language interface? This paper explores how to simplify reward design by using a large language model (LLM) such as GPT-3 as a proxy reward function, where the user provides a textual prompt containing a few examples (few-shot) or a description (zero-shot) of desired behavior. Our approach leverages this proxy reward function in an RL framework. Specifically, users specify a prompt once at the beginning of training. During training, the LLM evaluates an RL agent&#39;s behavior against the desired behavior described by the prompt and outputs a corresponding reward signal. The RL agent then uses this reward to update its behavior. We evaluate whether our approach can train agents aligned with user objectives in the Ultimatum Game, matrix games, and the DealOrNoDeal negotiation task. In all three tasks, we show that RL agents trained with our framework are well-aligned with the user&#39;s objectives and outperforms RL agents trained with reward functions learned via supervised learning. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                764. <h><a href=https://openreview.net/pdf?id=P8YIphWNEGO>MLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaotian Han, Tong Zhao, Yozen Liu, Xia Hu, Neil Shah</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=P8YIphWNEGO>6.5 - [5, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> we propose an embarrassingly simple, yet hugely effective initialization for GNN training acceleration by initializing GNN with full trained MLP.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/P8YIphWNEGO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training graph neural networks (GNNs) on large graphs is complex and extremely time consuming. This is attributed to overheads caused by sparse matrix multiplication, which are sidestepped when training multi-layer perceptrons (MLPs) with only node features. MLPs, by ignoring graph context, are simple and faster for graph data, however they usually sacrifice prediction accuracy, limiting their applications for graph data. We observe that for most message passing-based GNNs, we can trivially derive an analog MLP (we call this a PeerMLP) with an equivalent weight space, by setting the trainable parameters with the same shapes, making us curious about how do GNNs using weights from a fully trained PeerMLP perform? Surprisingly, we find that GNNs initialized with such weights significantly outperform their PeerMLPs, motivating us to use PeerMLP training as a precursor, initialization step to GNN training. To this end, we propose an embarrassingly simple, yet hugely effective initialization method for GNN training acceleration, called \mlpinit. Our extensive experiments on multiple large-scale graph datasets with diverse GNN architectures validate that MLPInit can accelerate the training of GNNs (up to 33× speedup on OGB-Products) and often improve prediction performance (e.g., up to $7.97\%$ improvement for GraphSAGE across $7$ datasets for node classification, and up to $17.81\%$ improvement across $4$ datasets for link prediction on metric Hits@10). The code is available at https://github.com/snap-research/MLPInit-for-GNNs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                765. <h><a href=https://openreview.net/pdf?id=mFDU0fP3EQH>Discovering Evolution Strategies via Meta-Black-Box Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Robert Tjarko Lange, Tom Schaul, Yutian Chen, Tom Zahavy, Valentin Dalibard, Chris Lu, Satinder Singh, Sebastian Flennerhag</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mFDU0fP3EQH>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We meta-learn evolution strategies, which flexibly generalize to unseen optimization problems, population sizes and optimization horizons. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mFDU0fP3EQH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Optimizing functions without access to gradients is the remit of black-box meth- ods such as evolution strategies. While highly general, their learning dynamics are often times heuristic and inflexible — exactly the limitations that meta-learning can address. Hence, we propose to discover effective update rules for evolution strategies via meta-learning. Concretely, our approach employs a search strategy parametrized by a self-attention-based architecture, which guarantees the update rule is invariant to the ordering of the candidate solutions. We show that meta-evolving this system on a small set of representative low-dimensional analytic optimization problems is sufficient to discover new evolution strategies capable of generalizing to unseen optimization problems, population sizes and optimization horizons. Furthermore, the same learned evolution strategy can outperform established neuroevolution baselines on supervised and continuous control tasks. As additional contributions, we ablate the individual neural network components of our method; reverse engineer the learned strategy into an explicit heuristic form, which remains highly competitive; and show that it is possible to self-referentially train an evolution strategy from scratch, with the learned update rule used to drive the outer meta-learning loop.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                766. <h><a href=https://openreview.net/pdf?id=IDJx97BC38>SQA3D: Situated Question Answering in 3D Scenes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, Siyuan Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IDJx97BC38>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a grand challenge for embodied agents to understand situations and reason about 3D scenes accordingly.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IDJx97BC38.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                767. <h><a href=https://openreview.net/pdf?id=vqSyt8D3ny>Towards Robust Object Detection Invariant to Real-World Domain Shifts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qi Fan, Mattia Segu, Yu-Wing Tai, Fisher Yu, Chi-Keung Tang, Bernt Schiele, Dengxin Dai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vqSyt8D3ny>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We perturb feature channel statistics to generalize object detectors under real-world domain shifts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vqSyt8D3ny.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Safety-critical applications such as autonomous driving require robust object detection invariant to real-world domain shifts. Such shifts can be regarded as different domain styles, which can vary substantially due to environment changes and sensor noises, but deep models only know the training domain style. Such domain style gap impedes object detection generalization on diverse real-world domains. Existing classification domain generalization (DG) methods cannot effectively solve the robust object detection problem, because they either rely on multiple source domains with large style variance or destroy the content structures of the original images. In this paper, we analyze and investigate effective solutions to overcome domain style overfitting for robust object detection without the above shortcomings. Our method, dubbed as Normalization Perturbation (NP), perturbs the channel statistics of source domain low-level features to synthesize various latent styles, so that the trained deep model can perceive diverse potential domains and generalizes well even without observations of target domain data in training. This approach is motivated by the observation that feature channel statistics of the target domain images deviate around the source domain statistics. We further explore the style-sensitive channels for effective style synthesis. Normalization Perturbation only relies on a single source domain and is surprisingly simple and effective, contributing a practical solution by effectively adapting or generalizing classification DG methods to robust object detection. Extensive experiments demonstrate the effectiveness of our method for generalizing object detectors under real-world domain shifts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                768. <h><a href=https://openreview.net/pdf?id=TdBaDGCpjly>Transformer-based World Models Are Happy With 100k Interactions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jan Robine, Marc Höftmann, Tobias Uelwer, Stefan Harmeling</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TdBaDGCpjly>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TdBaDGCpjly.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks have been successful in many reinforcement learning settings. However, compared to human learners they are overly data hungry. To build a sample-efficient world model, we apply a transformer to real-world episodes in an autoregressive manner: not only the compact latent states and the taken actions but also the experienced or predicted rewards are fed into the transformer, so that it can attend flexibly to all three modalities at different time steps. The transformer allows our world model to access previous states directly, instead of viewing them through a compressed recurrent state. By utilizing the Transformer-XL architecture, it is able to learn long-term dependencies while staying computationally efficient. Our transformer-based world model (TWM) generates meaningful, new experience, which is used to train a policy that outperforms previous model-free and model-based reinforcement learning algorithms on the Atari 100k benchmark. Our code is available at https://github.com/jrobine/twm.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                769. <h><a href=https://openreview.net/pdf?id=ctmLBs8lITa>Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Linbo Liu, Youngsuk Park, Trong Nghia Hoang, Hilaf Hasson, Luke Huan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ctmLBs8lITa>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Designs of Adversarial Attacks and Defense Mechanisms for Multivariate Forecasting Models
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ctmLBs8lITa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                770. <h><a href=https://openreview.net/pdf?id=qcJmsP3oE9>Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao Tang, XIAOJUAN QI, Guolei Sun, Dan Xu, Nicu Sebe, Radu Timofte, Luc Van Gool</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qcJmsP3oE9>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel contrastive learning based edge guided GAN for semantic image synthesis.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qcJmsP3oE9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a novel \underline{e}dge guided \underline{g}enerative \underline{a}dversarial \underline{n}etwork with \underline{c}ontrastive learning (ECGAN) for the challenging semantic image synthesis task. Although considerable improvement has been achieved, the quality of synthesized images is far from satisfactory due to three largely unresolved challenges. 1) The semantic labels do not provide detailed structural information, making it difficult to synthesize local details and structures. 2) The widely adopted CNN operations such as convolution, down-sampling, and normalization usually cause spatial resolution loss and thus cannot fully preserve the original semantic information, leading to semantically inconsistent results (e.g., missing small objects). 3) Existing semantic image synthesis methods focus on modeling ``local&#39;&#39; semantic information from a single input semantic layout. However, they ignore ``global&#39;&#39; semantic information of multiple input semantic layouts, i.e., semantic cross-relations between pixels across different input layouts. To tackle 1), we propose to use edge as an intermediate representation which is further adopted to guide image generation via a proposed attention guided edge transfer module. Edge information is produced by a convolutional generator and introduces detailed structure information. To tackle 2), we design an effective module to selectively highlight class-dependent feature maps according to the original semantic layout to preserve the semantic information. To tackle 3), inspired by current methods in contrastive learning, we propose a novel contrastive learning method, which aims to enforce pixel embeddings belonging to the same semantic class to generate more similar image content than those from different classes. Doing so can capture more semantic relations by explicitly exploring the structures of labeled pixels from multiple input semantic layouts. Experiments on three challenging datasets show that our ECGAN achieves significantly better results than state-of-the-art methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                771. <h><a href=https://openreview.net/pdf?id=Kf7Yyf4O0u>CANIFE: Crafting Canaries for Empirical Privacy Measurement in Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samuel Maddock, Alexandre Sablayrolles, Pierre Stock</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Kf7Yyf4O0u>6.5 - [5, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Crafting canaries to measure empirical privacy of DP-FL training under a realistic threat model
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Kf7Yyf4O0u.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated Learning (FL) is a setting for training machine learning models in distributed environments where the clients do not share their raw data but instead send model updates to a server. However, model updates can be subject to attacks and leak private information. Differential Privacy (DP) is a leading mitigation strategy which involves adding noise to clipped model updates, trading off performance for strong theoretical privacy guarantees. Previous work has shown that the threat model of DP is conservative and that the obtained guarantees may be vacuous or may overestimate information leakage in practice. In this paper, we aim to achieve a tighter measurement of the model exposure by considering a realistic threat model. We propose a novel method, CANIFE, that uses canaries - carefully crafted samples by a strong adversary to evaluate the empirical privacy of a training round. We apply this attack to vision models trained on CIFAR-10 and CelebA and to language models trained on Sent140 and Shakespeare. In particular, in realistic FL scenarios, we demonstrate that the empirical per-round epsilon obtained with CANIFE is 4 -- 5$\times$ lower than the theoretical bound.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                772. <h><a href=https://openreview.net/pdf?id=-nm-rHXi5ga>On the Data-Efficiency with Contrastive Image Transformation in Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sicong Liu, Xi Sheryl Zhang, Yushuo Li, Yifan Zhang, Jian Cheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-nm-rHXi5ga>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> CoIT is a learnable image transformation for sample-efficiency improvement.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-nm-rHXi5ga.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Data-efficiency has always been an essential issue in pixel-based reinforcement learning (RL). As the agent not only learns decision-making but also meaningful representations from images. The line of reinforcement learning with data augmentation shows significant improvements in sample-efficiency. However, it is challenging to guarantee the optimality invariant transformation, that is, the augmented data are readily recognized as a completely different state by the agent. In the end, we propose a contrastive invariant transformation (CoIT), a simple yet promising learnable data augmentation combined with standard model-free algorithms to improve sample-efficiency. Concretely, the differentiable CoIT leverages original samples with augmented samples and hastens the state encoder for a contrastive invariant embedding. We evaluate our approach on DeepMind Control Suite and Atari100K. Empirical results verify advances using CoIT, enabling it to outperform the new state-of-the-art on various tasks. Source code is available at https://github.com/mooricAnna/CoIT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                773. <h><a href=https://openreview.net/pdf?id=LGkmUauBUL>Distributional Meta-Gradient Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haiyan Yin, Shuicheng YAN, Zhongwen Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LGkmUauBUL>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A model-free meta gradient RL algorithm with distributional return
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LGkmUauBUL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Meta-gradient reinforcement learning (RL) algorithms have substantially boosted the performance of RL agents by learning an adaptive return. All the existing algorithms adhere to the same reward learning principle, where the adaptive return is simply formulated in the form of expected cumulative rewards, upon which the policy and critic update rules are specified under well-adopted distance metrics. In this paper, we present a novel algorithm that builds on the success of meta-gradient RL algorithms and effectively improves such algorithms by following a simple recipe, i.e., going beyond the expected return to formulate and learn the return in a more expressive form, value distributions. To this end, we first formulate a distributional return that could effectively capture bootstrapping and discounting behaviors over distributions, to form an informative distributional return target in value update. Then we derive an efficient meta update rule to learn the adaptive distributional return with meta-gradients. For empirical evaluation, we first present an illustrative example on a toy two-color grid-world domain, which validates the benefit of learning distributional return over expectation; then we conduct extensive comparisons on a large-scale RL benchmark Atari 2600, where we confirm that our proposed method with distributional return works seamlessly well with the actor-critic framework and leads to state-of-the-art median human normalized score among meta-gradient RL literature.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                774. <h><a href=https://openreview.net/pdf?id=PaEUQiY40Dk>Towards Understanding Why Mask Reconstruction Pretraining Helps in Downstream Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiachun Pan, Pan Zhou, Shuicheng YAN</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PaEUQiY40Dk>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PaEUQiY40Dk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    For unsupervised pretraining, mask-reconstruction pretraining (MRP) approaches, e.g. MAE and data2vec, randomly mask input patches and then reconstruct the pixels or semantic features of these masked patches via an auto-encoder. Then for a downstream task, supervised fine-tuning the pretrained encoder remarkably surpasses the conventional &#34;supervised learning&#34; (SL) trained from scratch. However, it is still unclear 1) how MRP performs semantic (feature) learning in the pretraining phase and 2) why it helps in downstream tasks.  To solve these problems, we first theoretically show that on an auto-encoder of a two/one-layered convolution encoder/decoder, MRP can capture all discriminative semantics of each potential semantic class in the pretraining dataset. Then considering the fact that the pretraining dataset is of huge size and high diversity and thus covers most semantics in downstream dataset, in fine-tuning phase, the pretrained encoder can capture as much semantics as it can in downstream datasets, and would not lost these semantics with theoretical guarantees.  In contrast, SL only randomly captures some semantics due to lottery ticket hypothesis. So  MRP provably achieves better performance than SL  on the classification tasks.   Experimental results testify to our data assumptions and also our theoretical implications. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                775. <h><a href=https://openreview.net/pdf?id=5lgD4vU-l24s>Recursive Time Series Data Augmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Amine Mohamed Aboussalah, Minjae Kwon, Raj G Patel, Cheng Chi, Chi-Guhn Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5lgD4vU-l24s>6.5 - [10, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5lgD4vU-l24s.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Time series observations can be seen as realizations of an underlying dynamical system governed by rules that we typically do not know. For time series learning tasks we create our model using available data. Training on available realizations, where data is limited, often induces severe over-fitting thereby preventing generalization. To address this issue, we introduce a general recursive framework for time series augmentation, which we call the Recursive Interpolation Method (RIM). New augmented time series are generated using a recursive interpolation function from the original time series for use in training. We perform theoretical analysis to characterize the proposed RIM and to guarantee its performance under certain conditions. We apply RIM to diverse synthetic and real-world time series cases to achieve strong performance over non-augmented data on a variety of learning tasks. Our method is also computationally more efficient and leads to better performance when compared to state of the art time series data augmentation.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                776. <h><a href=https://openreview.net/pdf?id=wcNtbEtcGIC>Robust and Controllable Object-Centric Learning through Energy-based Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruixiang ZHANG, Tong Che, Boris Ivanovic, Renhao Wang, Marco Pavone, Yoshua Bengio, Liam Paull</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wcNtbEtcGIC>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wcNtbEtcGIC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Humans are remarkably good at understanding and reasoning about complex visual scenes. The capability of decomposing low-level observations into discrete objects allows us to build a grounded abstract representation and identify the compositional structure of the world. Thus it is a crucial step for machine learning models to be capable of inferring objects and their properties from visual scene without explicit supervision. However, existing works on object-centric representation learning are either relying on tailor-made neural network modules or assuming sophisticated models of underlying generative and inference processes. In this work, we present EGO, a conceptually simple and general approach to learning object-centric representation through energy-based model. By forming a permutation-invariant energy function using vanilla attention blocks that are readily available in Transformers, we can infer object-centric latent variables via gradient-based MCMC methods where permutation equivariance is automatically guaranteed. We show that EGO can be easily integrated into existing architectures, and can effectively extract high-quality object-centric representations, leading to better segmentation accuracy and competitive downstream task performance. We empirically evaluate the robustness of the learned representation from EGO against distribution shift. Finally, we demonstrate the effectiveness of EGO in systematic compositional generalization, by recomposing learned energy functions for novel scene generation and manipulation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                777. <h><a href=https://openreview.net/pdf?id=8gd4M-_Rj1>Hebbian Deep Learning Without Feedback</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Adrien Journé, Hector Garcia Rodriguez, Qinghai Guo, Timoleon Moraitis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8gd4M-_Rj1>6.5 - [6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Advancing the state of the art in bio-plausible Deep Learning, and the plausibility of DL, through Hebbian plasticity and soft winner-take-all nets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8gd4M-_Rj1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent approximations to backpropagation (BP) have mitigated many of BP&#39;s computational inefficiencies and incompatibilities with biology, but important limitations still remain. Moreover, the approximations significantly decrease accuracy in benchmarks, suggesting that an entirely different approach may be more fruitful. Here, grounded on recent theory for Hebbian learning in soft winner-take-all networks, we present multilayer SoftHebb, i.e. an algorithm that trains deep neural networks, without any feedback, target, or error signals. As a result, it achieves efficiency by avoiding weight transport, non-local plasticity, time-locking of layer updates, iterative equilibria, and (self-) supervisory or other feedback signals – which were necessary in other approaches. Its increased efficiency and biological compatibility do not trade off accuracy compared to state-of-the-art bio-plausible learning, but rather improve it. With up to five hidden layers and an added linear classifier, accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%, 80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radically different approach from BP that Deep Learning over few layers may be plausible in the brain and increases the accuracy of bio-plausible machine learning. Code is available at https://github.com/NeuromorphicComputing/SoftHebb.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                778. <h><a href=https://openreview.net/pdf?id=XqcQhVUr2h0>Limitless Stability for Graph Convolutional Networks </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Christian Koke</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XqcQhVUr2h0>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a general and novel stability theory for graph convolutional networks able to deal with undirected graphs as well as topology-changing perturbations
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XqcQhVUr2h0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work establishes rigorous, novel and widely applicable stability guarantees and transferability bounds for general graph convolutional networks  -- without reference to any underlying limit object or statistical distribution. Crucially, utilized graph-shift operators are not necessarily assumed to be normal, allowing for the treatment of networks on both directed- and undirected graphs within the developed framework. In the undirected setting, stability to node-level perturbations is related to an &#39;adequate spectral covering&#39; property of the filters in each layer. Stability to edge-level perturbations is discussed and related to properties of the utilized filters such as their Lipschitz constants. Results on stability to  vertex-set non-preserving perturbations are obtained by utilizing recently developed mathematical-physics based tools. As an exemplifying application of the developed theory, it is showcased that
general graph convolutional networks utilizing the un-normalized graph Laplacian as graph-shift-operator  can be rendered stable to collapsing strong edges in the underlying graph if filters are mandated to be constant at infinity. These theoretical results are supported by corresponding numerical investigations showcasing the response of filters and networks to such perturbations. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                779. <h><a href=https://openreview.net/pdf?id=vtVDI3w_BLL>AANG : Automating Auxiliary Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lucio M. Dery, Paul Michel, Mikhail Khodak, Graham Neubig, Ameet Talwalkar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vtVDI3w_BLL>6.5 - [5, 5, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We automatically generate a suite of auxiliary objectives and give a theoretically informed, efficient algorithm for searching the space of generated objectives to find those most useful to a specified end-task.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vtVDI3w_BLL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Auxiliary objectives, supplementary learning signals that are introduced to help aid learning on data-starved or highly complex end-tasks, are commonplace in machine learning. Whilst much work has been done to formulate useful auxiliary objectives, their construction is still an art which proceeds by slow and tedious hand-design. Intuition for how and when these objectives improve end-task performance has also had limited theoretical backing. In this work, we present an approach for automatically generating a suite of auxiliary objectives. We achieve this by deconstructing existing objectives within a novel unified taxonomy, identifying connections between them, and generating new ones based on the uncovered structure. Next, we theoretically formalize widely-held intuitions about how auxiliary learning improves generalization on the end-task. This leads us to a principled and efficient algorithm for searching the space of generated objectives to find those most useful to a specified end-task.
With natural language processing (NLP) as our domain of study, we demonstrate that our automated auxiliary learning pipeline leads to strong improvements over competitive baselines across continued training experiments on a pre-trained model on 5 NLP end-tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                780. <h><a href=https://openreview.net/pdf?id=NE2911Kq1sp>CktGNN:  Circuit Graph Neural Network for Electronic Design Automation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zehao Dong, Weidong Cao, Muhan Zhang, Dacheng Tao, Yixin Chen, Xuan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NE2911Kq1sp>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NE2911Kq1sp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have only been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves efficiency by reducing the number of subgraphs to perform message passing.

Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains $10$K distinct operational amplifiers with carefully-extracted circuit specifications from physical implementations. OCB also equips with communicative circuit generation and evaluation capabilities such that it can be used to generalize the applicability of CktGNN to design various analog circuits by efficiently producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and manual design from human experts. Our work paves the way toward a learning-based open-sourced design automation flow for analog circuits.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                781. <h><a href=https://openreview.net/pdf?id=4u42KCQxCn8>Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Albert Yu, Ray Mooney</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4u42KCQxCn8>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Conditioning robotic manipulation policies on both demonstrations and language instructions improves sample efficiency and generalization to novel tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4u42KCQxCn8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Demonstrations and natural language instructions are two common ways to specify and teach robots novel tasks. However, for many complex tasks, a demonstration or language instruction alone contains ambiguities, preventing tasks from being specified clearly. In such cases, a combination of both a demonstration and an instruction more concisely and effectively conveys the task to the robot than either modality alone. To instantiate this problem setting, we train a single multi-task policy on a few hundred challenging robotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task Conditioning), a method for conditioning a robotic policy on task embeddings comprised of two components: a visual demonstration and a language instruction. By allowing these two modalities to mutually disambiguate and clarify each other during novel task specification, DeL-TaCo (1) substantially decreases the teacher effort needed to specify a new task and (2) achieves better generalization performance on novel objects and instructions over previous task-conditioning methods. To our knowledge, this is the first work to show that simultaneously conditioning a multi-task robotic manipulation policy on both demonstration and language embeddings improves sample efficiency and generalization over conditioning on either modality alone.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                782. <h><a href=https://openreview.net/pdf?id=Q_Jexl8-qDi>De Novo Molecular Generation via Connection-aware Motif Mining</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zijie Geng, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Jie Wang, Yongdong Zhang, Feng Wu, Tie-Yan Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Q_Jexl8-qDi>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a fragment-based model for molecular generation. It first mines connection-aware motifs from the molecule library and then leverage a connection-aware generator to generate novel drug candidates.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Q_Jexl8-qDi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    De novo molecular generation is an essential task for science discovery. Recently, fragment-based deep generative models have attracted much research attention due to their flexibility in generating novel molecules based on existing molecule fragments. However, the motif vocabulary, i.e., the collection of frequent fragments, is usually built upon heuristic rules, which brings difficulties to capturing common substructures from large amounts of molecules. In this work, we propose MiCaM to generate molecules based on mined connection-aware motifs. Specifically, it leverages a data-driven algorithm to automatically discover motifs from a molecule library by iteratively merging subgraphs based on their frequency. The obtained motif vocabulary consists of not only molecular motifs (i.e., the frequent fragments), but also their connection information, indicating how the motifs are connected with each other. Based on the mined connection-aware motifs, MiCaM builds a connection-aware generator, which simultaneously picks up motifs and determines how they are connected. We test our method on distribution-learning benchmarks (i.e., generating novel molecules to resemble the distribution of a given training set) and goal-directed benchmarks (i.e., generating molecules with target properties), and achieve significant improvements over previous fragment-based baselines. Furthermore, we demonstrate that our method can effectively mine domain-specific motifs for different tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                783. <h><a href=https://openreview.net/pdf?id=5HLoTvVGDe>Dual Diffusion Implicit Bridges for Image-to-Image Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xuan Su, Jiaming Song, Chenlin Meng, Stefano Ermon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5HLoTvVGDe>6.5 - [6, 10, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5HLoTvVGDe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Common image-to-image translation methods rely on joint training over data from both source and target domains. The training process requires concurrent access to both datasets, which hinders data separation and privacy protection; and existing models cannot be easily adapted for translation of new domain pairs. We present Dual Diffusion Implicit Bridges (DDIBs), an image translation method based on diffusion models, that circumvents training on domain pairs. Image translation with DDIBs relies on two diffusion models trained independently on each domain, and is a two-step process: DDIBs first obtain latent encodings for source images with the source diffusion model, and then decode such encodings using the target model to construct target images. Both steps are defined via ordinary differential equations (ODEs), thus the process is cycle consistent only up to discretization errors of the ODE solvers. Theoretically, we interpret DDIBs as concatenation of source to latent, and latent to target Schrodinger Bridges, a form of entropy-regularized optimal transport, to explain the efficacy of the method. Experimentally, we apply DDIBs on synthetic and high-resolution image datasets, to demonstrate their utility in a wide variety of translation tasks and their inherent optimal transport properties.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                784. <h><a href=https://openreview.net/pdf?id=tPKKXeW33YU>WiNeRT: Towards Neural Ray Tracing for Wireless Channel Modelling and Differentiable Simulations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tribhuvanesh Orekondy, Pratik Kumar, Shreya Kadambi, Hao Ye, Joseph Soriaga, Arash Behboodi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tPKKXeW33YU>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Neural wireless ray tracer
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tPKKXeW33YU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we work towards a neural surrogate to model wireless electro-magnetic propagation effects in indoor environments.
Such neural surrogates provide a fast, differentiable, and continuous representation of the environment and enables end-to-end optimization for downstream tasks (e.g., network planning). Specifically, the goal of the paper is to render the wireless signal (e.g., time-of-flights, power of each path) in an environment as a function of the sensor&#39;s spatial configuration (e.g., placement of transmit and receive antennas). NeRF-based approaches have shown promising results in the visual setting (RGB image signal, with a camera sensor), where the key idea is to algorithmically evaluate the &#39;global&#39; signal (e.g., using volumetric rendering) by breaking it down in a sequence of &#39;local&#39; evaluations (e.g., using co-ordinate neural networks). In a similar spirit, we model the time-angle channel impulse response (the global wireless signal) as a superposition of multiple paths. The wireless characteristics (e.g., power) of each path is a result of multiple evaluations of a neural network that learns implicit ray-surface interaction properties. We evaluate our approach in multiple indoor scenarios and demonstrate that our model achieves strong performance (e.g., $&lt;$0.33ns error in time-of-flight predictions). Furthermore, we demonstrate that our neural surrogate whitens the `black-box&#39; wireless simulators, and thus enables inverse rendering applications (e.g., user localization).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                785. <h><a href=https://openreview.net/pdf?id=qLOaeRvteqbx>Disparate Impact in Differential Privacy from Gradient Misalignment</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Maria S. Esipova, Atiyeh Ashari Ghomi, Yaqiao Luo, Jesse C Cresswell</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qLOaeRvteqbx>6.5 - [8, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> DPSGD can have unfair outcomes on protected groups because of direction errors caused by per-sample gradient clipping, but unfairness can be dramatically reduced with a global clipping technique.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qLOaeRvteqbx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As machine learning becomes more widespread throughout society, aspects including data privacy and fairness must be carefully considered, and are crucial for deployment in highly regulated industries. Unfortunately, the application of privacy enhancing technologies can worsen unfair tendencies in models. In particular, one of the most widely used techniques for private model training, differentially private stochastic gradient descent (DPSGD), frequently intensifies disparate impact on groups within data. In this work we study the fine-grained causes of unfairness in DPSGD and identify gradient misalignment due to inequitable gradient clipping as the most significant source. This observation leads us to a new method for reducing unfairness by preventing gradient misalignment in DPSGD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                786. <h><a href=https://openreview.net/pdf?id=3dnrKbeVatv>Energy-Based Test Sample Adaptation for Domain Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zehao Xiao, Xiantong Zhen, Shengcai Liao, Cees G. M. Snoek</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3dnrKbeVatv>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a discriminative energy-based model to adapt target samples to the source domain distributions for domain generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3dnrKbeVatv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we propose energy-based sample adaptation at test time for domain generalization. Where previous works adapt their models to target domains, we adapt the unseen target samples to source-trained models. To this end, we design a discriminative energy-based model, which is trained on source domains to jointly model the conditional distribution for classification and data distribution for sample adaptation. The model is optimized to simultaneously learn a classifier and an energy function. To adapt target samples to source distributions, we iteratively update the samples by energy minimization with stochastic gradient Langevin dynamics. Moreover, to preserve the categorical information in the sample during adaptation, we introduce a categorical latent variable into the energy-based model. The latent variable is learned from the original sample before adaptation by variational inference and fixed as a condition to guide the sample update. Experiments on six benchmarks for classification of images and microblog threads demonstrate the effectiveness of our proposal.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                787. <h><a href=https://openreview.net/pdf?id=GVSf7Z7DbYL>Teacher Guided Training: An Efficient Framework for Knowledge Transfer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Manzil Zaheer, Ankit Singh Rawat, Seungyeon Kim, Chong You, Himanshu Jain, Andreas Veit, Rob Fergus, Sanjiv Kumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GVSf7Z7DbYL>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose and theoretically analyze a novel way to improve the training efficiency of compact student models that better leverages the knowledge of pretrained generative (teacher) models compared to standard distillation methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GVSf7Z7DbYL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The remarkable performance gains realized by large pretrained models, e.g., GPT-3, hinge on the massive amounts of data they are exposed to during training. Analogously, distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data. In this paper, we propose the teacher-guided training (TGT) framework for training a high-quality compact model that leverages the knowledge acquired by pretrained generative models, while obviating the need to go through a large volume of data. TGT exploits the fact that the teacher has acquired a good representation of the underlying data domain, which typically corresponds to a much lower dimensional manifold than the input space. Furthermore, we can use the teacher to explore input space more efficiently through sampling or gradient-based methods; thus, making TGT especially attractive for limited data or long-tail settings. We formally capture this benefit of proposed data-domain exploration in our generalization bounds. We find that TGT can improve accuracy on several image classification benchmarks as well as a range of text classification and retrieval tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                788. <h><a href=https://openreview.net/pdf?id=8KYeilT3Ow>NAGphormer: A Tokenized Graph Transformer for Node Classification in Large Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinsong Chen, Kaiyuan Gao, Gaichao Li, Kun He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8KYeilT3Ow>6.5 - [5, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel Graph Transformer that utilizes the neighborhood aggregation of multiple hops to build the input sequence of token vectors and thereby can handle large graphs efficiently.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8KYeilT3Ow.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The graph Transformer emerges as a new architecture and has shown superior performance on various graph mining tasks. In this work, we observe that existing graph Transformers treat nodes as independent tokens and construct a single long sequence composed of all node tokens so as to train the Transformer model, causing it hard to scale to large graphs due to the quadratic complexity on the number of nodes for the self-attention computation. To this end, we propose a Neighborhood Aggregation Graph Transformer (NAGphormer) that treats each node as a sequence containing a series of tokens constructed by our proposed Hop2Token module. For each node, Hop2Token aggregates the neighborhood features from different hops into different representations and thereby produces a sequence of token vectors as one input. In this way, NAGphormer could be trained in a mini-batch manner and thus could scale to large graphs. Moreover, we mathematically show that as compared to a category of advanced Graph Neural Networks (GNNs), the decoupled Graph Convolutional Network, NAGphormer could learn more informative node representations from the multi-hop neighborhoods. Extensive experiments on benchmark datasets from small to large are conducted to demonstrate that NAGphormer consistently outperforms existing graph Transformers and mainstream GNNs. Code is available at https://github.com/JHL-HUST/NAGphormer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                789. <h><a href=https://openreview.net/pdf?id=H4Ncs5jhTCu>Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Palenicek, Michael Lutter, Joao Carvalho, Jan Peters</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=H4Ncs5jhTCu>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/H4Ncs5jhTCu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivaled sample efficiency but that the bottleneck lies elsewhere.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                790. <h><a href=https://openreview.net/pdf?id=YUDiZcZTI8>Bit-Pruning: A Sparse Multiplication-Less Dot-Product</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yusuke Sekikawa, Shingo Yashima</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YUDiZcZTI8>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Mult-less dot-product comprised of add and shift; add is pruned during training to reduce energy 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YUDiZcZTI8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dot-product is a central building block in neural networks.
However, multiplication ($\texttt{mult}$) in dot-product consumes intensive energy and space costs that challenge deployment on resource-constrained edge devices.
In this study, we realize energy-efficient neural networks by exploiting a $\texttt{mult}$-less, sparse dot-product. We first reformulate a dot-product between an integer weight and activation into an equivalent operation comprised of additions followed by bit-shifts ($\texttt{add-shift-add}$).
In this formulation, the number of $\texttt{add}$  operations equals the number of bits of the integer weight in binary format. 
Leveraging this observation, we propose Bit-Pruning, which removes unnecessary bits in each weight value during training to reduce the energy consumption of $\texttt{add-shift-add}$. Bit-Pruning can be seen as soft Weight-Pruning as it prunes bits, not the whole weight element.
In extensive experiments, we demonstrate that sparse $\texttt{mult}$-less networks trained with Bit-Pruning show a better accuracy-energy trade-off than sparse $\texttt{mult}$ networks trained with Weight-Pruning. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                791. <h><a href=https://openreview.net/pdf?id=r0otLtOwYW>Equivariant Energy-Guided SDE for Inverse Molecular Design</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, Jun Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=r0otLtOwYW>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Equivariant energy-guided stochastic differential equations for inverse molecular design.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/r0otLtOwYW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inverse molecular design is critical in material science and drug discovery, where the generated molecules should satisfy certain desirable properties. In this paper, we propose equivariant energy-guided stochastic differential equations (EEGSDE), a flexible framework for controllable 3D molecule generation under the guidance of an energy function in diffusion models. Formally, we show that EEGSDE naturally exploits the geometric symmetry in 3D molecular conformation, as long as the energy function is invariant to orthogonal transformations. Empirically, under the guidance of designed energy functions, EEGSDE significantly improves the baseline on QM9, in inverse molecular design targeted to quantum properties and molecular structures. Furthermore, EEGSDE is able to generate molecules with multiple target properties by combining the corresponding energy functions linearly.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                792. <h><a href=https://openreview.net/pdf?id=oztkQizr3kk>$\Lambda$-DARTS: Mitigating Performance Collapse by Harmonizing Operation Selection among Cells</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sajad Movahedi, Melika Adabinejad, Ayyoob Imani, Arezou Keshavarz, Mostafa Dehghani, Azadeh Shakery, Babak N Araabi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oztkQizr3kk>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We pinpoint the reason for performance collapse in DARTS and provide theoretical and empirical analysis on that as well as a solution to remedy the performance collapse via harmonizing the decisions of different cell.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oztkQizr3kk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Differentiable neural architecture search (DARTS) is a popular method for neural architecture search (NAS), which performs cell-search and utilizes continuous relaxation to improve the search efficiency via gradient-based optimization. The main shortcoming of DARTS is performance collapse, where the discovered architecture suffers from a pattern of declining quality during search. Performance collapse has become an important topic of research, with many methods trying to solve the issue through either regularization or fundamental changes to DARTS.
However, the weight-sharing framework used for cell-search in DARTS and the convergence of architecture parameters has not been analyzed yet. In this paper, we provide a thorough and novel theoretical and empirical analysis on DARTS and its point of convergence.
We show that DARTS suffers from a specific structural flaw due to its weight-sharing framework that limits the convergence of DARTS to saturation points of the softmax function. This point of convergence gives an unfair advantage to layers closer to the output in choosing the optimal architecture, causing performance collapse. We then propose two new regularization terms that aim to prevent performance collapse by harmonizing operation selection via aligning gradients of layers. 
Experimental results on six different search spaces and three different datasets show that our method ($\Lambda$-DARTS) does indeed prevent performance collapse, providing justification for our theoretical analysis and the proposed remedy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                793. <h><a href=https://openreview.net/pdf?id=-lGvSmht7a>Sequential Gradient Coding For Straggler Mitigation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nikhil Krishnan Muralee Krishnan, MohammadReza Ebrahimi, Ashish J Khisti</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-lGvSmht7a>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose to improve gradient coding by exploiting the temporal dimension while training deep learning models in distributed cloud systems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-lGvSmht7a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In distributed computing, slower nodes (stragglers) usually become a bottleneck. Gradient Coding (GC), introduced by Tandon et al., is an efficient technique that uses principles of error-correcting codes to distribute gradient computation in the presence of stragglers. In this paper, we consider the distributed computation of a sequence of gradients $\{g(1),g(2),\ldots,g(J)\}$, where processing of each gradient $g(t)$ starts in round-$t$ and finishes by round-$(t+T)$. Here $T\geq 0$ denotes a delay parameter. For the GC scheme, coding is only across computing nodes and this results in a solution where $T=0$. On the other hand, having $T&gt;0$ allows for designing schemes which exploit the temporal dimension as well. In this work, we propose two schemes that demonstrate improved performance compared to GC. Our first scheme combines GC with selective repetition of previously unfinished tasks and achieves improved straggler mitigation. In our second scheme, which constitutes our main contribution, we apply GC  to a subset of the tasks and repetition for the remainder of the tasks. We then multiplex these two classes of tasks across workers and rounds in an adaptive manner, based on past straggler patterns. Using theoretical analysis, we demonstrate that our second scheme achieves significant reduction in the computational load. In our experiments, we study a practical setting of concurrently training multiple neural networks over an AWS Lambda cluster involving 256 worker nodes, where our framework naturally applies. We demonstrate that the latter scheme can yield a 16\% improvement in runtime over the baseline GC scheme, in the presence of naturally occurring, non-simulated stragglers.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                794. <h><a href=https://openreview.net/pdf?id=51GXyzOKOp>Characterizing the Influence of Graph Elements</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zizhang Chen, Peizhao Li, Hongfu Liu, Pengyu Hong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=51GXyzOKOp>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Use influence functions to model the influence of elements in graphs, and understand the model behavior of graph convolution networks. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/51GXyzOKOp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Influence function, a method from the robust statistics, measures the changes of model parameters or some functions about model parameters with respect to the removal or modification of training instances. It is an efficient and useful post-hoc method for studying the interpretability of machine learning models without the need of expensive model re-training. Recently, graph convolution networks (GCNs), which operate on graph data, have attracted a great deal of attention. However, there is no preceding research on the influence functions of GCNs to shed light on the effects of removing training nodes/edges from an input graph. Since the nodes/edges in a graph are interdependent in GCNs, it is challenging to derive influence functions for GCNs. To fill this gap, we started with the simple graph convolution (SGC) model that operates on an attributed graph, and formulated an influence function to approximate the changes of model parameters when a node or an edge is removed from an attributed graph. Moreover, we theoretically analyzed the error bound of the estimated influence of removing an edge. We experimentally validated the accuracy and effectiveness of our influence estimation function. In addition, we showed that the influence function of a SGC model could be used to estimate the impact of removing training nodes/edges on the test performance of the SGC without re-training the model. Finally, we demonstrated how to use influence functions to effectively guide the adversarial attacks on GCNs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                795. <h><a href=https://openreview.net/pdf?id=nUmCcZ5RKF>IS SYNTHETIC DATA FROM GENERATIVE MODELS READY FOR IMAGE RECOGNITION?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song Bai, XIAOJUAN QI</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nUmCcZ5RKF>6.5 - [8, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present the first study on the state-of-the-art text-to-image generation models for image recognition.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nUmCcZ5RKF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent text-to-image generation models have shown promising results in generating high-fidelity photo-realistic images. Though the results are astonishing to human eyes, how applicable these generated images are for recognition tasks remains under-explored. In this work, we extensively study whether and how synthetic images generated from state-of-the-art text-to-image generation models can be used for image recognition tasks, and focus on two perspectives: synthetic data for improving classification models in the data-scare settings (i.e. zero-shot and few-shot), and synthetic data for large-scale model pre-training for transfer learning. We showcase the powerfulness and shortcomings of synthetic data from existing generative models, and propose strategies for better applying synthetic data for recognition tasks. Code: https://github.com/CVMI-Lab/SyntheticData. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                796. <h><a href=https://openreview.net/pdf?id=ConT6H7MWL>IDEAL: Query-Efficient Data-Free Learning from Black-Box Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jie Zhang, Chen Chen, Lingjuan Lyu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ConT6H7MWL>6.5 - [5, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> query-efficiently learn from black-box model APIs to train a good student without any real data
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ConT6H7MWL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge Distillation (KD) is a typical method for training a lightweight student model with the help of a well-trained teacher model. 
However, most KD methods require access to either the teacher&#39;s training data or model parameter, which is unrealistic. To tackle this problem, recent works study KD under data-free and black-box settings. Nevertheless, these works require a large number of queries to the teacher model, which incurs significant monetary and computational costs. To address these problems, we propose a novel method called \emph{query-effIcient Data-free lEarning from blAck-box modeLs} (IDEAL), which aims to query-efficiently learn from black-box model APIs to train a good student without any real data.   In detail, IDEAL trains the student model in two stages: data generation and model distillation. Note that IDEAL does not require any query in the data generation stage and queries the teacher only once for each sample in the distillation stage. Extensive experiments on various real-world datasets show the effectiveness of the proposed IDEAL. For instance, IDEAL can improve the performance of the best baseline method DFME by 5.83\% on CIFAR10 dataset with only $0.02\times$ the query budget of DFME.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                797. <h><a href=https://openreview.net/pdf?id=N9Pk5iSCzAn>Towards Open Temporal Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaituo Feng, Changsheng Li, Xiaolu Zhang, JUN ZHOU</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=N9Pk5iSCzAn>6.5 - [8, 6, 6, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we propose a general and principled learning approach for open temporal graphs where the class set for nodes is open.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/N9Pk5iSCzAn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural networks (GNNs) for temporal graphs have recently attracted increasing attentions, where a common assumption is that the class set for nodes is closed. However, in real-world scenarios, it often faces the open set problem with the dynamically increased class set as the time passes by. This will bring two big challenges to the existing dynamic GNN methods: (i) How to dynamically propagate appropriate information in an open temporal graph, where new class nodes are often linked to old class nodes. This case will lead to a sharp contradiction. This is because typical GNNs are prone to make the embeddings of connected nodes become similar, while we expect the embeddings of these two interactive nodes to be distinguishable since they belong to different classes. (ii) How to avoid catastrophic knowledge forgetting over old classes when learning new classes occurred in temporal graphs. In this paper, we propose a general and principled learning approach for open temporal graphs, called OTGNet, with the goal of addressing the above two challenges. We assume the knowledge of a node can be disentangled into class-relevant and class-agnostic one, and thus explore a new message passing mechanism by extending the information bottleneck principle to only propagate class-agnostic knowledge between nodes of different classes, avoiding aggregating conflictive information. Moreover, we devise a strategy to select both important and diverse triad sub-graph structures for effective class-incremental learning. Extensive experiments on three real-world datasets of different domains demonstrate the superiority of our method, compared to the baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                798. <h><a href=https://openreview.net/pdf?id=8aeSJNbmbQq>Deep Variational Implicit Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Luis A. Ortega, Simon Rodriguez Santana, Daniel Hernández-Lobato</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8aeSJNbmbQq>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b>  We propose here a multi-layer generalization of IPs called the Deep Variational Implicit process, similar to that of deep GPs over GPs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8aeSJNbmbQq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Implicit processes (IPs) are a generalization of Gaussian processes (GPs). IPs may lack a closed-form expression but are easy to sample from. Examples include, among others, Bayesian neural networks or neural samplers. IPs can be used as priors over functions, resulting in flexible models with well-calibrated prediction uncertainty estimates. Methods based on IPs usually carry out function-space approximate inference, which overcomes some of the difficulties of parameter-space approximate inference. Nevertheless, the approximations employed often limit the expressiveness of the final model, resulting, e.g., in a Gaussian predictive distribution, which can be restrictive. We propose here a multi-layer generalization of IPs called the Deep Variational Implicit process (DVIP). This generalization is similar to that of deep GPs over GPs, but it is more flexible due to the use of IPs as the prior distribution over the latent functions. We describe a scalable variational inference algorithm for training DVIP and show that it outperforms previous IP-based methods and also deep GPs. We support these claims via extensive regression and classification experiments. We also evaluate DVIP on large datasets with up to several million data instances to illustrate its good scalability and performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                799. <h><a href=https://openreview.net/pdf?id=OxNQXyZK-K8>Boosting Multiagent Reinforcement Learning via Permutation Invariant and Permutation Equivariant Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianye HAO, Xiaotian Hao, Hangyu Mao, Weixun Wang, Yaodong Yang, Dong Li, YAN ZHENG, Zhen Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OxNQXyZK-K8>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OxNQXyZK-K8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The state space in Multiagent Reinforcement Learning (MARL) grows exponentially with the agent number. Such a curse of dimensionality results in poor scalability and low sample efficiency, inhibiting MARL for decades. To break this curse, we propose a unified agent permutation framework that exploits the permutation invariance (PI) and permutation equivariance (PE) inductive biases to reduce the multiagent state space. Our insight is that permuting the order of entities in the factored multiagent state space does not change the information. Specifically, we propose two novel implementations: a Dynamic Permutation Network (DPN) and a Hyper Policy Network (HPN). The core idea is to build separate entity-wise PI input and PE output network modules to connect the entity-factored state space and action space in an end-to-end way. DPN achieves such connections by two separate module selection networks, which consistently assign the same input module to the same input entity (guarantee PI) and assign the same output module to the same entity-related output (guarantee PE). To enhance the representation capability, HPN replaces the module selection networks of DPN with hypernetworks to directly generate the corresponding module weights. Extensive experiments in SMAC, Google Research Football and MPE validate that the proposed methods significantly boost the performance and the learning efficiency of existing MARL algorithms. Remarkably, in SMAC, we achieve 100% win rates in almost all hard and super-hard scenarios (never achieved before).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                800. <h><a href=https://openreview.net/pdf?id=s-c96mSU0u5>SCoMoE: Efficient Mixtures of Experts with Structured Communication</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>zhiyuan zeng, Deyi Xiong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=s-c96mSU0u5>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/s-c96mSU0u5.jpg loading="lazy">
                </div>
                <div class="abstract">
                      Mixture-of-Experts (MoE) models are promising architectures for massively multilingual neural machine translation and large language models due to the advantage of sublinear scaling. However, the training of large MoE models is usually bottlenecked by the all-to-all communication (Lepikhin et al., 2020). To reduce the communication cost, we propose SCoMoE, an MoE architecture with structured all-to-all communication, inspired by the hierarchical architecture of the communication topology. SCoMoE encourages data to be communicated across devices through fast intra-accelerator/node communication channels, reducing communication throughput in the slow inter-node communication channel. We slice the data on the sequence dimension (SCoMoE-Seq) into three communication groups and project the data on the feature dimension (SCoMoE-Feat) into low-dimensional representations. To compensate the potential performance drop caused by the routing locality in SCoMoE, we further propose a token clustering approach to aggregating related tokens from different devices before the MoE layers. The sigmoid gating in the balanced router used in the token clustering is substituted with the softmax gating with differential sorting. Experiments on bilingual and massively multilingual machine translation demonstrate that SCoMoE achieves a speedup of 1.44x over GShard with comparable performance, and substantially outperforms Gshard (2.8 BLEU) on OPUS-100 with a speedup of 1.25x.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                801. <h><a href=https://openreview.net/pdf?id=tFvr-kYWs_Y>On the Saturation Effect of Kernel Ridge Regression</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yicheng Li, Haobo Zhang, Qian Lin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tFvr-kYWs_Y>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tFvr-kYWs_Y.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The saturation effect refers to the  phenomenon that the kernel ridge regression (KRR) fails to achieve the information theoretical lower bound when the smoothness of the underground truth function exceeds certain level. The saturation effect  has been widely observed in practices and a saturation lower bound of KRR has been conjectured for decades. In this paper, we provide a proof of this long-standing conjecture.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                802. <h><a href=https://openreview.net/pdf?id=ILQVw4cA5F9>LDMIC: Learning-based Distributed Multi-view Image Coding</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xinjie Zhang, Jiawei Shao, Jun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ILQVw4cA5F9>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design a multi-view image compression framework based on symmetric distributed source coding paradigm, which achieves higher compression performance than previous multi-view image compression methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ILQVw4cA5F9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relationships between images. Experimental results show that LDMIC significantly outperforms both traditional and learning-based MIC methods while enjoying fast encoding speed. Code is released at https://github.com/Xinjie-Q/LDMIC.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                803. <h><a href=https://openreview.net/pdf?id=-2zfgNS917>BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qinhong Jiang, Feng Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-2zfgNS917>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We leverage LiDAR-based knowledge into multi-view 3d detectors with cross-modal BEV distillation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-2zfgNS917.jpg loading="lazy">
                </div>
                <div class="abstract">
                    3D object detection from multiple image views is a fundamental and challenging task for visual scene understanding. Owing to its low cost and high efficiency, multi-view 3D object detection has demonstrated promising application prospects. However, accurately detecting objects through perspective views is extremely difficult due to the lack of depth information. Current approaches tend to adopt heavy backbones for image encoders, making them inapplicable for real-world deployment. Different from the images, LiDAR points are superior in providing spatial cues, resulting in highly precise localization. In this paper, we explore the incorporation of LiDAR-based detectors for multi-view 3D object detection. Instead of directly training a depth prediction network, we unify the image and LiDAR features in the Bird-Eye-View (BEV) space and adaptively transfer knowledge across non-homogenous representations in a teacher-student paradigm. To this end, we propose BEVDistill, a cross-modal BEV knowledge distillation (KD) framework for multi-view 3D object detection. 
Extensive experiments demonstrate that the proposed method outperforms current KD approaches on a highly-competitive baseline, BEVFormer, without introducing any extra cost in the inference phase. Notably, our best model achieves 59.4 NDS on the nuScenes test leaderboard, achieving new state-of-the-arts in comparison with various image-based detectors. Code will be available at https://github.com/zehuichen123/BEVDistill.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                804. <h><a href=https://openreview.net/pdf?id=Nk2pDtuhTq>Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, Huan Sun, Yoon Kim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Nk2pDtuhTq>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose multitask prompt tuning which learns a single transferable prompt by decomposing and distilling knowledge from multiple task-specific source prompts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Nk2pDtuhTq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning $0.035\%$ as many task-specific parameters.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                805. <h><a href=https://openreview.net/pdf?id=fe2S7736sNS>$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, Yongdong Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fe2S7736sNS>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We enable data scaling under the gradient-free paradigm of large language models using kNN inference, and bring substantial improvements over standard In-Context Learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fe2S7736sNS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art calibration-based methods under comparable few-shot scenario. 2) Beyond-Context: $k$NN Prompting can further scale up effectively with as many training data as are available, continually bringing substantial improvements. The scaling trend holds across 10 orders of magnitude ranging from 2 shots to 1024 shots as well as different LLMs scales ranging from 0.8B to 30B. It successfully bridges data scaling into model scaling, and brings new potentials for the gradient-free paradigm of LLM deployment. Code is publicly available at https://github.com/BenfengXu/KNNPrompting
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                806. <h><a href=https://openreview.net/pdf?id=jevY-DtiZTR>Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, Stan Z. Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jevY-DtiZTR>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We explain the negative transfer in molecular graph pre-training and develop two novel pre-training strategies to alleviate this issue.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jevY-DtiZTR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent years have witnessed the prosperity of pre-training graph neural networks (GNNs) for molecules. Typically, atom types as node attributes are randomly masked, and GNNs are then trained to predict masked types as in AttrMask \citep{hu2020strategies}, following the Masked Language Modeling (MLM) task of BERT~\citep{devlin2019bert}. However, unlike MLM with a large vocabulary, the AttrMask pre-training does not learn informative molecular representations due to small and unbalanced atom `vocabulary&#39;. To amend this problem, we propose a variant of VQ-VAE~\citep{van2017neural} as a context-aware tokenizer to encode atom attributes into chemically meaningful discrete codes. This can enlarge the atom vocabulary size and mitigate the quantitative divergence between dominant (e.g., carbons) and rare atoms (e.g., phosphorus). With the enlarged atom `vocabulary&#39;, we propose a novel node-level pre-training task, dubbed Masked Atoms Modeling (\textbf{MAM}), to mask some discrete codes randomly and then pre-train GNNs to predict them. MAM also mitigates another issue of AttrMask, namely the negative transfer. It can be easily combined with various pre-training tasks to improve their performance. Furthermore, we propose triplet masked contrastive learning (\textbf{TMCL}) for graph-level pre-training to model the heterogeneous semantic similarity between molecules for effective molecule retrieval. MAM and TMCL constitute a novel pre-training framework, \textbf{Mole-BERT}, which can match or outperform state-of-the-art methods in a fully data-driven manner. We release the code at \textcolor{magenta}{\url{https://github.com/junxia97/Mole-BERT}}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                807. <h><a href=https://openreview.net/pdf?id=v8JIQdiN9Sh>On the Effectiveness of Out-of-Distribution Data in Self-Supervised Long-Tail Learning.</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianhong Bai, Zuozhu Liu, Hualiang Wang, Jin Hao, YANG FENG, Huanpeng Chu, Haoji Hu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=v8JIQdiN9Sh>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/v8JIQdiN9Sh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Though Self-supervised learning (SSL) has been widely studied as a promising technique for representation learning, it doesn&#39;t generalize well on long-tailed datasets due to the majority classes dominating the feature space. Recent work shows that the long-tailed learning performance could be boosted by sampling extra in-domain (ID) data for self-supervised training, however, large-scale ID data which can rebalance the minority classes are expensive to collect. In this paper, we propose an alternative but easy-to-use and effective solution, \textbf{C}ontrastive with \textbf{O}ut-of-distribution (OOD) data for \textbf{L}ong-\textbf{T}ail learning (COLT), which can effectively exploit OOD data to dynamically re-balance the feature space. We empirically identify the counter-intuitive usefulness of OOD samples in SSL long-tailed learning and principally design a novel SSL method. Concretely, we first localize the `\emph{head}&#39; and `\emph{tail}&#39; samples by assigning a tailness score to each OOD sample based on its neighborhoods in the feature space. Then, we propose an online OOD sampling strategy to dynamically re-balance the feature space. Finally, we enforce the model to be capable of distinguishing ID and OOD samples by a distribution-level supervised contrastive loss. Extensive experiments are conducted on various datasets and several state-of-the-art SSL frameworks to verify the effectiveness of the proposed method. The results show that our method significantly improves the performance of SSL on long-tailed datasets by a large margin, and even outperforms previous work which uses external ID data. Our code is available at \url{https://github.com/JianhongBai/COLT}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                808. <h><a href=https://openreview.net/pdf?id=VA1YpcNr7ul>DASHA: Distributed Nonconvex Optimization with Communication Compression and Optimal Oracle Complexity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexander Tyurin, Peter Richtárik</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VA1YpcNr7ul>6.5 - [6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a new method that improves the state-of-the-art theoretical complexity of distributed optimization methods with compressed communication in the nonconvex regime.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VA1YpcNr7ul.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We develop and analyze  DASHA: a new family of methods for nonconvex distributed optimization problems. When the local functions at the nodes have a finite-sum or an expectation form, our new methods, DASHA-PAGE, DASHA-MVR and DASHA-SYNC-MVR, improve the theoretical oracle and communication complexity of the previous state-of-the-art method MARINA by Gorbunov et al. (2020). In particular, to achieve an $\varepsilon$-stationary point, and considering the random sparsifier Rand$K$ as an example, our methods compute the optimal number of gradients $\mathcal{O}\left(\frac{\sqrt{m}}{\varepsilon\sqrt{n}}\right)$ and $\mathcal{O}\left(\frac{\sigma}{\varepsilon^{3/2}n}\right)$ in finite-sum and expectation form cases, respectively, while maintaining the SOTA communication complexity $\mathcal{O}\left(\frac{d}{\varepsilon \sqrt{n}}\right)$. Furthermore, unlike MARINA, the new methods DASHA, DASHA-PAGE and DASHA-MVR send compressed vectors only, which makes them more practical for federated learning. We extend our results to the case when the functions satisfy the Polyak-Lojasiewicz condition. Finally, our theory is corroborated in practice: we see a significant improvement in experiments with nonconvex classification and training of deep learning models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                809. <h><a href=https://openreview.net/pdf?id=jXQ0ipgMdU>Spherical Sliced-Wasserstein</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Clément Bonet, Paul Berg, Nicolas Courty, François Septier, Lucas Drumetz, Minh Tan Pham</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jXQ0ipgMdU>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a SW discrepancy on the sphere using only tools intrinsic to the manifold.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jXQ0ipgMdU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many variants of the Wasserstein distance have been introduced to reduce its original computational burden. In particular the Sliced-Wasserstein distance (SW), which leverages one-dimensional projections for which a closed-form solution of the Wasserstein distance is available, has received a lot of interest. Yet, it is restricted to data living in Euclidean spaces, while the Wasserstein distance has been studied and used recently on manifolds. We focus more specifically on the sphere, for which we define a novel SW discrepancy, which we call spherical Sliced-Wasserstein, making a first step towards defining SW discrepancies on manifolds. Our construction is notably based on closed-form solutions of the Wasserstein distance on the circle, together with a new spherical Radon transform. Along with efficient algorithms and the corresponding implementations, we illustrate its properties in several machine learning use cases where spherical representations of data are at stake: sampling on the sphere, density estimation on real eath data or hyperspherical auto-encoders.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                810. <h><a href=https://openreview.net/pdf?id=BPwIgvf5iQ>Jointly Learning Visual and Auditory Speech Representations from Raw Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexandros Haliassos, Pingchuan Ma, Rodrigo Mira, Stavros Petridis, Maja Pantic</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=BPwIgvf5iQ>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a self-supervised audiovisual approach to jointly learn visual and auditory speech representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/BPwIgvf5iQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present RAVEn, a self-supervised multi-modal approach to jointly learn visual and auditory speech representations. Our pre-training objective involves encoding masked inputs, and then predicting contextualised targets generated by slowly-evolving momentum encoders. Driven by the inherent differences between video and audio, our design is asymmetric w.r.t. the two modalities&#39; pretext tasks: Whereas the auditory stream predicts both the visual and auditory targets, the visual one predicts only the auditory targets. We observe strong results in low- and high-resource labelled data settings when fine-tuning the visual and auditory encoders resulting from a single pre-training stage, in which the encoders are jointly trained. Notably, RAVEn surpasses all self-supervised methods on visual speech recognition (VSR) on LRS3, and combining RAVEn with self-training using only 30 hours of labelled data even outperforms a recent semi-supervised method trained on 90,000 hours of non-public data. At the same time, we achieve state-of-the-art results in the LRS3 low-resource setting for auditory speech recognition (as well as for VSR). Our findings point to the viability of learning powerful speech representations entirely from raw video and audio, i.e., without relying on handcrafted features. Code and models are available at https://github.com/ahaliassos/raven.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                811. <h><a href=https://openreview.net/pdf?id=VV0hSE8AxCw>Sparse Token Transformer with Attention Back Tracking</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Heejun Lee, Minki Kang, Youngwan Lee, Sung Ju Hwang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VV0hSE8AxCw>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an attention back-tracking method that tracks the importance of each attention in a Transformer architecture from the outputs to the inputs, to preserve the tokens that have large impact on the final predictions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VV0hSE8AxCw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the success of Transformers in various applications from text, vision, and speech domains, they are yet to become standard architectures for mobile and edge device applications due to their heavy memory and computational requirements. While there exist many different approaches to reduce the complexities of the Transformers, such as the pruning of the weights/attentions/tokens, quantization, and distillation, we focus on token pruning, which reduces not only the complexity of the attention operations, but also the linear layers, which have non-negligible computational costs. However, previous token pruning approaches often remove tokens during the feed-forward stage without consideration of their impact on later layers&#39; attentions, which has a potential risk of dropping out important tokens for the given task. To tackle this issue, we propose an attention back-tracking method that tracks the importance of each attention in a Transformer architecture from the outputs to the inputs, to preserve the tokens that have a large impact on the final predictions. We experimentally validate the effectiveness of the method on both NLP and CV benchmarks, using Transformer architectures for both domains, and the results show that the proposed attention back-tracking allows the model to better retain the full models&#39; performance even at high sparsity rates, significantly outperforming all baselines. Qualitative analysis of the examples further shows that our method does preserve semantically meaningful tokens.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                812. <h><a href=https://openreview.net/pdf?id=6lUEy1J5R7p>Imitating Graph-Based Planning with Goal-Conditioned Policies</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junsu Kim, Younggyo Seo, Sungsoo Ahn, Kyunghwan Son, Jinwoo Shin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6lUEy1J5R7p>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train goal-conditioned policies guided by decisions from graph-based planning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6lUEy1J5R7p.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, graph-based planning algorithms have gained much attention to solve goal-conditioned reinforcement learning (RL) tasks: they provide a sequence of subgoals to reach the target-goal, and the agents learn to execute subgoal-conditioned policies. However, the sample-efficiency of such RL schemes still remains a challenge, particularly for long-horizon tasks.  To address this issue, we present a simple yet effective self-imitation scheme which distills a subgoal-conditioned policy into the target-goal-conditioned policy. Our intuition here is that to reach a target-goal, an agent should pass through a subgoal, so target-goal- and subgoal- conditioned policies should be similar to each other. We also propose a novel scheme of stochastically skipping executed subgoals in a planned path, which further improves performance. Unlike prior methods that only utilize graph-based planning in an execution phase, our method transfers knowledge from a planner along with a graph into policy learning. We empirically show that our method can significantly boost the sample-efficiency of the existing goal-conditioned RL methods under various long-horizon control tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                813. <h><a href=https://openreview.net/pdf?id=RQY2AXFMRiu>Solving Constrained Variational Inequalities via a First-order Interior Point-based Method</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tong Yang, Michael Jordan, Tatjana Chavdarova</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RQY2AXFMRiu>6.5 - [6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We derive a first-order method for solving constrained variational inequality problem when given general constraints, by combining interior-point methods and ADMM.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RQY2AXFMRiu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We develop an interior-point approach to solve constrained variational inequality (cVI) problems. Inspired by the efficacy of the alternating direction method of multipliers (ADMM) method in the single-objective context, we generalize ADMM to derive a first-order method for cVIs, that we refer to as ADMM-based interior-point method for constrained VIs (ACVI). We provide convergence guarantees for ACVI in two general classes of problems: (i) when the operator is $\xi$-monotone, and (ii) when it is monotone, some constraints are active and the game is not purely rotational. When the operator is in addition L-Lipschitz for the latter case, we match known lower bounds on rates for the gap function of $\mathcal{O}(1/\sqrt{K})$ and $\mathcal{O}(1/K)$ for the last and average iterate, respectively. To the best of our knowledge, this is the first presentation of a first-order interior-point method for the general cVI problem that has a global convergence guarantee. Moreover, unlike previous work in this setting, ACVI provides a means to solve cVIs when the constraints are nontrivial. Empirical analyses demonstrate clear advantages of ACVI over common first-order methods. In particular, (i) cyclical behavior is notably reduced as our methods approach the solution from the analytic center, and (ii) unlike projection-based methods that zigzag when near a constraint, ACVI efficiently handles the constraints.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                814. <h><a href=https://openreview.net/pdf?id=HPdxC1THU8T>Revisiting adapters with adversarial training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sylvestre-Alvise Rebuffi, Francesco Croce, Sven Gowal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HPdxC1THU8T>6.5 - [6, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HPdxC1THU8T.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While adversarial training is generally used as a defense mechanism, recent works show that it can also act as a regularizer. By co-training a neural network on clean and adversarial inputs, it is possible to improve classification accuracy on the clean, non-adversarial inputs. We demonstrate that, contrary to previous findings, it is not necessary to separate batch statistics when co-training on clean and adversarial inputs, and that it is sufficient to use adapters with few domain-specific parameters for each type of input. We establish that using the classification token of a Vision Transformer (ViT) as an adapter is enough to match the classification performance of dual normalization layers, while using significantly less additional parameters. First, we improve upon the top-1 accuracy of a non-adversarially trained ViT-B16 model by +1.12% on ImageNet (reaching 83.76% top-1 accuracy). Second, and more importantly, we show that training with adapters enables model soups through linear combinations of the clean and adversarial tokens. These model soups, which we call adversarial model soups, allow us to trade-off between clean and robust accuracy without sacrificing efficiency. Finally, we show that we can easily adapt the resulting models in the face of distribution shifts. Our ViT-B16 obtains top-1 accuracies on ImageNet variants that are on average +4.00% better than those obtained with Masked Autoencoders.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                815. <h><a href=https://openreview.net/pdf?id=cMAjKYftNwx>Extracting Robust Models with Uncertain Examples</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guanlin Li, Guowen Xu, Shangwei Guo, Han Qiu, Jiwei Li, Tianwei Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cMAjKYftNwx>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cMAjKYftNwx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Model extraction attacks are proven to be a severe privacy threat to Machine Learning as a Service (MLaaS). A variety of techniques have been designed to steal a remote machine learning model with high accuracy and fidelity. However, how to extract a robust model with similar resilience against adversarial attacks is never investigated. This paper presents the first study toward this goal. We first analyze those existing extraction solutions either fail to maintain the model accuracy or model robustness or lead to the robust overfitting issue. Then we propose Boundary Entropy Searching Thief (BEST), a novel model extraction attack to achieve both accuracy and robustness extraction under restricted attack budgets. BEST generates a new kind of uncertain examples for querying and reconstructing the victim model. These samples have uniform confidence scores across different classes, which can perfectly balance the trade-off between model accuracy and robustness. Extensive experiments demonstrate that BEST outperforms existing attack methods over different datasets and model architectures under limited data. It can also effectively invalidate state-of-the-art extraction defenses.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                816. <h><a href=https://openreview.net/pdf?id=Z_tmYu060Kr>Squeeze Training for Adversarial Robustness</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Z_tmYu060Kr>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We highlight that some collaborative examples, which show extremely lower prediction loss, can be utilized to enhance adversarial training. A novel method called squeeze training (ST) is thus proposed.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Z_tmYu060Kr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The vulnerability of deep neural networks (DNNs) to adversarial examples has attracted great attention in the machine learning community. The problem is related to non-flatness and non-smoothness of normally obtained loss landscapes. Training augmented with adversarial examples (a.k.a., adversarial training) is considered as an effective remedy. In this paper, we highlight that some collaborative examples, nearly perceptually indistinguishable from both adversarial and benign examples yet show extremely lower prediction loss, can be utilized to enhance adversarial training. A novel method is therefore proposed to achieve new state-of-the-arts in adversarial robustness. Code: https://github.com/qizhangli/ST-AT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                817. <h><a href=https://openreview.net/pdf?id=7GEvPKxjtt>Towards Robustness Certification Against Universal Perturbations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yi Zeng, Zhouxing Shi, Ming Jin, Feiyang Kang, Lingjuan Lyu, Cho-Jui Hsieh, Ruoxi Jia</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7GEvPKxjtt>6.5 - [5, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A robustness certification framework against universal perturbations (including both universal adversarial noise and backdoor attacks).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7GEvPKxjtt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we investigate the problem of certifying neural network robustness against universal perturbations (UPs), which have been widely used in universal adversarial attacks and backdoor attacks. Existing robustness certification methods aim to provide robustness guarantees for each sample with respect to the worst-case perturbations given a neural network. However, those sample-wise bounds will be loose when considering the UP threat model as they overlook the important constraint that the perturbation should be shared across all samples. We propose a method based on a combination of linear relaxation-based perturbation analysis and Mixed Integer Linear Programming to establish the first robust certification method for UP. In addition, we develop a theoretical framework for computing error bounds on the entire population using the certification results from a randomly sampled batch. Aside from an extensive evaluation of the proposed certification, we further show how the certification facilitates efficient comparison of robustness among different models or efficacy among different universal adversarial attack defenses and enables accurate detection of backdoor target classes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                818. <h><a href=https://openreview.net/pdf?id=y5W8tpojhtJ>Neural Collapse Inspired Feature-Classifier Alignment for Few-Shot Class-Incremental Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, Dacheng Tao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=y5W8tpojhtJ>6.5 - [6, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An interpretable solution inspired by neural collapse for few-shot class-incremental learning
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/y5W8tpojhtJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Few-shot class-incremental learning (FSCIL) has been a challenging problem as only a few training samples are accessible for each novel class in the new sessions. Finetuning the backbone or adjusting the classifier prototypes trained in the prior sessions would inevitably cause a misalignment between the feature and classifier of old classes, which explains the well-known catastrophic forgetting problem. In this paper, we deal with this misalignment dilemma in FSCIL inspired by the recently discovered phenomenon named neural collapse, which reveals that the last-layer features of the same class will collapse into a vertex, and the vertices of all classes are aligned with the classifier prototypes, which are formed as a simplex equiangular tight frame (ETF). It corresponds to an optimal geometric structure for classification due to the maximized Fisher Discriminant Ratio. We propose a neural collapse inspired framework for FSCIL. A group of classifier prototypes are pre-assigned as a simplex ETF for the whole label space, including the base session and all the incremental sessions. During training, the classifier prototypes are not learnable, and we adopt a novel loss function that drives the features into their corresponding prototypes. Theoretical analysis shows that our method holds the neural collapse optimality and does not break the feature-classifier alignment in an incremental fashion. Experiments on the miniImageNet, CUB-200, and CIFAR-100 datasets demonstrate that our proposed framework outperforms the state-of-the-art performances. Code address: https://github.com/NeuralCollapseApplications/FSCIL 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                819. <h><a href=https://openreview.net/pdf?id=M9u_ctqFUlg>Deep Generative Modeling on Limited Data with Regularization by Nontransferable Pre-trained Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yong Zhong, Hong Tao Liu, Xiaodong Liu, Fan Bao, Weiran Shen, Chongxuan Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=M9u_ctqFUlg>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/M9u_ctqFUlg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep generative models (DGMs) are data-eager because learning a complex model on limited data suffers from a large variance and easily overfits. Inspired by the classical perspective of the bias-variance tradeoff, we propose regularized deep generative model (Reg-DGM), which leverages a nontransferable pre-trained model to reduce the variance of generative modeling with limited data. Formally, Reg-DGM optimizes a weighted sum of a certain divergence and the expectation of an energy function, where the divergence is between the data and the model distributions, and the energy function is defined by the pre-trained model w.r.t. the model distribution. We analyze a simple yet representative Gaussian-fitting case to demonstrate how the weighting hyperparameter trades off the bias and the variance. Theoretically, we characterize the existence and the uniqueness of the global minimum of Reg-DGM in a non-parametric setting and prove its convergence with neural networks trained by gradient-based methods. Empirically, with various pre-trained feature extractors and a data-dependent energy function, Reg-DGM consistently improves the generation performance of strong DGMs with limited data and achieves competitive results to the state-of-the-art methods. Our implementation is available at https://github.com/ML-GSAI/Reg-ADA-APA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                820. <h><a href=https://openreview.net/pdf?id=d3QNWD_pcFv>Neural Lagrangian Schr\&#34;{o}dinger Bridge: Diffusion Modeling for Population Dynamics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Takeshi Koshizuka, Issei Sato</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=d3QNWD_pcFv>6.5 - [6, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/d3QNWD_pcFv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Population dynamics is the study of temporal and spatial variation in the size of populations of organisms and is a major part of population ecology. One of the main difficulties in analyzing population dynamics is that we can only obtain observation data with coarse time intervals from fixed-point observations due to experimental costs or measurement constraints. Recently, modeling population dynamics by using continuous normalizing flows (CNFs) and dynamic optimal transport has been proposed to infer the sample trajectories from a fixed-point observed population. While the sample behavior in CNFs is deterministic, the actual sample in biological systems moves in an essentially random yet directional manner. Moreover, when a sample moves from point A to point B in dynamical systems, its trajectory typically follows the principle of least action in which the corresponding action has the smallest possible value. To satisfy these requirements of the sample trajectories, we formulate the Lagrangian Schrödinger bridge (LSB) problem and propose to solve it approximately by modeling the advection-diffusion process with regularized neural SDE. We also develop a model architecture that enables faster computation of the loss function. Experimental results show that the proposed method can efficiently approximate the population-level dynamics even for high-dimensional data and that using the prior knowledge introduced by the Lagrangian enables us to estimate the sample-level dynamics with stochastic behavior.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                821. <h><a href=https://openreview.net/pdf?id=5KUPKjHYD-l>MAST: Masked Augmentation Subspace Training for Generalizable Self-Supervised Priors</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chen Huang, Hanlin Goh, Jiatao Gu, Joshua M. Susskind</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5KUPKjHYD-l>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Disentangled and uncertainty-aware learning of augmentation invariances during SSL improves generalization on downstream tasks
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5KUPKjHYD-l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent Self-Supervised Learning (SSL) methods are able to learn feature representations that are invariant to different data augmentations, which can then be transferred to downstream tasks of interest. However, different downstream tasks require different invariances for their best performance, so the optimal choice of augmentations for SSL depends on the target task. In this paper, we aim to learn self-supervised features that generalize well across a variety of downstream tasks (e.g., object classification, detection and instance segmentation) without knowing any task information beforehand. We do so by Masked Augmentation Subspace Training (or MAST) to encode in the single feature space the priors from different data augmentations in a factorized way. Specifically, we disentangle the feature space into separate subspaces, each induced by a learnable mask that selects relevant feature dimensions to model invariance to a specific augmentation. We show the success of MAST in jointly capturing generalizable priors from different augmentations, using both unique and shared features across the subspaces. We further show that MAST benefits from uncertainty modeling to reweight ambiguous samples from strong augmentations that may cause similarity mismatch in each subspace. Experiments demonstrate that MAST consistently improves generalization on various downstream tasks, while being task-agnostic and efficient during SSL. We also provide interesting insights about how different augmentations are related and how uncertainty reflects learning difficulty.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                822. <h><a href=https://openreview.net/pdf?id=pd1P2eUBVfq>Diffusion Models Already Have A Semantic Latent Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingi Kwon, Jaeseok Jeong, Youngjung Uh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pd1P2eUBVfq>6.5 - [6, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We discover the semantic latent space of pretrained diffusion models by introducing asymmetric reverse process.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pd1P2eUBVfq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models achieve outstanding generative performance in various domains. Despite their great success, they lack semantic latent space which is essential for controlling the generative process. To address the problem, we propose asymmetric reverse process (Asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. Our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. In addition, we measure editing strength and quality deficiency of a generative process at timesteps to provide a principled design of the process for versatility and quality improvements. Our method is applicable to various architectures (DDPM++, iDDPM, and ADM) and datasets (CelebA-HQ, AFHQ-dog, LSUN-church, LSUN-bedroom, and METFACES).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                823. <h><a href=https://openreview.net/pdf?id=pgHNOcxEdRI>Function-Consistent Feature Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dongyang Liu, Meina Kan, Shiguang Shan, Xilin CHEN</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pgHNOcxEdRI>6.5 - [5, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pgHNOcxEdRI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Feature distillation makes the student mimic the intermediate features of the teacher. Nearly all existing feature-distillation methods use L2 distance or its slight variants as the distance metric between teacher and student features. However, while L2 distance is isotropic w.r.t. all dimensions, the neural network’s operation on different dimensions is usually anisotropic, i.e., perturbations with the same 2-norm but in different dimensions of intermediate features lead to changes in the final output with largely different magnitude. Considering this, we argue that the similarity between teacher and student features should \textit{not} be measured merely based on their appearance (i.e., L2 distance), but should, more importantly, be measured by their difference in function, namely how later layers of the network will read, decode, and process them. Therefore, we propose Function-Consistent Feature Distillation (FCFD), which explicitly optimizes the functional similarity between teacher and student features. The core idea of FCFD is to make teacher and student features not only numerically similar, but more importantly produce similar outputs when fed to the later part of the same network. With FCFD, the student mimics the teacher more faithfully and learns more from the teacher. Extensive experiments on image classification and object detection demonstrate the superiority of FCFD to existing methods. Furthermore, we can combine FCFD with many existing methods to obtain even higher accuracy. Our codes are available at https://github.com/LiuDongyang6/FCFD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                824. <h><a href=https://openreview.net/pdf?id=E3ip6qBLF7>Distributionally Robust Recourse Action</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Duy Nguyen, Ngoc Bui, Viet Anh Nguyen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=E3ip6qBLF7>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Distributionally Robust Recourse Action framework generates a recourse action that has high probability of being valid under a mixture of model shifts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/E3ip6qBLF7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A recourse action aims to explain a particular algorithmic decision by showing one specific way in which the instance could be modified to receive an alternate outcome. Existing recourse generation methods often assume that the machine learning model does not change over time. However, this assumption does not always hold in practice because of data distribution shifts, and in this case, the recourse action may become invalid. To redress this shortcoming, we propose the Distributionally Robust Recourse Action (DiRRAc) framework, which generates a recourse action that has high probability of being valid under a mixture of model shifts. We first formulate the robustified recourse setup as a min-max optimization problem, where the max problem is specified by Gelbrich distance over an ambiguity set around the distribution of model parameters. Then we suggest a projected gradient descent algorithm to find a robust recourse according to the min-max objective. We also show that our DiRRAc framework can be extended to hedge against the misspecification of the mixture weights. Numerical experiments with both synthetic and three real-world datasets demonstrate the benefits of our proposed framework over the state-of-the-art recourse methods, which generate robust recourses.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                825. <h><a href=https://openreview.net/pdf?id=a2jNdqE2102>Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, Jianshu Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=a2jNdqE2102>6.5 - [6, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a novel knowledge-rich semi-parametric model, KiC, that is able to achieve superior zeros-hot performance on unseen task with a much smaller model size.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/a2jNdqE2102.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge:  entity,  dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly, we find that KiC can be identified as a special mixture-of-experts (MoE) model, where the knowledge selector plays the role of a router that is used to determine the sequence-to-expert assignment in MoE. This key observation inspires us to develop a novel algorithm for training KiC with an instance-adaptive knowledge selector. As a knowledge-rich semi-parametric language model, KiC only needs a much smaller parametric part to achieve superior zero-shot performance on unseen tasks. By evaluating on 40+ different tasks, we show that KiC-Large with 770M parameters easily outperforms large language models that are 4-39x larger. In addition, KiC also exhibits emergent abilities at a much smaller model scale compared to the fully-parametric models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                826. <h><a href=https://openreview.net/pdf?id=ZxdkjTgK_Dl>BSTT: A Bayesian Spatial-Temporal Transformer for Sleep Staging</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuchen Liu, Ziyu Jia</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZxdkjTgK_Dl>6.5 - [5, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZxdkjTgK_Dl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sleep staging is helpful in assessing sleep quality and diagnosing sleep disorders. However, how to adequately capture the temporal and spatial relations of the brain during sleep remains a challenge. In particular, existing methods cannot adaptively infer spatial-temporal relations of the brain under different sleep stages. In this paper, we propose a novel Bayesian spatial-temporal relation inference neural network, named Bayesian spatial-temporal transformer (BSTT), for sleep staging. Our model is able to adaptively infer brain spatial-temporal relations during sleep for spatial-temporal feature modeling through a well-designed Bayesian relation inference component. Meanwhile, our model also includes a spatial transformer for extracting brain spatial features and a temporal transformer for capturing temporal features. Experiments show that our BSTT outperforms state-of-the-art baselines on ISRUC and MASS datasets. In addition, the visual analysis shows that the spatial-temporal relations obtained by BSTT inference have certain interpretability for sleep staging.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                827. <h><a href=https://openreview.net/pdf?id=xLr0I_xYGAs>The Devil is in the Wrongly-classified Samples: Towards Unified Open-set Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jun CEN, Di Luan, Shiwei Zhang, Yixuan Pei, Yingya Zhang, Deli Zhao, Shaojie Shen, Qifeng Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xLr0I_xYGAs>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xLr0I_xYGAs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Open-set Recognition (OSR) aims to identify test samples whose classes are not seen during the training process. Recently, Unified Open-set Recognition (UOSR) has been proposed to reject not only unknown samples but also known but wrongly classified samples, which tends to be more practical in real-world applications. In this paper, we deeply analyze the UOSR task under different training and evaluation settings to shed light on this promising research direction. For this purpose, we first evaluate the UOSR performance of several OSR methods and show a significant finding that the uncertainty distribution of almost all these methods is actually closer to the expectation of UOSR than OSR. We show that the reason lies in the known but wrongly classified samples, as their uncertainty distribution is extremely close to unknown samples rather than known and correctly classified samples. Second, we analyze how the two training settings of OSR (i.e., pre-training and outlier exposure) influence the UOSR. We find although they are both beneficial for distinguishing known and correctly classified samples from unknown samples, pre-training is also helpful for identifying known but wrongly classified samples while outlier exposure is not. In addition to different training settings, we also formulate a new evaluation setting for UOSR which is called few-shot UOSR, where only one or five samples per unknown class are available during evaluation to help identify unknown samples. We propose FS-KNNS for the few-shot UOSR to achieve state-of-the-art performance under all settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                828. <h><a href=https://openreview.net/pdf?id=YgC62m4CY3r>Learning with Auxiliary Activation for Memory-Efficient Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sunghyeon Woo, Dongsuk Jeon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YgC62m4CY3r>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The proposed learning rule reduces training memory requirements without reduction in training speed while achieving high performance close to backpropagation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YgC62m4CY3r.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While deep learning has achieved great success in various fields, a large amount of memory is necessary to train deep neural networks, which hinders the development of massive state-of-the-art models. The reason is the conventional learning rule, backpropagation, should temporarily store input activations of all the layers in the network. To overcome this, recent studies suggested various memory-efficient implementations of backpropagation. However, those approaches incur computational overhead due to the recomputation of activations, slowing down neural network training. In this work, we propose a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation. The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored. We mathematically show that our learning rule can reliably train the networks whose loss landscape is convex if the auxiliary activation satisfies certain conditions. Based on this observation, we suggest candidates of auxiliary activation that satisfy those conditions. Experimental results confirm that the proposed learning rule achieves competitive performance compared to backpropagation in various models such as ResNet, Transformer, BERT, ViT, and MLP-Mixer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                829. <h><a href=https://openreview.net/pdf?id=4oYUGeGBPm>Transformer-Patcher: One Mistake Worth One Neuron</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, Zhang Xiong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4oYUGeGBPm>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A Sequential Model Editor to correct model&#39;s output on the specific input.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4oYUGeGBPm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large Transformer-based Pretrained Language Models (PLMs) dominate almost all Natural Language Processing (NLP) tasks. Nevertheless, they still make mistakes from time to time. For a model deployed in an industrial environment, fixing these mistakes quickly and robustly is vital to improve user experiences. Previous works formalize such problems as Model Editing (ME) and mostly focus on fixing one mistake. However, the one-mistake-fixing scenario is not an accurate abstraction of the real-world challenge. In the deployment of AI services, there are ever-emerging mistakes, and the same mistake may recur if not corrected in time. Thus a preferable solution is to rectify the mistakes as soon as they appear nonstop. Therefore, we extend the existing ME into the Sequential Model Editing (SME) to help develop more practical editing methods. Our study shows that current ME methods either fail to make a sequence of edits or to remember previous edits. We then introduce Transformer-Patcher, a novel model editor that can shift the behavior of transformer-based models by simply adding and training a few neurons in the last Feed-Forward Network layer. Experimental results on both classification and generation tasks show that Transformer-Patcher can successively correct up to thousands of errors (Reliability) and generalize to their equivalent inputs (Generality) while retaining the model’s accuracy on irrelevant inputs (Locality). Our method outperforms previous fine-tuning and HyperNetwork-based methods and achieves state-of-the-art performance for Sequential Model Editing (SME).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                830. <h><a href=https://openreview.net/pdf?id=OhUAblg27z>Harnessing Mixed Offline Reinforcement Learning Datasets via Trajectory Weighting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhang-Wei Hong, Pulkit Agrawal, Remi Tachet des Combes, Romain Laroche</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OhUAblg27z>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a sample selection strategy that enables offline reinforcement learning algorithms to learn a better policy in mixed datasets with sparse high-return trajectories.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OhUAblg27z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Most offline reinforcement learning (RL) algorithms return a target policy maximizing a trade-off between (1) the expected performance gain over the behavior policy that collected the dataset, and (2) the risk stemming from the out-of-distribution-ness of the induced state-action occupancy. It follows that the performance of the target policy is strongly related to the performance of the behavior policy and, thus, the trajectory return distribution of the dataset. We show that in mixed datasets consisting of mostly low-return trajectories and minor high-return trajectories, state-of-the-art offline RL algorithms are overly restrained by low-return trajectories and fail to exploit high-performing trajectories to the fullest. To overcome this issue, we show that, in deterministic MDPs with stochastic initial states, the dataset sampling can be re-weighted to induce an artificial dataset whose behavior policy has a higher return. This re-weighted sampling strategy may be combined with any offline RL algorithm. We further analyze that the opportunity for performance improvement over the behavior policy correlates with the positive-sided variance of the returns of the trajectories in the dataset. We empirically show that while CQL, IQL, and TD3+BC achieve only a part of this potential policy improvement, these same algorithms combined with our reweighted sampling strategy fully exploit the dataset. Furthermore, we empirically demonstrate that, despite its theoretical limitation, the approach may still be efficient in stochastic environments. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                831. <h><a href=https://openreview.net/pdf?id=lRdhvzMpVYV>A Differential Geometric View and Explainability of GNN on Evolving Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yazheng Liu, Xi Zhang, Sihong Xie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lRdhvzMpVYV>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lRdhvzMpVYV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graphs are ubiquitous in social networks and biochemistry, where Graph Neural Networks (GNN) are the state-of-the-art models for prediction. Graphs can be evolving and it is vital to formally model and understand how a trained GNN responds to graph evolution. We propose a smooth parameterization of the GNN predicted distributions using axiomatic attribution, where the distributions are on a low dimensional manifold within a high-dimensional embedding space. We exploit the differential geometric viewpoint to model distributional evolution as smooth curves on the manifold. We reparameterize families of curves on the manifold and design a convex optimization problem to find a unique curve that concisely approximates the distributional evolution for human interpretation. Extensive experiments on node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better sparsity, faithfulness, and intuitiveness of the proposed method over the state-of-the-art methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                832. <h><a href=https://openreview.net/pdf?id=CRNwGauQpb6>NORM: Knowledge Distillation via N-to-One Representation Matching</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaolong Liu, LUJUN LI, Chao Li, Anbang Yao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CRNwGauQpb6>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents a new knowledge distillation method via n-to-one representation matching
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CRNwGauQpb6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing feature distillation methods commonly adopt the One-to-one Representation Matching between any pre-selected teacher-student layer pair. In this paper, we present $N$-to-$O$ne $R$epresentation $M$atching (NORM), a new two-stage knowledge distillation method, which relies on a simpleFeature Transform (FT) module consisting of two linear layers. In view of preserving the intact information learnt by the teacher network, during training, our FT module is merely inserted after the last convolutional layer of the student network. The first linear layer projects the student representation to a feature space having $N$ times feature channels than the teacher representation from the last convolutional layer, and the second linear layer contracts the expanded output back to the original feature space. By sequentially splitting the expanded student representation into $N$ non-overlapping feature segments having the same number of feature channels as the teacher&#39;s, they can be readily forced to approximate the intact teacher representation simultaneously, formulating a novel many-to-one representation matching mechanism conditioned on a single teacher-student layer pair. After training, such an FT module will be naturally merged into the subsequent fully connected layer thanks to its linear property, introducing no extra parameters or architectural modifications to the student network at inference. Extensive experiments on different visual recognition benchmarks demonstrate the leading performance of our method. For instance, the ResNet18|MobileNet|ResNet50-1/4 model trained by NORM reaches 72.14%|74.26%|68.03% top-1 accuracy on the ImageNet dataset when using a pre-trained ResNet34|ResNet50|ResNet50 model as the teacher, achieving an absolute improvement of 2.01%|4.63%|3.03% against the individually trained counterpart. Code is available at https://github.com/OSVAI/NORM.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                833. <h><a href=https://openreview.net/pdf?id=VGI9dSmTgPF>Schema Inference for Interpretable Image Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haofei Zhang, Mengqi Xue, Xiaokang Liu, Kaixuan Chen, Jie Song, Mingli Song</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VGI9dSmTgPF>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VGI9dSmTgPF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we study a novel inference paradigm, termed as schema inference, that learns to deductively infer the explainable predictions by rebuilding the prior deep neural network (DNN) forwarding scheme, guided by the prevalent philosophical cognitive concept of schema. We strive to reformulate the conventional model inference pipeline into a graph matching policy that associates the extracted visual concepts of an image with the pre-computed scene impression, by analogy with human reasoning mechanism via impression matching. To this end, we devise an elaborated architecture, termed as SchemaNet, as a dedicated instantiation of the proposed schema inference concept, that models both the visual semantics of input instances and the learned abstract imaginations of target categories as topological relational graphs. Meanwhile, to capture and leverage the compositional contributions of visual semantics in a global view, we also introduce a universal Feat2Graph scheme in SchemaNet to establish the relational graphs that contain abundant interaction information. Both the theoretical analysis and the experimental results on several benchmarks demonstrate that the proposed schema inference achieves encouraging performance and meanwhile yields a clear picture of the deductive process leading to the predictions. Our code is available at https://github.com/zhfeing/SchemaNet-PyTorch.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                834. <h><a href=https://openreview.net/pdf?id=w9WUQkBvpI>Subsampling in Large Graphs Using Ricci Curvature</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shushan Wu, Huimin Cheng, Jiazhang Cai, Ping Ma, Wenxuan Zhong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=w9WUQkBvpI>6.5 - [8, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/w9WUQkBvpI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the past decades, many large graphs with millions of nodes have been collected/constructed. The high computational cost and significant visualization difficulty hinder the analysis of large graphs. To overcome the difficulties, researchers have developed many graph subsampling approaches to provide a rough sketch that preserves global properties. By selecting representative nodes, these graph subsampling methods can help researchers estimate the graph statistics, e.g., the number of communities, of the large graph from the subsample. However, the available subsampling methods, e.g., degree node sampler and random walk sampler, tend to leave out minority communities because nodes with high degrees are more likely to be sampled. To overcome the shortcomings of the existing methods, we are motivated to apply the community information hidden in the graph to the subsampling method. Though the community structure is unavailable, community structure information can be obtained by applying geometric methods to a graph. An analog of Ricci curvature in the manifold is defined for the graph, i.e., Ollivier Ricci curvature. Based on the asymptotic results about the within-community edge and between-community edge&#39;s OR curvature, we propose a subsampling algorithm based on our theoretical results, the Ollivier-Ricci curvature Gradient-based subsampling (ORG-sub) algorithm. The proposed ORG-sub algorithm has two main contributions: First, ORG-sub provides a rigorous theoretical guarantee that the probability of ORG-sub taking all communities into the final subgraph converges to one. Second, extensive experiments on synthetic and benchmark datasets demonstrate the advantages of our algorithm.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                835. <h><a href=https://openreview.net/pdf?id=VZ5EaTI6dqa>Scale-invariant Bayesian Neural Networks with Connectivity Tangent Kernel</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>SungYub Kim, Sihwan Park, Kyung-Su Kim, Eunho Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VZ5EaTI6dqa>6.5 - [6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VZ5EaTI6dqa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Studying the loss landscapes of neural networks is critical to identifying generalizations and avoiding overconfident predictions. Flatness, which measures the perturbation resilience of pre-trained parameters for loss values, is widely acknowledged as an essential predictor of generalization. While the concept of flatness has been formalized as a PAC-Bayes bound, it has been observed that the generalization bounds can vary arbitrarily depending on the scale of the model parameters. Despite previous attempts to address this issue, generalization bounds remain vulnerable to function-preserving scaling transformations or are limited to impractical network structures. In this paper, we introduce new PAC-Bayes prior and posterior distributions invariant to scaling transformations, achieved through the \textit{decomposition of perturbations into scale and connectivity components}. In this way, this approach expands the range of networks to which the resulting generalization bound can be applied, including those with practical transformations such as weight decay with batch normalization. Moreover, we demonstrate that scale-dependency issues of flatness can adversely affect the uncertainty calibration of Laplace approximation, and we propose a solution using our invariant posterior. Our proposed invariant posterior allows for effective measurement of flatness and calibration with low complexity while remaining invariant to practical parameter transformations, also applying it as a reliable predictor of neural network generalization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                836. <h><a href=https://openreview.net/pdf?id=vk-j5pQY3Gv>Discovering Informative and Robust Positives for Video Domain Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chang Liu, Kunpeng Li, Michael Stopa, Jun Amano, Yun Fu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vk-j5pQY3Gv>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce the bottlenecks of in existing contrastive-based video DA methods and propose a unified solution to address them without relying on negatives by mining informative and robust intra-domain positives and cross-domain positives.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vk-j5pQY3Gv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised domain adaptation for video recognition is challenging where the domain shift includes both spatial variations and temporal dynamics. Previous works have focused on exploring contrastive learning for cross-domain alignment. However, limited variations in intra-domain positives, false cross-domain positives, and false negatives hinder contrastive learning from fulfilling intra-domain discrimination and cross-domain closeness. This paper presents a non-contrastive learning framework without relying on negative samples for unsupervised video domain adaptation. To address the limited variations in intra-domain positives, we set unlabeled target videos as anchors and explored to mine &#34;informative intra-domain positives&#34; in the form of spatial/temporal augmentations and target nearest neighbors (NNs).
To tackle the false cross-domain positives led by noisy pseudo-labels, we reversely set source videos as anchors and sample the synthesized target videos as &#34;robust cross-domain positives&#34; from an estimated target distribution, which are naturally more robust to the pseudo-label noise. Our approach is demonstrated to be superior to state-of-the-art methods through extensive experiments on several cross-domain action recognition benchmarks.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                837. <h><a href=https://openreview.net/pdf?id=rwo-ls5GqGn>ZiCo: Zero-shot NAS via inverse Coefficient of Variation on Gradients</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guihong Li, Yuedong Yang, Kartikeya Bhardwaj, Radu Marculescu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rwo-ls5GqGn>6.5 - [8, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A theoretically grounded SOTA proxy for zero-shot NAS under various inference budgets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rwo-ls5GqGn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Architecture Search (NAS) is widely used to automatically obtain the neural network with the best performance among a large number of candidate architectures. To reduce the search time, zero-shot NAS aims at designing training-free proxies that can predict the test performance of a given architecture. However, as shown recently, none of the zero-shot proxies proposed to date can actually work consistently better than a naive proxy, namely, the number of network parameters (#Params). To improve this state of affairs, as the main theoretical contribution, we first reveal how some specific gradient properties across different samples impact the convergence rate and generalization capacity of neural networks. Based on this theoretical analysis, we propose a new zero-shot proxy, ZiCo, the first proxy that works consistently better than #Params. We demonstrate that ZiCo works better than State-Of-The-Art (SOTA) proxies on several popular NAS-Benchmarks (NASBench101, NATSBench-SSS/TSS, TransNASBench-101) for multiple applications (e.g., image classification/reconstruction and pixel-level prediction). Finally, we demonstrate that the optimal architectures found via ZiCo are as competitive as the ones found by one-shot and multi-shot NAS methods, but with much less search time. For example, ZiCo-based NAS can find optimal architectures with 78.1%, 79.4%, and 80.4% test accuracy under inference budgets of 450M, 600M, and 1000M FLOPs, respectively, on ImageNet within 0.4 GPU days. Our code is available at https://github.com/SLDGroup/ZiCo.


                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                838. <h><a href=https://openreview.net/pdf?id=29V3AWjVAFi>The Best of Both Worlds: Accurate Global and Personalized Models through Federated Learning with Data-Free Hyper-Knowledge Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huancheng Chen, Chaining Wang, Haris Vikalo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=29V3AWjVAFi>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/29V3AWjVAFi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Heterogeneity of data distributed across clients limits the performance of global models trained through federated learning, especially in the settings with highly imbalanced class distributions of local datasets. In recent years, personalized federated learning (pFL) has emerged as a potential solution to the challenges presented by heterogeneous data. However, existing pFL methods typically enhance performance of local models at the expense of the global model&#39;s accuracy. We propose FedHKD (Federated Hyper-Knowledge Distillation), a novel FL algorithm in which clients rely on knowledge distillation (KD) to train local models. In particular, each client extracts and sends to the server the means of local data representations and the corresponding soft predictions -- information that we refer to as ``hyper-knowledge&#34;. The server aggregates this information and broadcasts it to the clients in support of local training. Notably, unlike other KD-based pFL methods, FedHKD does not rely on a public dataset nor it deploys a generative model at the server. We analyze convergence of FedHKD and conduct extensive experiments on visual datasets in a variety of scenarios, demonstrating that FedHKD provides significant improvement in both personalized as well as global model performance compared to state-of-the-art FL methods designed for heterogeneous data settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                839. <h><a href=https://openreview.net/pdf?id=Zb6c8A-Fghk>Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Polina Kirichenko, Pavel Izmailov, Andrew Gordon Wilson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Zb6c8A-Fghk>6.5 - [8, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple method based on retraining the last layer of a neural network which achieves strong results on spurious correlation benchmarks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Zb6c8A-Fghk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural network classifiers can largely rely on simple spurious features, such as image backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                840. <h><a href=https://openreview.net/pdf?id=yyLvxYBJV1B>AnyDA: Anytime Domain Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Omprakash Chakraborty, Aadarsh Sahoo, Rameswar Panda, Abir Das</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yyLvxYBJV1B>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yyLvxYBJV1B.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised domain adaptation is an open and challenging problem in computer vision. While existing research shows encouraging results in addressing cross-domain distribution shift on common benchmarks, they are often constrained to testing under a specific target setting, limiting their impact for many real-world applications. In this paper, we introduce a simple yet effective framework for anytime domain adaptation that is executable with dynamic resource constraints to achieve accuracy-efficiency trade-offs under domain-shifts. We achieve this by training a single shared network using both labeled source and unlabeled data, with switchable depth, width and input resolutions on the fly to enable testing under a wide range of computation budgets. Starting with a teacher network trained from a label-rich source domain, we utilize bootstrapped recursive knowledge distillation as a nexus between source and target domains to jointly train the student network with switchable subnetworks. Experiments on multiple datasets well demonstrate the superiority of our approach over state-of-the-art methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                841. <h><a href=https://openreview.net/pdf?id=_nGgzQjzaRy>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_nGgzQjzaRy>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new few-shot prompting approach to solve complex task by decomposing complex tasks into a shared library of prompts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_nGgzQjzaRy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired.
We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                842. <h><a href=https://openreview.net/pdf?id=9MbhFHqrti9>ImaginaryNet: Learning Object Detectors without Real Images and Annotations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minheng Ni, Zitong Huang, Kailai Feng, Wangmeng Zuo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9MbhFHqrti9>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper propose ImaginaryNet obtain about 70% performance in object detection trained without real images or annotations and improve the performance by incorporating real images and annotations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9MbhFHqrti9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Without the demand of training in reality, humans are able of detecting a new category of object simply based on the language description on its visual characteristics. Empowering deep learning with this ability undoubtedly enables the neural network to handle complex vision tasks, e.g., object detection, without collecting and annotating real images. To this end, this paper introduces a novel challenging learning paradigm Imaginary-Supervised Object Detection (ISOD), where neither real images nor manual annotations are allowed for training object detectors. To resolve this challenge, we propose ImaginaryNet, a framework to synthesize images by combining pretrained language model and text-to-image synthesis model. Given a class label, the language model is used to generate a full description of a scene with a target object, and the text-to-image model is deployed to generate a photo-realistic image. With the synthesized images and class labels, weakly supervised object detection can then be leveraged to accomplish ISOD. By gradually introducing real images and manual annotations, ImaginaryNet can collaborate with other supervision settings to further boost detection performance. Experiments show that ImaginaryNet can (i) obtain about 75% performance in ISOD compared with the weakly supervised counterpart of the same backbone trained on real data, (ii) significantly improve the baseline while achieving state-of-the-art or comparable performance by incorporating ImaginaryNet with other supervision settings.  Our code will be publicly available at https://github.com/kodenii/ImaginaryNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                843. <h><a href=https://openreview.net/pdf?id=NkJOhtNKX91>DamoFD: Digging into Backbone Design on Face Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yang Liu, Jiankang Deng, Fei Wang, Lei Shang, Xuansong Xie, Baigui Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NkJOhtNKX91>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel DDSAR score to characterize stage-wise detection ability, based on which, we employ off-the-shelf NAS technology to search FD-friendly backbone architectures.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NkJOhtNKX91.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Face detection (FD) has achieved remarkable success over the past few years, yet,
these leaps often arrive when consuming enormous computation costs. Moreover,
when considering a realistic situation, i.e., building a lightweight face detector
under a computation-scarce scenario, such heavy computation cost limits the application
of the face detector. To remedy this, several pioneering works design
tiny face detectors through off-the-shelf neural architecture search (NAS) technologies,
which are usually applied to the classification task. Thus, the searched
architectures are sub-optimal for the face detection task since some design criteria
between detection and classification task are different. As a representative, the
face detection backbone design needs to guarantee the stage-level detection ability
while it is not required for the classification backbone. Furthermore, the detection
backbone consumes a vast body of inference budgets in the whole detection framework.
Considering the intrinsic design requirement and the virtual importance role
of the face detection backbone, we thus ask a critical question: How to employ
NAS to search FD-friendly backbone architecture? To cope with this question,
we propose a distribution-dependent stage-aware ranking score (DDSAR-Score)
to explicitly characterize the stage-level expressivity and identify the individual
importance of each stage, thus satisfying the aforementioned design criterion of
the FD backbone. Based on our proposed DDSAR-Score, we conduct comprehensive
experiments on the challenging Wider Face benchmark dataset and achieve
dominant performance across a wide range of compute regimes. In particular,
compared to the tiniest face detector SCRFD-0.5GF, our method is +2.5 % better
in Average Precision (AP) score when using the same amount of FLOPs. The
code is avaliable at https://github.com/ly19965/FaceMaas/tree/master/face_project/face_detection/DamoFD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                844. <h><a href=https://openreview.net/pdf?id=zzqBoIFOQ1>Guiding Safe Exploration with Weakest Preconditions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Greg Anderson, Swarat Chaudhuri, Isil Dillig</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zzqBoIFOQ1>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use an online, weakest-precondition-based approach to ensure safety during exploration without interfering with performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zzqBoIFOQ1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                845. <h><a href=https://openreview.net/pdf?id=aEFaE0W5pAd>How to Exploit Hyperspherical Embeddings for Out-of-Distribution Detection?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yifei Ming, Yiyou Sun, Ousmane Dia, Yixuan Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aEFaE0W5pAd>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aEFaE0W5pAd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Out-of-distribution (OOD) detection is a critical task for reliable machine learning. Recent advances in representation learning give rise to distance-based OOD detection, where testing samples are detected as OOD if they are relatively far away from the centroids or prototypes of in-distribution (ID) classes. However, prior methods directly take off-the-shelf contrastive losses that suffice for classifying ID samples, but are not optimally designed when test inputs contain OOD samples. In this work, we propose CIDER, a novel representation learning framework that exploits hyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses to promote strong ID-OOD separability: a dispersion loss that promotes large angular distances among different class prototypes, and a compactness loss that encourages samples to be close to their class prototypes. We analyze and establish the unexplored relationship between OOD detection performance and the embedding properties in the hyperspherical space, and demonstrate the importance of dispersion and compactness. CIDER establishes superior performance, outperforming the latest rival by 13.33% in FPR95. Code is available at https://github.com/deeplearning-wisc/cider.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                846. <h><a href=https://openreview.net/pdf?id=W918Ora75q>Towards Smooth Video Composition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qihang Zhang, Ceyuan Yang, Yujun Shen, Yinghao Xu, Bolei Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=W918Ora75q>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a simple yet strong baseline for smooth video generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/W918Ora75q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Video generation, with the purpose of producing a sequence of frames, requires synthesizing consistent and persistent dynamic contents over time. This work investigates how to model the temporal relations for composing a video with arbitrary number of frames, from a few to even infinite, using generative adversarial networks (GANs). First, towards composing adjacent frames, we show that the alias-free operation for single image generation, together with adequately pre-learned knowledge, bring a smooth frame transition without harming the per-frame quality. Second, through incorporating a temporal shift module (TSM), which is originally designed for video understanding, into the discriminator, we manage to advance the generator in synthesizing more reasonable dynamics. Third, we develop a novel B-Spline based motion representation to ensure the temporal smoothness, and hence achieve infinite-length video generation, going beyond the frame number used in training. We evaluate our approach on a range of datasets and show substantial improvements over baselines on video generation. Code and models are publicly available at \url{https://genforce.github.io/StyleSV}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                847. <h><a href=https://openreview.net/pdf?id=3RhuF8foyPW>Single-shot General Hyper-parameter Optimization for Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, Heiko Ludwig</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3RhuF8foyPW>6.5 - [8, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a single-shot hyperparameter optimization scheme for Federated Learning systems with theoretical performance guarantees and strong empirical performance against baselines.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3RhuF8foyPW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We address the problem of hyper-parameter optimization (HPO) for federated learning (FL-HPO). We introduce Federated Loss SuRface Aggregation (FLoRA), a general FL-HPO solution framework that can address use cases of tabular data and any Machine Learning (ML) model including gradient boosting training algorithms, SVMs, neural networks, among others and thereby further expands the scope of FL-HPO. FLoRA enables single-shot FL-HPO: identifying a single set of good hyper-parameters that are subsequently used in a single FL training. Thus, it enables FL-HPO solutions with minimal additional communication overhead compared to FL training without HPO. Utilizing standard smoothness assumptions, we theoretically characterize the optimality gap of FLoRA for any convex and non-convex loss functions, which explicitly accounts for the heterogeneous nature of the parties&#39; local data distributions, a dominant characteristic of FL systems. Our empirical evaluation of FLoRA for multiple FL algorithms on seven OpenML datasets demonstrates significant model accuracy improvements over the baselines, and robustness to increasing number of parties involved in FL-HPO training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                848. <h><a href=https://openreview.net/pdf?id=O_lFCPaF48t>Structure by Architecture: Structured Representations without Regularization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Felix Leeb, Giulia Lanzillotta, Yashas Annadani, Michel Besserve, Stefan Bauer, Bernhard Schölkopf</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=O_lFCPaF48t>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel autoencoder architecture to structure the learned representation without regularizing the objective and improve sampling for generative modeling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/O_lFCPaF48t.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the problem of self-supervised structured representation learning using autoencoders for downstream tasks such as generative modeling. Unlike most methods which rely on matching an arbitrary, relatively unstructured, prior distribution for sampling, we propose a sampling technique that relies solely on the independence of latent variables, thereby avoiding the trade-off between reconstruction quality and generative performance typically observed in VAEs. We design a novel autoencoder architecture capable of learning a structured representation without the need for aggressive regularization. Our structural decoders learn a hierarchy of latent variables, thereby ordering the information without any additional regularization or supervision. We demonstrate how these models learn a representation that improves results in a variety of downstream tasks including generation, disentanglement, and extrapolation using several challenging and natural image datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                849. <h><a href=https://openreview.net/pdf?id=SM7XkJouWHm>ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaojun Guo, Yifei Wang, Tianqi Du, Yisen Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SM7XkJouWHm>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SM7XkJouWHm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance degenerates as the layer goes deeper. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the power of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under some conditions. Our proposed normalization layer can be easily inserted into GNNs and Transformers with negligible parameter overhead. Experiments on various real-world datasets verify the effectiveness of our method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                850. <h><a href=https://openreview.net/pdf?id=ih0uFRFhaZZ>Continual Unsupervised Disentangling of Self-Organizing Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhiyuan Li, Xiajun Jiang, Ryan Missel, Prashnna Kumar Gyawali, Nilesh Kumar, Linwei Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ih0uFRFhaZZ>6.5 - [6, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed a novel generative model describing a topologically-connected mixture of spike-and-slab distributions in the latent space for continual unsupervised learning and disentangling representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ih0uFRFhaZZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Limited progress has been made in continual unsupervised learning of representations, especially in reusing, expanding, and continually disentangling learned semantic factors across data environments. We argue that this is because existing approaches treat continually-arrived data independently, without considering how they are related based on the underlying semantic factors. We address this by a new generative model describing a topologically-connected mixture of spike-and-slab distributions in the latent space, learned end-to-end in a continual fashion via principled variational inference. The learned mixture is able to automatically discover the active semantic factors underlying each data environment and to accumulate their relational structure based on that. This distilled knowledge of different data environments can further be used for generative replay and guiding continual disentangling of new semantic factors. We tested the presented method on a split version of 3DShapes to provide the first quantitative disentanglement evaluation of continually learned representations, and further demonstrated its ability to continually disentangle new representations in benchmark datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                851. <h><a href=https://openreview.net/pdf?id=IpGgfpMucHj>Voint Cloud: Multi-View Point Cloud Representation for 3D Understanding </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Abdullah Hamdi, Silvio Giancola, Bernard Ghanem</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IpGgfpMucHj>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose voint cloud, a novel 3D data structure, that combines multi-view and point clouds for robust 3D understanding tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IpGgfpMucHj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multi-view projection methods have demonstrated promising performance on 3D understanding tasks like 3D classification and segmentation. However, it remains unclear how to combine such multi-view methods with the widely available 3D point clouds. Previous methods use unlearned heuristics to combine features at the point level. To this end, we introduce the concept of the multi-view point cloud (Voint cloud), representing each 3D point as a set of features extracted from several view-points. This novel 3D Voint cloud representation combines the compactness of 3D point cloud representation with the natural view-awareness of multi-view representation. Naturally, we can equip this new representation with convolutional and pooling operations. We deploy a Voint neural network (VointNet) to learn representations in the Voint space. Our novel representation achieves state-of-the-art performance on 3D classification, shape retrieval, and robust 3D part segmentation on standard benchmarks ( ScanObjectNN, ShapeNet Core55, and ShapeNet Parts). Further analysis shows that VointNet improves the robustness to occlusion compared to other methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                852. <h><a href=https://openreview.net/pdf?id=rimcq1oIFeR>A Control-Centric Benchmark for Video Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Stephen Tian, Chelsea Finn, Jiajun Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rimcq1oIFeR>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We find that existing video evaluation metrics are not always indicative of a model&#39;s performance during control, and propose a benchmark that directly evaluates video prediction models on simulated manipulation tasks by using them for planning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rimcq1oIFeR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Video is a promising source of knowledge for embodied agents to learn models of the world&#39;s dynamics. Large deep networks have become increasingly effective at modeling complex video data in a self-supervised manner, as evaluated by metrics based on human perceptual similarity or pixel-wise comparison. However, it remains unclear whether current metrics are accurate indicators of performance on downstream tasks. We find empirically that for planning robotic manipulation, existing metrics can be unreliable at predicting execution success. To address this, we propose a benchmark for action-conditioned video prediction in the form of a control benchmark that evaluates a given model for simulated robotic manipulation through sampling-based planning. Our benchmark, Video Prediction for Visual Planning ($\text{VP}^2$), includes simulated environments with $11$ task categories and $310$ task instance definitions, a full planning implementation, and training datasets containing scripted interaction trajectories for each task category. A central design goal of our benchmark is to expose a simple interface -- a single forward prediction call -- so it is straightforward to evaluate almost any action-conditioned video prediction model. We then leverage our benchmark to study the effects of scaling model size, quantity of training data, and model ensembling by analyzing five highly-performant video prediction models, finding that while scale can improve perceptual quality when modelling visually diverse settings, other attributes such as uncertainty awareness can also aid planning performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                853. <h><a href=https://openreview.net/pdf?id=elDEe8LYW7->NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyeong-Seok Choi, Jinhyeok Yang, Juheon Lee, Hyeongju Kim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=elDEe8LYW7->6.5 - [8, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper introduces a unified voice synthesis framework that tackles four tasks, zero-shot voice conversion, text-to-speech, singing voice synthesis, and voice designing.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/elDEe8LYW7-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Various applications of voice synthesis have been developed independently despite the fact that they generate “voice” as output in common. In addition, most of the voice synthesis models still require a large number of audio data paired with annotated labels (e.g., text transcription and music score) for training. To this end, we propose a unified framework of synthesizing and manipulating voice signals from analysis features, dubbed NANSY++. The backbone network of NANSY++ is trained in a self-supervised manner that does not require any annotations paired with audio. After training the backbone network, we efficiently tackle four voice applications - i.e. voice conversion, text-to-speech, singing voice synthesis, and voice designing - by partially modeling the analysis features required for each task. Extensive experiments show that the proposed framework offers competitive advantages such as controllability, data efficiency, and fast training convergence, while providing high quality synthesis. Audio samples: tinyurl.com/8tnsy3uc.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                854. <h><a href=https://openreview.net/pdf?id=8HRvyxc606>Reliability of CKA as a Similarity Measure in Deep Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, Eugene Belilovsky</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8HRvyxc606>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We extensively study a broad class of cases where the very popular CKA analysis method for deep representations can give unreliable results.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8HRvyxc606.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Comparing learned neural representations in neural networks is a challenging but important problem, which has been approached in different ways. The Centered Kernel Alignment (CKA) similarity metric, particularly its linear variant, has recently become a popular approach and has been widely used to compare representations of a network&#39;s different layers, of architecturally similar networks trained differently, or of models with different architectures trained on the same data. A wide variety of claims about similarity and dissimilarity of these various representations have been made using CKA results. In this work we present analysis that formally characterizes CKA sensitivity to a large class of simple transformations, which can naturally occur in the context of modern machine learning. This provides a concrete explanation to CKA sensitivity to outliers, which has been observed in past works, and to transformations that preserve the linear separability of the data, an important generalization attribute. We empirically investigate several weaknesses of the CKA similarity metric, demonstrating situations in which it gives unexpected or counterintuitive results. Finally we study approaches for modifying representations to maintain functional behaviour while changing the CKA value. Our results illustrate that, in many cases, the CKA value can be easily manipulated without substantial changes to the functional behaviour of the models, and call for caution when leveraging activation alignment metrics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                855. <h><a href=https://openreview.net/pdf?id=XCTVFJwS9LJ>Flow Annealed Importance Sampling Bootstrap</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Laurence Illing Midgley, Vincent Stimper, Gregor N. C. Simm, Bernhard Schölkopf, José Miguel Hernández-Lobato</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XCTVFJwS9LJ>6.5 - [6, 8, 8, 6, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train normalizing flows to fit multi-modal target distributions by generating samples where the flow is a poor approximation of the target using an annealed importance sampling bootstrap procedure.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XCTVFJwS9LJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Normalizing flows are tractable density models that can approximate complicated target distributions, e.g. Boltzmann distributions of physical systems. However, current methods for training flows either suffer from mode-seeking behavior, use samples from the target generated beforehand by expensive MCMC methods, or use stochastic losses that have high variance. To avoid these problems, we augment flows with annealed importance sampling (AIS) and minimize the mass-covering $\alpha$-divergence with $\alpha=2$, which minimizes importance weight variance. Our method, Flow AIS Bootstrap (FAB), uses AIS to generate samples in regions where the flow is a poor approximation of the target, facilitating the discovery of new modes. We apply FAB to multimodal targets and show that we can approximate them very accurately where previous methods fail. To the best of our knowledge, we are the first to learn the Boltzmann distribution of the alanine dipeptide molecule using only the unnormalized target density, without access to samples generated via Molecular Dynamics (MD) simulations: FAB produces better results than training via maximum likelihood on MD samples while using 100 times fewer target evaluations. After reweighting the samples, we obtain unbiased histograms of dihedral angles that are almost identical to the ground truth.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                856. <h><a href=https://openreview.net/pdf?id=k1FHgri5y3->Sparse Random Networks for Communication-Efficient Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Berivan Isik, Francesco Pase, Deniz Gunduz, Tsachy Weissman, Zorzi Michele</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k1FHgri5y3->6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an FL framework, where clients find a sparse random network using a stochastic strategy; and provide (1) lower communication cost, (2) higher accuracy, (3) faster convergence, and (4) at the end of the training, a compressed final model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k1FHgri5y3-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    One main challenge in federated learning is the large communication cost of exchanging weight updates from clients to the server at each round. While prior work has made great progress in compressing the weight updates through gradient compression methods, we propose a radically different approach that does not update the weights at all. Instead, our method freezes the weights at their initial \emph{random} values and learns how to sparsify the random network for the best performance. To this end, the clients collaborate in training a \emph{stochastic} binary mask to find the optimal sparse random network within the original one. At the end of the training, the final model is a sparse network with random weights -- or a subnetwork inside the dense random network. We show improvements in accuracy, communication (less than $1$ bit per parameter (bpp)), convergence speed, and final model size (less than $1$ bpp) over relevant baselines on MNIST, EMNIST, CIFAR-10, and CIFAR-100 datasets, in the low bitrate regime.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                857. <h><a href=https://openreview.net/pdf?id=jgmuRzM-sb6>DAG Matters! GFlowNets Enhanced Explainer for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenqian Li, Yinchuan Li, Zhigang Li, Jianye HAO, Yan Pang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jgmuRzM-sb6>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jgmuRzM-sb6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure – GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its’ reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                858. <h><a href=https://openreview.net/pdf?id=PINRbk7h01>Restricted Strong Convexity of Deep Learning Models with Smooth Activations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Arindam Banerjee, Pedro Cisneros-Velarde, Libin Zhu, Misha Belkin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PINRbk7h01>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PINRbk7h01.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the problem of optimization of deep learning models with smooth activation functions. While there exist influential results on the problem from the ``near initialization&#39;&#39; perspective, we shed considerable new light on the problem. In particular, we make two key technical contributions for such models with $L$ layers, $m$ width, and $\sigma_0^2$ initialization variance. First, for suitable $\sigma_0^2$, we establish a $O(\frac{\text{poly}(L)}{\sqrt{m}})$ upper bound on the spectral norm of the Hessian of such models, considerably sharpening prior results. Second, we introduce a new analysis of optimization based on Restricted Strong Convexity (RSC) which holds as long as the squared norm of the average gradient of predictors is $\Omega(\frac{\text{poly}(L)}{\sqrt{m}})$ for the square loss. We also present results for more general losses. The RSC based analysis does not need the ``near initialization&#34; perspective and guarantees geometric convergence for gradient descent (GD). To the best of our knowledge, ours is the first result on establishing geometric convergence of GD based on RSC for deep learning models, thus becoming an alternative sufficient condition for convergence that does not depend on the widely-used Neural Tangent Kernel (NTK). We share preliminary experimental results supporting our theoretical advances.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                859. <h><a href=https://openreview.net/pdf?id=RDy3IbvjMqT>$\mathrm{SE}(3)$-Equivariant Attention Networks for Shape Reconstruction in Function Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Evangelos Chatzipantazis, Stefanos Pertigkiozoglou, Edgar Dobriban, Kostas Daniilidis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RDy3IbvjMqT>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RDy3IbvjMqT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a method for 3D shape reconstruction from unoriented point clouds. Our method consists of a novel SE(3)-equivariant coordinate-based network (TF-ONet), that parametrizes the occupancy field of the shape and respects the inherent symmetries of the problem. In contrast to previous shape reconstruction methods that align the input to a regular grid, we operate directly on the irregular point cloud. Our architecture leverages equivariant attention layers that operate on local tokens. This mechanism enables local shape modelling, a crucial property for scalability to large scenes. Given an unoriented, sparse, noisy point cloud as input, we produce equivariant features for each point. These serve as keys and values for the subsequent equivariant cross-attention blocks that parametrize the occupancy field. By querying an arbitrary point in space, we predict its occupancy score. We show that our method outperforms previous SO(3)-equivariant methods, as well as non-equivariant methods trained on SO(3)-augmented datasets. More importantly, local modelling together with SE(3)-equivariance create an ideal setting for SE(3) scene reconstruction. We show that by training only on single, aligned objects and without any pre-segmentation, we can reconstruct novel scenes containing arbitrarily many objects in random poses without any performance loss. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                860. <h><a href=https://openreview.net/pdf?id=cxvEGLCHpgl>Semi-supervised Community Detection via Structural Similarity Metrics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yicong Jiang, Tracy Ke</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cxvEGLCHpgl>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a fast semi-supervised community detection algorithm AngleMin+ based on the structural similarity metric of DCBM, which is able to address degree heterogeneity and non-assortative network and possesses nice theoretical guarantees.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cxvEGLCHpgl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Motivated by the interests of social network analysis and network-based recommendation systems, we consider a semi-supervised community detection problem, where the goal is to estimate the community label of a new node by leveraging on the network structure and partially observed community labels of existing nodes. 
We model the network with a degree-corrected stochastic block model, which allows for severe degree heterogeneity and potentially non-assortative communities. 
We propose a fast algorithm that computes a `structural similarity metric&#39; between the new node and each of the $K$ communities, aggregating information in labeled and unlabeled data. The estimated label of the new node is equal to the value of $k$  that maximizes this similarity metric. Our method is computationally fast and compares favorably with existing semi-supervised algorithms on numerical performance. In theory, we derive explicit bounds for the misclassification error and show the efficiency of our method by comparing it with an ideal classifier. To our best knowledge, our results provide the first semi-supervised community detection algorithm with theoretical guarantees.  
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                861. <h><a href=https://openreview.net/pdf?id=XomEU3eNeSQ>Code Translation with Compiler Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marc Szafraniec, Baptiste Roziere, Hugh James Leather, Patrick Labatut, Francois Charton, Gabriel Synnaeve</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XomEU3eNeSQ>6.5 - [5, 5, 6, 10] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We leverage compiler intermediate representations (IR) for the unsupervised neural machine translation of programming languages and get state-of-the-art results
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XomEU3eNeSQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we leverage low-level compiler intermediate representations (IR) code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11% on average, and up to 79% for the Java → Rust pair with greedy decoding. With beam search, it increases the number of correct translations by 5.5% in average. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                862. <h><a href=https://openreview.net/pdf?id=KLrGlNoxzb4>Video Scene Graph Generation from Single-Frame Weak Supervision</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Siqi Chen, Jun Xiao, Long Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KLrGlNoxzb4>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel method for weakly-supervised VidSGG task with only single-frame weak supervision.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KLrGlNoxzb4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Video scene graph generation (VidSGG) aims to generate a sequence of graph-structure representations for the given video. However, all existing VidSGG methods are fully-supervised, i.e., they need dense and costly manual annotations. In this paper, we propose the first weakly-supervised VidSGG task with only single-frame weak supervision: SF-VidSGG. By ``weakly-supervised&#34;, we mean that SF-VidSGG relaxes the training supervision from two different levels: 1) It only provides single-frame annotations instead of all-frame annotations. 2) The single-frame ground-truth annotation is still a weak image SGG annotation, i.e., an unlocalized scene graph. To solve this new task, we also propose a novel Pseudo Label Assignment based method, dubbed as PLA. PLA is a two-stage method, which generates pseudo visual relation annotations for the given video at the first stage, and then trains a fully-supervised VidSGG model with these pseudo labels. Specifically, PLA consists of three modules: an object PLA module, a predicate PLA module, and a future predicate prediction (FPP) module. Firstly, in the object PLA, we localize all objects for every frame. Then, in the predicate PLA, we design two different teachers to assign pseudo predicate labels. Lastly, in the FPP module, we fusion these two predicate pseudo labels by the regularity of relation transition in videos. Extensive ablations and results on the benchmark Action Genome have demonstrated the effectiveness of our PLA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                863. <h><a href=https://openreview.net/pdf?id=U2g8OGONA_V>Multi-domain image generation and translation with identifiability guarantees</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shaoan Xie, Lingjing Kong, Mingming Gong, Kun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=U2g8OGONA_V>6.5 - [6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a way to learn the pairing information from unpaired data with theoretial guarantees, with direct applications in learning tasks such as image-to-image translation
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/U2g8OGONA_V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multi-domain image generation and unpaired image-to-to-image translation are two important and related computer vision problems. The common technique for the two tasks is the learning of a joint distribution from multiple marginal distributions. However, it is well known that there can be infinitely many joint distributions that can derive the same marginals. Hence, it is necessary to formulate suitable constraints to address this highly ill-posed problem. Inspired by the recent advances in nonlinear Independent Component Analysis (ICA) theory, we propose a new method to learn the joint distribution from the marginals by enforcing a specific type of minimal change across domains. We report one of the first results connecting multi-domain generative models to identifiability and shows why identifiability is essential and how to achieve it theoretically and practically. We apply our method to five multi-domain image generation and six image-to-image translation tasks. The superior performance of our model supports our theory and demonstrates the effectiveness of our method. The training code are available at https://github.com/Mid-Push/i-stylegan.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                864. <h><a href=https://openreview.net/pdf?id=-G1kjTFsSs>Learning Kernelized Contextual Bandits in a Distributed and Asynchronous Environment</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chuanhao Li, Huazheng Wang, Mengdi Wang, Hongning Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-G1kjTFsSs>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose and analyze a communication efficient asynchronous Kernel UCB algorithm with Nystrom approximation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-G1kjTFsSs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the recent advances in communication-efficient distributed bandit learning, most existing solutions are restricted to parametric models, e.g., linear bandits and generalized linear bandits (GLB). In comparison, kernel bandits, which search for non-parametric functions in a reproducing kernel Hilbert space (RKHS), offer higher modeling capacity. But the only existing work in distributed kernel bandits adopts a synchronous communication protocol, which greatly limits its practical use (e.g., every synchronization step requires all clients to participate and wait for data exchange).
In this paper, in order to improve the robustness against delays and unavailability of clients that are common in practice, we propose the first asynchronous solution based on approximated kernel regression for distributed kernel bandit learning. A set of effective treatments are developed to ensure approximation quality and communication efficiency. Rigorous theoretical analysis about the regret and communication cost is provided; and extensive empirical evaluations demonstrate the effectiveness of our solution.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                865. <h><a href=https://openreview.net/pdf?id=ZCStthyW-TD>Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Uday Kamal, Saurabh Dash, Saibal Mukhopadhyay</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZCStthyW-TD>6.5 - [6, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose EventFormer, an asynchronous spatiotemporal representation learning framework augmented by an associative memory to efficiently perform event-based perception.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZCStthyW-TD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose $\textit{EventFormer}$, a computationally efficient event-based representation learning framework for asynchronously processing event camera data. EventFormer treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism. An associative memory-augmented recurrent module is used to correlate with the stored representation computed from past events. A memory addressing mechanism is proposed to store and retrieve the latent states only $\textit{where}$ these events occur and update them only $\textit{when}$ they occur. The representation learning shift from input space to the latent memory space resulting in reduced computation cost for processing each event. We show that EventFormer achieves 0.5$\%$ and 9$\%$ better accuracy with 30000$\times$ and 200$\times$ less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                866. <h><a href=https://openreview.net/pdf?id=1J-ZTr7aypY>Differentiable Mathematical Programming for Object-Centric Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Adeel Pervez, Phillip Lippe, Efstratios Gavves</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1J-ZTr7aypY>6.5 - [5, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1J-ZTr7aypY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose topology-aware feature partitioning into $k$ disjoint partitions for given scene features as a method for object-centric representation learning. To this end, we propose to use minimum $s$-$t$ graph cuts as a partitioning method which is represented as a linear program. The method is topologically aware since it explicitly encodes neighborhood relationships in the image graph. To solve the graph cuts our solution relies on an efficient, scalable, and differentiable quadratic programming approximation. Optimizations specific to cut problems allow us to solve the quadratic programs and compute their gradients significantly more efficiently compared with the general quadratic programming approach. Our results show that our approach is scalable and outperforms existing methods on object discovery tasks with textured scenes and objects.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                867. <h><a href=https://openreview.net/pdf?id=dBk3hsg-n6>Artificial Neuronal Ensembles with Learned Context Dependent Gating</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthew James Tilley, Michelle Miller, David Freedman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dBk3hsg-n6>6.5 - [8, 5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method to alleviate catastrophic forgetting in artificial neural networks using learned context dependent activity gates
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dBk3hsg-n6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Biological neural networks are capable of recruiting different sets of neurons to encode different memories. However, when training artificial neural networks on a set of tasks, typically, no mechanism is employed for selectively producing anything analogous to these neuronal ensembles. Further, artificial neural networks suffer from catastrophic forgetting, where the network&#39;s performance rapidly deteriorates as tasks are learned sequentially. By contrast, sequential learning is possible for a range of biological organisms. We introduce Learned Context Dependent Gating (LXDG), a method to flexibly allocate and recall `artificial neuronal ensembles&#39;, using a particular network structure and a new set of regularization terms. Activities in the hidden layers of the network are modulated by gates, which are dynamically produced during training. The gates are outputs of networks themselves, trained with a sigmoid output activation. The regularization terms we have introduced correspond to properties exhibited by biological neuronal ensembles. The first term penalizes low gate sparsity, ensuring that only a specified fraction of the network is used. The second term ensures that previously learned gates are recalled when the network is presented with input from previously learned tasks. Finally, there is a regularization term responsible for ensuring that new tasks are encoded in gates that are as orthogonal as possible from previously used ones. We demonstrate the ability of this method to alleviate catastrophic forgetting on continual learning benchmarks. When the new regularization terms are included in the model along with Elastic Weight Consolidation (EWC) it achieves better performance on the benchmark `permuted MNIST&#39; than with EWC alone. The benchmark `rotated MNIST&#39; demonstrates how similar tasks recruit similar neurons to the artificial neuronal ensemble. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                868. <h><a href=https://openreview.net/pdf?id=ymt1zQXBDiF>SoftMatch: Addressing the Quantity-Quality Tradeoff in Semi-supervised Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, Marios Savvides</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ymt1zQXBDiF>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper revisit the quantity-quality tradeoff with a unified sample weighting function of pseudo-labeling/consistency loss. From the analysis, we propose SoftMatch, which better utilizes unlabeled data while reducing the enrolled error rate. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ymt1zQXBDiF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model&#39;s generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                869. <h><a href=https://openreview.net/pdf?id=ySCL-NG_I3>Learning Harmonic Molecular Representations on Riemannian Manifold</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yiqun Wang, Yuning Shen, Shi Chen, Lihao Wang, Fei YE, Hao Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ySCL-NG_I3>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a harmonic molecular representation learning framework to achieve multi-resolution molecular encoding on 2D Riemannian manifold.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ySCL-NG_I3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Molecular representation learning plays a crucial role in AI-assisted drug discovery research. Encoding 3D molecular structures through Euclidean neural networks has become the prevailing method in the geometric deep learning community. However, the equivariance constraints and message passing in Euclidean space may limit the network expressive power. In this work, we propose a Harmonic Molecular Representation learning (HMR) framework, which represents a molecule using the Laplace-Beltrami eigenfunctions of the molecular surface. HMR offers a multi-resolution representation of molecular geometric and chemical properties on 2D Riemannian manifold. We also introduce a harmonic message passing method to realize efficient spectral message passing over the surface manifold for better molecular encoding. Our proposed method shows comparable predictive power to current models in small molecule property prediction, and outperforms the state-of-the-art deep learning models for the rigid protein docking challenge, demonstrating its versatility in molecular representation learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                870. <h><a href=https://openreview.net/pdf?id=L2MUOUp0beo>CoRTX: Contrastive Framework for Real-time Explanation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yu-Neng Chuang, Guanchu Wang, Fan Yang, Quan Zhou, Pushkar Tripathi, Xuanting Cai, Xia Hu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=L2MUOUp0beo>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Learning real-time model explainer with limited explanation labels.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/L2MUOUp0beo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent advancements in explainable machine learning provide effective and faithful solutions for interpreting model behaviors. However, many explanation methods encounter efficiency issues, which largely limit their deployments in practical scenarios. Real-time explainer (RTX) frameworks have thus been proposed to accelerate the model explanation process by learning an one-feed-forward explainer. Existing RTX frameworks typically build the explainer under the supervised learning paradigm, which requires large amounts of explanation labels as the ground truth. Considering that accurate explanation labels are usually hard to obtain, due to constrained computational resources and limited human efforts, effective explainer training is still challenging in practice. In this work, we propose a COntrastive Real-Time eXplanation (CoRTX) framework to learn the explanation-oriented representation and relieve the intensive dependence of explainer training on explanation labels. Specifically, we design a synthetic strategy to select positive and negative instances for explanation representation learning. Theoretical analysis show that our selection strategy can benefit the contrastive learning process on explanation tasks. Experimental results on three real-world datasets further demonstrate the efficiency and efficacy of our proposed CoRTX framework.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                871. <h><a href=https://openreview.net/pdf?id=socffUzSIlx>Anisotropic Message Passing: Graph Neural Networks with Directional and Long-Range Interactions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Moritz Thürlemann, Sereina Riniker</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=socffUzSIlx>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Modified message passing for the efficient description of long-range and directional interactions with applications to quantum-chemical systems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/socffUzSIlx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural networks have shown great potential for the description of a variety of chemical systems.
However, standard message passing does not explicitly account for long-range and directional interactions, for instance due to electrostatics.
In this work, an anisotropic state based on Cartesian multipoles is proposed as an addition to the existing hidden features.
With the anisotropic state, message passing can be modified to explicitly account for directional interactions.
Compared to existing models, this modification results in relatively little additional computational cost. 
Most importantly, the proposed formalism offers as a distinct advantage the seamless integration of (1) anisotropic long-range interactions, (2) interactions with surrounding fields and particles that are not part of the graph, and (3) the fast multipole method.
As an exemplary use case, the application to quantum mechanics/molecular mechanics (QM/MM) systems is demonstrated.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                872. <h><a href=https://openreview.net/pdf?id=CPIy9TWFYBG>Proactive Multi-Camera Collaboration for 3D Human Pose Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hai Ci, Mickel Liu, Xuehai Pan, fangwei zhong, Yizhou Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CPIy9TWFYBG>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel MARL framework to solve proactive multi-camrea collaborations for 3D HPE in human crowds
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CPIy9TWFYBG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents a multi-agent reinforcement learning (MARL) scheme for proactive Multi-Camera Collaboration in 3D Human Pose Estimation in dynamic human crowds. Traditional fixed-viewpoint multi-camera solutions for human motion capture (MoCap) are limited in capture space and susceptible to dynamic occlusions. Active camera approaches proactively control camera poses to find optimal viewpoints for 3D reconstruction. However, current methods still face challenges with credit assignment and environment dynamics. To address these issues, our proposed method introduces a novel Collaborative Triangulation Contribution Reward (CTCR) that improves convergence and alleviates multi-agent credit assignment issues resulting from using 3D reconstruction accuracy as the shared reward. Additionally, we jointly train our model with multiple world dynamics learning tasks to better capture environment dynamics and encourage anticipatory behaviors for occlusion avoidance. We evaluate our proposed method in four photo-realistic UE4 environments to ensure validity and generalizability. Empirical results show that our method outperforms fixed and active baselines in various scenarios with different numbers of cameras and humans.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                873. <h><a href=https://openreview.net/pdf?id=uu1GBD9SlLe>Simple and Scalable Nearest Neighbor Machine Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuhan Dai, Zhirui Zhang, Qiuzhi Liu, Qu Cui, Weihua Li, Yichao Du, Tong Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uu1GBD9SlLe>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple and scalable nearest neighbor machine translation framework to drastically improve the decoding and storage efficiency of $k$NN-MT
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uu1GBD9SlLe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    $k$NN-MT is a straightforward yet powerful approach for fast domain adaptation, which directly plugs the pre-trained neural machine translation (NMT) models with domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve domain adaptation without retraining. Despite being conceptually attractive, $k$NN-MT is burdened with massive storage requirements and high computational complexity since it conducts nearest neighbor searches over the entire reference corpus. In this paper, we propose a simple and scalable nearest neighbor machine translation framework to drastically promote the decoding and storage efficiency of $k$NN-based models while maintaining the translation performance. To this end, we dynamically construct a extremely small datastore for each input via sentence-level retrieval to avoid searching the entire datastore in vanilla $k$NN-MT, based on which we further introduce a distance-aware adapter to adaptively incorporate the $k$NN retrieval results into the pre-trained NMT models. Experiments on machine translation in two general settings, static domain adaptation, and online learning, demonstrate that our proposed approach not only achieves almost 90% speed as the NMT model without performance degradation, but also significantly reduces the storage requirements of $k$NN-MT. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                874. <h><a href=https://openreview.net/pdf?id=UYcIheNY9Pf>Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Deyao Zhu, Li Erran Li, Mohamed Elhoseiny</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UYcIheNY9Pf>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UYcIheNY9Pf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline RL dataset. Together with an action translator that converts the abstract graph actions in VMG to real actions in the original environment, VMG controls agents to maximize episode returns. Our experiments on the D4RL benchmark show that VMG can outperform state-of-the-art offline RL methods in several tasks, especially when environments have sparse rewards and long temporal horizons. Code is available at https://github.com/TsuTikgiau/ValueMemoryGraph
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                875. <h><a href=https://openreview.net/pdf?id=FE99-fDrWd5>Semi-Parametric Inducing Point Networks and Neural Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Richa Rastogi, Yair Schiff, Alon Hacohen, Zhaozhi Li, Ian Lee, Yuntian Deng, Mert R. Sabuncu, Volodymyr Kuleshov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FE99-fDrWd5>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FE99-fDrWd5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in meta-learning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of meta-learning tasks, and improves state-of-the-art performance on an important practical problem, genotype imputation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                876. <h><a href=https://openreview.net/pdf?id=07tc5kKRIo>Delving into Semantic Scale Imbalance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yanbiao Ma, Licheng Jiao, Fang Liu, Yuxin Li, Shuyuan Yang, Xu Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=07tc5kKRIo>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our proposed semantic scale, like the number of samples, is a natural measure of class imbalance and does not depend on the model’s predictions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/07tc5kKRIo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Model bias triggered by long-tailed data has been widely studied. However, measure based on the number of samples cannot explicate three phenomena simultaneously: (1) Given enough data, the classification performance gain is marginal with additional samples. (2) Classification performance decays precipitously as the number of training samples decreases when there is insufficient data. (3) Model trained on sample-balanced datasets still has different biases for different classes. In this work, we define and quantify the semantic scale of classes, which is equivalent to the feature diversity of classes. It is exciting to find experimentally that there is a marginal effect of semantic scale, which perfectly describes the first two phenomena. Further, the quantitative measurement of semantic scale imbalance is proposed, which can accurately reflect model bias on multiple datasets, even on sample-balanced data, revealing a novel perspective for the study of class imbalance. Due to the prevalence of semantic scale imbalance, we propose semantic-scale-balanced learning, including a general loss improvement scheme and a dynamic re-weighting training framework that overcomes the challenge of calculating semantic scales in real-time during iterations. Comprehensive experiments show that dynamic semantic-scale-balanced learning consistently enables the model to perform superiorly on large-scale long-tailed and non-long-tailed datasets, which is a good starting point for mitigating the prevalent but unnoticed model bias. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                877. <h><a href=https://openreview.net/pdf?id=462z-gLgSht>DCI-ES: An Extended Disentanglement Framework with Connections to Identifiability</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Cian Eastwood, Andrei Liviu Nicolicioiu, Julius Von Kügelgen, Armin Kekić, Frederik Träuble, Andrea Dittadi, Bernhard Schölkopf</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=462z-gLgSht>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We extend the DCI framework for evaluating disentangled representations and connect it to identifiability.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/462z-gLgSht.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In representation learning, a common approach is to seek representations which disentangle the underlying factors of variation. Eastwood &amp; Williams (2018) proposed three metrics for quantifying the quality of such disentangled representations: disentanglement (D), completeness (C) and informativeness (I). In this work, we first connect this DCI framework to two common notions of linear and nonlinear identifiability, thereby establishing a formal link between disentanglement and the closely-related field of independent component analysis. We then propose an extended DCI-ES framework with two new measures of representation quality—explicitness (E) and size (S)—and point out how D and C can be computed for black-box predictors. Our main idea is that the functional capacity required to use a representation is an important but thus-far neglected aspect of representation quality, which we quantify using explicitness or ease-of-use (E). We illustrate the relevance of our extensions on the MPI3D and Cars3D datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                878. <h><a href=https://openreview.net/pdf?id=9_gsMA8MRKQ>Pseudoinverse-Guided Diffusion Models for Inverse Problems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiaming Song, Arash Vahdat, Morteza Mardani, Jan Kautz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9_gsMA8MRKQ>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce pseudoinverse guidance, an approach to solve inverse problems with generative diffusion models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9_gsMA8MRKQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models have become competitive candidates for solving various inverse problems. Models trained for specific inverse problems work well but are limited to their particular use cases, whereas methods that use problem-agnostic models are general but often perform worse empirically. To address this dilemma, we introduce Pseudoinverse-guided Diffusion Models ($\Pi$GDM), an approach that uses problem-agnostic models to close the gap in performance. $\Pi$GDM directly estimates conditional scores from the measurement model of the inverse problem without additional training. It can address inverse problems with noisy, non-linear, or even non-differentiable measurements, in contrast to many existing approaches that are limited to noiseless linear ones. We illustrate the empirical effectiveness of $\Pi$GDM on several image restoration tasks, including super-resolution, inpainting and JPEG restoration. On ImageNet, $\Pi$GDM is competitive with state-of-the-art diffusion models trained on specific tasks, and is the first to achieve this with problem-agnostic diffusion models. $\Pi$GDM can also solve a wider set of inverse problems where the measurement processes are composed of several simpler ones.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                879. <h><a href=https://openreview.net/pdf?id=W-nZDQyuy8D>GOOD: Exploring geometric cues for detecting objects in an open world</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haiwen Huang, Andreas Geiger, Dan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=W-nZDQyuy8D>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose incorporating geometric cues into open-world object detector training and make significant improvements on various benchmarks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/W-nZDQyuy8D.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We address the task of open-world class-agnostic object detection, i.e., detecting every object in an image by learning from a limited number of base object classes. State-of-the-art RGB-based models suffer from overfitting the training classes and often fail at detecting novel-looking objects. This is because RGB-based models primarily rely on appearance similarity to detect novel objects and are also prone to overfitting short-cut cues such as textures and discriminative parts. To address these shortcomings of RGB-based object detectors, we propose incorporating geometric cues such as depth and normals, predicted by general-purpose monocular estimators. Specifically, we use the geometric cues to train an object proposal network for pseudo-labeling unannotated novel objects in the training set. Our resulting Geometry-guided Open-world Object Detector (GOOD) significantly improves detection recall for novel object categories and already performs well with only a few training classes. Using a single ``person&#39;&#39; class for training on the COCO dataset, GOOD  surpasses SOTA methods by 5.0% AR@100, a relative improvement of 24%. The code has been made available at https://github.com/autonomousvision/good.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                880. <h><a href=https://openreview.net/pdf?id=cHf1DcCwcH3>LipsFormer: Introducing Lipschitz Continuity to Vision Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xianbiao Qi, Jianan Wang, Yihao Chen, Yukai Shi, Lei Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cHf1DcCwcH3>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a Lipschitz continuous Transformer.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cHf1DcCwcH3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a Lipschitz continuous Transformer, called LipsFormer, to pursue training stability both theoretically and empirically for Transformer-based models. In contrast to previous practical tricks that address training instability by learning rate warmup, layer normalization, attention formulation, and weight initialization, we show that Lipschitz continuity is a more essential property to ensure training stability. In LipsFormer, we replace unstable Transformer component modules with Lipschitz continuous counterparts:  CenterNorm instead of LayerNorm, spectral initialization instead of Xavier initialization, scaled cosine similarity attention instead of dot-product attention, and weighted residual shortcut. We prove that these introduced modules are Lipschitz continuous and derive an upper bound on the Lipschitz constant of LipsFormer. Our experiments show that LipsFormer allows stable training of deep Transformer architectures without the need of careful learning rate tuning such as warmup, yielding a faster convergence and better generalization. As a result,  on the ImageNet 1K dataset, LipsFormer-Tiny training for 100 epochs without learning rate warmup attains a top-1 accuracy of 81.6\% which is higher than Swin Transformer-Tiny training for 300 epochs with warmup. Moreover, LipsFormer-Tiny training for 300 epochs achieves a top-1 accuracy of 83.5\% with 4.7G FLOPs and 24M parameters. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                881. <h><a href=https://openreview.net/pdf?id=cVFD6qE8gnY>Planning with Sequence Models through Iterative Energy Minimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongyi Chen, Yilun Du, Yiye Chen, Joshua B. Tenenbaum, Patricio A. Vela</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cVFD6qE8gnY>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Planning with Transformers through the energy minimization (MCMC sampling)
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cVFD6qE8gnY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent works have shown that language modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing language models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of language models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with language models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We further demonstrate unique benefits of our iterative optimization procedure, involving new task generalization, test-time constraints adaptation, and the ability to compose plans together. Project webpage: https://hychen-naza.github.io/projects/LEAP/index.html
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                882. <h><a href=https://openreview.net/pdf?id=TVY6GoURrw>Private Federated Learning Without a Trusted Server: Optimal Algorithms for Convex Losses</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrew Lowy, Meisam Razaviyayn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TVY6GoURrw>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Optimal algorithms for differentially private convex/strongly convex federated learning with data from people who do not trust the server or other silos/clients. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TVY6GoURrw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper studies federated learning (FL)—especially cross-silo FL—with data from people who do not trust the server or other silos. In this setting, each silo (e.g. hospital) has data from different people (e.g. patients) and must maintain the privacy of each person’s data (e.g. medical record), even if the server or other silos act as adversarial eavesdroppers. This requirement motivates the study of Inter-Silo Record-Level Differential Privacy (ISRL-DP), which requires silo $i$’s communications to satisfy record/item-level differential privacy (DP). ISRL-DP ensures that the data of each person (e.g. patient) in silo $i$ (e.g. hospital $i$) cannot be leaked. ISRL-DP is different from well-studied privacy notions. Central and user-level DP assume that people trust the server/other silos. On the other end of the spectrum, local DP assumes that people do not trust anyone at all (even their own silo). Sitting between central and local DP, ISRL-DP makes the realistic assumption (in cross-silo FL) that people trust their own silo, but not the server or other silos. In this work, we provide tight (up to logarithms) upper and lower bounds for ISRL-DP FL with convex/strongly convex loss functions and homogeneous (i.i.d.) silo data. Remarkably, we show that similar bounds are attainable for smooth losses with arbitrary heterogeneous silo data distributions, via an accelerated ISRL-DP algorithm. We also provide tight upper and lower bounds for ISRL-DP federated empirical risk minimization, and use acceleration to attain the optimal bounds in fewer rounds of communication than the state-of-the-art. Finally, with a secure “shuffler” to anonymize silo messages (but without a trusted server), our algorithm attains the optimal central DP rates under more practical trust assumptions. Numerical experiments show favorable privacy-accuracy tradeoffs for our algorithm in classification and regression tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                883. <h><a href=https://openreview.net/pdf?id=-htnolWDLvP>Transferable Unlearnable Examples</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jie Ren, Han Xu, Yuxuan Wan, Xingjun Ma, Lichao Sun, Jiliang Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-htnolWDLvP>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-htnolWDLvP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With more people publishing their personal data online, unauthorized data usage has become a serious concern. The unlearnable examples strategies have been introduced to prevent third parties from training on the data without permission. They add perturbations to the users’ data before publishing, so as to make the models trained on the perturbed published dataset invalidated. These perturbations have been generated for a specific training setting and a target dataset. However, their unlearnable effects significantly decrease when used in other training settings or datasets. To tackle this issue, we propose a novel unlearnable strategy based on Class-wise Separability Discriminant (CSD), which boosts the transferability of the unlearnable perturbations by enhancing the linear separability. Extensive experiments demonstrate the transferability of the unlearnable examples crafted by our proposed method across training settings and datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                884. <h><a href=https://openreview.net/pdf?id=g2oB_k-18b>Measure the Predictive Heterogeneity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiashuo Liu, Jiayun Wu, Renjie Pi, Renzhe Xu, Xingxuan Zhang, Bo Li, Peng Cui</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=g2oB_k-18b>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this work, we propose the predictive heterogeneity to measure the data heterogeneity that affects prediction. Theoretical analysis and empirical results validate the rationality of the proposed measure.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/g2oB_k-18b.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As an intrinsic and fundamental property of big data, data heterogeneity exists in a variety of real-world applications, such as in agriculture, sociology, health care, etc. For machine learning algorithms, the ignorance of data heterogeneity will significantly hurt the generalization performance and the algorithmic fairness, since the prediction mechanisms among different sub-populations are likely to differ. In this work, we focus on the data heterogeneity that affects the prediction of machine learning models, and first formalize the Predictive Heterogeneity, which takes into account the model capacity and computational constraints. We prove that it can be reliably estimated from finite data with PAC bounds even in high dimensions. Additionally, we propose the Information Maximization (IM) algorithm, a bi-level optimization algorithm, to explore the predictive heterogeneity of data. Empirically, the explored predictive heterogeneity provides insights for sub-population divisions in agriculture, sociology, and object recognition, and leveraging such heterogeneity benefits the out-of-distribution generalization performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                885. <h><a href=https://openreview.net/pdf?id=eb_cpjZZ3GH>Scalable and Equivariant Spherical CNNs by Discrete-Continuous (DISCO) Convolutions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeremy Ocampo, Matthew Alexander Price, Jason McEwen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eb_cpjZZ3GH>6.5 - [5, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A discrete-continuous (DISCO) spherical CNN framework that is simultaneously rotationally equivariant and computationally scalable and achieves state-of-the-art on numerous benchmarks
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eb_cpjZZ3GH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    No existing spherical convolutional neural network (CNN) framework is both computationally scalable and rotationally equivariant.  Continuous approaches capture rotational equivariance but are often prohibitively computationally demanding.  Discrete approaches offer more favorable computational performance but at the cost of equivariance. We develop a hybrid discrete-continuous (DISCO) group convolution that is simultaneously equivariant and computationally scalable to high-resolution. While our framework can be applied to any compact group, we specialize to the sphere.  Our DISCO spherical convolutions exhibit $\text{SO}(3)$ rotational equivariance, where $\text{SO}(n)$ is the special orthogonal group representing rotations in $n$-dimensions.  When restricting rotations of the convolution to the quotient space $\text{SO}(3)/\text{SO}(2)$ for further computational enhancements, we recover a form of asymptotic $\text{SO}(3)$ rotational equivariance.  Through a sparse tensor implementation we achieve linear scaling in number of pixels on the sphere for both computational cost and memory usage.  For 4k spherical images we realize a saving of $10^9$ in computational cost and $10^4$ in memory usage when compared to the most efficient alternative equivariant spherical convolution. We apply the DISCO spherical CNN framework to a number of benchmark dense-prediction problems on the sphere, such as semantic segmentation and depth estimation, on all of which we achieve the state-of-the-art performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                886. <h><a href=https://openreview.net/pdf?id=Hu4r-dedqR0>Weakly Supervised Explainable Phrasal Reasoning with Neural Fuzzy Logic</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zijun Wu, Zi Xuan Zhang, Atharva Naik, Zhijian Mei, Mauajama Firdaus, Lili Mou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Hu4r-dedqR0>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Hu4r-dedqR0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Natural language inference (NLI) aims to determine the logical relationship between two sentences, such as Entailment, Contradiction, and Neutral. In recent years, deep learning models have become a prevailing approach to NLI, but they lack interpretability and explainability. In this work, we address the explainability of NLI by weakly supervised logical reasoning, and propose an Explainable Phrasal Reasoning (EPR) approach. Our model first detects phrases as the semantic unit and aligns corresponding phrases in the two sentences. Then, the model predicts the NLI label for the aligned phrases, and induces the sentence label by fuzzy logic formulas. Our EPR is almost everywhere differentiable and thus the system can be trained end to end. In this way, we are able to provide explicit explanations of phrasal logical relationships in a weakly supervised manner. We further show that such reasoning results help textual explanation generation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                887. <h><a href=https://openreview.net/pdf?id=086pmarAris>Fantastic Rewards and How to Tame Them: A Case Study on Reward Learning for Task-oriented Dialogue Systems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yihao Feng, Shentao Yang, Shujian Zhang, Jianguo Zhang, Caiming Xiong, Mingyuan Zhou, Huan Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=086pmarAris>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> we propose techniques for learning and utilizing reward functions that can be used for training task-oriented dialogue agents
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/086pmarAris.jpg loading="lazy">
                </div>
                <div class="abstract">
                    When learning task-oriented dialogue (ToD) agents, reinforcement learning (RL) techniques can naturally be utilized to train dialogue strategies to achieve user-specific goals. Prior works mainly focus on adopting advanced RL techniques to train the ToD agents, while the design of the reward function is not well studied. This paper aims at answering the question of how to efficiently learn and leverage a reward function for training end-to-end (E2E) ToD agents. Specifically, we introduce two generalized objectives for reward-function learning, inspired by the classical learning-to-rank literature. Further, we utilize the learned reward function to guide the training of the E2E ToD agent. With the proposed techniques, we achieve competitive results on the E2E response-generation task on the Multiwoz 2.0 dataset. Source code and checkpoints are publicly released at https://github.com/Shentao-YANG/Fantastic_Reward_ICLR2023.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                888. <h><a href=https://openreview.net/pdf?id=R_OL5mLhsv>Interpretability with full complexity by constraining feature information</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kieran A Murphy, Danielle Bassett</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=R_OL5mLhsv>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/R_OL5mLhsv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Interpretability is a pressing issue for machine learning. Common approaches to interpretable machine learning constrain interactions between features of the input, sacrificing model complexity in order to render more comprehensible the effects of those features on the model&#39;s output. We approach interpretability from a new angle: constrain the information about the features without restricting the complexity of the model. We use the Distributed Information Bottleneck to optimally compress each feature so as to maximally preserve information about the output. The learned information allocation, by feature and by feature value, provides rich opportunities for interpretation, particularly in problems with many features and complex feature interactions. The central object of analysis is not a single trained model, but rather a spectrum of models serving as approximations that leverage variable amounts of information about the inputs. Information is allocated to features by their relevance to the output, thereby solving the problem of feature selection by constructing a learned continuum of feature inclusion-to-exclusion. The optimal compression of each feature---at every stage of approximation---allows fine-grained inspection of the distinctions among feature values that are most impactful for prediction. We develop a framework for extracting insight from the spectrum of approximate models and demonstrate its utility on a range of tabular datasets. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                889. <h><a href=https://openreview.net/pdf?id=p0yrSRbN5Bu>Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>XIANGYU PENG, Chen Xing, Prafulla Kumar Choubey, Chien-Sheng Wu, Caiming Xiong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p0yrSRbN5Bu>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> For improving few-shot prompt tuning, we propose a Sample-specific Ensemble of Source Models to transfer knowledge from soft prompts trained on source tasks to target tasks by adjusting the contribution of source models for each target sample.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p0yrSRbN5Bu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks with abundant training samples. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and  propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of ensemble methods and simultaneously captures the sample-specific competence of each source prompt.  We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-\{base, large, XL\}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                890. <h><a href=https://openreview.net/pdf?id=QUK1ExlbbA>DENSE RGB SLAM WITH NEURAL IMPLICIT MAPS</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Heng Li, Xiaodong Gu, Weihao Yuan, luwei yang, Zilong Dong, Ping Tan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QUK1ExlbbA>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QUK1ExlbbA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There is an emerging trend of using neural implicit functions for map representation in Simultaneous Localization and Mapping (SLAM). Some pioneer works have achieved encouraging results on RGB-D SLAM. In this paper, we present a dense RGB SLAM method with neural implicit map representation. To reach this challenging goal without depth input, we introduce a hierarchical feature volume to facilitate the implicit map decoder. This design effectively fuses shape cues across different scales to facilitate map reconstruction. Our method simultaneously solves the camera motion and the neural implicit map by matching the rendered and input video frames. To facilitate optimization, we further propose a photometric warping loss in the spirit of multi-view stereo to better constrain the camera pose and scene geometry. We evaluate our method on commonly used benchmarks and compare it with modern RGB and RGB-D SLAM systems. Our method achieves favorable results than previous methods and even surpasses some recent RGB-D SLAM methods.The code is at poptree.github.io/DIM-SLAM/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                891. <h><a href=https://openreview.net/pdf?id=nchvKfvNeX0>Learning ReLU networks to high uniform accuracy is intractable</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Julius Berner, Philipp Grohs, Felix Voigtlaender</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nchvKfvNeX0>6.5 - [6, 8, 6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Learning target classes of ReLU networks to high uniform accuracy needs exponentially many samples.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nchvKfvNeX0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Statistical learning theory provides bounds on the necessary number of training samples needed to reach a prescribed accuracy in a learning problem formulated over a given target class. This accuracy is typically measured in terms of a generalization error, that is, an expected value of a given loss function. However, for several applications --- for example in a security-critical context or for problems in the computational sciences --- accuracy in this sense is not sufficient. In such cases, one would like to have guarantees for high accuracy on every input value, that is, with respect to the uniform norm. In this paper we precisely quantify the number of training samples needed for any conceivable training algorithm to guarantee a given uniform accuracy on any learning problem formulated over target classes containing (or consisting of) ReLU neural networks of a prescribed architecture. We prove that, under very general assumptions, the minimal number of training samples for this task scales exponentially both in the depth and the input dimension of the network architecture.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                892. <h><a href=https://openreview.net/pdf?id=Yo06F8kfMa1>How Much Space Has Been Explored? Measuring the Chemical Space Covered by Databases and Machine-Generated Molecules</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yutong Xie, Ziqiao Xu, Jiaqi Ma, Qiaozhu Mei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Yo06F8kfMa1>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel evaluation framework for measures of the chemical space in the context of drug discovery.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Yo06F8kfMa1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Forming a molecular candidate set that contains a wide range of potentially effective compounds is crucial to the success of drug discovery. While most databases and machine-learning-based generation models aim to optimize particular chemical properties, there is limited literature on how to properly measure the coverage of the chemical space by those candidates included or generated. This problem is challenging due to the lack of formal criteria to select good measures of the chemical space. In this paper, we propose a novel evaluation framework for measures of the chemical space based on two analyses: an axiomatic analysis with three intuitive axioms that a good measure should obey, and an empirical analysis on the correlation between a measure and a proxy gold standard. Using this framework, we are able to identify #Circles, a new measure of chemical space coverage, which is superior to existing measures both analytically and empirically. We further evaluate how well the existing databases and generation models cover the chemical space in terms of #Circles. The results suggest that many generation models fail to explore a larger space over existing databases, which leads to new opportunities for improving generation models by encouraging exploration. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                893. <h><a href=https://openreview.net/pdf?id=3UHoYrglYkG>Differentially Private $L_2$-Heavy Hitters in the Sliding Window Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeremiah Blocki, Seunghoon Lee, Tamalika Mukherjee, Samson Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3UHoYrglYkG>6.5 - [5, 5, 8, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3UHoYrglYkG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The data management of large companies often prioritize more recent data, as a source of higher accuracy prediction than outdated data. For example, the Facebook data policy retains user search histories for $6$ months while the Google data retention policy states that browser information may be stored for up to $9$ months. These policies are captured by the sliding window model, in which only the most recent $W$ statistics form the underlying dataset. In this paper, we consider the problem of privately releasing the $L_2$-heavy hitters in the sliding window model, which include $L_p$-heavy hitters for $p\le 2$ and in some sense are the strongest possible guarantees that can be achieved using polylogarithmic space, but cannot be handled by existing techniques due to the sub-additivity of the $L_2$ norm. Moreover, existing non-private sliding window algorithms use the smooth histogram framework, which has high sensitivity. To overcome these barriers, we introduce the first differentially private algorithm for $L_2$-heavy hitters in the sliding window model by initiating a number of $L_2$-heavy hitter algorithms across the stream with significantly lower threshold. Similarly, we augment the algorithms with an approximate frequency tracking algorithm with significantly higher accuracy. We then use smooth sensitivity and statistical distance arguments to show that we can add noise proportional to an estimation of the $L_2$ norm. To the best of our knowledge, our techniques are the first to privately release statistics that are related to a sub-additive function in the sliding window model, and may be of independent interest to future differentially private algorithmic design in the sliding window model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                894. <h><a href=https://openreview.net/pdf?id=b0JxQC7JLWh>Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Maksym Yatsura, Kaspar Sakmann, N. Grace Hua, Matthias Hein, Jan Hendrik Metzen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b0JxQC7JLWh>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the first approach for certified recovery and certified detection against adversarial patch attacks on semantic segmentation, which is based on novel masking schemes and image inpainting.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b0JxQC7JLWh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Adversarial patch attacks are an emerging security threat for real world deep learning applications. We present Demasked Smoothing, the first approach (up to our knowledge) to certify the robustness of semantic segmentation models against this threat model. Previous work on certifiably defending against patch attacks has mostly focused on image classification task and often required changes in the model architecture and additional training which is undesirable and computationally expensive. In Demasked Smoothing, any segmentation model can be applied without particular training, fine-tuning, or restriction of the architecture. Using different masking strategies, Demasked Smoothing can be applied both for certified detection and certified recovery. In extensive experiments we show that Demasked Smoothing can on average certify 63% of the pixel predictions for a 1% patch in the detection task and 46% against a 0.5% patch for the recovery task on the ADE20K dataset. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                895. <h><a href=https://openreview.net/pdf?id=wzlWiO_WY4>Calibration Matters: Tackling Maximization Bias in Large-scale Advertising Recommendation Systems</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yewen Fan, Nian Si, Kun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wzlWiO_WY4>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We solve the maximization bias problem in Large-scale Advertising Recommendation Systems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wzlWiO_WY4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Calibration is defined as the ratio of the average predicted click rate to the true click rate. The optimization of calibration is essential to many online advertising recommendation systems because it directly affects the downstream bids in ads auctions and the amount of money charged to advertisers. Despite its importance, calibration often suffers from a problem called “maximization bias”. Maximization bias refers to the phenomenon that the maximum of predicted values overestimates the true maximum. The problem is introduced because the calibration is computed on the set selected by the prediction model itself. It persists even if unbiased predictions are achieved on every datapoint and worsens when covariate shifts exist between the training and test sets. To mitigate this problem, we quantify maximization bias and propose a variance-adjusting debiasing (VAD) meta-algorithm in this paper. The algorithm is efficient, robust, and practical as it is able to mitigate maximization bias problem under covariate shifts, without incurring additional online serving costs or compromising the ranking performance. We demonstrate the effectiveness of the proposed algorithm  using a state-of-the-art recommendation neural network model on a large-scale real-world dataset.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                896. <h><a href=https://openreview.net/pdf?id=6JMXLWX68Kj>On the Performance of Temporal Difference Learning With Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>HAOXING TIAN, Ioannis Paschalidis, Alex Olshevsky</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6JMXLWX68Kj>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6JMXLWX68Kj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Temporal Difference (TD) Learning is an approximate temporal difference method for policy evaluation that uses a neural network for function approximation. Analysis of Neural TD Learning has proven to be challenging. In this paper we provide a convergence analysis of Neural TD Learning with a projection onto $B(\theta_0, \omega)$, a ball of fixed radius $\omega$ around the initial point $\theta_0$. We show an approximation bound of $O(\epsilon + 1/\sqrt{m})$ where $\epsilon$ is the approximation quality of the best neural network in $B(\theta_0, \omega)$ and $m$ is the width of all hidden layers in the network. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                897. <h><a href=https://openreview.net/pdf?id=gcD2UtCGMc2>Replicable Bandits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hossein Esfandiari, Alkis Kalavasis, Amin Karbasi, Andreas Krause, Vahab Mirrokni, Grigoris Velegkas</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gcD2UtCGMc2>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a definition of reproducibility in the context of stochastic bandit problems and we develop algorithms with low regret in various environments.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gcD2UtCGMc2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we introduce the notion of replicable policies in the context of stochastic bandits, one of the canonical problems in interactive learning. A policy in the bandit environment is called replicable if it pulls, with high probability, the exact same sequence of arms in two different and independent executions (i.e., under independent reward realizations). We show that not only do replicable policies exist, but also they achieve almost the same optimal (non-replicable) regret bounds in terms of the time horizon. More specifically, in the stochastic multi-armed bandits setting, we develop a policy with an optimal problem-dependent regret bound whose dependence on the replicability parameter is also optimal. Similarly, for stochastic linear bandits (with finitely and infinitely many arms) we develop replicable policies that achieve the best-known problem-independent regret bounds with an optimal dependency on the replicability parameter. Our results show that even though randomization is crucial for the exploration-exploitation trade-off, an optimal balance can still be achieved while pulling the exact same arms in two different rounds of executions. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                898. <h><a href=https://openreview.net/pdf?id=5vM51iamNeL>Augmentation Component Analysis: Modeling Similarity via the Augmentation Overlaps</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lu Han, Han-Jia Ye, De-Chuan Zhan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5vM51iamNeL>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5vM51iamNeL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-supervised learning aims to learn a embedding space where semantically similar samples are close. Contrastive learning methods pull views of samples together and push different samples away, which utilizes semantic invariance of augmentation but ignores the relationship between samples. To better exploit the power of augmentation, we observe that semantically similar samples are more likely to have similar augmented views. Therefore, we can take the augmented views as a special description of a sample. In this paper, we model such a description as the augmentation distribution, and we call it augmentation feature. The similarity in augmentation feature reflects how much the views of two samples overlap and is related to their semantical similarity. Without computational burdens to explicitly estimate values of the augmentation feature, we propose Augmentation Component Analysis (ACA) with a contrastive-like loss to learn principal components and an on-the-fly projection loss to embed data. ACA equals an efficient dimension reduction by PCA and extracts low-dimensional embeddings, theoretically preserving the similarity of augmentation distribution between samples. Empirical results show that our method can achieve competitive results against various traditional contrastive learning methods on different benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                899. <h><a href=https://openreview.net/pdf?id=0_TxFpAsEI>A law of adversarial risk, interpolation, and label noise</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Paleka, Amartya Sanyal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0_TxFpAsEI>6.5 - [6, 6, 6, 6, 6, 6, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Laws for how interpolating label noise increases adversarial risk, with stronger guarantees in presence of inductive bias and distributional assumptions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0_TxFpAsEI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In supervised learning, it has been shown that label noise in the data can be interpolated without penalties on test accuracy.  We show that interpolating label noise induces adversarial vulnerability, and prove the first theorem showing the relationship between label noise and adversarial risk for any data distribution.  Our results are almost tight if we do not make any assumptions on the inductive bias of the learning algorithm. We then investigate how different components of this problem affect this result including properties of the distribution. We also discuss non-uniform label noise distributions; and prove a new theorem showing uniform label noise induces nearly as large an adversarial risk as the worst poisoning with the same noise rate.  Then, we provide theoretical and empirical evidence that uniform label noise is more harmful than typical real-world label noise.  Finally, we show how inductive biases amplify the effect of label noise and argue the need for future work in this direction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                900. <h><a href=https://openreview.net/pdf?id=TQ5WUwS_4ai>On amortizing convex conjugates for optimal transport</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Brandon Amos</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TQ5WUwS_4ai>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> State-of-the art continuous Wasserstein-2 potential learning, and along the way I improved Jax&#39;s L-BFGS implementation to run in 3% of the time for solving batches of optimization problems
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TQ5WUwS_4ai.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper focuses on computing the convex conjugate operation that arises when solving Euclidean Wasserstein-2 optimal transport problems. This conjugation, which is also referred to as the Legendre-Fenchel conjugate or c-transform,is considered difficult to compute and in practice,Wasserstein-2 methods are limited by not being able to exactly conjugate the dual potentials in continuous space. To overcome this, the computation of the conjugate can be approximated with amortized optimization, which learns a model to predict the conjugate. I show that combining amortized approximations to the conjugate with a solver for fine-tuning significantly improves the quality of transport maps learned for the Wasserstein-2 benchmark by Korotin et al. (2021a) and is able to model many 2-dimensional couplings and flows considered in the literature. All of the baselines, methods, and solvers in this paper are available at http://github.com/facebookresearch/w2ot.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                901. <h><a href=https://openreview.net/pdf?id=sOXU-PEJSgQ>Confidence Estimation Using Unlabeled Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chen Li, Xiaoling Hu, Chao Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sOXU-PEJSgQ>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sOXU-PEJSgQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Overconfidence is a common issue for deep neural networks, limiting their deployment in real-world applications. To better estimate confidence, existing methods mostly focus on fully-supervised scenarios and rely on training labels. In this paper, we propose the first confidence estimation method for a semi-supervised setting, when most training labels are unavailable. We stipulate that even with limited training labels, we can still reasonably approximate the confidence of model on unlabeled samples by inspecting the prediction consistency through the training process.  We use training consistency as a surrogate function and propose a consistency ranking loss for confidence estimation. On both image classification and segmentation tasks, our method achieves state-of-the-art performances in confidence estimation. Furthermore, we show the benefit of the proposed method through a downstream active learning task. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                902. <h><a href=https://openreview.net/pdf?id=NDWl9qcUpvy>Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zihan Zhou, Animesh Garg</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NDWl9qcUpvy>6.5 - [5, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NDWl9qcUpvy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose Structured Exploration with Achievements (SEA), a multi-stage reinforcement learning algorithm designed for achievement-based environments, a particular type of environment with an internal achievement set. SEA first uses offline data to learn a representation of the known achievements with a determinant loss function, then recovers the dependency graph of the learned achievements with a heuristic algorithm, and finally interacts with the environment online to learn policies that master known achievements and explore new ones with a controller built with the recovered dependency graph. We empirically demonstrate that SEA can recover the achievement structure accurately and improve exploration in hard domains such as Crafter that are procedurally generated with high-dimensional observations like images.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                903. <h><a href=https://openreview.net/pdf?id=UClBPxIZqnY>Deep Declarative Dynamic Time Warping for End-to-End Learning of Alignment Paths</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ming Xu, Sourav Garg, Michael Milford, Stephen Gould</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UClBPxIZqnY>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a novel differentiable dynamic time warping layer based on continuous time warps and implicit differentiation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UClBPxIZqnY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper addresses learning end-to-end models for time series data that include a temporal alignment step via dynamic time warping (DTW). Existing approaches to differentiable DTW either differentiate through a fixed warping path or apply a differentiable relaxation to the min operator found in the recursive steps used to solve the DTW problem. We instead propose a DTW layer based around bi-level optimisation and deep declarative networks, which we name DecDTW. By formulating DTW as a continuous, inequality constrained optimisation problem, we can compute gradients for the solution of the optimal alignment (with respect to the underlying time series) using implicit differentiation. An interesting byproduct of this formulation is that DecDTW outputs the optimal warping path between two time series as opposed to a soft approximation, recoverable from Soft-DTW. We show that this property is particularly useful for applications where downstream loss functions are defined on the optimal alignment path itself. This naturally occurs, for instance, when learning to improve the accuracy of predicted alignments against ground truth alignments. We evaluate DecDTW on two such applications, namely the audio-to-score alignment task in music information retrieval and the visual place recognition task in robotics, demonstrating state-of-the-art results in both.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                904. <h><a href=https://openreview.net/pdf?id=bsZULlDGXe>The Dark Side of AutoML: Towards Architectural Backdoor Search</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ren Pang, Changjiang Li, Zhaohan Xi, Shouling Ji, Ting Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bsZULlDGXe>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents EVAS, a new attack to leverage NAS to find neural architectures with exploitable backdoor vulnerability.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bsZULlDGXe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper asks the intriguing question: is it possible to exploit neural architecture search (NAS) as a new attack vector to launch previously improbable attacks? Specifically, we present EVAS, a new attack that leverages NAS to find neural architectures with inherent backdoors and exploits such vulnerability using input-aware triggers. Compared with existing attacks, EVAS demonstrates many interesting properties: (i) it does not require polluting training data or perturbing model parameters; (ii) it is agnostic to downstream fine-tuning or even re-training from scratch; (iii) it naturally evades defenses that rely on inspecting model parameters or training data. With extensive evaluation on benchmark datasets, we show that EVAS features high evasiveness, transferability, and robustness, thereby expanding the adversary&#39;s design spectrum. We further characterize the mechanisms underlying EVAS, which are possibly explainable by architecture-level ``shortcuts&#39;&#39; that recognize trigger patterns. This work showcases that NAS can be exploited in a harmful way to find architectures with inherent backdoor vulnerability. The code is available at https://github.com/ain-soph/nas_backdoor.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                905. <h><a href=https://openreview.net/pdf?id=B4maZQLLW0_>Stateful Active Facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dianbo Liu, Vedant Shah, Oussama Boussif, Cristian Meo, Anirudh Goyal, Tianmin Shu, Michael Curtis Mozer, Nicolas Heess, Yoshua Bengio</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=B4maZQLLW0_>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/B4maZQLLW0_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In cooperative multi-agent reinforcement learning, a team of agents works together
to achieve a common goal. Different environments or tasks may require varying
degrees of coordination among agents in order to achieve the goal in an optimal
way. The nature of coordination will depend on properties of the environment—its
spatial layout, distribution of obstacles, dynamics, etc. We term this variation
of properties within an environment as heterogeneity. Existing literature has not
sufficiently addressed the fact that different environments may have different levels
of heterogeneity. We formalize the notions of coordination level and heterogeneity
level of an environment and present HECOGrid, a suite of multi-agent RL
environments that facilitates empirical evaluation of different MARL approaches
across different levels of coordination and environmental heterogeneity by providing
a quantitative control over coordination and heterogeneity levels of the
environment. Further, we propose a Centralized Training Decentralized Execution
learning approach called Stateful Active Facilitator (SAF) that enables agents to
work efficiently in high-coordination and high-heterogeneity environments through
a differentiable and shared knowledge source used during training and dynamic
selection from a shared pool of policies. We evaluate SAF and compare its performance
against baselines IPPO and MAPPO on HECOGrid. Our results show
that SAF consistently outperforms the baselines across different tasks and different
heterogeneity and coordination levels.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                906. <h><a href=https://openreview.net/pdf?id=dPs6BGO2QT0>Learning Locality and Isotropy in Dialogue Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Han Wu, Haochen Tan, Mingjie Zhan, Gangming Zhao, Shaoqing Lu, Ding Liang, Linqi Song</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dPs6BGO2QT0>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a simple dialogue representation calibration method to learn isotropic and conversational features during the dialogue modeling stage.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dPs6BGO2QT0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing dialogue modeling methods have achieved promising performance on various dialogue tasks with the aid of Transformer and the large-scale pre-trained language models. However, some recent studies revealed that the context representations produced by these methods suffer the problem of anisotropy. In this paper, we find that the generated representations are also not conversational, losing the conversation structure information during the context modeling stage. To this end, we identify two properties in dialogue modeling, i.e., locality and isotropy, and present a simple method for dialogue representation calibration, namely SimDRC, to build isotropic and conversational feature spaces. Experimental results show that our approach significantly outperforms current state-of-the-art models on three open-domain dialogue tasks with eight benchmarks. More in-depth analyses further confirm the effectiveness of our proposed approach. We release the code at https://github.com/hahahawu/SimDRC.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                907. <h><a href=https://openreview.net/pdf?id=1-MBdJssZ-S>Discrete Contrastive Diffusion for Cross-Modal Music and Image Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ye Zhu, Yu Wu, Kyle Olszewski, Jian Ren, Sergey Tulyakov, Yan Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1-MBdJssZ-S>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a conditional contrastive diffusion approach for better input-output correspondence via maximized mutual information, applicable for music and image generations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1-MBdJssZ-S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion probabilistic models (DPMs) have become a popular approach to conditional generation, due to their promising results and support for cross-modal synthesis. A key desideratum in conditional synthesis is to achieve high correspondence between the conditioning input and generated output. Most existing methods learn such relationships implicitly, by incorporating the prior into the variational lower bound. In this work, we take a different route---we explicitly enhance input-output connections by maximizing their mutual information. To this end, we introduce a Conditional Discrete Contrastive Diffusion (CDCD) loss and design two contrastive diffusion mechanisms to effectively incorporate it into the denoising process, combining the diffusion training and contrastive learning for the first time by connecting it with the conventional variational objectives. We demonstrate the efficacy of our approach in evaluations with diverse multimodal conditional synthesis tasks: dance-to-music generation, text-to-image synthesis, as well as class-conditioned image synthesis. On each, we enhance the input-output correspondence and achieve higher or competitive general synthesis quality. Furthermore, the proposed approach improves the convergence of diffusion models, reducing the number of required diffusion steps by more than 35% on two benchmarks, significantly increasing the inference speed.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                908. <h><a href=https://openreview.net/pdf?id=gx2yJS-ENqI>S-NeRF: Neural Radiance Fields for Street Views</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, Li Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gx2yJS-ENqI>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gx2yJS-ENqI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Radiance Fields (NeRFs) aim to synthesize novel views of objects and scenes, given the object-centric camera views with large overlaps. However, we conjugate that this paradigm does not fit the nature of the street views that are collected by many self-driving cars from the large-scale unbounded scenes. Also, the onboard cameras perceive scenes without much overlapping. Thus, existing NeRFs often produce blurs, &#34;floaters&#34; and other artifacts on  street-view synthesis. In this paper, we propose a new street-view NeRF (S-NeRF) that considers novel view synthesis of both the large-scale background scenes and the foreground moving vehicles jointly. Specifically, we improve the scene parameterization function and the camera poses for learning better neural representations from street views. We also use the the noisy and sparse LiDAR points to boost the training and learn a robust geometry and reprojection based confidence to address the depth outliers.   Moreover, we extend our S-NeRF for reconstructing moving vehicles that is impracticable for conventional NeRFs. Thorough experiments on the large-scale driving datasets (e.g., nuScenes and Waymo) demonstrate that our method beats the state-of-the-art rivals by reducing 7～40% of the mean-squared error in the street-view synthesis and a 45% PSNR gain for the moving vehicles rendering.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                909. <h><a href=https://openreview.net/pdf?id=M95oDwJXayG>Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marius-Constantin Dinu, Markus Holzleitner, Maximilian Beck, Hoan Duc Nguyen, Andrea Huber, Hamid Eghbal-zadeh, Bernhard A. Moser, Sergei Pereverzyev, Sepp Hochreiter, Werner Zellinger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=M95oDwJXayG>6.5 - [6, 6, 6, 8] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method for addressing the issue of hyper-parameter selection in unsupervised domain adaptation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/M95oDwJXayG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the problem of choosing algorithm hyper-parameters in unsupervised domain adaptation, i.e., with labeled data in a source domain and unlabeled data in a target domain, drawn from a different input distribution. We follow the strategy to compute several models using different hyper-parameters, and, to subsequently compute a linear aggregation of the models. While several heuristics exist that follow this strategy, methods are still missing that rely on thorough theories for bounding the target error. In this turn, we propose a method that extends weighted least squares to vector-valued functions, e.g., deep neural networks. We show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. We also perform a large scale empirical comparative study on several datasets, including text, images, electroencephalogram, body sensor signals and signals from mobile phones. Our method outperforms deep embedded validation (DEV) and importance weighted validation (IWV) on all datasets, setting a new state-of-the-art performance for solving parameter choice issues in unsupervised domain adaptation with theoretical error guarantees. We further study several competitive heuristics, all outperforming IWV and DEV on at least five datasets. However, our method outperforms each heuristic on at least five of seven datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                910. <h><a href=https://openreview.net/pdf?id=bAz2DBS35i>Noise-Robust De-Duplication at Scale</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Emily Silcock, Luca D&#39;Amico-Wong, Jinglin Yang, Melissa Dell</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bAz2DBS35i>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bAz2DBS35i.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Identifying near duplicates within large, noisy text corpora has a myriad of applications that range from de-duplicating training datasets, reducing privacy risk, and evaluating test set leakage, to identifying reproduced news articles and literature within large corpora. Across these diverse applications, the overwhelming majority of work relies on $N$-grams. Limited efforts have been made to evaluate how well $N$-gram methods perform, in part because it is unclear how one could create an unbiased evaluation dataset for a massive corpus. This study uses the unique timeliness of historical news wires to create a 27,210 document dataset, with 122,876 positive duplicate pairs, for studying noise-robust de-duplication. The time-sensitivity of news makes comprehensive hand labelling feasible - despite the massive overall size of the corpus - as duplicates occur within a narrow date range. The study then develops and evaluates a range of de-duplication methods: hashing and $N$-gram overlap (which predominate in the literature), a contrastively trained bi-encoder, and a ``re-rank&#39;&#39; style approach combining a bi- and cross-encoder. The neural approaches significantly outperform hashing and $N$-gram overlap. We show that the bi-encoder scales well, de-duplicating a 10 million article corpus on a single GPU card in a matter of hours. We also apply our pre-trained model to the RealNews and patent portions of C4 (Colossal Clean Crawled Corpus), illustrating that a neural approach can identify many near duplicates missed by hashing, in the presence of various types of noise. The public release of our NEWS-COPY de-duplication dataset, codebase, and the pre-trained models will facilitate further research and applications.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                911. <h><a href=https://openreview.net/pdf?id=18XzeuYZh_>Online Bias Correction for Task-Free Continual Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aristotelis Chrysakis, Marie-Francine Moens</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=18XzeuYZh_>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/18XzeuYZh_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Task-free continual learning is the machine-learning setting where a model is trained online with data generated by a nonstationary stream. Conventional wisdom suggests that, in this setting, models are trained using an approach called experience replay, where the risk is computed both with respect to current stream observations and to a small subset of past observations. In this work, we explain both theoretically and empirically how experience replay biases the outputs of the model towards recent stream observations. Moreover, we propose a simple approach to mitigate this bias online, by changing how the output layer of the model is optimized. We show that our approach improves significantly the learning performance of experience-replay approaches over different datasets. Our findings suggest that, when performing experience replay, the output layer of the model should be optimized separately from the preceding layers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                912. <h><a href=https://openreview.net/pdf?id=9ZpciCOunFb>Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, Nima Dehmamy</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9ZpciCOunFb>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a framework for finding linear and nonlinear continuous symmetries in deep learning and show how they lead to extended local minima and conserved quantities
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9ZpciCOunFb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part of the global minimum. By relating conserved quantities to convergence rate and sharpness of the minimum, we provide insights on how initialization impacts convergence and generalizability.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                913. <h><a href=https://openreview.net/pdf?id=kKF8_K-mBbS>DiffDock: Diffusion Steps, Twists, and Turns for Molecular Docking</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gabriele Corso, Hannes Stärk, Bowen Jing, Regina Barzilay, Tommi S. Jaakkola</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kKF8_K-mBbS>6.5 - [5, 10, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Molecular docking via non-Euclidean diffusion modeling and confidence estimation
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kKF8_K-mBbS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Predicting the binding structure of a small molecule ligand to a protein---a task known as molecular docking---is critical to drug design. Recent deep learning methods that treat docking as a regression problem have decreased runtime compared to traditional search-based methods but have yet to offer substantial improvements in accuracy. We instead frame molecular docking as a generative modeling problem and develop DiffDock, a diffusion generative model over the non-Euclidean manifold of ligand poses. To do so, we map this manifold to the product space of the degrees of freedom (translational, rotational, and torsional) involved in docking and develop an efficient diffusion process on this space. Empirically, DiffDock obtains a 38% top-1 success rate (RMSD&lt;2A) on PDBBind, significantly outperforming the previous state-of-the-art of traditional docking (23%) and deep learning (20%) methods. Moreover, while previous methods are not able to dock on computationally folded structures (maximum accuracy 10.4%), DiffDock maintains significantly higher precision (21.7%). Finally, DiffDock has fast inference times and provides confidence estimates with high selective accuracy. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                914. <h><a href=https://openreview.net/pdf?id=L8iZdgeKmI6>Deja Vu: Continual Model Generalization for Unseen Domains</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenxi Liu, Lixu Wang, Lingjuan Lyu, Chen Sun, Xiao Wang, Qi Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=L8iZdgeKmI6>6.5 - [8, 5, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> To tackle continual domain shift in real-world applications, this work proposes a novel framework for achieving target domain generalization, target domain adaptation, and forgetting compensation at the same time.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/L8iZdgeKmI6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In real-world applications, deep learning models often run in non-stationary environments where the target data distribution continually shifts over time. There have been numerous domain adaptation (DA) methods in both online and offline modes to improve cross-domain adaptation ability. However, these DA methods typically only provide good performance after a long period of adaptation, and perform poorly on new domains before and during adaptation – in what we call the “Unfamiliar Period”, especially when domain shifts happen suddenly and significantly. On the other hand, domain generalization (DG) methods have been proposed to improve the model generalization ability on unadapted domains. However, existing DG works are ineffective for continually changing domains due to severe catastrophic forgetting of learned knowledge. To overcome these limitations of DA and DG in handling the Unfamiliar Period during continual domain shift, we propose RaTP, a framework that focuses on improving models’ target domain generalization (TDG) capability, while also achieving effective target domain adaptation (TDA) capability right after training on certain domains and forgetting alleviation (FA) capability on past domains. RaTP includes a training-free data augmentation module to prepare data for TDG, a novel pseudo-labeling mechanism to provide reliable supervision for TDA, and a prototype contrastive alignment algorithm to align different domains for achieving TDG, TDA and FA. Extensive experiments on Digits, PACS, and DomainNet demonstrate that RaTP significantly outperforms state-of-the-art works from Continual DA, Source-Free DA, Test-Time/Online DA, Single DG, Multiple DG and Unified DA&amp;DG in TDG, and achieves comparable TDA and FA capabilities.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                915. <h><a href=https://openreview.net/pdf?id=i2_TvOFmEml>MultiViz: Towards Visualizing and Understanding Multimodal Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, Ruslan Salakhutdinov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=i2_TvOFmEml>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> MultiViz is a framework for visualizing &amp; understanding multimodal models across unimodal importance, cross-modal interactions, multimodal representations &amp; multimodal prediction that enables model understanding, error analysis &amp; model debugging.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/i2_TvOFmEml.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                916. <h><a href=https://openreview.net/pdf?id=6TxBxqNME1Y>Diffusion Probabilistic Modeling of Protein Backbones in 3D for the motif-scaffolding problem</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Brian L. Trippe, Jason Yim, Doug Tischer, David Baker, Tamara Broderick, Regina Barzilay, Tommi S. Jaakkola</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6TxBxqNME1Y>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We have created the first generative modeling approach to motif-scaffolding by developing a diffusion probabilistic model of protein backbones and a procedure for generating scaffolds conditional on a motif.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6TxBxqNME1Y.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Construction of a scaffold structure that supports a desired motif, conferring protein function, shows promise for the design of vaccines and enzymes. But a general solution to this motif-scaffolding problem remains open. Current machine-learning techniques for scaffold design are either limited to unrealistically small scaffolds (up to length 20) or struggle to produce multiple diverse scaffolds. We propose to learn a distribution over diverse and longer protein backbone structures via an E(3)-equivariant graph neural network. We develop SMCDiff to efficiently sample scaffolds from this distribution conditioned on a given motif; our algorithm is the first to theoretically guarantee conditional samples from a diffusion model in the large-compute limit. We evaluate our designed backbones by how well they align with AlphaFold2-predicted structures. We show that our method can (1) sample scaffolds up to 80 residues and (2) achieve structurally diverse scaffolds for a fixed motif.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                917. <h><a href=https://openreview.net/pdf?id=6P9Y25Pljl6>FedDAR: Federated Domain-Aware Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aoxiao Zhong, Hao He, Zhaolin Ren, Na Li, Quanzheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6P9Y25Pljl6>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6P9Y25Pljl6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Cross-silo Federated learning (FL) has become a promising tool in machine learning applications for healthcare. It allows hospitals/institutions to train models with sufficient data while the data is kept private. To make sure the FL model is robust when facing heterogeneous data among FL clients, most efforts focus on personalizing models for clients. However, the latent relationships between clients&#39; data are ignored. In this work, we focus on a special non-iid FL problem, called Domain-mixed FL, where each client&#39;s data distribution is assumed to be a mixture of several predefined domains.  Recognizing the diversity of domains and the similarity within domains, we propose a novel method, FedDAR, which learns a domain shared representation and domain-wise personalized prediction heads in a decoupled manner. For simplified linear regression settings, we have theoretically proved that FedDAR enjoys a linear convergence rate.  For general settings, we have performed intensive empirical studies on both synthetic and real-world medical datasets which demonstrate its superiority over prior FL methods. Our code is available at https://github.com/zlz0414/FedDAR.     
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                918. <h><a href=https://openreview.net/pdf?id=vOEXS39nOF>Phenaki: Variable Length Video Generation from Open Domain Textual Descriptions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, Dumitru Erhan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vOEXS39nOF>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Generating videos from text, with prompts that can change over time, and videos that can be as long as multiple minutes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vOEXS39nOF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present Phenaki, a model capable of realistic video synthesis given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new causal model for learning video representation which compresses the video to a small discrete tokens representation. This tokenizer is auto-regressive in time, which allows it to work with video representations of different length. 
To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                919. <h><a href=https://openreview.net/pdf?id=yyBis80iUuU>Hybrid RL: Using both offline and online data can make RL efficient</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, Wen Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yyBis80iUuU>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new hybrid RL framework with access to both offline dataset and online interaction, and design a hybrid RL algorithm that is statistically and computationally efficient.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yyBis80iUuU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an offline dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure offline and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outperforms state-of-the-art online, offline, and hybrid RL baselines on challenging benchmarks, including Montezuma’s Revenge.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                920. <h><a href=https://openreview.net/pdf?id=B-z41MBL_tH>Causal Imitation Learning via Inverse Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kangrui Ruan, Junzhe Zhang, Xuan Di, Elias Bareinboim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=B-z41MBL_tH>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes novel inverse reinforcement learning methods to learn effective imitating policies from the expert&#39;s demonstrations when unobserved confounders are present.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/B-z41MBL_tH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    One of the most common ways children learn when unfamiliar with the environment is by mimicking adults. Imitation learning concerns an imitator learning to behave in an unknown environment from an expert&#39;s demonstration; reward signals remain latent to the imitator. This paper studies imitation learning through causal lenses and extends the analysis and tools developed for behavior cloning (Zhang, Kumor, Bareinboim, 2020) to inverse reinforcement learning. First, we propose novel graphical conditions that allow the imitator to learn a policy performing as well as the expert&#39;s behavior policy, even when the imitator and the expert&#39;s state-action space disagree, and unobserved confounders (UCs) are present. When provided with parametric knowledge about the unknown reward function, such a policy may outperform the expert&#39;s. Also, our method is easily extensible and allows one to leverage existing IRL algorithms even when UCs are present, including the multiplicative-weights algorithm (MWAL) (Syed &amp; Schapire, 2008) and the generative adversarial imitation learning (GAIL) (Ho &amp; Ermon, 2016). Finally, we validate our framework by simulations using real-world and synthetic data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                921. <h><a href=https://openreview.net/pdf?id=qU6NIcpaSi->Learning Heterogeneous Interaction Strengths by Trajectory Prediction with Graph Neural Network</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Seungwoong Ha, Hawoong Jeong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qU6NIcpaSi->6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a neural architecture that infers a continuous interaction graph rather than a conventional discrete one, solely from trajectories in an unsupervised manner.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qU6NIcpaSi-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dynamical systems with interacting agents are universal in nature, commonly modeled by a graph of relationships between their constituents. Recently, various works have been presented to tackle the problem of inferring those relationships from the system trajectories via deep neural networks, but most of the studies assume binary or discrete types of interactions for simplicity. In the real world, the interaction kernels often involve continuous interaction strengths, which cannot be accurately approximated by discrete relations. In this work, we propose the relational attentive inference network (RAIN) to infer continuously weighted interaction graphs without any ground-truth interaction strengths. Our model employs a novel pairwise attention (PA) mechanism to refine the trajectory representations and a graph transformer to extract heterogeneous interaction weights for each pair of agents. We show that our RAIN model with the PA mechanism accurately infers continuous interaction strengths for simulated physical systems in an unsupervised manner. Further, RAIN with PA successfully predicts trajectories from motion capture data with an interpretable interaction graph, demonstrating the virtue of modeling unknown dynamics with continuous weights.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                922. <h><a href=https://openreview.net/pdf?id=Nayau9fwXU>Diffusion-based Image Translation using disentangled style and content representation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gihyun Kwon, Jong Chul Ye</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Nayau9fwXU>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new method which enables image translation using Denoising Diffusion Probabilistic Model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Nayau9fwXU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion-based image translation guided by  semantic texts   or a single target image   has enabled flexible style transfer which is not limited to the specific domains. 
Unfortunately, due to the stochastic nature of diffusion models, it is often  difficult to maintain the original content of the image  during the reverse diffusion.
To address this, here we present a novel diffusion-based unsupervised image translation method, dubbed as DiffuseIT, using disentangled style and content representation.
 Specifically, inspired by the  slicing Vision Transformer, we extract intermediate keys of multihead self attention layer  from ViT model and used them as the content preservation loss. Then, an image guided style transfer is performed by matching the [CLS] classification token from the denoised samples and target image, whereas additional CLIP loss is used for the text-driven style transfer.
  To further accelerate the semantic change during the reverse  diffusion, we also propose a novel semantic divergence loss and resampling strategy. 
 Our experimental results show that the proposed method outperforms state-of-the-art baseline models in both text-guided and image-guided translation tasks. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                923. <h><a href=https://openreview.net/pdf?id=aCQt_BrkSjC>Learning Hyper Label Model for Programmatic Weak Supervision</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Renzhi Wu, Shen-En Chen, Jieyu Zhang, Xu Chu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aCQt_BrkSjC>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A hyper label model to aggregate weak labels from multiple weak supervision sources to infer the ground-truth labels in a single forward pass
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aCQt_BrkSjC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    To reduce the human annotation efforts, the programmatic weak supervision (PWS) paradigm abstracts weak supervision sources as labeling functions (LFs) and involves a label model to aggregate the output of multiple LFs to produce training labels. Most existing label models require a parameter learning step for each dataset. In this work, we present a hyper label model that (once learned) infers the ground-truth labels for each dataset in a single forward pass without dataset-specific parameter learning. The hyper label model approximates an optimal analytical (yet computationally intractable) solution of the ground-truth labels. We train the model on synthetic data generated in the way that ensures the model approximates the analytical optimal solution, and build the model upon Graph Neural Network (GNN) to ensure the model prediction being invariant (or equivariant) to the permutation of LFs (or data points). On 14 real-world datasets, our hyper label model outperforms the best existing methods in both accuracy (by 1.4 points on average) and efficiency (by six times on average). Our code is available at https://github.com/wurenzhi/hyper_label_model
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                924. <h><a href=https://openreview.net/pdf?id=U9yFP90jU0>FedFA:  Federated Feature Augmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianfei Zhou, Ender Konukoglu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=U9yFP90jU0>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple, flexible, effective, and robust method, named as FedFA, to solve federated learning from a novel perspective of federated feature augmentation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/U9yFP90jU0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning is a distributed paradigm that allows multiple parties to collaboratively train deep models without exchanging the raw data. However, the data distribution among clients is naturally non-i.i.d., which leads to severe degradation of the learnt model. The primary goal of this paper is to develop a robust federated learning algorithm to address feature shift in clients’ samples, which can be caused by various factors, e.g., acquisition differences in medical imaging. To reach this goal, we propose FedFA to tackle federated learning from a dis- tinct perspective of federated feature augmentation. FedFA is based on a major insight that each client’s data distribution can be characterized by statistics (i.e., mean and standard deviation) of latent features; and it is likely to manipulate these local statistics globally, i.e., based on information in the entire federation, to let clients have a better sense of the underlying distribution and therefore alleviate local data bias. Based on this insight, we propose to augment each local feature statistic probabilistically based on a normal distribution, whose mean is the original statistic and variance quantifies the augmentation scope. Key to our approach is the determination of a meaningful Gaussian variance, which is accomplished by taking into account not only biased data of each individual client, but also underlying feature statistics characterized by all participating clients. We offer both theoretical and empirical justifications to verify the effectiveness of FedFA. Our code is available at https://github.com/tfzhou/FedFA.
	
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                925. <h><a href=https://openreview.net/pdf?id=wNUgn1n6esQ>Safe Exploration Incurs Nearly No Additional Sample Complexity for Reward-Free RL</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruiquan Huang, Jing Yang, Yingbin Liang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wNUgn1n6esQ>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We developed a novel reward-free RL framework with safety constraint, and provide a unified provably efficient algorithm.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wNUgn1n6esQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reward-free reinforcement learning (RF-RL), a recently introduced RL paradigm, relies on random action-taking to explore the unknown environment without any reward feedback information. While the primary goal of the exploration phase in RF-RL is to reduce the uncertainty in the estimated model with minimum number of trajectories, in practice, the agent often needs to abide by certain safety constraint at the same time. It remains unclear how such safe exploration requirement would affect the corresponding sample complexity in order to achieve the desired optimality of the obtained policy in planning. In this work, we make a first attempt to answer this question. In particular, we consider the scenario where a safe baseline policy is known beforehand, and propose a unified Safe reWard-frEe ExploraTion (SWEET) framework. We then particularize the SWEET framework to the tabular and the low-rank MDP settings, and develop algorithms coined Tabular-SWEET and Low-rank-SWEET, respectively. Both algorithms leverage the concavity and continuity of the newly introduced truncated value functions, and are guaranteed to achieve zero constraint violation during exploration with high probability. Furthermore, both algorithms can provably find a near-optimal policy subject to any constraint in the planning phase. Remarkably, the sample complexities under both algorithms match or even outperform the state of the art in their constraint-free counterparts up to some constant factors, proving that safety constraint hardly increases the sample complexity for RF-RL.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                926. <h><a href=https://openreview.net/pdf?id=j8IiQUM33s>Task-customized Masked Autoencoder via Mixture of Cluster-conditional Experts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhili LIU, Kai Chen, Jianhua Han, Lanqing HONG, Hang Xu, Zhenguo Li, James Kwok</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=j8IiQUM33s>6.5 - [8, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/j8IiQUM33s.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Masked Autoencoder (MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE’s scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\% on average. It also obtains new state-of-the-art self-supervised learning results on detection and segmentation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                927. <h><a href=https://openreview.net/pdf?id=3DIpIf3wQMC>Spatial Attention Kinetic Networks with E(n)-Equivariance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuanqing Wang, John Chodera</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3DIpIf3wQMC>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Equivariant functional form termed spatial attention uses neurally parametrized linear combinations of edge vectors to equivariantly yet describe node environments 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3DIpIf3wQMC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks that are equivariant to rotations, translations, reflections, and permutations on $n$-dimensional geometric space have shown promise in physical modeling for tasks such as accurately but inexpensively modeling complex potential energy surfaces to guiding the sampling of complex dynamical systems or forecasting their time evolution.
Current state-of-the-art methods employ spherical harmonics to encode higher-order interactions among particles, which are computationally expensive.
In this paper, we propose a simple alternative functional form that uses neurally parametrized linear combinations of edge vectors to achieve equivariance while still universally approximating node environments.
Incorporating this insight, we design \emph{spatial attention kinetic networks} with E(n)-equivariance, or SAKE, which are competitive in many-body system modeling tasks while being significantly faster.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                928. <h><a href=https://openreview.net/pdf?id=3Bh6sRPKS3J>Hyperbolic Self-paced Learning for Self-supervised Skeleton-based Action Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Luca Franco, Paolo Mandica, Bharti Munjal, Fabio Galasso</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3Bh6sRPKS3J>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3Bh6sRPKS3J.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training.
We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeletonbased action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the established Euclidean SSL framework counterparts.
When tested on three established skeleton-based action recognition datasets, HYSP outperforms the state-of-the-art on PKU-MMD I, as well as on 2 out of 3 downstream tasks on NTU-60 and NTU-120. Additionally, HYSP only uses positive pairs and bypasses therefore the complex and computationally-demanding mining procedures required for the negatives in contrastive techniques.
Code is available at https://github.com/paolomandica/HYSP.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                929. <h><a href=https://openreview.net/pdf?id=vDFA1tpuLvk>Retrieval-based Controllable Molecule Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, Anima Anandkumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vDFA1tpuLvk>6.5 - [6, 6, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a first-of-its-kind retrieval-based framework for controllable molecule generation which can effectively extrapolate beyond the retrieval database and achieves state-of-the-art performance on various benchmarks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vDFA1tpuLvk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generating new molecules with specified chemical and biological properties via generative models has emerged as a promising direction for drug discovery. However, existing methods require extensive training/fine-tuning with a large dataset, often unavailable in real-world generation tasks. In this work, we propose a new retrieval-based framework for controllable molecule generation. We use a small set of exemplar molecules,  i.e., those that (partially) satisfy the design criteria, to steer the pre-trained generative model towards synthesizing molecules that satisfy the given design criteria. We design a retrieval mechanism that retrieves and fuses the exemplar molecules with the input molecule, which is trained by a new self-supervised objective that predicts the nearest neighbor of the input molecule. We also propose an iterative refinement process to dynamically update the generated molecules and retrieval database for better generalization. Our approach is agnostic to the choice of generative models and requires no task-specific fine-tuning. On various tasks ranging from simple design criteria to a challenging real-world scenario for designing lead compounds that bind to the SARS-CoV-2 main protease, we demonstrate our approach extrapolates well beyond the retrieval database, and achieves better performance and wider applicability than previous methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                930. <h><a href=https://openreview.net/pdf?id=H7M_5K5qKJV>Progressive Mix-Up for Few-Shot Supervised Multi-Source Domain Transfer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ronghang Zhu, Ronghang Zhu, Xiang Yu, Sheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=H7M_5K5qKJV>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/H7M_5K5qKJV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper targets at a new and challenging setting of knowledge transfer from multiple source domains to a single target domain, where target data is few shot or even one shot with label. Traditional domain generalization or adaptation methods cannot directly work since there is no sufficient target domain distribution serving as the transfer object. The multi-source setting further prevents the transfer task as excessive domain gap introduced from all the source domains. To tackle this problem, we newly propose a progressive mix-up (P-Mixup) mechanism to introduce an intermediate mix-up domain, pushing both the source domains and the few-shot target domain aligned to this mix-up domain. Further by enforcing the mix-up domain to progressively move towards the source domains, we achieve the domain transfer from multi-source domains to the single one-shot target domain. Our P-Mixup is different from traditional mix-up that ours is with a progressive and adaptive mix-up ratio, following the curriculum learning spirit to better align the source and target domains. Moreover, our P-Mixup combines both pixel-level and feature-level mix-up to better enrich the data diversity. Experiments on two benchmarks show that our P-Mixup significantly outperforms the state-of-the-art methods, i.e., 6.0\% and 6.8\% improvements on Office-Home and DomainNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                931. <h><a href=https://openreview.net/pdf?id=JdTnc9gjVfJ>MoDem: Accelerating Visual Model-Based Reinforcement Learning with Demonstrations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nicklas Hansen, Yixin Lin, Hao Su, Xiaolong Wang, Vikash Kumar, Aravind Rajeswaran</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JdTnc9gjVfJ>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL, but requires three key ingredients: policy pretraining, targeted exploration, and oversampling of demonstration data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JdTnc9gjVfJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Poor sample efficiency continues to be the primary challenge for deployment of deep Reinforcement Learning (RL) algorithms for real-world applications, and in particular for visuo-motor control. Model-based RL has the potential to be highly sample efficient by concurrently learning a world model and using synthetic rollouts for planning and policy improvement. However, in practice, sample-efficient learning with model-based RL is bottlenecked by the exploration challenge. In this work, we find that leveraging just a handful of demonstrations can dramatically improve the sample-efficiency of model-based RL. Simply appending demonstrations to the interaction dataset, however, does not suffice. We identify key ingredients for leveraging demonstrations in model learning -- policy pretraining, targeted exploration, and oversampling of demonstration data -- which forms the three phases of our model-based RL framework. We empirically study three complex visuo-motor control domains and find that our method is 160%-250% more successful in completing sparse reward tasks compared to prior approaches in the low data regime (100k interaction steps, 5 demonstrations). Code and videos are available at https://nicklashansen.github.io/modemrl.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                932. <h><a href=https://openreview.net/pdf?id=_eTZBs-yedr>CrAM: A Compression-Aware Minimizer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexandra Peste, Adrian Vladu, Eldar Kurtic, Christoph H Lampert, Dan Alistarh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_eTZBs-yedr>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a method for training accurate models that are robust to compression in a single step. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_eTZBs-yedr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks (DNNs) often have to be compressed, via pruning and/or quantization, before they can be deployed in practical settings. In this work we propose a new compression-aware minimizer dubbed CrAM that modifies the optimization step in a principled way, in order to produce models whose local loss behavior is stable under compression operations such as pruning. Thus, dense models trained via CrAM should be compressible post-training, in a single step, without significant accuracy loss. Experimental results on standard benchmarks, such as residual networks for ImageNet classification and BERT models for language modelling, show that CrAM produces dense models that can be more accurate than the standard SGD/Adam-based baselines, but which are stable under weight pruning: specifically, we can prune models in one-shot to 70-80% sparsity with almost no accuracy loss, and to 90% with reasonable (∼ 1%) accuracy loss, which is competitive with gradual compression methods. Additionally, CrAM can produce sparse models which perform well for transfer learning, and it also works for semi-structured 2:4 pruning patterns supported by GPU hardware. The code for reproducing the results is available at: https://github.com/IST-DASLab/CrAM .

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                933. <h><a href=https://openreview.net/pdf?id=6axIMJA7ME3>Compositional Task Representations for Large Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>NAN SHAO, Zefan Cai, Hanwei xu, Chonghua Liao, Yanan Zheng, Zhilin Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6axIMJA7ME3>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6axIMJA7ME3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models have shown a remarkable cross-task generalization ability. Most prior work assumed that prompts effectively extract knowledge from language models to facilitate generalization to new tasks. This perspective led to numerous studies on improving prompts. In contrast, we introduce a new perspective, compositional generalization, that views each task as a composition of latent codes and generalizes to test tasks by a new composition of seen codes. To this end, we propose a novel prompt-free approach, Compositional Task Representations (CTR), that employs multi-task training to learn a discrete, compositional codebook. Empirically, our CTR substantially outperforms prompt-based methods in zero-label learning on average. According to our analysis, some of the learned CTR codes are interpretable to human and demonstrate a certain degree of controllability.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                934. <h><a href=https://openreview.net/pdf?id=YfUICnZMwk7>Weighted Clock Logic Point Process</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruixuan Yan, Yunshi Wen, Debarun Bhattacharjya, Ronny Luss, Tengfei Ma, Achille Fokoue, Anak Agung Julius</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YfUICnZMwk7>6.5 - [5, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel neuro-symbolic framework for modeling temporal point processes with interpretability and high computation efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YfUICnZMwk7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Datasets involving multivariate event streams are prevalent in numerous applications. We present a novel framework for modeling temporal point processes called clock logic neural networks (CLNN) which learn weighted clock logic (wCL) formulas as interpretable temporal rules by which some events promote or inhibit other events. Specifically, CLNN models temporal relations between events using conditional intensity rates informed by a set of wCL formulas, which are more expressive than related prior work. Unlike conventional approaches of searching for generative rules through expensive combinatorial optimization, we design smooth activation functions for components of wCL formulas that enable a continuous relaxation of the discrete search space and efficient learning of wCL formulas using gradient-based methods. Experiments on synthetic datasets manifest our model&#39;s ability to recover the ground-truth rules and improve computational efficiency. In addition, experiments on real-world datasets show that our models perform competitively when compared with state-of-the-art models. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                935. <h><a href=https://openreview.net/pdf?id=7bJizxLKrR>Measuring Forgetting of Memorized Training Examples</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini, Eric Wallace, Shuang Song, Abhradeep Guha Thakurta, Nicolas Papernot, Chiyuan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7bJizxLKrR>6.5 - [6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> When models are trained on large datasets, we show that privacy attacks become less effective on examples seen early in training, and investigate why.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7bJizxLKrR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning models exhibit two seemingly contradictory phenomena: training data memorization and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena.
We propose a technique to measure to what extent models ``forget&#39;&#39; the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently.
We show that, while non-convexity can prevent forgetting from happening in the worst-case, standard image,speech, and language models empirically do forget examples over time.
We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget.
Our results suggest that examples seen early when training with extremely large datasets---for instance those examples used to pre-train a model---may observe privacy benefits at the expense of examples seen later.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                936. <h><a href=https://openreview.net/pdf?id=dL35lx-mTEs>Is Forgetting Less a Good Inductive Bias for Forward Transfer?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiefeng Chen, Timothy Nguyen, Dilan Gorur, Arslan Chaudhry</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dL35lx-mTEs>6.5 - [6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Study if forgetting less is a good inductive bias for forward transfer. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dL35lx-mTEs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    One of the main motivations of studying continual learning is that the problem setting allows a model to accrue knowledge from past tasks to learn new tasks more efficiently. However, recent studies suggest that the key metric that continual learning algorithms optimize, reduction in catastrophic forgetting, does not correlate well with the forward transfer of knowledge. We believe that the conclusion previous works reached is due to the way they measure forward transfer. We argue that the measure of forward transfer to a task should not be affected by the restrictions placed on the continual learner in order to preserve knowledge of previous tasks. Instead, forward transfer should be measured by how easy it is to learn a new task given a set of representations produced by continual learning on previous tasks. Under this notion of forward transfer, we evaluate different continual learning algorithms on a variety of image classification benchmarks. Our results indicate that less forgetful representations lead to a better forward transfer suggesting a strong correlation between retaining past information and learning efficiency on new tasks. Further, we found less forgetful representations to be more diverse and discriminative compared to their forgetful counterparts. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                937. <h><a href=https://openreview.net/pdf?id=F91SROvVJ_6>Causal Balancing for Domain Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xinyi Wang, Michael Saxon, Jiachen Li, Hongyang Zhang, Kun Zhang, William Yang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=F91SROvVJ_6>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a balanced mini-batch sampling strategy to reduce spurious correlations for domain generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/F91SROvVJ_6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While machine learning models rapidly advance the state-of-the-art on various real-world tasks, out-of-domain (OOD) generalization remains a challenging problem given the vulnerability of these models to spurious correlations. We propose a balanced mini-batch sampling strategy to transform a biased data distribution into a spurious-free balanced distribution, based on the invariance of the underlying causal mechanisms for the data generation process. We argue that the Bayes optimal classifiers trained on such balanced distribution are minimax optimal across a diverse enough environment space. We also provide an identifiability guarantee of the latent variable model of the proposed data generation process, when utilizing enough train environments. Experiments are conducted on DomainBed, demonstrating empirically that our method obtains the best performance across 20 baselines reported on the benchmark.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                938. <h><a href=https://openreview.net/pdf?id=FlCg47MNvBA>Label-free Concept Bottleneck Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tuomas Oikarinen, Subhro Das, Lam M. Nguyen, Tsui-Wei Weng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FlCg47MNvBA>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Scalable, automated and efficient way to create Concept Bottleneck Models without labeled concept data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FlCg47MNvBA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Concept bottleneck models (CBM) are a popular way of creating more interpretable neural networks by having hidden layer neurons correspond to human-understandable concepts. However, existing CBMs and their variants have two crucial limitations:  first, they need to collect labeled data for each of the predefined concepts, which is time consuming and labor intensive; second, the accuracy of a CBM is often significantly lower than that of a standard neural network, especially on more complex datasets. This poor performance creates a barrier for adopting CBMs in practical real world applications. Motivated by these challenges, we propose Label-free CBM which is a novel framework to transform any neural network into an interpretable CBM without labeled concept data, while retaining a high accuracy. Our Label-free CBM has many advantages, it is: scalable - we present the first CBM scaled to ImageNet, efficient - creating a CBM takes only a few hours even for very large datasets, and automated - training it for a new dataset requires minimal human effort. Our code is available at https://github.com/Trustworthy-ML-Lab/Label-free-CBM.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                939. <h><a href=https://openreview.net/pdf?id=pOnhudsvzR>Everybody Needs Good Neighbours: An Unsupervised Locality-based Method for Bias Mitigation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xudong Han, Timothy Baldwin, Trevor Cohn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pOnhudsvzR>6.5 - [6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pOnhudsvzR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning models from human behavioural data often leads to outputs that are biased with respect to user demographics, such as gender or race. This effect can be controlled by explicit mitigation methods, but this typically presupposes access to demographically-labelled training data. Such data is often not available, motivating the need for unsupervised debiasing methods. To this end, we propose a new meta-algorithm for debiasing representation learning models, which combines the notions of data locality and accuracy of model fit, such that a supervised debiasing method can optimise fairness between neighbourhoods of poorly vs. well modelled instances as identified by our method. Results over five datasets, spanning natural language processing and structured data classification tasks, show that our technique recovers proxy labels that correlate with unknown demographic data, and that our method outperforms all unsupervised baselines, while also achieving competitive performance with state-of-the-art supervised methods which are given access to demographic labels.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                940. <h><a href=https://openreview.net/pdf?id=kJWcI39kXY>ODAM: Gradient-based Instance-Specific Visual Explanations for Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenyang ZHAO, Antoni B. Chan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kJWcI39kXY>6.5 - [8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> ODAM: a gradient-based instance-specific explanation technique for object detectors; ODAM-Train: improve the explanation ability on object discrimination; ODAM-NMS: distinguish the duplicate detected objects with the help of ODAM.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kJWcI39kXY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose the Gradient-weighted Object Detector Activation Mapping (Grad-ODAM), a visualized explanation technique for interpreting the predictions of object detectors. Utilizing the gradients of detector targets flowing into the intermediate feature maps, Grad-ODAM produces heat maps that show the influence of regions on the detector&#39;s decision. Compared to previous classification activation mapping works, Grad-ODAM generates instance-specific explanations rather than class-specific ones. We show that Grad-ODAM is applicable to both one-stage detectors such as FCOS and two-stage detectors such as Faster R-CNN, and produces higher-quality visual explanations than the state-of-the-art both effectively and efficiently. We next propose a training scheme, ODAM-Train, to improve the explanation ability on object discrimination of the detector through encouraging consistency between explanations for detections on the same object, and distinct explanations for detections on different objects. Based on the heat maps produced by Grad-ODAM with ODAM-Train, we propose ODAM-NMS, which considers the information of the model&#39;s explanation for each prediction to distinguish the duplicate detected objects. We present a detailed analysis of the visualized explanations of detectors and carry out extensive experiments to validate the effectiveness of the proposed ODAM.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                941. <h><a href=https://openreview.net/pdf?id=Dvs-a3aymPe>Searching Lottery Tickets in Graph Neural Networks: A Dual Perspective</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kun Wang, Yuxuan Liang, Pengkun Wang, Xu Wang, Pengfei Gu, Junfeng Fang, Yang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Dvs-a3aymPe>6.4 - [6, 6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper generalizes Dual Lottery Ticket Hypothesis (DLTH) to the graph to address information loss and aggregation failure issues caused by sampling-based GNN pruning algorithms
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Dvs-a3aymPe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph Neural Networks (GNNs) have shown great promise in various graph learning tasks. However, the computational overheads of fitting GNNs to large-scale graphs grow rapidly, posing obstacles to GNNs from scaling up to real-world applications. To tackle this issue, Graph Lottery Ticket (GLT) hypothesis articulates that there always exists a sparse subnetwork/subgraph with admirable performance in GNNs with random initialization. Such a pair of core subgraph and sparse subnetwork (called graph lottery tickets) can be uncovered by iteratively applying a novel sparsification method. While GLT provides new insights for GNN compression, it requires a full pretraining process to obtain graph lottery tickets, which is not universal and friendly to real-world applications. Moreover, the graph sparsification in GLT utilizes sampling techniques, which may result in massive information loss and aggregation failure. In this paper, we explore the searching of graph lottery tickets from a complementary perspective -- transforming a random ticket into a graph lottery ticket, which allows us to more comprehensively explore the relationships between the original network/graph and their sparse counterpart. To achieve this, we propose regularization-based network pruning and hierarchical graph sparsification, leading to our Dual Graph Lottery Ticket (DGLT) framework for a joint sparsification of network and graph. Compared to GLT, our DGLT helps achieve a triple-win situation of graph lottery tickets with high sparsity, admirable performance, and good explainability. More importantly, we rigorously prove that our model can eliminate noise and maintain reliable information in substructures using the graph information bottleneck theory. Extensive experimental results on various graph-related tasks validate the effectiveness of our framework.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                942. <h><a href=https://openreview.net/pdf?id=pfuqQQCB34>Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eduard Gorbunov, Samuel Horváth, Peter Richtárik, Gauthier Gidel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pfuqQQCB34>6.4 - [8, 6, 5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new Byzantine-tolerant method with variance reduction, communication compression, and theoretical guarantees superior to previously known results
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pfuqQQCB34.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning. However, many fruitful directions, such as the usage of variance reduction for achieving robustness and communication compression for reducing communication costs, remain weakly explored in the field. This work addresses this gap and proposes Byz-VR-MARINA -- a new Byzantine-tolerant method with variance reduction and compression. A key message of our paper is that variance reduction is key to fighting Byzantine workers more effectively. At the same time, communication compression is a bonus that makes the process more communication efficient. We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-Lojasiewicz loss functions. Unlike the concurrent Byzantine-robust methods with variance reduction and/or compression, our complexity results are tight and do not rely on restrictive assumptions such as boundedness of the gradients or limited compression. Moreover, we provide the first analysis of a Byzantine-tolerant method supporting non-uniform sampling of stochastic gradients. Numerical experiments corroborate our theoretical findings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                943. <h><a href=https://openreview.net/pdf?id=RgUPdudkWlN>CUDA: Curriculum of Data Augmentation for Long-tailed Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sumyeong Ahn, Jongwoo Ko, Se-Young Yun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RgUPdudkWlN>6.4 - [6, 6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a class-wise data augmentation method by designing the curriculum of data augmentation, which is based on our findings that stronger augmentation on major classes improves the performance on long-tailed recognition.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RgUPdudkWlN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Class imbalance problems frequently occur in real-world tasks, and conventional deep learning algorithms are well known for performance degradation on imbalanced training datasets. To mitigate this problem, many approaches have aimed to balance among given classes by re-weighting or re-sampling training samples. These re-balancing methods increase the impact of minority classes and reduce the influence of majority classes on the output of models. However, the extracted representations may be of poor quality owing to the limited number of minority samples. To handle this restriction, several methods have been developed that increase the representations of minority samples by leveraging the features of the majority samples. Despite extensive recent studies, no deep analysis has been conducted on determination of classes to be augmented and strength of augmentation has been conducted. In this study, we first investigate the correlation between the degree of augmentation and class-wise performance, and find that the proper degree of augmentation must be allocated for each class to mitigate class imbalance problems. Motivated by this finding, we propose a simple and efficient novel curriculum, which is designed to find the appropriate per-class strength of data augmentation, called CUDA: CUrriculum of Data Augmentation for long-tailed recognition. CUDA can simply be integrated into existing long-tailed recognition methods. We present the results of experiments showing that CUDA effectively achieves better generalization performance compared to the state-of-the-art method on various imbalanced datasets such as CIFAR-100-LT, ImageNet-LT, and iNaturalist 2018. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                944. <h><a href=https://openreview.net/pdf?id=65XDF_nwI61>A Graph Neural Network Approach to Automated Model Building in Cryo-EM Maps</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kiarash Jamali, Dari Kimanius, Sjors HW Scheres</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=65XDF_nwI61>6.4 - [5, 8, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Using graph neural networks to automatically build atomic models in cryo-EM maps
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/65XDF_nwI61.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Electron cryo-microscopy (cryo-EM) produces three-dimensional (3D) maps of the electrostatic potential of biological macromolecules, including proteins. At sufficient resolution, the cryo-EM maps, along with some knowledge about the imaged molecules, allow de novo atomic modelling. Typically, this is done through a laborious manual process. Recent advances in machine learning applications to protein structure prediction show potential for automating this process. Taking inspiration from these techniques, we have built ModelAngelo for automated model building of proteins in cryo-EM maps. ModelAngelo first uses a residual convolutional neural network (CNN) to initialize a graph representation with nodes assigned to individual amino acids of the proteins in the map and edges representing the protein chain. The graph is then refined with a graph neural network (GNN) that combines the cryo-EM data, the amino acid sequence data and prior knowledge about protein geometries. The GNN refines the geometry of the protein chain and classifies the amino acids for each of its nodes. The final graph is post-processed with a hidden Markov model (HMM) search to map each protein chain to entries in a user provided sequence file. Application to 28 test cases shows that ModelAngelo outperforms state-of-the-art and approximates manual building for cryo-EM maps with resolutions better than 3.5 A.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                945. <h><a href=https://openreview.net/pdf?id=4TyNEhI2GdN>TypeT5: Seq2seq Type Inference using Static Analysis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiayi Wei, Greg Durrett, Isil Dillig</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4TyNEhI2GdN>6.4 - [6, 6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Combining the strengths of CodeT5 and static analysis to predict Python type annotations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4TyNEhI2GdN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model.  We also propose an iterative decoding scheme that incorporates previous type predictions in the model&#39;s input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors---while enabling easy user intervention.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                946. <h><a href=https://openreview.net/pdf?id=s7oOe6cNRT8>M-L2O: Towards Generalizable Learning-to-Optimize by Test-Time Fast Self-Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junjie Yang, Xuxi Chen, Tianlong Chen, Zhangyang Wang, Yingbin Liang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=s7oOe6cNRT8>6.4 - [6, 8, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/s7oOe6cNRT8.jpg loading="lazy">
                </div>
                <div class="abstract">
                     Learning to Optimize (L2O) has drawn increasing attention as it often remarkably accelerates the optimization procedure of complex tasks by &#34;overfitting&#34; specific task type, leading to enhanced performance compared to analytical optimizers. Generally, L2O develops a parameterized optimization method (i.e., &#34;optimizer&#34;) by learning from solving sample problems. This data-driven procedure yields L2O that can efficiently solve problems similar to those seen in training, that is, drawn from the same &#34;task distribution&#34;. However, such learned optimizers often struggle when new test problems come with a substantially deviation from the training task distribution. This paper investigates a potential solution to this open challenge, by meta-training an L2O optimizer that can perform fast test-time self-adaptation to a out-of-distribution task, in only a few steps. We theoretically characterize the generalization of L2O, and further show that our proposed framework (termed as M-L2O) provably facilitates rapid task adaptation by locating well-adapted initial points for the optimizer weight. Empirical observations on several classic tasks like LASSO and Quadratic, demonstrate that M-L2O converges significantly faster than vanilla L2O with only $5$ steps of adaptation, echoing our theoretical results. Codes are available in https://github.com/VITA-Group/M-L2O.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                947. <h><a href=https://openreview.net/pdf?id=dnjZSPGmY5O>Equivariant Descriptor Fields: SE(3)-Equivariant Energy-Based Models for End-to-End Visual Robotic Manipulation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyunwoo Ryu, Hong-in Lee, Jeong-Hoon Lee, Jongeun Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dnjZSPGmY5O>6.4 - [8, 6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present SE(3)-equivariant energy based models that can learn robotic manipulation tasks end-to-end from only few demonstrations without any prior knowledge.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dnjZSPGmY5O.jpg loading="lazy">
                </div>
                <div class="abstract">
                    End-to-end learning for visual robotic manipulation is known to suffer from sample inefficiency, requiring large numbers of demonstrations. The spatial roto-translation equivariance, or the SE(3)-equivariance can be exploited to improve the sample efficiency for learning robotic manipulation. In this paper, we present SE(3)-equivariant models for visual robotic manipulation from point clouds that can be trained fully end-to-end. By utilizing the representation theory of the Lie group, we construct novel SE(3)-equivariant energy-based models that allow highly sample efficient end-to-end learning. We show that our models can learn from scratch without prior knowledge and yet are highly sample efficient (5~10 demonstrations are enough). Furthermore, we show that our models can generalize to tasks with (i) previously unseen target object poses, (ii) previously unseen target object instances of the category, and (iii) previously unseen visual distractors. We experiment with 6-DoF robotic manipulation tasks to validate our models&#39; sample efficiency and generalizability. Codes are available at: https://github.com/tomato1mule/edf
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                948. <h><a href=https://openreview.net/pdf?id=B_pCIsX8KL_>GRACE-C: Generalized Rate Agnostic Causal Estimation via Constraints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mohammadsajad Abavisani, David Danks, Sergey Plis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=B_pCIsX8KL_>6.4 - [6, 6, 8, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel method for causal structure discovery in undersampled time-series with three orders of magnitude speedup under the same theoretical guarantees.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/B_pCIsX8KL_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graphical structures estimated by causal learning algorithms from time series data can provide highly misleading causal information if the causal timescale of the generating process fails to match the measurement timescale of the data. Existing algorithms provide limited resources to respond to this challenge, and so researchers must either use models that they know are likely misleading, or else forego causal learning entirely. Existing methods face up-to-four distinct shortfalls, as they might a) require that the difference between causal and measurement timescales is known; b) only handle very small number of random variables when the timescale difference is unknown; c) only apply to pairs of variables (albeit with fewer assumptions about prior knowledge); or d) be unable to find a solution given statistical noise in the data. This paper aims to address these challenges. We present an approach that combines constraint programming with both theoretical insights into the problem structure and prior information about admissible causal interactions to achieve speed up of multiple orders of magnitude. The resulting system scales to significantly larger sets of random variables ($&gt;100$) without knowledge of the timescale difference while maintaining  theoretical guarantees. This method is also robust to edge misidentification and can use parametric connection strengths, while optionally finding the optimal among many possible solutions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                949. <h><a href=https://openreview.net/pdf?id=Fsd-6ax4T1m>Evaluating Representations with Readout Model Switching</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yazhe Li, Jorg Bornschein, Marcus Hutter</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Fsd-6ax4T1m>6.4 - [6, 6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an evaluation framework that is based on MDL and model switching for evaluating representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Fsd-6ax4T1m.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Although much of the success of Deep Learning builds on learning good representations, a rigorous method to evaluate their quality is lacking. In this paper, we treat the evaluation of representations as a model selection problem and propose to use the Minimum Description Length (MDL) principle to devise an evaluation metric. Contrary to the established practice of limiting the capacity of the readout model, we design a hybrid discrete and continuous-valued model space for the readout models and employ a switching strategy to combine their predictions. The MDL score takes model complexity, as well as data efficiency into account. As a result, the most appropriate model for the specific task and representation will be chosen, making it a unified measure for comparison. The proposed metric can be efficiently computed with an online method and we present results for pre-trained vision encoders of various architectures (ResNet and ViT) and objective functions (supervised and self-supervised) on a range of downstream tasks. We compare our methods with accuracy-based approaches and show that the latter are inconsistent when multiple readout models are used. Finally, we discuss important properties revealed by our evaluations such as model scaling, preferred readout model, and data efficiency.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                950. <h><a href=https://openreview.net/pdf?id=lcSfirnflpW>ManyDG: Many-domain Generalization for Healthcare Applications</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chaoqi Yang, M Brandon Westover, Jimeng Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lcSfirnflpW>6.4 - [3, 8, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> New &#34;many-domain generalization&#34; setting and new approach ManyDG for the setting in healthcare applications
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lcSfirnflpW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The vast amount of health data has been continuously collected for each patient, providing opportunities to support diverse healthcare predictive tasks such as seizure detection and hospitalization prediction. Existing models are mostly trained on other patients’ data and evaluated on new patients. Many of them might suffer from poor generalizability. One key reason can be overfitting due to the unique information related to patient identities and their data collection environments, referred to as patient covariates in the paper. These patient covariates usually do not contribute to predicting the targets but are often difficult to remove. As a result, they can bias the model training process and impede generalization. In healthcare applications, most existing domain generalization methods assume a small number of domains. In this paper, considering the diversity of patient covariates, we propose a new setting by treating each patient as a separate domain (leading to many domains). We develop a new domain generalization method ManyDG, that can scale to such many-domain problems. Our method identifies the patient do- main covariates by mutual reconstruction, and removes them via an orthogonal projection step. Extensive experiments show that ManyDG can boost the generalization performance on multiple real-world healthcare tasks (e.g., 3.7% Jaccard improvements on MIMIC drug recommendation) and support realistic but challenging settings such as insufficient data and continuous learning. The code is available at https://github.com/ycq091044/ManyDG. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                951. <h><a href=https://openreview.net/pdf?id=syfgJE6nFRW>PASHA: Efficient HPO and NAS with Progressive Resource Allocation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ondrej Bohdal, Lukas Balles, Martin Wistuba, Beyza Ermis, Cedric Archambeau, Giovanni Zappella</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=syfgJE6nFRW>6.4 - [6, 6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Efficient multi-fidelity method with progressive resource allocation for HPO and NAS.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/syfgJE6nFRW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Hyperparameter optimization (HPO) and neural architecture search (NAS) are methods of choice to obtain the best-in-class machine learning models, but in practice they can be costly to run. When models are trained on large datasets, tuning them with HPO or NAS rapidly becomes prohibitively expensive for practitioners, even when efficient multi-fidelity methods are employed. We propose an approach to tackle the challenge of tuning machine learning models trained on large datasets with limited computational resources. Our approach, named PASHA, extends ASHA and is able to dynamically allocate maximum resources for the tuning procedure depending on the need. The experimental comparison shows that PASHA identifies well-performing hyperparameter configurations and architectures while consuming significantly fewer computational resources than ASHA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                952. <h><a href=https://openreview.net/pdf?id=dQNL7Zsta3>Malign Overfitting: Interpolation and Invariance are Fundamentally at Odds</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yoav Wald, Gal Yona, Uri Shalit, Yair Carmon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dQNL7Zsta3>6.4 - [8, 5, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Proof that interpolating classifiers cannot satisfy common invariance and fairness criteria; Provides insight on empirical observations and possible effective solutions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dQNL7Zsta3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learned classifiers should often possess certain invariance properties meant to encourage fairness, robustness, or out-of-distribution generalization. 
However, multiple recent works empirically demonstrate that common invariance-inducing regularizers are ineffective in the over-parameterized regime, in which classifiers perfectly fit (i.e. interpolate) the training data. This suggests that the phenomenon of ``benign overfitting,&#34; in which models generalize well despite interpolating, might not favorably extend to settings in which robustness or fairness are desirable. 

In this work, we provide a theoretical justification for these observations. We prove that---even in the simplest of settings---any interpolating learning rule (with an arbitrarily small margin) will not satisfy these invariance properties. We then propose and analyze an algorithm that---in the same setting---successfully learns a non-interpolating classifier that is provably invariant. We validate our theoretical observations on simulated data and the Waterbirds dataset.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                953. <h><a href=https://openreview.net/pdf?id=MQcmfgRxf7a>Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Raj Ghugare, Homanga Bharadhwaj, Benjamin Eysenbach, Sergey Levine, Russ Salakhutdinov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MQcmfgRxf7a>6.4 - [6, 6, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a joint objective for latent space model based RL which lower bounds the RL objective. Maximising this bound jointly with the encoder, model and the policy matches the performance of SOTA methods, while being 6-10 times faster. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MQcmfgRxf7a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging.
Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear.
In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-free RL methods. While sample efficient methods typically are computationally demanding, our method attains the performance of SAC in about 50\% less wall-clock time.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                954. <h><a href=https://openreview.net/pdf?id=38m4h8HcNRL>Federated Neural Bandits</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhongxiang Dai, Yao Shu, Arun Verma, Flint Xiaofeng Fan, Bryan Kian Hsiang Low, Patrick Jaillet</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=38m4h8HcNRL>6.4 - [6, 6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce federated neural-UCB, which uses a weighted combination of two UCBs that respectively, (a) accelerates exploration using observations from other agents and (b) improves reward prediction using a neural network with aggregated parameters.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/38m4h8HcNRL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent works on neural contextual bandits have achieved compelling performances due to their ability to leverage the strong representation power of neural networks (NNs) for reward prediction. Many applications of contextual bandits involve multiple agents who collaborate without sharing raw observations, thus giving rise to the setting of federated contextual bandits}. Existing works on federated contextual bandits rely on linear or kernelized bandits, which may fall short when modeling complex real-world reward functions. So, this paper introduces the federated neural-upper confidence bound (FN-UCB) algorithm. To better exploit the federated setting, FN-UCB adopts a weighted combination of two UCBs: $\text{UCB}^{a}$ allows every agent to additionally use the observations from the other agents to accelerate exploration (without sharing raw observations), while $\text{UCB}^{b}$ uses an NN with aggregated parameters for reward prediction in a similar way to federated averaging for supervised learning. Notably, the weight between the two UCBs required by our theoretical analysis is amenable to an interesting interpretation, which emphasizes $\text{UCB}^{a}$ initially for accelerated exploration and relies more on $\text{UCB}^{b}$ later after enough observations have been collected to train the NNs for accurate reward prediction (i.e., reliable exploitation). We prove sub-linear upper bounds on both the cumulative regret and the number of communication rounds of FN-UCB, and empirically demonstrate its competitive performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                955. <h><a href=https://openreview.net/pdf?id=b4t9_XASt6G>On the Word Boundaries of Emergent Languages Based on Harris&#39;s Articulation Scheme</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ryo Ueda, Taiga Ishii, Yusuke Miyao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b4t9_XASt6G>6.4 - [8, 5, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper investigates whether Harris&#39;s articulation scheme (HAS) also holds in emergent languages.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b4t9_XASt6G.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper shows that emergent languages in signaling games lack meaningful word boundaries in terms of Harris&#39;s Articulation Scheme (HAS), a universal property of natural language. Emergent Languages are artificial communication protocols arising among agents. However, it is not obvious whether such a simulated language would have the same properties as natural language. In this paper, we test if they satisfy HAS. HAS states that word boundaries can be obtained solely from phonemes in natural language. We adopt HAS-based word segmentation and verify whether emergent languages have meaningful word segments. The experiment suggested they do not have, although they meet some preconditions for HAS. We discovered a gap between emergent and natural languages to be bridged, indicating that the standard signaling game satisfies prerequisites but is still missing some necessary ingredients.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                956. <h><a href=https://openreview.net/pdf?id=TAVBJ4aHsWt>SemPPL: Predicting Pseudo-Labels for Better Contrastive Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matko Bošnjak, Pierre Harvey Richemond, Nenad Tomasev, Florian Strub, Jacob C Walker, Felix Hill, Lars Holger Buesing, Razvan Pascanu, Charles Blundell, Jovana Mitrovic</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TAVBJ4aHsWt>6.4 - [6, 6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TAVBJ4aHsWt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning from large amounts of unsupervised data and a small amount of supervision is an important open problem in computer vision. We propose a new semi-supervised learning method, Semantic Positives via Pseudo-Labels (SEMPPL), that combines labelled and unlabelled data to learn informative representations. Our method extends self-supervised contrastive learning—where representations are shaped by distinguishing whether two samples represent the same underlying datum (positives) or not (negatives)—with a novel approach to selecting  positives. To enrich the set of positives, we leverage the few existing ground-truth labels to predict the missing  ones through a k-nearest neighbors classifier by using the learned embeddings of the labelled data. We thus extend the set of positives with datapoints having the same pseudo-label and call these semantic positives. We jointly learn the representation and predict bootstrapped pseudo-labels. This creates a reinforcing cycle. Strong initial representations enable better pseudo-label predictions which then improve the selection of semantic positives and lead to even better representations. SEMPPL outperforms competing semi-supervised methods setting new state-of-the-art performance of 76% and 68.5% top-1accuracy when using a ResNet-50 and training on 10% and 1% of labels on ImageNet, respectively. Furthermore, when using selective kernels, SEMPPL significantly outperforms previous state-of-the-art achieving 72.3% and 78.3% top-1accuracy on ImageNet with 1% and 10% labels, respectively, which improves absolute +7.8% and +6.2% over previous work. SEMPPL also exhibits state-of-the-art performance over larger ResNet models as well as strong robustness, out-of-distribution and transfer performance. We release the checkpoints and the evaluation code at https://github.com/deepmind/semppl.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                957. <h><a href=https://openreview.net/pdf?id=TJ2nxciYCk->The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J. Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, Sanjiv Kumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TJ2nxciYCk->6.4 - [6, 5, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Learned Transformers for NLP (e.g., T5) and Vision (e.g., ViT) tasks produce sparse representations in their MLP layers. The sparsity may be leveraged to improve robustness, calibration, and computational efficiency of Transformer models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TJ2nxciYCk-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper studies a curious phenomenon that machine learning model with Transformer architectures have sparse activation maps. By activation map we refer to the intermediate output of the multi-layer perceptrons (MLPs) after a ReLU activation function, and by &#34;sparse&#34; we mean that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero for each input to MLP. Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser as measured by the percentage of nonzero entries. Through extensive experiments we demonstrate that the emergence of sparsity is a prevalent phenomenon that occurs for both natural language processing and vision tasks, on both training and evaluation data, for Transformers of various configurations, at layers of all depth levels. We discuss how sparsity immediately implies a way to significantly reduce the FLOP count and improve efficiency for Transformers. Moreover, we demonstrate perhaps surprisingly that enforcing an even sparser activation via Top-k thresholding with a small k brings a collection of desired properties, namely less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                958. <h><a href=https://openreview.net/pdf?id=PDG4-Y3aboN>FIT: A Metric for Model Sensitivity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ben Zandonati, Adrian Alan Pol, Maurizio Pierini, Olya Sirkin, Tal Kopetz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PDG4-Y3aboN>6.4 - [8, 5, 3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the Fisher Information Trace (FIT) metric, to quantify the effects of mixed-precision quantization. FIT facilitates zero-shot performance prediction of quantized models, and is fast to compute.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PDG4-Y3aboN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Model compression is vital to the deployment of deep learning on edge devices. Low precision representations, achieved via quantization of weights and activations, can reduce inference time and memory requirements. However, quantifying and predicting the response of a model to the changes associated with this procedure remains challenging. This response is non-linear and heterogeneous throughout the network. Understanding which groups of parameters and activations are more sensitive to quantization than others is a critical stage in maximizing efficiency. For this purpose, we propose FIT. Motivated by an information geometric perspective, FIT combines the Fisher information with a model of quantization. We find that FIT can estimate the final performance of a network without retraining. FIT effectively fuses contributions from both parameter and activation quantization into a single metric. Additionally, FIT is fast to compute when compared to existing methods, demonstrating favourable convergence properties. These properties are validated experimentally across hundreds of quantization configurations, with a focus on layer-wise mixed-precision quantization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                959. <h><a href=https://openreview.net/pdf?id=PEuxUXIMLlA>Compositional Law Parsing with Latent Random Functions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fan Shi, Bin Li, Xiangyang Xue</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PEuxUXIMLlA>6.4 - [6, 6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PEuxUXIMLlA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human cognition has compositionality. We understand a scene by decomposing the scene into different concepts (e.g., shape and position of an object) and learning the respective laws of these concepts, which may be either natural (e.g., laws of motion) or man-made (e.g., laws of a game). The automatic parsing of these laws indicates the model&#39;s ability to understand the scene, which makes law parsing play a central role in many visual tasks. This paper proposes a deep latent variable model for Compositional LAw Parsing (CLAP), which achieves the human-like compositionality ability through an encoding-decoding architecture to represent concepts in the scene as latent variables. CLAP employs concept-specific latent random functions instantiated with Neural Processes to capture the law of concepts. Our experimental results demonstrate that CLAP outperforms the baseline methods in multiple visual tasks such as intuitive physics, abstract visual reasoning, and scene representation. The law manipulation experiments illustrate CLAP&#39;s interpretability by modifying specific latent random functions on samples. For example, CLAP learns the laws of position-changing and appearance constancy from the moving balls in a scene, making it possible to exchange laws between samples or compose existing laws into novel laws.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                960. <h><a href=https://openreview.net/pdf?id=j9m-mVnndbm>MIMT: Masked Image Modeling Transformer for Video Compression</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinxi Xiang, Kuan Tian, Jun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=j9m-mVnndbm>6.4 - [6, 6, 6, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/j9m-mVnndbm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning video compression outperforms its hand-craft counterparts with enhanced flexibility and capacity. One key component of the learned video codec is the autoregressive entropy model conditioned on spatial and temporal priors. Operating autoregressive on raster scanning order naively treats the context as unidirectional. This is neither efficient nor optimal, considering that conditional information probably locates at the end of the sequence. We thus introduce an entropy model based on a masked image modeling transformer (MIMT) to learn the spatial-temporal dependencies. Video frames are first encoded into sequences of tokens and then processed with the transformer encoder as priors.   The transformer decoder learns the probability mass functions (PMFs) \emph{conditioned} on the  priors and masked inputs. Then it is capable of selecting optimal decoding orders without a fixed direction.  During training, MIMT aims to predict the PMFs of randomly masked tokens by attending to tokens in all directions. This allows MIMT to capture the temporal dependencies from encoded priors and the spatial dependencies from the unmasked tokens, i.e., decoded tokens. At inference time, the model begins with generating  PMFs of all masked tokens in parallel and then decodes the frame iteratively from the previously-selected decoded tokens (i.e., with high confidence). In addition, we improve the overall performance with more techniques, e.g.,  manifold conditional priors accumulating a long range of information,  shifted window attention to reduce complexity. Extensive experiments demonstrate the proposed MIMT framework equipped with the new transformer entropy model achieves state-of-the-art performance on HEVC, UVG, and MCL-JCV datasets, generally outperforming the VVC in terms of PSNR and SSIM. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                961. <h><a href=https://openreview.net/pdf?id=U_2kuqoTcB>Identifiability Results for Multimodal Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Imant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, Julia E Vogt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=U_2kuqoTcB>6.4 - [6, 6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that multimodal contrastive learning can block-identify latent factors shared between heterogenous modalities (e.g., images and captions), even in the presence of nontrivial statistical and causal dependencies.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/U_2kuqoTcB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contrastive learning is a cornerstone underlying recent progress in multi-view and multimodal learning, e.g., in representation learning with image/caption pairs. While its effectiveness is not yet fully understood, a line of recent work reveals that contrastive learning can invert the data generating process and recover ground truth latent factors shared between views. In this work, we present new identifiability results for multimodal contrastive learning, showing that it is possible to recover shared factors in a more general setup than the multi-view setting studied previously. Specifically, we distinguish between the multi-view setting with one generative mechanism (e.g., multiple cameras of the same type) and the multimodal setting that is characterized by distinct mechanisms (e.g., cameras and microphones). Our work generalizes previous identifiability results by redefining the generative process in terms of distinct mechanisms with modality-specific latent variables. We prove that contrastive learning can block-identify latent factors shared between modalities, even when there are nontrivial dependencies between factors. We empirically verify our identifiability results with numerical simulations and corroborate our findings on a complex multimodal dataset of image/text pairs. Zooming out, our work provides a theoretical basis for multimodal representation learning and explains in which settings multimodal contrastive learning can be effective in practice.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                962. <h><a href=https://openreview.net/pdf?id=qUKsCztWlKq>The KFIoU Loss for Rotated Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xue Yang, Yue Zhou, Gefan Zhang, Jirui Yang, Wentao Wang, Junchi Yan, XIAOPENG ZHANG, Qi Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qUKsCztWlKq>6.4 - [6, 6, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qUKsCztWlKq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Differing from the well-developed horizontal object detection area whereby the computing-friendly IoU based loss is readily adopted and well fits with the detection metrics, rotation detectors often involve a more complicated loss based on SkewIoU which is unfriendly to gradient-based training. In this paper, we propose an effective approximate SkewIoU loss based on Gaussian modeling and Gaussian product, which mainly consists of two items. The first term is a scale-insensitive center point loss, which is used to quickly narrow the distance between the center points of the two bounding boxes. In the distance-independent second term, the product of the Gaussian distributions is adopted to inherently mimic the mechanism of SkewIoU by its definition, and show its alignment with the SkewIoU loss at trend-level within a certain distance (i.e. within 9 pixels). This is in contrast to recent Gaussian modeling based rotation detectors e.g. GWD loss and KLD loss that involve a human-specified distribution distance metric which require additional hyperparameter tuning that vary across datasets and detectors. The resulting new loss called KFIoU loss is easier to implement and works better compared with exact SkewIoU loss, thanks to its full differentiability and ability to handle the non-overlapping cases. We further extend our technique to the 3-D case which also suffers from the same issues as 2-D. Extensive results on various datasets with different base detectors show the effectiveness of our approach. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                963. <h><a href=https://openreview.net/pdf?id=hp_RwhKDJ5>Learning to Induce Causal Structure </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nan Rosemary Ke, Silvia Chiappa, Jane X Wang, Jorg Bornschein, Anirudh Goyal, Melanie Rey, Theophane Weber, Matthew Botvinick, Michael Curtis Mozer, Danilo Jimenez Rezende</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hp_RwhKDJ5>6.4 - [8, 5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hp_RwhKDJ5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The fundamental challenge in causal induction is to infer the underlying graph structure given observational and/or interventional data. Most existing causal induction algorithms operate by generating candidate graphs and evaluating them using either score-based methods (including continuous optimization) or independence tests. In our work, we instead treat the inference process as a black box and design a neural network architecture that learns the mapping from both observational and interventional data to graph structures via supervised training on synthetic graphs. The learned model generalizes to new synthetic graphs, is robust to train-test distribution shifts, and achieves state-of-the-art performance on naturalistic graphs for low sample complexity.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                964. <h><a href=https://openreview.net/pdf?id=Qamz7Q_Ta1k>Direct Embedding of Temporal Network Edges via Time-Decayed Line Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sudhanshu Chanpuriya, Ryan A. Rossi, Sungchul Kim, Tong Yu, Jane Hoffswell, Nedim Lipka, Shunan Guo, Cameron N Musco</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Qamz7Q_Ta1k>6.38 - [5, 6, 8, 8, 3, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a line graph-based method for temporal networks which directly embeds temporal edges.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Qamz7Q_Ta1k.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Temporal networks model a variety of important phenomena involving timed interactions between entities. Existing methods for machine learning on temporal networks generally exhibit at least one of two limitations. First, many methods assume time to be discretized, so if the time data is continuous, the user must determine the discretization and discard precise time information. Second, edge representations can only be calculated indirectly from the nodes, which may be suboptimal for tasks like edge classification. We present a simple method that avoids both shortcomings: construct the line graph of the network, which includes a node for each interaction, and weigh the edges of this graph based on the difference in time between interactions. From this derived graph, edge representations for the original network can be computed with efficient classical methods. The simplicity of this approach facilitates explicit theoretical analysis: we can constructively show the effectiveness of our method&#39;s representations for a natural synthetic model of temporal networks. Empirical results on real-world networks demonstrate our method&#39;s efficacy and efficiency on both link classification and prediction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                965. <h><a href=https://openreview.net/pdf?id=09hVcSDkea>Corrupted Image Modeling for Self-Supervised Visual Pre-Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuxin Fang, Li Dong, Hangbo Bao, Xinggang Wang, Furu Wei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=09hVcSDkea>6.33 - [6, 5, 8, 8, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/09hVcSDkea.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce Corrupted Image Modeling (CIM) for self-supervised visual pre-training. CIM uses an auxiliary generator with a small trainable BEiT to corrupt the input image instead of using artificial [MASK] tokens, where some patches are randomly selected and replaced with plausible alternatives sampled from the BEiT output distribution. Given this corrupted image, an enhancer network learns to either recover all the original image pixels, or predict whether each visual token is replaced by a generator sample or not. The generator and the enhancer are simultaneously trained and synergistically updated. After pre-training, the enhancer can be used as a high-capacity visual encoder for downstream tasks. CIM is a general and flexible visual pre-training framework that is suitable for various network architectures. For the first time, CIM demonstrates that both ViT and CNN can learn rich visual representations using a unified, non-Siamese framework. Experimental results show that our approach achieves compelling results in vision benchmarks, such as ImageNet classification and ADE20K semantic segmentation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                966. <h><a href=https://openreview.net/pdf?id=chDrutUTs0K>POPGym: Benchmarking Partially Observable Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, Amanda Prorok</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=chDrutUTs0K>6.33 - [3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose POPGym, an RL library containing 14 partially observable gym environments and 13 different memory architectures
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/chDrutUTs0K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Real world applications of Reinforcement Learning (RL) are often partially observable, thus requiring memory. Despite this, partial observability is still largely ignored by contemporary RL benchmarks and libraries. We introduce Partially Observable Process Gym (POPGym), a two-part library containing (1) a diverse collection of 15 partially observable environments, each with multiple difficulties and (2) implementations of 13 memory model baselines -- the most in a single RL library. Existing partially observable benchmarks tend to fixate on 3D visual navigation, which is computationally expensive and only one type of POMDP. In contrast, POPGym environments are diverse, produce smaller observations, use less memory, and often converge within two hours of training on a consumer-grade GPU. We implement our high-level memory API and memory baselines on top of the popular RLlib framework, providing plug-and-play compatibility with various training algorithms, exploration strategies, and distributed training paradigms. Using POPGym, we execute the largest comparison across RL memory models to date. POPGym is available at https://github.com/proroklab/popgym.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                967. <h><a href=https://openreview.net/pdf?id=gfWNItGOES6>Guiding continuous operator learning through Physics-based boundary constraints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nadim Saad, Gaurav Gupta, Shima Alizadeh, Danielle C. Maddix</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gfWNItGOES6>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose novel kernel correction mechanisms for neural operators to satisfy physical boundary constraints which are effective in improving the overall performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gfWNItGOES6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Boundary conditions (BCs) are important groups of physics-enforced constraints that are necessary for solutions of Partial Differential Equations (PDEs) to satisfy at specific spatial locations. These constraints carry important physical meaning, and guarantee the existence and the uniqueness of the PDE solution. Current neural-network based approaches that aim to solve PDEs rely only on training data to help the model learn BCs implicitly, however, there is no guarantee of BC satisfaction by these models during evaluation. In this work, we propose Boundary enforcing Operator Network (BOON) that enables the BC satisfaction of neural operators by making structural changes to the operator kernel. We provide our refinement procedure, and demonstrate the satisfaction of physics-based BCs such as Dirichlet, Neumann, and periodic by the solutions obtained by BOON. Numerical experiments based on multiple PDEs with a wide variety of applications indicate that the proposed approach ensures satisfaction of BCs, and leads to more accurate solutions over the whole domain. The proposed method exhibits a (2X-20X) improvement in accuracy (0.000084 relative $L^2$ error for Burgers&#39; equation). Code available at: https://github.com/amazon-science/boon.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                968. <h><a href=https://openreview.net/pdf?id=SEh5SfEQtqB>Meta-prediction Model for Distillation-Aware NAS on Unseen Datasets</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hayeon Lee, Sohyun An, Minseon Kim, Sung Ju Hwang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SEh5SfEQtqB>6.33 - [8, 8, 3] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a one-shot meta accuracy prediction model which can predict a given architecture&#39;s final performances on a dataset when performing KD with a given teacher, without having to actually train it on the target task. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SEh5SfEQtqB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Distillation-aware Neural Architecture Search (DaNAS) aims to search for an optimal student architecture that obtains the best performance and/or efficiency when distilling the knowledge from a given teacher model. Previous DaNAS methods have mostly tackled the search for the neural architecture for fixed datasets and the teacher, which are not generalized well on a new task consisting of an unseen dataset and an unseen teacher, thus need to perform a costly search for any new combination of the datasets and the teachers. For standard NAS tasks without KD, meta-learning-based computationally efficient NAS methods have been proposed, which learn the generalized search process over multiple tasks (datasets) and transfer the knowledge obtained over those tasks to a new task. However, since they assume learning from scratch without KD from a teacher, they might not be ideal for DaNAS scenarios. To eliminate the excessive computational cost of DaNAS methods and the sub-optimality of rapid NAS methods, we propose a distillation-aware meta-accuracy prediction model, DaSS (Distillation-aware Student Search), which can predict a given architecture&#39;s final performances on a dataset when performing KD with a given teacher, without having actually to train it on the target task. The experimental results demonstrate that our proposed meta-prediction model successfully generalizes to multiple unseen datasets for DaNAS tasks, largely outperforming existing meta-NAS methods and rapid NAS baselines. Code is available at https://github.com/CownowAn/DaSS.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                969. <h><a href=https://openreview.net/pdf?id=4WM4cy42B81>Dirichlet-based Uncertainty Calibration for Active Domain Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mixue Xie, Shuang Li, Rui Zhang, Chi Harold Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4WM4cy42B81>6.33 - [5, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4WM4cy42B81.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Active domain adaptation (DA) aims to maximally boost the model adaptation on a new target domain by actively selecting limited target data to annotate, whereas traditional active learning methods may be less effective since they do not consider the domain shift issue. Despite active DA methods address this by further proposing targetness to measure the representativeness of target domain characteristics, their predictive uncertainty is usually based on the prediction of deterministic models, which can easily be miscalibrated on data with distribution shift. Considering this, we propose a Dirichlet-based Uncertainty Calibration (DUC) approach for active DA, which simultaneously achieves the mitigation of miscalibration and the selection of informative target samples. Specifically, we place a Dirichlet prior on the prediction and interpret the prediction as a distribution on the probability simplex, rather than a point estimate like deterministic models. This manner enables us to consider all possible predictions, mitigating the miscalibration of unilateral prediction. Then a two-round selection strategy based on different uncertainty origins is designed to select target samples that are both representative of target domain and conducive to discriminability. Extensive experiments on cross-domain image classification and semantic segmentation validate the superiority of DUC.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                970. <h><a href=https://openreview.net/pdf?id=f2wN4v_2__W>Learning Symbolic Models for Graph-structured Physical Mechanism</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongzhi Shi, Jingtao Ding, Yufan Cao, quanming yao, Li Liu, Yong Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=f2wN4v_2__W>6.33 - [8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We generalize symbolic regression to graph data and propose a novel method with the key insight of learning formula skeleton by searching message-passing flows of graph neural networks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/f2wN4v_2__W.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph-structured physical mechanisms are ubiquitous in real-world scenarios, thus revealing underneath formulas is of great importance for scientific discovery. However, classical symbolic regression methods fail on this task since they can only handle input-output pairs that are not graph-structured. In this paper, we propose a new approach that generalizes symbolic regression to graph-structured physical mechanisms. The essence of our method is to model the formula skeleton with a message-passing flow, which helps transform the discovery of the skeleton into the search for the message-passing flow. Such a transformation guarantees that we are able to search a message-passing flow, which is efficient and Pareto-optimal in terms of both accuracy and simplicity. Subsequently, the underneath formulas can be identified by interpreting component functions of the searched message-passing flow, reusing classical symbolic regression methods. We conduct extensive experiments on datasets from different physical domains, including mechanics, electricity, and thermology, and on real-world datasets of pedestrian dynamics without ground-truth formulas. The experimental results not only verify the rationale of our design but also demonstrate that the proposed method can automatically learn precise and interpretable formulas for graph-structured physical mechanisms. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                971. <h><a href=https://openreview.net/pdf?id=3F6I-0-57SC>HiViT: A Simpler and More Efficient Design of Hierarchical Vision Transformer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, Qi Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3F6I-0-57SC>6.33 - [5, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel hierarchical vision transformer that is stronger and faster when applied to masked image modeling
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3F6I-0-57SC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There has been a debate on the choice of plain vs. hierarchical vision transformers, where researchers often believe that the former (e.g., ViT) has a simpler design but the latter (e.g., Swin) enjoys higher recognition accuracy. Recently, the emerge of masked image modeling (MIM), a self-supervised visual pre-training method, raised a new challenge to vision transformers in terms of flexibility, i.e., part of image patches or tokens are to be discarded, which seems to claim the advantages of plain vision transformers. In this paper, we delve deep into the comparison between ViT and Swin, revealing that (i) the performance gain of Swin is mainly brought by a deepened backbone and relative positional encoding, (ii) the hierarchical design of Swin can be simplified into hierarchical patch embedding (proposed in this work), and (iii) other designs such as shifted-window attentions can be removed. By removing the unnecessary operations, we come up with a new architecture named HiViT (short for hierarchical ViT), which is simpler and more efficient than Swin yet further improves its performance on fully-supervised and self-supervised visual representation learning. In particular, after pre-trained using masked autoencoder (MAE) on ImageNet-1K, HiViT-B reports a 84.6% accuracy on ImageNet-1K classification, a 53.3% box AP on COCO detection, and a 52.8% mIoU on ADE20K segmentation, significantly surpassing the baseline. Code is available at https://github.com/zhangxiaosong18/hivit.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                972. <h><a href=https://openreview.net/pdf?id=xZD10GhCvM>Toward Adversarial Training on Contextualized Language Representation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongqiu Wu, Yongxiang Liu, Hanwen Shi, hai zhao, Min Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xZD10GhCvM>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Adversarial training optimized to deviate contextualized language representation with all-powerful performance gain
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xZD10GhCvM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Beyond the success story of adversarial training (AT) in the recent text domain on top of pre-trained language models (PLMs), our empirical study showcases the inconsistent gains from AT on some tasks, e.g. commonsense reasoning, named entity recognition. This paper investigates AT from the perspective of the contextualized language representation outputted by PLM encoders. We find the current AT attacks lean to generate sub-optimal adversarial examples that can fool the decoder part but have a minor effect on the encoder. However, we find it necessary to effectively deviate the latter one to allow AT to gain. Based on the observation, we propose simple yet effective \textit{Contextualized representation-Adversarial Training} (CreAT), in which the attack is explicitly optimized to deviate the contextualized representation of the encoder. It allows a global optimization of adversarial examples that can fool the entire model. We also find CreAT gives rise to a better direction to optimize the adversarial examples, to let them less sensitive to hyperparameters. Compared to AT, CreAT produces consistent performance gains on a wider range of tasks and is proven to be more effective for language pre-training where only the encoder part is kept for downstream tasks. We achieve the new state-of-the-art performances on a series of challenging benchmarks, e.g. AdvGLUE (59.1 $ \rightarrow $ 61.1), HellaSWAG (93.0  $ \rightarrow $ 94.9), ANLI (68.1  $ \rightarrow $ 69.3).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                973. <h><a href=https://openreview.net/pdf?id=1fZd4owfJP6>Masked Image Modeling with Denoising Contrast</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kun Yi, Yixiao Ge, Xiaotong Li, Shusheng Yang, Dian Li, Jianping Wu, Ying Shan, Xiaohu Qie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1fZd4owfJP6>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We first treat masked patch prediction as denoising contrastive learning in self-supervised image pre-training, achieving state-of-the-art results.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1fZd4owfJP6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Since the development of self-supervised visual representation learning from contrastive learning to masked image modeling (MIM), there is no significant difference in essence, that is, how to design proper pretext tasks for vision dictionary look-up. MIM recently dominates this line of research with state-of-the-art performance on vision Transformers (ViTs), where the core is to enhance the patch-level visual context capturing of the network via denoising auto-encoding mechanism. Rather than tailoring image tokenizers with extra training stages as in previous works, we unleash the great potential of contrastive learning on de- noising auto-encoding and introduce a pure MIM method, ConMIM, to produce simple intra-image inter-patch contrastive constraints as the sole learning objectives for masked patch prediction. We further strengthen the denoising mechanism with asymmetric designs, including image perturbations and model progress rates, to improve the network pre-training. ConMIM-pretrained models with various scales achieve competitive results on downstream image classification, semantic segmentation, object detection, and instance segmentation tasks, e.g., on ImageNet-1K classification, we achieve 83.9% top-1 accuracy with ViT-Small and 85.3% with ViT-Base without extra data for pre-training. Code will be available at https://github.com/TencentARC/ConMIM.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                974. <h><a href=https://openreview.net/pdf?id=NRxydtWup1S>Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Keyu Tian, Yi Jiang, qishuai diao, Chen Lin, Liwei Wang, Zehuan Yuan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NRxydtWup1S>6.33 - [5, 8, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents a simple yet powerful framework to pre-train convolutional network (convnet) with Sparse masKed modeling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NRxydtWup1S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We identify and overcome two key obstacles in extending the success of BERT-style pre-training, or masked image modeling, to convolutional networks (convnets): (i) convolution operation cannot handle irregular, randomly masked input images; (ii) the single-scale nature of BERT pre-training is inconsistent with convnet’s hierarchical structure. For (i), we treat unmasked pixels as sparse voxels of 3D point clouds and use sparse convolution to encode. This is the first use of sparse convolution for 2D masked modeling. For (ii), we develop a hierarchical decoder to reconstruct images from multi-scale encoded features. Our method, called Sparse masKed modeling (SparK), is general: it can be used directly on any convolutional model without backbone modifications. We validate it on both classical (ResNet) and modern (ConvNeXt) models: on three downstream tasks, it surpasses both state-of-the-art contrastive learning and transformer-based masked modeling by similarly large margins (around +1.0%). The improvements on object detection and instance segmentation are more significant (up to +3.5%), validating the strong transferability of features learned. We also find SparK’s favorable scaling behavior by observing more gains on larger networks. All of these findings support the promising future of generative pre-training on convnets. Both codes and pre-trained models have been released at https://github.com/keyu-tian/SparK.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                975. <h><a href=https://openreview.net/pdf?id=LSz-gQyd0zE>Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhengrui Ma, Chenze Shao, Shangtong Gui, Min Zhang, Yang Feng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LSz-gQyd0zE>6.33 - [8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a fuzzy alignment objective in Directed Acyclic Graph for NAT, setting a new state of the art for NAT on the raw training data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LSz-gQyd0zE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Non-autoregressive translation (NAT) reduces the decoding latency but suffers from performance degradation due to the multi-modality problem. Recently, the structure of directed acyclic graph has achieved great success in NAT, which tackles the multi-modality problem by introducing dependency between vertices. However, training it with negative log-likelihood loss implicitly requires a strict alignment between reference tokens and vertices, weakening its ability to handle multiple translation modalities. In this paper, we hold the view that all paths in the graph are fuzzily aligned with the reference sentence. We do not require the exact alignment but train the model to maximize a fuzzy alignment score between the graph and reference, which takes captured translations in all modalities into account. Extensive experiments on major WMT benchmarks show that our method substantially improves translation performance and increases prediction confidence, setting a new state of the art for NAT on the raw training data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                976. <h><a href=https://openreview.net/pdf?id=0paCJSFW7j>ISAAC Newton: Input-based Approximate Curvature for Newton&#39;s Method</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Felix Petersen, Tobias Sutter, Christian Borgelt, Dongsung Huh, Hilde Kuehne, Yuekai Sun, Oliver Deussen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0paCJSFW7j>6.33 - [8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0paCJSFW7j.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present ISAAC (Input-baSed ApproximAte Curvature), a novel method that conditions the gradient using selected second-order information and has an asymptotically vanishing computational overhead, assuming a batch size smaller than the number of neurons. We show that it is possible to compute a good conditioner based on only the input to a respective layer without a substantial computational overhead. The proposed method allows effective training even in small-batch stochastic regimes, which makes it competitive to first-order as well as second-order methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                977. <h><a href=https://openreview.net/pdf?id=6w1k-IixnL8>Beyond calibration: estimating the grouping loss of modern neural networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexandre Perez-Lebel, Marine Le Morvan, Gael Varoquaux</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6w1k-IixnL8>6.33 - [3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide an estimator to evaluate confidence scores beyond calibration, revealing the subgroups heterogeneities that undermine individual predicted probabilities.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6w1k-IixnL8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to ensure that a classifier gives reliable confidence scores is essential to ensure informed decision-making. To this end, recent work has focused on miscalibration, i.e., the over or under confidence of model scores. Yet calibration is not enough: even a perfectly calibrated classifier with the best possible accuracy can have confidence scores that are far from the true posterior probabilities. This is due to the grouping loss, created by samples with the same confidence scores but different true posterior probabilities. Proper scoring rule theory shows that given the calibration loss, the missing piece to characterize individual errors is the grouping loss. While there are many estimators of the calibration loss, none exists for the grouping loss in standard settings. Here, we propose an estimator to approximate the grouping loss. We show that modern neural network architectures in vision and NLP exhibit grouping loss, notably in distribution shifts settings, which highlights the importance of pre-production validation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                978. <h><a href=https://openreview.net/pdf?id=QjQibO3scV_>Revocable Deep Reinforcement Learning with Affinity Regularization for Outlier-Robust Graph Matching</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chang Liu, Zetian Jiang, Runzhong Wang, Lingxiao Huang, Pinyan Lu, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QjQibO3scV_>6.33 - [5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QjQibO3scV_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph matching (GM) has been a building block in various areas including computer vision and pattern recognition. Despite recent impressive progress, existing deep GM methods often have obvious difficulty in handling outliers, which are ubiquitous in practice. We propose a deep reinforcement learning based approach RGM, whose sequential node matching scheme naturally fits the strategy for selective inlier matching against outliers. A revocable action framework is devised to improve the agent&#39;s flexibility against the complex constrained GM. Moreover, we propose a quadratic approximation technique to regularize the affinity score, in the presence of outliers. As such, the agent can finish inlier matching timely when the affinity score stops growing, for which otherwise an additional parameter i.e. the number of inliers is needed to avoid matching outliers. In this paper, we focus on learning the back-end solver under the most general form of GM: the Lawler&#39;s QAP, whose input is the affinity matrix. Especially, our approach can also boost existing GM methods that use such input. Experiments on multiple real-world datasets demonstrate its performance regarding both accuracy and robustness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                979. <h><a href=https://openreview.net/pdf?id=78xgBm6ckZr>Sparse tree-based Initialization for Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Patrick Lutz, Ludovic Arnould, Claire Boyer, Erwan Scornet</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=78xgBm6ckZr>6.33 - [5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/78xgBm6ckZr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Dedicated neural network (NN) architectures have been designed to handle specific data types (such as CNN for images or RNN for text), which ranks them among state-of-the-art methods for dealing with these data. Unfortunately, no architecture has been found for dealing with tabular data yet, for which tree ensemble methods (tree boosting, random forests) usually show the best predictive performances. In this work, we propose a new sparse initialization technique for (potentially deep) multilayer perceptrons (MLP):  we first train a tree-based procedure to detect feature interactions and use the resulting information to initialize the network, which is subsequently trained via standard gradient descent (GD) strategies. Numerical experiments on several tabular data sets showthe benefits of this new, simple and easy-to-use method, both in terms of generalization capacity and computation time, compared to default MLP initialization and even to existing complex deep learning solutions. In fact, this wise MLP initialization raises the performances of the resulting NN methods to that of gradient boosting on tabular data. Besides, such initializations are able to preserve the sparsity of weights introduced in the first layers of the network throughout the training, which emphasizes that the first layers act as a sparse feature extractor (like convolutional layers in CNN).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                980. <h><a href=https://openreview.net/pdf?id=WmIwYTd0YTF>Stable Target Field for Reduced Variance Score Estimation in Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yilun Xu, Shangyuan Tong, Tommi S. Jaakkola</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WmIwYTd0YTF>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a low variance objective to improve the training of score-based models
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WmIwYTd0YTF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models generate samples by reversing a fixed forward diffusion process. Despite already providing impressive empirical results, these diffusion models algorithms can be further improved by reducing the variance of the training targets in their denoising score-matching objective. We argue that the source of such variance lies in the handling of intermediate noise-variance scales, where multiple modes in the data affect the direction of reverse paths. We propose to remedy the problem by incorporating a reference batch which we use to calculate weighted conditional scores as more stable training targets. We show that the procedure indeed helps in the challenging intermediate regime by reducing (the trace of) the covariance of training targets. The new stable targets can be seen as trading bias for reduced variance, where the bias vanishes with increasing reference batch size. Empirically, we show that the new objective improves the image quality, stability, and training speed of various popular diffusion models across datasets with both general ODE and SDE solvers. When used in combination with EDM, our method yields a current SOTA FID of 1.90 with 35 network evaluations on the unconditional CIFAR-10 generation task. The code is available at https://github.com/Newbeeer/stf
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                981. <h><a href=https://openreview.net/pdf?id=hdghx6wbGuD>Out-of-distribution Detection with Implicit Outlier Transformation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qizhou Wang, Junjie Ye, Feng Liu, Quanyu Dai, Marcus Kalander, Tongliang Liu, Jianye HAO, Bo Han</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hdghx6wbGuD>6.33 - [8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hdghx6wbGuD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Outlier exposure (OE) is powerful in out-of-distribution (OOD) detection, enhancing detection capability via model fine-tuning with surrogate OOD data. However, surrogate data typically deviate from test OOD data. Thus, the performance of OE when facing unseen OOD data, can be weaken. To address this issue, we propose a novel OE-based approach that makes the model perform well for unseen OOD situations, even for unseen OOD cases. It leads to a min-max learning scheme---searching to synthesize OOD data that leads to worst judgments and learning from such OOD data for the uniform performance in OOD detection. In our realization, these worst OOD data are synthesized by transforming original surrogate ones, where the associated transform functions are learned implicitly based on our novel insight that model perturbation leads to data transformation. Our methodology offers an efficient way of synthesizing OOD data, which can further benefit the detection model, besides the surrogate OOD data. We conduct extensive experiments under various OOD detection setups, demonstrating the effectiveness of our method against its advanced counterparts. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                982. <h><a href=https://openreview.net/pdf?id=1FxRPKrH8bw>MCAL: Minimum Cost Human-Machine Active Labeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hang Qiu, Krishna Chintalapudi, Ramesh Govindan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1FxRPKrH8bw>6.33 - [8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A framework to address the prohibitive data labeling cost challenge using hybrid human-machine labeling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1FxRPKrH8bw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Today, ground-truth generation uses data sets annotated by cloud-based annotation services. These services rely on human annotation, which can be prohibitively expensive. In this paper, we consider the problem of hybrid human-machine labeling, which trains a classifier to accurately auto-label part of the data set. However, training the classifier can be expensive too. We propose an iterative approach that minimizes total overall cost by, at each step, jointly determining which samples to label using humans and which to label using the trained classifier. We validate our approach on well known public data sets such as Fashion-MNIST, CIFAR-10, CIFAR-100, and ImageNet. In some cases, our approach has 6× lower overall cost relative to human labeling the entire data set, and is always cheaper than the cheapest competing strategy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                983. <h><a href=https://openreview.net/pdf?id=_X9Yl1K2mD>Rotamer Density Estimator is an Unsupervised Learner of the Effect of Mutations on Protein-Protein Interaction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shitong Luo, Yufeng Su, Zuofan Wu, Chenpeng Su, Jian Peng, Jianzhu Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_X9Yl1K2mD>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_X9Yl1K2mD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Protein-protein interactions are crucial to many biological processes, and predicting the effect of amino acid mutations on binding is important for protein engineering. While data-driven approaches using deep learning have shown promise, the scarcity of annotated experimental data remains a major challenge. In this work, we propose a new approach that predicts mutational effects on binding using the change in conformational flexibility of the protein-protein interface. Our approach, named Rotamer Density Estimator (RDE), employs a flow-based generative model to estimate the probability distribution of protein side-chain conformations and uses entropy to measure flexibility. RDE is trained solely on protein structures and does not require the supervision of experimental values of changes in binding affinities. Furthermore, the unsupervised representations extracted by RDE can be used for downstream neural network predictions with even greater accuracy. Our method outperforms empirical energy functions and other machine learning-based approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                984. <h><a href=https://openreview.net/pdf?id=7d-g8KozkiE>Cycle to Clique (Cy2C) Graph Neural Network: A Sight to See beyond Neighborhood Aggregation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yun Young Choi, Sun Woo Park, Youngho Woo, U Jin Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7d-g8KozkiE>6.33 - [5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7d-g8KozkiE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural networks have been successfully adapted for learning vector representations of graphs through various neighborhood aggregation schemes. Previous researches suggest, however, that they possess limitations in incorporating key non-Euclidean topological properties of graphs. This paper mathematically identifies the caliber of graph neural networks in classifying isomorphism classes of graphs with continuous node attributes up to their local topological properties. In light of these observations, we construct the Cycle to Clique graph neural network, a novel yet simple algorithm which topologically enriches the input data of conventional graph neural networks while preserving their architectural components. This method theoretically outperforms conventional graph neural networks in classifying isomorphism classes of graphs while ensuring comparable time complexity in representing random graphs. Empirical results further support that the novel algorithm produces comparable or enhanced results in classifying benchmark graph data sets compared to contemporary variants of graph neural networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                985. <h><a href=https://openreview.net/pdf?id=lj1Eb1OPeNw>Maximizing Spatio-Temporal Entropy of Deep 3D CNNs for Efficient Video Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Junyan Wang, Zhenhong Sun, Yichen Qian, Dong Gong, Xiuyu Sun, Ming Lin, Maurice Pagnucco, Yang Song</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lj1Eb1OPeNw>6.33 - [8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lj1Eb1OPeNw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    3D convolution neural networks (CNNs) have been the prevailing option for video recognition. To capture the temporal information, 3D convolutions are computed along the sequences, leading to cubically growing and expensive computations. To reduce the computational cost, previous methods resort to manually designed 3D/2D CNN structures with approximations or automatic search, which sacrifice the modeling ability or make training time-consuming. In this work, we propose to automatically design efficient 3D CNN architectures via a novel training-free neural architecture search approach tailored for 3D CNNs considering the model complexity. To measure the expressiveness of 3D CNNs efficiently, we formulate a 3D CNN as an information system and derive an analytic entropy score, based on the Maximum Entropy Principle. Specifically, we propose a spatio-temporal entropy score (STEntr-Score) with a refinement factor to handle the discrepancy of visual information in spatial and temporal dimensions, through dynamically leveraging the correlation between the feature map size and kernel size depth-wisely. Highly efficient and expressive 3D CNN architectures, i.e., entropy-based 3D CNNs (E3D family),  can then be efficiently searched by maximizing the STEntr-Score under a given computational budget, via an evolutionary algorithm without training the network parameters. Extensive experiments on Something-Something V1&amp;V2 and Kinetics400 demonstrate that the E3D family achieves state-of-the-art performance with higher computational efficiency.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                986. <h><a href=https://openreview.net/pdf?id=WVRb98rwbv9>Truthful Self-Play</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shohei Ohsawa</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WVRb98rwbv9>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> TSP is a general framework for evolutionary learning to emergent unbiased state representation without any supervision. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WVRb98rwbv9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a general framework for evolutionary learning to emergent unbiased state representation without any supervision. Evolutionary frameworks such as self-play converge to bad local optima in case of multi-agent reinforcement learning in non-cooperative partially observable environments with communication due to information asymmetry.  Our proposed framework is a simple modification of self-play inspired by mechanism design, also known as {\em reverse game theory}, to elicit truthful signals and make the agents cooperative. The key idea is to add imaginary rewards using the peer prediction method, i.e., a mechanism for evaluating the validity of information exchanged between agents in a decentralized environment. Numerical experiments with predator prey, traffic junction and StarCraft tasks demonstrate that the state-of-the-art performance of our framework.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                987. <h><a href=https://openreview.net/pdf?id=RUzSobdYy0V>Quantifying and Mitigating the Impact of Label Errors on Model Disparity Metrics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Julius Adebayo, Melissa Hall, Bowen Yu, Bobbie Chern</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RUzSobdYy0V>6.33 - [5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RUzSobdYy0V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Errors in labels obtained via human annotation adversely affect a trained model&#39;s performance. Existing approaches propose ways to mitigate the effect of label error on a model&#39;s downstream accuracy, yet little is known about its impact on a model&#39;s group-based disparity metrics\footnote{Group-based disparity metrics like subgroup calibration, false positive rate, false negative rate, equalized odds, and equal opportunity are more often known, colloquially, as \textit{fairness metrics} in the literature. We use the term group-based disparity metrics in this work.}. Here we study the effect of label error on a model&#39;s group-based disparity metrics like group calibration. We empirically characterize how varying levels of label error, in both training and test data, affect these disparity metrics. We find that group calibration and other metrics are sensitive to train-time and test-time label error---particularly for minority groups. For the same level of label error, the percentage change in group calibration error for the minority group is on average 1.5 times larger than the change for the majority group. Towards mitigating the impact of training-time label error, we present an approach to estimate how changing a single training input&#39;s label affects a model&#39;s group disparity metric on a test set. We empirically assess the proposed approach on a variety of datasets and find a 10-40\% improvement, compared to alternative approaches, in identifying training inputs that improve a model&#39;s disparity metric. The proposed approach can help surface training inputs that may need to be corrected for improving a model&#39;s group-based disparity metrics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                988. <h><a href=https://openreview.net/pdf?id=osei3IzUia>Where to Diffuse, How to Diffuse, and How to Get Back: Automated Learning for Multivariate Diffusions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Raghav Singhal, Mark Goldstein, Rajesh Ranganath</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=osei3IzUia>6.33 - [8, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/osei3IzUia.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion-based generative models (DBGMs) perturb data to a target noise distribution and reverse this process to generate samples.  The choice of noising process, or inference diffusion process, affects both likelihoods and sample quality. For example, extending the inference process with auxiliary variables leads to improved sample quality. While there are many such multivariate diffusions to explore, each new one requires significant model-specific analysis, hindering rapid prototyping and evaluation. In this work, we study Multivariate Diffusion Models (MDMs). For any number of auxiliary variables, we provide a recipe for maximizing a lower-bound on the MDMs likelihood without requiring any model-specific analysis. We then demonstrate how to parameterize the diffusion for a specified target noise distribution; these two points together enable optimizing the inference diffusion process. Optimizing the diffusion expands easy experimentation from just a few well-known processes to an automatic search over all linear diffusions. To demonstrate these ideas, we introduce two new specific diffusions as well as learn a diffusion process on the MNIST, CIFAR10, and ImageNet32 datasets. We show learned MDMs match or surpass bits-per-dims (BPDs) relative to fixed choices of diffusions for a given dataset and model architecture.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                989. <h><a href=https://openreview.net/pdf?id=cP2QVK-uygd>On Representing Linear Programs by Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziang Chen, Jialin Liu, Xinshang Wang, Wotao Yin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cP2QVK-uygd>6.33 - [5, 6, 8] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cP2QVK-uygd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning to optimize is a rapidly growing area that aims to solve optimization problems or improve existing optimization algorithms using machine learning (ML). In particular, the graph neural network (GNN) is considered a suitable ML model for optimization problems whose variables and constraints are permutation--invariant, for example, the linear program (LP). While the literature has reported encouraging numerical results, this paper establishes the theoretical foundation of applying GNNs to solving LPs. Given any size limit of LPs, we construct a GNN that maps different LPs to different outputs. We show that properly built GNNs can reliably predict feasibility, boundedness, and an optimal solution for each LP in a broad class. Our proofs are based upon the recently--discovered connections between the Weisfeiler--Lehman isomorphism test and the GNN. To validate our results, we train a simple GNN and present its accuracy in mapping LPs to their feasibilities and solutions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                990. <h><a href=https://openreview.net/pdf?id=fGG6vHp3W9W>Hierarchical Abstraction for Combinatorial Generalization in Object Rearrangement</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Chang, Alyssa Li Dayan, Franziska Meier, Thomas L. Griffiths, Sergey Levine, Amy Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fGG6vHp3W9W>6.33 - [8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We demonstrate how to generalize over a combinatorially large space of rearrangement tasks from only pixel observations by constructing from video demonstrations a factorized transition graph over entity state transitions that we use for control.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fGG6vHp3W9W.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Object rearrangement is a challenge for embodied agents because solving these tasks requires generalizing across a combinatorially large set of configurations of entities and their locations. Worse, the representations of these entities are unknown and must be inferred from sensory percepts. We present a hierarchical abstraction approach to uncover these underlying entities and achieve combinatorial generalization from unstructured visual inputs. By constructing a factorized transition graph over clusters of entity representations inferred from pixels, we show how to learn a correspondence between intervening on states of entities in the agent&#39;s model and acting on objects in the environment. We use this correspondence to develop a method for control that generalizes to different numbers and configurations of objects, which outperforms current offline deep RL methods when evaluated on simulated rearrangement tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                991. <h><a href=https://openreview.net/pdf?id=M0_sUuEyHs>Better Teacher Better Student: Dynamic Prior Knowledge for Knowledge Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Martin Zong, Zengyu Qiu, Xinzhu Ma, Kunlin Yang, Chunya Liu, Jun Hou, Shuai Yi, Wanli Ouyang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=M0_sUuEyHs>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The proposed method dynamically introduces part of teacher&#39;s features to student as prior knowledge before applying knowledge distillation. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/M0_sUuEyHs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge distillation (KD) has shown very promising capabilities in transferring learning representations from large models (teachers) to small models (students). However, as the capacity gap between students and teachers becomes larger, existing KD methods fail to achieve better results. Our work shows that the &#39;prior knowledge&#39; is vital to KD, especially when applying large teachers.  Particularly, we propose the dynamic prior knowledge (DPK), which integrates part of teacher&#39;s features as the prior knowledge before the feature distillation. This means that our method also takes the teacher&#39;s feature as `input&#39;, not just `target&#39;. Besides, we dynamically adjust the ratio of the prior knowledge during the training phase according to the feature gap, thus guiding the student in an appropriate difficulty. To evaluate the proposed method, we conduct extensive experiments on two image classification benchmarks (i.e. CIFAR100 and ImageNet) and an object detection benchmark (\i.e. MS COCO). The results demonstrate the superiority of our method in performance under varying settings. Besides, our DPK makes the performance of the student model positively correlated with that of the teacher model, which means that we can further boost the accuracy of students by applying larger teachers. More importantly, DPK provides a fast solution in teacher model selection for any given model. Our codes will be publicly available for reproducibility.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                992. <h><a href=https://openreview.net/pdf?id=4oLK1_k71Tz>Mid-Vision Feedback</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Maynord, Eadom T Dessalene, Cornelia Fermuller, Yiannis Aloimonos</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4oLK1_k71Tz>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4oLK1_k71Tz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Feedback plays a prominent role in biological vision, where perception is modulated based on agents&#39; evolving expectations and world model. We introduce a novel mechanism which modulates perception based on high level categorical expectations: Mid-Vision Feedback (MVF). MVF associates high level contexts with linear transformations. When a context is &#34;expected&#34; its associated linear transformation is applied over feature vectors in a mid level of a network. The result is that mid-level network representations are biased towards conformance with high level expectations, improving overall accuracy and contextual consistency. Additionally, during training mid-level feature vectors are biased through introduction of a loss term which increases the distance between feature vectors associated with different contexts. MVF is agnostic as to the source of contextual expectations, and can serve as a mechanism for top down integration of symbolic systems with deep vision architectures. We show the superior performance of MVF to post-hoc filtering for incorporation of contextual knowledge, and show superior performance of configurations using predicted context (when no context is known a priori) over configurations with no context awareness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                993. <h><a href=https://openreview.net/pdf?id=z57WK5lGeHd>On the Soft-Subnetwork for Few-Shot Class Incremental Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haeyong Kang, Jaehong Yoon, Sultan Rizky Hikmawan Madjid, Sung Ju Hwang, Chang D. Yoo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=z57WK5lGeHd>6.33 - [8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> SoftNet jointly learns the model weights and adaptive non-binary soft masks at a base training session in which each mask consists of the major and minor subnetwork.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/z57WK5lGeHd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inspired by Regularized Lottery Ticket Hypothesis, which states that competitive smooth (non-binary) subnetworks exist within a dense network, we propose a few-shot class-incremental learning method referred to as Soft-SubNetworks (SoftNet). Our objective is to learn a sequence of sessions incrementally, where each session only includes a few training instances per class while preserving the knowledge of the previously learned ones. SoftNet jointly learns the model weights and adaptive non-binary soft masks at a base training session in which each mask consists of the major and minor subnetwork; the former aims to minimize catastrophic forgetting during training, and the latter aims to avoid overfitting to a few samples in each new training session. We provide comprehensive empirical validations demonstrating that our SoftNet effectively tackles the few-shot incremental learning problem by surpassing the performance of state-of-the-art baselines over benchmark datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                994. <h><a href=https://openreview.net/pdf?id=3pfNb4pZBNp>Random Laplacian Features for Learning with Hyperbolic Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tao Yu, Christopher De Sa</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3pfNb4pZBNp>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3pfNb4pZBNp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Due to its geometric properties, hyperbolic space can support high-fidelity embeddings of tree- and graph-structured data, upon which various hyperbolic networks have been developed. Existing hyperbolic networks encode geometric priors not only for the input, but also at every layer of the network. This approach involves repeatedly mapping to and from hyperbolic space, which makes these networks complicated to implement, computationally expensive to scale, and numerically unstable to train. In this paper, we propose a simpler approach: learn a hyperbolic embedding of the input, then map once from it to Euclidean space using a mapping that encodes geometric priors by respecting the isometries of hyperbolic space, and finish with a standard Euclidean network. The key insight is to use a random feature mapping via the eigenfunctions of the Laplace operator, which we show can approximate any isometry-invariant kernel on hyperbolic space. Our method can be used together with any graph neural networks: using even a linear graph model yields significant improvements in both efficiency and performance over other hyperbolic baselines in both transductive and inductive tasks. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                995. <h><a href=https://openreview.net/pdf?id=aBH_DydEvoH>Offline RL for Natural Language Generation with Implicit Language Q Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Charlie Victor Snell, Ilya Kostrikov, Yi Su, Sherry Yang, Sergey Levine</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aBH_DydEvoH>6.33 - [3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel offline RL method, implicit language Q-learning (ILQL), for use on language models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aBH_DydEvoH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language generation settings, demonstrating how it can be a more effective utility optimizer than prior approaches for end-to-end dialogue, and how it can effectively optimize high variance reward functions based on subjective judgement, such as whether to label a comment as toxic or not.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                996. <h><a href=https://openreview.net/pdf?id=JLR_B7n_Wqr>Latent Graph Inference using Product Manifolds</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haitz Sáez de Ocáriz Borde, Anees Kazi, Federico Barbero, Pietro Lio</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JLR_B7n_Wqr>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JLR_B7n_Wqr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph Neural Networks usually rely on the assumption that the graph topology is available to the network as well as optimal for the downstream task. Latent graph inference allows models to dynamically learn the intrinsic graph structure of problems where the connectivity patterns of data may not be directly accessible. In this work, we generalize the discrete Differentiable Graph Module (dDGM) for latent graph learning. The original dDGM architecture used the Euclidean plane to encode latent features based on which the latent graphs were generated. By incorporating Riemannian geometry into the model and generating more complex embedding spaces, we can improve the performance of the latent graph inference system. In particular, we propose a computationally tractable approach to produce product manifolds of constant curvature model spaces that can encode latent features of varying structure. The latent representations mapped onto the inferred product manifold are used to compute richer similarity measures that are leveraged by the latent graph learning model to obtain optimized latent graphs. Moreover, the curvature of the product manifold is learned during training alongside the rest of the network parameters and based on the downstream task, rather than it being a static embedding space. Our novel approach is tested on a wide range of datasets, and outperforms the original dDGM model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                997. <h><a href=https://openreview.net/pdf?id=mMaInr0r0c>A View From Somewhere: Human-Centric Face Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jerone Theodore Alexander Andrews, Przemyslaw Joniak, Alice Xiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mMaInr0r0c>6.33 - [5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Implicit discovery of face-varying dimensions and annotator bias by learning on a novel face similarity dataset
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mMaInr0r0c.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Few datasets contain self-identified demographic information, inferring demographic information risks introducing additional biases, and collecting and storing data on sensitive attributes can carry legal risks. Besides, categorical demographic labels do not necessarily capture all the relevant dimensions of human diversity. We propose to implicitly learn a set of continuous face-varying dimensions, without ever asking an annotator to explicitly categorize a person. We uncover the dimensions by learning on A View From Somewhere (AVFS) dataset of 638,180 human judgments of face similarity. We demonstrate the utility of our learned embedding space for predicting face similarity judgments, collecting continuous face attribute values, attribute classification, and comparative dataset diversity auditing. Moreover, using a novel conditional framework, we show that an annotator&#39;s demographics influences the \emph{importance} they place on different attributes when judging similarity, underscoring the \emph{need} for diverse annotator groups to avoid biases. Data and code are available at \url{https://github.com/SonyAI/a_view_from_somewhere}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                998. <h><a href=https://openreview.net/pdf?id=iBdwKIsg4m>f-DM: A Multi-stage Diffusion Model via Progressive Signal Transformation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Miguel Ángel Bautista, Joshua M. Susskind</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iBdwKIsg4m>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a generalized family of diffusion models that incorporates progressive signal transformation including downsampling, blurring and VAEs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iBdwKIsg4m.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models (DMs) have recently emerged as SoTA tools for generative modeling in various domains. Standard DMs can be viewed as an instantiation of hierarchical variational autoencoders (VAEs) where the latent variables are inferred from input-centered Gaussian distributions with fixed scales and variances. Unlike VAEs, this formulation constrains DMs from changing the latent spaces and
learning abstract representations. In this work, we propose f-DM, a generalized family of DMs which allows progressive signal transformation. More precisely, we extend DMs to incorporate a set of (hand-designed or learned) transformations, where the transformed input is the mean of each diffusion step. We propose a generalized formulation and derive the corresponding de-noising objective with a modified sampling algorithm. As a demonstration, we apply f-DM in image generation tasks with a range of functions, including down-sampling, blurring, and learned transformations based on the encoder of pretrained VAEs. In addition, we identify the importance of adjusting the noise levels whenever the signal is sub-sampled and propose a simple rescaling recipe. f-DM can produce high-quality samples on standard image generation benchmarks like FFHQ, AFHQ, LSUN, and ImageNet with better efficiency and semantic interpretation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                999. <h><a href=https://openreview.net/pdf?id=wsZsjOSytRA>3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ho Hin Lee, Shunxing Bao, Yuankai Huo, Bennett A. Landman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wsZsjOSytRA>6.33 - [3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a lightweight network 3D UX-Net that simulates hierarchical transformer behavior with large kernel depthwise convolution and introduce pointwise depthwise scaling to scale features with less model parameters for volumetric segmentation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wsZsjOSytRA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The recent 3D medical ViTs (e.g., SwinUNETR) achieve the state-of-the-art performances on several 3D volumetric data benchmarks, including 3D medical image segmentation. Hierarchical transformers (e.g., Swin Transformers) reintroduced several ConvNet priors and further enhanced the practical viability of adapting volumetric segmentation in 3D medical datasets. The effectiveness of hybrid approaches is largely credited to the large receptive field for non-local self-attention and the large number of model parameters. We hypothesize that volumetric ConvNets can simulate the large receptive field behavior of these learning approaches with fewer model parameters using depth-wise convolution. In this work, we propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the hierarchical transformer using ConvNet modules for robust volumetric segmentation. Specifically, we revisit volumetric depth-wise convolutions with large kernel (LK) size (e.g. starting from $7\times7\times7$) to enable the larger global receptive fields, inspired by Swin Transformer. We further substitute the multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise depth convolutions and enhance model performances with fewer normalization and activation layers, thus reducing the number of model parameters. 3D UX-Net competes favorably with current SOTA transformers (e.g. SwinUNETR) using three challenging public datasets on volumetric brain and abdominal imaging: 1) MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI Challenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice (Feta2021). We further evaluate the transfer learning capability of 3D UX-Net with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880 to 0.900). The source code with our proposed model are available at https://github.com/MASILab/3DUX-Net.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1000. <h><a href=https://openreview.net/pdf?id=dqnNW2omZL6>Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenxiao Yang, Qitian Wu, Jiahua Wang, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dqnNW2omZL6>6.33 - [8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dqnNW2omZL6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural networks (GNNs), as the de-facto model class for representation learning on graphs, are built upon the multi-layer perceptrons (MLP) architecture with additional message passing layers to allow features to flow across nodes. While conventional wisdom commonly attributes the success of GNNs to their advanced expressivity, we conjecture that this is not the main cause of GNNs&#39; superiority in node-level prediction tasks. This paper pinpoints the major source of GNNs&#39; performance gain to their intrinsic generalization capability, by introducing an intermediate model class dubbed as P(ropagational)MLP, which is identical to standard MLP in training, but then adopts GNN&#39;s architecture in testing. Intriguingly, we observe that PMLPs consistently perform on par with (or even exceed) their GNN counterparts, while being much more efficient in training.

This finding provides a new perspective for understanding the learning behavior of GNNs, and can be used as an analytic tool for dissecting various GNN-related research problems including expressivity, generalization, over-smoothing and heterophily. As an initial step to analyze PMLP, we show its essential difference to MLP at infinite-width limit lies in the NTK feature map in the post-training stage. Moreover, through extrapolation analysis (i.e., generalization under distribution shifts), we find that though most GNNs and their PMLP counterparts cannot extrapolate non-linear functions for extreme out-of-distribution data, they have greater potential to generalize to testing data near the training data support as natural advantages of the GNN architecture used for inference.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1001. <h><a href=https://openreview.net/pdf?id=fPVRcJqspu>GOGGLE: Generative Modelling for Tabular Data by Learning Relational Structure</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tennison Liu, Zhaozhi Qian, Jeroen Berrevoets, Mihaela van der Schaar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fPVRcJqspu>6.33 - [8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fPVRcJqspu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep generative models learn highly complex and non-linear representations to generate realistic synthetic data. While they have achieved notable success in computer vision and natural language processing, similar advances have been less demonstrable in the tabular domain. This is partially because generative modelling of tabular data entails a particular set of challenges, including heterogeneous relationships, limited number of samples, and difficulties in incorporating prior knowledge. Additionally, unlike their counterparts in image and sequence domain, deep generative models for tabular data almost exclusively employ fully-connected layers, which encode weak inductive biases about relationships between inputs. Real-world data generating processes can often be represented using relational structures, which encode sparse, heterogeneous relationships between variables. In this work, we learn and exploit relational structure underlying tabular data to better model variable dependence, and as a natural means to introduce regularization on relationships and include prior knowledge. Specifically, we introduce GOGGLE, an end-to-end message passing scheme that jointly learns the relational structure and corresponding functional relationships as the basis of generating synthetic samples. Using real-world datasets, we provide empirical evidence that the proposed method is effective in generating realistic synthetic data and exploiting domain knowledge for downstream tasks. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1002. <h><a href=https://openreview.net/pdf?id=R98ZfMt-jE>Efficient Discrete Multi Marginal Optimal Transport Regularization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ronak Mehta, Jeffery Kline, Vishnu Suresh Lokhande, Glenn Fung, Vikas Singh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=R98ZfMt-jE>6.33 - [6, 8, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Using a fast algorithm for computing generalized earth mover&#39;s distances, we solve practical discrete multi-marginal optimal transport problems in neural network learning applications.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/R98ZfMt-jE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Optimal transport has emerged as a powerful tool for a variety of problems in machine learning, and it is frequently used to enforce distributional constraints. In this context, existing methods often use either a Wasserstein metric, or else they apply concurrent barycenter approaches when more than two distributions are considered. In this paper, we  leverage multi-marginal optimal transport (MMOT), where we take advantage of a procedure that computes a generalized earth mover&#39;s distance as a sub-routine. We show that not only is our algorithm computationally more efficient compared to other barycentric-based distance methods, but it has the additional advantage that gradients used for backpropagation can be efficiently computed during the forward pass computation itself, which leads to substantially faster model training. We provide technical details about this new regularization term and its properties, and we present experimental demonstrations of faster runtimes when compared to standard Wasserstein-style methods. Finally, on a range of experiments designed to assess effectiveness at enforcing fairness, we demonstrate our method compares well with alternatives.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1003. <h><a href=https://openreview.net/pdf?id=cB4N3G5udUS>RandProx: Primal-Dual Optimization Algorithms with Randomized Proximal Updates</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Laurent Condat, Peter Richtárik</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cB4N3G5udUS>6.33 - [6, 10, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cB4N3G5udUS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Proximal splitting algorithms are well suited to solving large-scale nonsmooth optimization problems, in particular those arising in machine learning. We propose a new primal–dual algorithm, in which the dual update is randomized; equivalently, the proximity operator of one of the function in the problem is replaced by a stochastic oracle. For instance, some randomly chosen dual variables, instead of all, are updated at each iteration. Or, the proximity operator of a function is called with some small probability only. A nonsmooth variance-reduction technique is implemented so that the algorithm finds an exact minimizer of the general problem involving smooth and nonsmooth functions, possibly composed with linear operators. We derive linear convergence results in presence of strong convexity; these results are new even in the deterministic case, when our algorithms reverts to the recently proposed Primal–Dual Davis–Yin algorithm. Some randomized algorithms of the literature are also recovered as particular cases (e.g., Point-SAGA). But our randomization technique is general and encompasses many unbiased mechanisms beyond sampling and probabilistic updates, including compression. Since the convergence speed depends on the slowest among the primal and dual contraction mechanisms, the iteration complexity might remain the same when randomness is used. On the other hand, the computation complexity can be significantly reduced. Overall, randomness helps getting faster algorithms. This has long been known for stochastic-gradient-type algorithms, and our work shows that this fully applies in the more general primal–dual setting as well.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1004. <h><a href=https://openreview.net/pdf?id=Dzmd-Cc8OI>How Does Semi-supervised Learning with Pseudo-labelers Work? A Case Study</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yiwen Kou, Zixiang Chen, Yuan Cao, Quanquan Gu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Dzmd-Cc8OI>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Dzmd-Cc8OI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Semi-supervised learning is a popular machine learning paradigm that utilizes a large amount of unlabeled data as well as a small amount of labeled data to facilitate learning tasks. While semi-supervised learning has achieved great success in training neural networks, its theoretical understanding remains largely open. In this paper, we aim to theoretically understand a semi-supervised learning approach based on pre-training and linear probing. In particular, the semi-supervised learning approach we consider first trains a two-layer neural network based on the unlabeled data with the help of pseudo-labelers. Then it linearly probes the pre-trained network on a small amount of labeled data. We prove that, under a certain toy data generation model and two-layer convolutional neural network, the semisupervised learning approach can achieve nearly zero test loss, while a neural network directly trained by supervised learning on the same amount of labeled data can only achieve constant test loss. Through this case study, we demonstrate a separation between semi-supervised learning and supervised learning in terms of test loss provided the same amount of labeled data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1005. <h><a href=https://openreview.net/pdf?id=JtC6yOHRoJJ>Human-level Atari 200x faster</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Steven Kapturowski, Víctor Campos, Ray Jiang, Nemanja Rakicevic, Hado van Hasselt, Charles Blundell, Adria Puigdomenech Badia</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JtC6yOHRoJJ>6.33 - [8, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an RL agent &#39;MEME&#39; that achieves human-level performance on all 57 Atari games within 390M environment frames, only 1/200 of the experience required by Agent57.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JtC6yOHRoJJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The task of building general agents that perform well over a wide range of tasks has been an important goal in reinforcement learning since its inception. The problem has been subject of research of a large body of work, with performance frequently measured by observing scores over the wide range of environments contained in the Atari 57 benchmark. Agent57 was the first agent to surpass the human benchmark on all 57 games, but this came at the cost of poor data-efficiency, requiring nearly 80 billion frames of experience to achieve. Taking Agent57 as a starting point, we employ a diverse set of strategies to achieve a 200-fold reduction of experience needed to outperform the human baseline, within our novel agent MEME. We investigate a range of instabilities and bottlenecks we encountered while reducing the data regime, and propose effective solutions to build a more robust and efficient agent. We also demonstrate competitive performance with high-performing methods such as Muesli and MuZero. Our contributions aim to achieve faster propagation of learning signals related to rare events, stabilize learning under differing value scales, improve the neural network architecture, and make updates more robust under a rapidly-changing policy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1006. <h><a href=https://openreview.net/pdf?id=Ovnwe_sDQW>BC-IRL: Learning Generalizable Reward Functions from Demonstrations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrew Szot, Amy Zhang, Dhruv Batra, Zsolt Kira, Franziska Meier</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Ovnwe_sDQW>6.33 - [8, 8, 3] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Ovnwe_sDQW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generalization settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1007. <h><a href=https://openreview.net/pdf?id=GPTjnA57h_3>Free Lunch for Domain Adversarial Training: Environment Label Smoothing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>YiFan Zhang, xue wang, Jian Liang, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GPTjnA57h_3>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose to smooth environment label for domain adversarial training methods, which is experimentally and theoretically shown able to improve training stability, local convergence, and robustness to noisy labels.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GPTjnA57h_3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A fundamental challenge for machine learning models is how to generalize learned models for out-of-distribution (OOD) data. Among various approaches, exploiting invariant features by Domain Adversarial Training (DAT) received widespread attention. Despite its success, we observe training instability from DAT, mostly due to over-confident domain discriminator and environment label noise. To address this issue, we proposed Environment Label Smoothing (ELS), which encourages the discriminator to output soft probability, which thus reduces the confidence of the discriminator and alleviates the impact of noisy environment labels. We demonstrate, both experimentally and theoretically, that ELS can improve training stability, local convergence, and robustness to noisy environment labels. By incorporating ELS with DAT methods, we are able to yield state-of-art results on a wide range of domain generalization/adaptation tasks, particularly when the environment labels are highly noisy. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1008. <h><a href=https://openreview.net/pdf?id=OAsXFPBfTBh>Autoregressive Conditional Neural Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wessel Bruinsma, Stratis Markou, James Requeima, Andrew Y. K. Foong, Tom Andersson, Anna Vaughan, Anthony Buonomo, Scott Hosking, Richard E Turner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OAsXFPBfTBh>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OAsXFPBfTBh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Conditional neural processes (CNPs; Garnelo et al., 2018a) are attractive meta-learning models which produce well-calibrated predictions and are trainable via a simple maximum likelihood procedure. Although CNPs have many advantages, they are unable to model dependencies in their predictions. Various works propose solutions to this, but these come at the cost of either requiring approximate inference or being limited to Gaussian predictions. In this work, we instead propose to change how CNPs are deployed at test time, without any modifications to the model or training procedure. Instead of making predictions independently for every target point, we autoregressively define a joint predictive distribution using the chain rule of probability, taking inspiration from the neural autoregressive density estimator (NADE) literature. We show that this simple procedure allows factorised Gaussian CNPs to model highly dependent, non-Gaussian predictive distributions. Perhaps surprisingly, in an extensive range of tasks with synthetic and real data, we show that CNPs in autoregressive (AR) mode not only significantly outperform non-AR CNPs, but are also competitive with more sophisticated models that are significantly more computationally expensive and challenging to train. This performance is remarkable given that AR CNPs are not trained to model joint dependencies. Our work provides an example of how ideas from neural distribution estimation can benefit neural processes, and motivates research into the AR deployment of other neural process models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1009. <h><a href=https://openreview.net/pdf?id=w2P7fMy_RH>Expressive Monotonic Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Niklas Nolte, Ouail Kitouni, Mike Williams</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=w2P7fMy_RH>6.33 - [3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper introduces a new method to universally approximate lipschitz functions that are monotonic in any subset of their inputs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/w2P7fMy_RH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce monotonic dependence, and is highly expressive. We show how the algorithm is used to train powerful, robust, and interpretable discriminators that achieve competitive performance compared to current state-of-the-art methods across various benchmarks, from social applications to the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1010. <h><a href=https://openreview.net/pdf?id=kQxry8Z6Fd9>Statistical Guarantees for Consensus Clustering</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhixin Zhou, Gautam Dudeja, Arash A Amini</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kQxry8Z6Fd9>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose spectral algorithms for aggregating labels from multiple clustering algorithms without knowing the optimal matching between clusters, and we provide theoretical performance bounds. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kQxry8Z6Fd9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Consider the problem of clustering $n$ objects. One can apply multiple algorithms to produce $N$ potentially different clustersings of the same objects, that is, partitions of the $n$ objects into $K$ groups. Even a single randomized algorithm can output different clusterings. This often happens when one samples from the posterior of a Bayesian model, or runs multiple MCMC chains from random initializations. A natural task is then to form a consensus among these different clusterings. The challenge in an unsupervised setting is that the optimal matching between clusters of different inputs is unknown. We model this problem as finding a barycenter (also known as Fr\&#39;{e}chet mean) relative to the misclassification rate. We show that by lifting the problem to the space of association matrices, one can derive aggregation algorithms that circumvent the knowledge of the optimal matchings. We analyze the statistical performance of aggregation algorithms under a stochastic label perturbation model, and show that a $K$-means type algorithm followed by a local refinement step can achieve near optimal performance, with a rate that decays exponentially fast in $N$. Numerical experiments show the effectiveness of the proposed methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1011. <h><a href=https://openreview.net/pdf?id=FVW7Mi2ph6C>PAC Reinforcement Learning for Predictive State Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenhao Zhan, Masatoshi Uehara, Wen Sun, Jason D. Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FVW7Mi2ph6C>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> PAC learning for PSRs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FVW7Mi2ph6C.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper we study online Reinforcement Learning (RL) in partially observable dynamical systems. We focus on the Predictive State Representations (PSRs) model, which is an expressive model that captures other well-known models such as Partially Observable Markov Decision Processes (POMDP). PSR represents the states using a set of predictions of future observations and is defined entirely using observable quantities. We develop a novel model-based algorithm for PSRs that can learn a near optimal policy in sample complexity scaling polynomially with respect to all the relevant parameters of the systems. Our algorithm naturally works with function approximation to extend to systems with potentially large state and observation spaces. We show that given a realizable model class, the sample complexity of learning the near optimal policy only scales polynomially with respect to the statistical complexity of the model class, without any explicit polynomial dependence on the size of the state and observation spaces. Notably, our work is the first work that shows polynomial sample complexities to compete with the globally optimal policy in PSRs. Finally, we demonstrate how our general theorem can be directly used to derive sample complexity bounds for special models including $m$-step weakly revealing and $m$-step decodable tabular POMDPs, POMDPs with low-rank latent transition, and POMDPs with linear emission and latent transition. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1012. <h><a href=https://openreview.net/pdf?id=CgCmwcfgEdH>PGrad: Learning Principal Gradients For Domain Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhe Wang, Jake Grigsby, Yanjun Qi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CgCmwcfgEdH>6.33 - [8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CgCmwcfgEdH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning models fail to perform when facing out-of-distribution (OOD) domains, a challenging task known as domain generalization (DG). In this work, we develop a novel DG training strategy, we call PGrad, to learn a robust gradient direction, improving models&#39; generalization ability on unseen domains.  The proposed gradient aggregates the principal directions of a sampled roll-out optimization trajectory that measures the training dynamics across all training domains. PGrad gradient design forces the DG training to ignore domain-dependent noise signals and updates all training domains with a robust direction covering main components of parameter dynamics.  We further improve PGrad via bijection-based computational refinement and directional plus length-based calibrations. Our theoretical proof connects PGrad to the spectral analysis of Hessian in training neural networks. Experiments on DomainBed and WILDS benchmarks demonstrate that our approach effectively enables robust DG optimization and leads to smoothly decreased loss curves.  Empirically, PGrad achieves competitive results across seven datasets, demonstrating its efficacy across both synthetic and real-world distributional shifts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1013. <h><a href=https://openreview.net/pdf?id=lid14UkLPd4>Learning to Extrapolate: A Transductive Approach</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aviv Netanyahu, Abhishek Gupta, Max Simchowitz, Kaiqing Zhang, Pulkit Agrawal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lid14UkLPd4>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lid14UkLPd4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning systems, especially with overparameterized deep neural networks, can generalize to novel test instances drawn from the same distribution as the training data. However, they fare poorly when evaluated on out-of-support test points. In this work, we tackle the problem of developing machine learning systems that retain the power of overparameterized function approximators while enabling extrapolation to out-of-support test points when possible. This is accomplished by noting that under certain conditions, a &#34;transductive&#34; reparameterization can convert an out-of-support extrapolation problem into a problem of within-support combinatorial generalization. We propose a simple strategy based on bilinear embeddings to enable this type of combinatorial generalization, thereby addressing the out-of-support extrapolation problem under certain conditions. We instantiate a simple, practical algorithm applicable to various supervised learning and imitation learning tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1014. <h><a href=https://openreview.net/pdf?id=q9VherQJd8_>Matching receptor to odorant with protein language and graph neural networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matej Hladiš, Maxence Lalis, Sebastien Fiorucci, Jérémie Topin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=q9VherQJd8_>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We leverage recent advances in protein representation learning and graph neural networks to predict olfactory receptor-molecule binding.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/q9VherQJd8_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Odor perception in mammals is triggered by interactions between volatile organic compounds and a subset of hundreds of proteins called olfactory receptors (ORs). Molecules activate these receptors in a complex combinatorial coding allowing mammals to discriminate a vast number of chemical stimuli. Recently, ORs have gained attention as new therapeutic targets following the discovery of their involvement in other physiological processes and diseases. To date, predicting molecule-induced activation for ORs is highly challenging since $43\%$ of ORs have no identified active compound. In this work, we combine [CLS] token from protBERT with a molecular graph and propose a tailored GNN architecture incorporating inductive biases from the protein-molecule binding. We abstract the biological process of protein-molecule activation as the injection of a molecule into a protein-specific environment. On a newly gathered dataset of $46$ $700$ OR-molecule pairs, this model outperforms state-of-the-art models on drug-target interaction prediction as well as standard GNN baselines. Moreover, by incorporating non-bonded interactions the model is able to work with mixtures of compounds. Finally, our predictions reveal a similar activation pattern for molecules within a given odor family, which is in agreement with the theory of combinatorial coding in olfaction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1015. <h><a href=https://openreview.net/pdf?id=5cFfz6yMVPU>$\mathcal{O}$-GNN: incorporating ring priors into molecular modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinhua Zhu, Kehan Wu, Bohan Wang, Yingce Xia, Shufang Xie, Qi Meng, Lijun Wu, Tao Qin, Wengang Zhou, Houqiang Li, Tie-Yan Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5cFfz6yMVPU>6.33 - [8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5cFfz6yMVPU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Cyclic compounds that contain at least one ring play an important role in drug design. Despite the recent success of molecular modeling with graph neural networks (GNNs), few models explicitly take rings in compounds into consideration, consequently limiting the expressiveness of the models. In this work, we design a new variant of GNN, ring-enhanced GNN ($\mathcal{O}$-GNN), that explicitly models rings in addition to atoms and bonds in compounds. In $\mathcal{O}$-GNN,  each ring is represented by a latent vector, which contributes to and is iteratively updated by atom and bond representations. Theoretical analysis shows that $\mathcal{O}$-GNN is able to distinguish two isomorphic subgraphs lying on different rings using only one layer while conventional graph convolutional neural networks require multiple layers to distinguish, demonstrating that $\mathcal{O}$-GNN is more expressive. Through experiments, $\mathcal{O}$-GNN shows good performance on $\bf{11}$ public datasets. In particular, it achieves state-of-the-art validation result on the PCQM4Mv1 benchmark (outperforming the previous KDDCup champion solution) and the drug-drug interaction prediction task on DrugBank. Furthermore, $\mathcal{O}$-GNN outperforms strong baselines (without modeling rings) on the molecular property prediction and retrosynthesis prediction tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1016. <h><a href=https://openreview.net/pdf?id=cA77NrVEuqn>Efficient Planning in a Compact Latent Action Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>zhengyao jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim Rocktäschel, Edward Grefenstette, Yuandong Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cA77NrVEuqn>6.33 - [8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose Trajectory Autoencoding Planner (TAP) a model-based RL method that learns a compact discrete latent action space for efficient planning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cA77NrVEuqn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Planning-based reinforcement learning has shown strong performance in tasks in discrete and low-dimensional continuous action spaces. However, planning usually brings significant computational overhead for decision making, so scaling such methods to high-dimensional action spaces remains challenging. To advance efficient planning for high-dimensional continuous control, we propose Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent action codes with a state-conditional VQ-VAE. The decoder of the VQ-VAE thus serves as a novel dynamics model that takes latent actions and current state as input and reconstructs long-horizon trajectories. During inference time, given a starting state, TAP searches over discrete latent actions to find trajectories that have both high probability under the training distribution and high predicted cumulative reward. Empirical evaluation in the offline RL setting demonstrates low decision latency which is indifferent to the growing raw action dimensionality. For Adroit robotic hand manipulation tasks with high-dimensional continuous action space, TAP surpasses existing model-based methods by a large margin and also beats strong model-free actor-critic baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1017. <h><a href=https://openreview.net/pdf?id=8jU7wy7N7mA>Supervision Complexity and its Role in Knowledge Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hrayr Harutyunyan, Ankit Singh Rawat, Aditya Krishna Menon, Seungyeon Kim, Sanjiv Kumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8jU7wy7N7mA>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a new theoretical perspective on knowledge distillation through the lens of supervision complexity -- a measure of alignment between the teacher-provided supervision and the student&#39;s neural tangent kernel.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8jU7wy7N7mA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the popularity and efficacy of knowledge distillation, there is limited understanding of why it helps. In order to study the generalization behavior of a distilled student, we propose a new theoretical framework that leverages supervision complexity: a measure of alignment between teacher-provided supervision and the student&#39;s neural tangent kernel. The framework highlights a delicate interplay among the teacher&#39;s accuracy, the student&#39;s margin with respect to the teacher predictions, and the complexity of the teacher predictions. Specifically, it provides a rigorous justification for the utility of various techniques that are prevalent in the context of distillation, such as early stopping and temperature scaling. Our analysis further suggests the use of online distillation, where a student receives increasingly more complex supervision from teachers in different stages of their training. We demonstrate efficacy of online distillation and validate the theoretical findings on a range of image classification benchmarks and model architectures.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1018. <h><a href=https://openreview.net/pdf?id=89GT-S49mGd>Function-space regularized Rényi divergences</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeremiah Birrell, Yannis Pantazis, Paul Dupuis, Luc Rey-Bellet, Markos Katsoulakis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=89GT-S49mGd>6.33 - [6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/89GT-S49mGd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a new family of regularized Rényi divergences parametrized not only by the order $\alpha$ but also by a variational function space. These new objects are defined by taking the infimal convolution of the standard Rényi divergence with the integral probability metric (IPM) associated with the chosen function space. We derive a novel dual variational representation that can be used to construct numerically tractable divergence estimators. This representation avoids risk-sensitive terms and therefore exhibits lower variance, making it well-behaved  when $\alpha&gt;1$; this addresses a notable weakness of prior approaches. We prove several properties of these new divergences, showing that they interpolate between the classical Rényi divergences and IPMs. We also study the $\alpha\to\infty$ limit, which leads to a regularized worst-case-regret and a new variational representation in the classical case. Moreover, we show that the proposed regularized Rényi divergences inherit features from IPMs such as the ability to compare distributions that are not absolutely continuous, e.g., empirical measures and distributions with low-dimensional support. We present numerical results on both synthetic and real datasets, showing the utility of these new divergences in both estimation and GAN training applications; in particular, we demonstrate significantly reduced variance and improved training performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1019. <h><a href=https://openreview.net/pdf?id=eDLwjKmtYFt>EquiMod: An Equivariance Module to Improve Visual Instance Discrimination</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexandre DEVILLERS, Mathieu Lefort</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eDLwjKmtYFt>6.33 - [8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a generic equivariance module that structures the learned latent space by learning to predict the displacement in the embedding space caused by augmentations; we show that it improves the representation of usual self-supervised methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eDLwjKmtYFt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent self-supervised visual representation methods are closing the gap with supervised learning performance. Most of these successful methods rely on maximizing the similarity between embeddings of related synthetic inputs created through data augmentations. This can be seen as a task that encourages embeddings to leave out factors modified by these augmentations, i.e. to be invariant to them. However, this only considers one side of the trade-off in the choice of the augmentations: they need to strongly modify the images to avoid simple solution shortcut learning (e.g. using only color histograms), but on the other hand, augmentations-related information may be lacking in the representations for some downstream tasks (e.g. literature shows that color is important for bird and flower classification). Few recent works proposed to mitigate this problem of using only an invariance task by exploring some form of equivariance to augmentations. This has been performed by learning additional embeddings space(s), where some augmentation(s) cause embeddings to differ, yet in a non-controlled way. In this work, we introduce EquiMod a generic equivariance module that structures the learned latent space, in the sense that our module learns to predict the displacement in the embedding space caused by the augmentations. We show that applying that module to state-of-the-art invariance models, such as BYOL and SimCLR, increases the performances on the usual CIFAR10 and ImageNet datasets. Moreover, while our model could collapse to a trivial equivariance, i.e. invariance, we observe that it instead automatically learns to keep some augmentations-related information beneficial to the representations. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1020. <h><a href=https://openreview.net/pdf?id=fR3wGCk-IXp>Language models are multilingual chain-of-thought reasoners</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fR3wGCk-IXp>6.33 - [6, 6, 6, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce the Multilingual Grade School Math (MGSM) benchmark, and analyze the multilingual multi-step reasoning abilities of large language models. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fR3wGCk-IXp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at AnonymousLink and the supplementary material.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1021. <h><a href=https://openreview.net/pdf?id=zlbci7019Z3>Error Sensitivity Modulation based Experience Replay: Mitigating Abrupt Representation Drift in Continual Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fahad Sarfraz, Elahe Arani, Bahram Zonooz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zlbci7019Z3>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel method that employs a principled mechanism for modulating the error sensitivity in a dual-memory rehearsal-based system for effective continual learning
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zlbci7019Z3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Humans excel at lifelong learning, as the brain has evolved to be robust to distribution shifts and noise in our ever-changing environment. Deep neural networks (DNNs), however, exhibit catastrophic forgetting and the learned representations drift drastically as they encounter a new task. This alludes to a different error-based learning mechanism in the brain. Unlike DNNs, where learning scales linearly with the magnitude of the error, the sensitivity to errors in the brain decreases as a function of their magnitude. To this end, we propose &#34;ESMER&#34; which employs a principled mechanism to modulate error sensitivity in a dual-memory rehearsal-based system. Concretely, it maintains a memory of past errors and uses it to modify the learning dynamics so that the model learns more from small consistent errors compared to large sudden errors. We also propose &#34;Error-Sensitive Reservoir Sampling&#34; to maintain episodic memory, which leverages the error history to pre-select low-loss samples as candidates for the buffer, which are better suited for retaining information. Empirical results show that ESMER effectively reduces forgetting and abrupt drift in representations at the task boundary by gradually adapting to the new task while consolidating knowledge. Remarkably, it also enables the model to learn under high levels of label noise, which is ubiquitous in real-world data streams.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1022. <h><a href=https://openreview.net/pdf?id=n6H86gW8u0d>TANGOS: Regularizing Tabular Neural Networks through Gradient Orthogonalization and Specialization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alan Jeffares, Tennison Liu, Jonathan Crabbé, Fergus Imrie, Mihaela van der Schaar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n6H86gW8u0d>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce TANGOS, a regularization method that orthogonalizes the gradient attribution of neurons to improve the generalization of deep neural networks on tabular data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n6H86gW8u0d.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite their success with unstructured data, deep neural networks are not yet a panacea for structured tabular data. In the tabular domain, their efficiency crucially relies on various forms of regularization to prevent overfitting and provide strong generalization performance. Existing regularization techniques include broad modelling decisions such as choice of architecture, loss functions, and optimization methods. In this work, we introduce Tabular Neural Gradient Orthogonalization and Specialization (TANGOS), a novel framework for regularization in the tabular setting built on latent unit attributions. The gradient attribution of an activation with respect to a given input feature suggests how the neuron attends to that feature, and is often employed to interpret the predictions of deep networks. In TANGOS, we take a different approach and incorporate neuron attributions directly into training to encourage orthogonalization and specialization of latent attributions in a fully-connected network. Our regularizer encourages neurons to focus on sparse, non-overlapping input features and results in a set of diverse and specialized latent units. In the tabular domain, we demonstrate that our approach can lead to improved out-of-sample generalization performance, outperforming other popular regularization methods. We provide insight into why our regularizer is effective and demonstrate that TANGOS can be applied jointly with existing methods to achieve even greater generalization performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1023. <h><a href=https://openreview.net/pdf?id=20gBzEzgtiI>Performance Bounds for Model and Policy Transfer in Hidden-parameter MDPs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haotian Fu, Jiayu Yao, Omer Gottesman, Finale Doshi-Velez, George Konidaris</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=20gBzEzgtiI>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/20gBzEzgtiI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the Hidden-Parameter MDP (HiP-MDP) framework, a family of reinforcement learning tasks is generated by varying hidden parameters specifying the dynamics and reward function for each individual task. HiP-MDP is a natural model for families of tasks in which meta- and lifelong-reinforcement learning approaches can succeed. Given a learned context encoder that infers the hidden parameters from previous experience, most existing algorithms fall into two categories: $\textit{model transfer}$ and $\textit{policy transfer}$, depending on which function the hidden parameters are used to parameterize. We characterize the robustness of model and policy transfer algorithms with respect to hidden parameter estimation error. We first show that the value function of HiP-MDPs is Lipschitz continuous under certain conditions. We then derive regret bounds for both settings through the lens of Lipschitz continuity. Finally, we empirically corroborate our theoretical analysis by experimentally varying the hyper-parameters governing the Lipschitz constants of two continuous control problems; the resulting performance is consistent with our predictions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1024. <h><a href=https://openreview.net/pdf?id=bjPPypbLre>Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qizhang Li, Yiwen Guo, Wangmeng Zuo, Hao Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bjPPypbLre>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bjPPypbLre.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The transferability of adversarial examples across deep neural networks (DNNs) is the crux of many black-box attacks. Many prior efforts have been devoted to improving the transferability via increasing the diversity in inputs of some substitute models. In this paper, by contrast, we opt for the diversity in substitute models and advocate to attack a Bayesian model for achieving desirable transferability. Deriving from the Bayesian formulation, we develop a principled strategy for possible finetuning, which can be combined with many off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive experiments have been conducted to verify the effectiveness of our method, on common benchmark datasets, and the results demonstrate that our method outperforms recent state-of-the-arts by large margins (roughly 19% absolute increase in average attack success rate on ImageNet), and, by combining with these recent methods, further performance gain can be obtained. Our code: https://github.com/qizhangli/MoreBayesian-attack. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1025. <h><a href=https://openreview.net/pdf?id=wtcud6HroZr>Learning to Decompose Visual Features with Latent Textual Prompts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Feng Wang, Manling Li, Xudong Lin, Hairong Lv, Alex Schwing, Heng Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wtcud6HroZr>6.33 - [5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wtcud6HroZr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent advances in pre-training vision-language models like CLIP have shown great potential in learning transferable visual representations. Nonetheless, for downstream inference, CLIP-like models suffer from either 1) degraded accuracy and robustness in the case of inaccurate text descriptions during retrieval-based inference (the challenge for zero-shot protocol); or 2) breaking the well-established vision-language alignment (the challenge for linear probing). To address them, we propose Decomposed Feature Prompting (DeFo). DeFo leverages a flexible number of learnable embeddings as textual input while maintaining the vision-language dual-model architecture, which enables the model to learn decomposed visual features with the help of feature-level textual prompts. We further use an additional linear layer to perform classification, allowing a scalable size of language inputs. Our empirical study shows DeFo&#39;s significance in improving the vision-language models. For example, DeFo obtains 73.2% test accuracy on ImageNet with a ResNet-50 backbone without tuning any pretrained weights of both the vision and language encoder, outperforming zero-shot CLIP by a large margin of 15.0%, and outperforming state-of-the-art vision-language prompt tuning method by 7.6%.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1026. <h><a href=https://openreview.net/pdf?id=dRjWsd3gwsm>MixPro: Data Augmentation with MaskMix and Progressive Attention Labeling for Vision Transformer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qihao Zhao, Yangyu Huang, Wei Hu, Fan Zhang, Jun Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dRjWsd3gwsm>6.33 - [8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dRjWsd3gwsm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The recently proposed data augmentation TransMix employs attention labels to help visual transformers (ViT) achieve better robustness and performance. However, TransMix is deficient in two aspects: 1) The image cropping method of TransMix may not be suitable for vision transformer. 2) At the early stage of training, the model produces unreliable attention maps. TransMix uses unreliable attention maps to compute mixed attention labels that can affect the model. To address the aforementioned issues, we propose MaskMix and Progressive Attention Labeling (PAL) in image and label space, respectively. In detail, from the perspective of image space, we design MaskMix, which mixes two images based on a patch-like grid mask. In particular, the size of each mask patch is adjustable and is a multiple of the image patch size, which ensures each image patch comes from only one image and contains more global contents. From the perspective of label space, we design PAL, which utilizes a progressive factor to dynamically re-weight the attention weights of the mixed attention label. Finally, we combine MaskMix and Progressive Attention Labeling as our new data augmentation method, named MixPro. The experimental results show that our method can improve various ViT-based models at scales on ImageNet classification (73.8% top-1 accuracy based on DeiT-T for 300 epochs). After being pre-trained with MixPro on ImageNet, the ViT-based models also demonstrate better transferability to semantic segmentation, object detection, and instance segmentation. Furthermore, compared to TransMix, MixPro also shows stronger robustness on several benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1027. <h><a href=https://openreview.net/pdf?id=AjC0KBjiMu>Contrastive Learning Can Find An Optimal Basis For Approximately View-Invariant Functions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel D. Johnson, Ayoub El Hanchi, Chris J. Maddison</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AjC0KBjiMu>6.33 - [5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that existing contrastive objectives approximate a &#34;positive-pair kernel&#34;, and that applying Kernel PCA produces a representation that is provably optimal for supervised learning of functions that assign similar values to positive pairs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AjC0KBjiMu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contrastive learning is a powerful framework for learning self-supervised representations that generalize well to downstream supervised tasks. We show that multiple existing contrastive learning methods can be reinterpeted as learning kernel functions that approximate a fixed *positive-pair kernel*. We then prove that a simple representation obtained by combining this kernel with PCA provably minimizes the worst-case approximation error of linear predictors, under a straightforward assumption that positive pairs have similar labels. Our analysis is based on a decomposition of the target function in terms of the eigenfunctions of a positive-pair Markov chain, and a surprising equivalence between these eigenfunctions and the output of Kernel PCA. We give generalization bounds for downstream linear prediction using our kernel PCA representation, and show empirically on a set of synthetic tasks that applying kernel PCA to contrastive learning models can indeed approximately recover the Markov chain eigenfunctions, although the accuracy depends on the kernel parameterization as well as on the augmentation strength.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1028. <h><a href=https://openreview.net/pdf?id=b3itJyarLM0>Distributed Extra-gradient with Optimal Complexity and Communication Guarantees</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ali Ramezani-Kebrya, Kimon Antonakopoulos, Igor Krawczuk, Justin Deschenaux, Volkan Cevher</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b3itJyarLM0>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose quantized generalized extra-gradient, which is an unbiased and adaptive compression method tailored to a generic unifying framework for solving variational inequality problems.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b3itJyarLM0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider monotone variational inequality (VI) problems in multi-GPU  settings where multiple processors/workers/clients have access to local stochastic dual vectors. This setting  includes a broad range of important problems from distributed convex minimization to min-max and games. Extra-gradient, which is a de facto algorithm  for monotone VI problems, has not been designed to be communication-efficient. To this end, we propose a quantized generalized extra-gradient (Q-GenX), which is an unbiased and adaptive compression method tailored to solve VIs. We provide an adaptive step-size rule, which  adapts to the respective noise profiles at hand and achieve a fast rate of  ${\cal O}(1/T)$ under relative noise, and an order-optimal ${\cal O}(1/\sqrt{T})$ under absolute noise  and show distributed training accelerates convergence. Finally, we validate our theoretical results by providing real-world experiments and training generative adversarial networks on multiple GPUs.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1029. <h><a href=https://openreview.net/pdf?id=Lt8bMlhiwx2>DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wei Li, Linchao Zhu, Longyin Wen, Yi Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Lt8bMlhiwx2>6.33 - [8, 6, 6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple framework with a lightweight visual-aware language decoder for zero-shot captioning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Lt8bMlhiwx2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large-scale pre-trained multi-modal models (e.g., CLIP) demonstrate strong zero-shot transfer capability in many discriminative tasks, e.g., image classification. Their adaptation to zero-shot image-conditioned text generation tasks has drawn increasing interest. Prior arts approach to zero-shot captioning by either utilizing the existing large language models (e.g., GPT-2) or pre-training the encoder-decoder network in an end-to-end manner. However, the large language models may not generate sensible descriptions due to the task discrepancy between captioning and language modeling, while the end-to-end pre-training requires paired data and extensive computational resources. In this work, we propose a simple framework, named DeCap, for zero-shot captioning. We introduce a lightweight visual-aware language decoder. This decoder is both data-efficient and computation-efficient: 1) it only requires the \textit{text} data for training, easing the burden on the collection of paired data. 2) it does not require end-to-end training. When trained with text-only data, the decoder takes the text embedding extracted from the off-the-shelf CLIP encoder as a prefix embedding. The challenge is that the decoder is trained on the text corpus but at the inference stage, it needs to generate captions based on visual inputs. Though the CLIP text embedding and the visual embedding are correlated, the \textit{modality gap} issue is widely observed in multi-modal contrastive models that prevents us from directly taking the visual embedding as the prefix embedding. We propose a training-free mechanism to reduce the modality gap. We project the visual embedding into the CLIP text embedding space, while the projected embedding retains the information of the visual input. Taking the projected embedding as the prefix embedding, the decoder generates high-quality descriptions that match the visual input. The experiments show that DeCap outperforms other zero-shot captioning methods and unpaired captioning methods by a large margin on the typical image captioning benchmarks, i.e., MSCOCO and NoCaps. We apply DeCap to video captioning and achieve state-of-the-art zero-shot performance on MSR-VTT and ActivityNet-Captions. The code is available at https://github.com/dhg-wei/DeCap.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1030. <h><a href=https://openreview.net/pdf?id=Fh97BDaR6I>On The Specialization of Neural Modules</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Devon Jarvis, Richard Klein, Benjamin Rosman, Andrew M Saxe</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Fh97BDaR6I>6.33 - [8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use the linear neural networks framework to mathematically study the ability of neural modules to specialize and facilitate systematic generalization in modular network architectures.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Fh97BDaR6I.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A number of machine learning models have been proposed with the goal of achieving systematic generalization: the ability to reason about new situations by combining aspects of previous experiences. These models leverage compositional architectures which aim to learn specialized modules dedicated to structures in a task that can be composed to solve novel problems with similar structures. While the compositionality of these architectures is guaranteed by design, the modules specializing is not. Here we theoretically study the ability of network modules to specialize to useful structures in a dataset and achieve systematic generalization. To this end we introduce a minimal space of datasets motivated by practical systematic generalization benchmarks. From this space of datasets we present a mathematical definition of systematicity and study the learning dynamics of linear neural modules when solving components of the task. Our results shed light on the difficulty of module specialization, what is required for modules to successfully specialize, and the necessity of modular architectures to achieve systematicity. Finally, we confirm that the theoretical results in our tractable setting generalize to more complex datasets and non-linear architectures.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1031. <h><a href=https://openreview.net/pdf?id=Yt-yM-JbYFO>Efficient Offline Policy Optimization with a Learned Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zichen Liu, Siyi Li, Wee Sun Lee, Shuicheng YAN, Zhongwen Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Yt-yM-JbYFO>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a regularized one-step model-based method that outperforms MuZero Unplugged on Atari benchmark.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Yt-yM-JbYFO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    MuZero Unplugged presents a promising approach for offline policy learning from logged data. It conducts Monte-Carlo Tree Search (MCTS) with a learned model and leverages Reanalyze algorithm to learn purely from offline data. For good performance, MCTS requires accurate learned models and a large number of simulations, thus costing huge computing time. This paper investigates a few hypotheses where MuZero Unplugged may not work well under the offline RL settings, including 1) learning with limited data coverage; 2) learning from offline data of stochastic environments; 3) improperly parameterized models given the offline data; 4) with a low compute budget. We propose to use a regularized one-step look-ahead approach to tackle the above issues. Instead of planning with the expensive MCTS, we use the learned model to construct an advantage estimation based on a one-step rollout. Policy improvements are towards the direction that maximizes the estimated advantage with regularization of the dataset. We conduct extensive empirical studies with BSuite environments to verify the hypotheses and then run our algorithm on the RL Unplugged Atari benchmark. Experimental results show that our proposed approach achieves stable performance even with an inaccurate learned model. On the large-scale Atari benchmark, the proposed method outperforms MuZero Unplugged by 43%. Most significantly, it uses only 5.6% wall-clock time (i.e., 1 hour) compared to MuZero Unplugged (i.e., 17.8 hours) to achieve a 150% IQM normalized score with the same hardware and software stacks. Our implementation is open-sourced at https://github.com/sail-sg/rosmo.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1032. <h><a href=https://openreview.net/pdf?id=8JsaP7j1cL0>Correlative Information Maximization Based Biologically Plausible Neural Networks for Correlated Source Separation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bariscan Bozkurt, Ateş İsfendiyaroğlu, Cengiz Pehlevan, Alper Tunga Erdogan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8JsaP7j1cL0>6.33 - [6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes biologically plausible neural networks for blind separation of correlated sources exploiting prior domain assumptions via an information maximization criterion.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8JsaP7j1cL0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The brain effortlessly extracts latent causes of stimuli, but how it does this at the network level remains unknown. Most prior attempts at this problem proposed neural networks that implement independent component analysis, which works under the limitation that latent elements are mutually independent. Here, we relax this limitation and propose a biologically plausible neural network that extracts correlated latent sources by exploiting information about their domains. To derive this network, we choose maximum correlative information transfer from inputs to outputs as the separation objective under the constraint that the outputs are restricted to their presumed sets. The online formulation of this optimization problem naturally leads to neural networks with local learning rules. Our framework incorporates infinitely many source domain choices and flexibly models complex latent structures. Choices of simplex or polytopic source domains result in networks with piecewise-linear activation functions. We provide numerical examples to demonstrate the superior correlated source separation capability for both synthetic and natural sources.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1033. <h><a href=https://openreview.net/pdf?id=kqHkCVS7wbj>Decision S4: Efficient Sequence-Based RL via State Spaces Layers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, Lior Wolf</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kqHkCVS7wbj>6.33 - [5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Replacing transformers with state-space layers for RL modeling. Also extended to on-policy training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kqHkCVS7wbj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, sequence learning methods have been applied to the problem of off-policy
Reinforcement Learning, including the seminal work on Decision Transformers,
which employs transformers for this task. Since transformers are parameter-heavy,
cannot benefit from history longer than a fixed window size, and are not computed
using recurrence, we set out to investigate the suitability of the S4 family of
models, which are based on state-space layers and have been shown to outperform
transformers, especially in modeling long-range dependencies. In this work, we
present two main algorithms: (i) an off-policy training procedure that works with
trajectories, while still maintaining the training efficiency of the S4 model. (ii) An
on-policy training procedure that is trained in a recurrent manner, benefits from
long-range dependencies, and is based on a novel stable actor-critic mechanism.
Our results indicate that our method outperforms multiple variants of decision
transformers, as well as the other baseline methods on most tasks, while reducing
the latency, number of parameters, and training time by several orders of magnitude,
making our approach more suitable for real-world RL
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1034. <h><a href=https://openreview.net/pdf?id=0v4VkCSkHNm>Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sasha Salter, Kristian Hartikainen, Walter Goodwin, Ingmar Posner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0v4VkCSkHNm>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce &#39;Attentive Priors for Expressive and Transferable Skills&#39; (APES), a hierarchical KL-regularized skill transfer method that automates the choice of information asymmetry thereby maximising transfer benefits.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0v4VkCSkHNm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Transferable Skills (APES), a hierarchical KL-regularized method, heavily benefiting from both priors and hierarchy. Unlike existing approaches, APES automates the choice of asymmetry by learning it in a data-driven, domain-dependent, way based on our expressivity-transferability theorems. Experiments over complex transfer domains of varying levels of extrapolation and sparsity, such as robot block stacking, demonstrate the criticality of the correct asymmetric choice, with APES drastically outperforming previous methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1035. <h><a href=https://openreview.net/pdf?id=vdv6CmGksr0>Learning differentiable solvers for systems with hard constraints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Geoffrey Négiar, Michael W. Mahoney, Aditi Krishnapriyan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vdv6CmGksr0>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a method to solve partial differential equations (PDEs) through enforcing constraints in neural networks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vdv6CmGksr0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce a practical method to enforce partial differential equation (PDE) constraints for functions defined by neural networks (NNs), with a high degree of accuracy and up to a desired tolerance. We develop a differentiable PDE-constrained layer that can be incorporated into any NN architecture. Our method leverages differentiable optimization and the implicit function theorem to effectively enforce physical constraints. Inspired by dictionary learning, our model learns a family of functions, each of which defines a mapping from PDE parameters to PDE solutions. At inference time, the model finds an optimal linear combination of the functions in the learned family by solving a PDE-constrained optimization problem. Our method provides continuous solutions over the domain of interest that accurately satisfy desired physical constraints. Our results show that incorporating hard constraints directly into the NN architecture achieves much lower test error when compared to training on an unconstrained objective.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1036. <h><a href=https://openreview.net/pdf?id=Loek7hfb46P>Fast Sampling of Diffusion Models with Exponential Integrator</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qinsheng Zhang, Yongxin Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Loek7hfb46P>6.25 - [5, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Training-free acceleration for diffusion model, 4.17 FID with 10 NFEs on CIFAR10
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Loek7hfb46P.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The past few years have witnessed the great success of Diffusion models~(DMs) in generating high-fidelity samples in generative modeling tasks. A major limitation of the DM is its notoriously slow sampling procedure which normally requires hundreds to thousands of time discretization steps of the learned diffusion process to reach the desired accuracy. Our goal is to develop a fast sampling method for DMs with a much less number of steps while retaining high sample quality. To this end, we systematically analyze the sampling procedure in DMs and identify key factors that affect the sample quality, among which the method of discretization is most crucial. By carefully examining the learned diffusion process, we propose Diffusion Exponential Integrator Sampler~(DEIS). It is based on the Exponential Integrator designed for discretizing ordinary differential equations (ODEs) and leverages a semilinear structure of the learned diffusion process to reduce the discretization error. The proposed method can be applied to any DMs and can generate high-fidelity samples in as few as 10 steps. Moreover, by directly using pre-trained DMs, we achieve state-of-art sampling performance when the number of score function evaluation~(NFE) is limited,  e.g., 4.17 FID with 10 NFEs, 2.86 FID with only 20 NFEs on CIFAR10.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1037. <h><a href=https://openreview.net/pdf?id=loIfC8WHevK>Characteristic Neural Ordinary Differential Equation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xingzi Xu, Ali Hasan, Khalil Elkhalil, Jie Ding, Vahid Tarokh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=loIfC8WHevK>6.25 - [8, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/loIfC8WHevK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose Characteristic-Neural Ordinary Differential Equations (C-NODEs), a framework for extending Neural Ordinary Differential Equations (NODEs) beyond ODEs. While NODE models the evolution of latent variables as the solution to an ODE, C-NODE models the evolution of the latent variables as the solution of a family of first-order partial differential equations (PDEs) along curves on which the PDEs reduce to ODEs, referred to as characteristic curves. This reduction along characteristic curves allows for analyzing PDEs through standard techniques used for ODEs, in particular the adjoint sensitivity method. We also derive C-NODE-based continuous normalizing flows, which describe the density evolution of latent variables along multiple dimensions. Empirical results demonstrate the improvements provided by the proposed method for irregularly sampled time series prediction on MuJoCo, PhysioNet, and Human Activity datasets and classification and density estimation on CIFAR-10, SVHN, and MNIST datasets given a similar computational budget as the existing NODE methods.
The results also provide empirical evidence that the learned curves improve the system efficiency using a lower number of parameters and function evaluations compared with those of the baselines. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1038. <h><a href=https://openreview.net/pdf?id=EQfeudmWLQ>TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyesu Lim, Byeonggeun Kim, Jaegul Choo, Sungha Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EQfeudmWLQ>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a test-time batch normalization method, which interpolates source and current batch statistics considering each layer&#39;s domain-shift sensitivity level that shows robust performance over various realistic evaluation scenarios..
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EQfeudmWLQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modified batch normalization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from the source data, i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degradation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous methods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adaptation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1039. <h><a href=https://openreview.net/pdf?id=6ZajpxqTlQ>Generalize Learned Heuristics to Solve Large-scale Vehicle Routing Problems in Real-time</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qingchun Hou, Jingwei Yang, Yiqiang Su, Xiaoqing Wang, Yuming Deng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6ZajpxqTlQ>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Propose a zero-shot method to generalize the data-driven heuristics trained on small-scale VRPs to solve large-scale VRPs in real-time
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6ZajpxqTlQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large-scale Vehicle Routing Problems (VRPs) are widely used in logistics, transportation, supply chain, and robotic systems. Recently, data-driven VRP heuristics are proposed to generate real-time VRP solutions with up to 100 nodes. Despite this progress, current heuristics for large-scale VRPs still face three major challenges: 1) Difficulty in generalizing the heuristics learned on small-scale VRPs to large-scale VRPs without retraining; 2) Challenge in generating real-time solutions for large-scale VRPs; 3) Difficulty in embedding global constraints into learned heuristics. We contribute in the three directions: We propose a Two-stage Divide Method (TAM) to generate sub-route sequence rather than node sequence for generalizing the heuristics learned on small-scale VRPs to solve large-scale VRPs in real-time. A  two-step reinforcement learning method with new reward and padding techniques is proposed to train our TAM.  A global mask function is proposed to keep the global constraints satisfied when dividing a large-scale VRP into several small-scale Traveling Salesman Problems (TSPs). As result, we can solve the small-scale TSPs in parallel quickly. The experiments on synthetic and real-world large-scale VRPs show our method could generalize the learned heuristics trained on datasets of VRP 100 to solve VRPs with over 5000 nodes in real-time while keeping the solution quality better than data-driven heuristics and competitive with traditional heuristics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1040. <h><a href=https://openreview.net/pdf?id=KwmPfARgOTD>Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yi-Lun Liao, Tess Smidt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KwmPfARgOTD>6.25 - [8, 6, 6, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an equivariant graph neural network based on Transformer networks and propose a novel attention mechanism, which improves upon self-attention in typical Transformers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KwmPfARgOTD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite their widespread success in various domains, Transformer networks have yet to perform well across datasets in the domain of 3D atomistic graphs such as molecules even when 3D-related inductive biases like translational invariance and rotational equivariance are considered. In this paper, we demonstrate that Transformers can generalize well to 3D atomistic graphs and present Equiformer, a graph neural network leveraging the strength of Transformer architectures and incorporating SE(3)/E(3)-equivariant features based on irreducible representations (irreps). First, we propose a simple and effective architecture by only replacing original operations in Transformers with their equivariant counterparts and including tensor products. Using equivariant operations enables encoding equivariant information in channels of irreps features without complicating graph structures. With minimal modifications to Transformers, this architecture has already achieved strong empirical results. Second, we propose a novel attention mechanism called equivariant graph attention, which improves upon typical attention in Transformers through replacing dot product attention with multi-layer perceptron attention and including non-linear message passing. With these two innovations, Equiformer achieves competitive results to previous models on QM9, MD17 and OC20 datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1041. <h><a href=https://openreview.net/pdf?id=cyg2YXn_BqF>Efficiently Controlling Multiple Risks with Pareto Testing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bracha Laufer-Goldshtein, Adam Fisch, Regina Barzilay, Tommi S. Jaakkola</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cyg2YXn_BqF>6.25 - [5, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents a statistically efficient strategy for performing multiple hypothesis testing in order to find risk-controlling model configurations that are also useful with respect to arbitrary auxiliary objectives. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cyg2YXn_BqF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning applications frequently come with multiple diverse objectives and constraints that can change over time. Accordingly, trained models can be tuned with sets of hyper-parameters that affect their predictive behavior (e.g., their run-time efficiency versus error rate). As the number of constraints and hyper-parameter dimensions grow, naively selected settings may lead to sub-optimal and/or unreliable results. We develop an efficient method for calibrating models such that their predictions provably satisfy multiple explicit and simultaneous statistical guarantees (e.g., upper-bounded error rates), while also optimizing any number of additional, unconstrained objectives (e.g., total run-time cost). Building on recent results in distribution-free, finite-sample risk control for general losses, we propose Pareto Testing: a two-stage process which combines multi-objective optimization with multiple hypothesis testing. The optimization stage constructs a set of promising combinations on the Pareto frontier. We then apply statistical testing to this frontier only to identify configurations that have (a) high utility with respect to our objectives, and (b) guaranteed risk levels with respect to our constraints, with specifiably high probability. We demonstrate the effectiveness of our approach to reliably accelerate the execution of large-scale Transformer models in natural language processing (NLP) applications. In particular, we show how Pareto Testing can be used to dynamically configure multiple inter-dependent model attributes—including the number of layers computed before exiting, number of attention heads pruned, or number of text tokens considered—to simultaneously control and optimize various accuracy and cost metrics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1042. <h><a href=https://openreview.net/pdf?id=hH36JeQZDaO>Generating Sequences by Learning to Self-Correct</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hH36JeQZDaO>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hH36JeQZDaO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that  Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1043. <h><a href=https://openreview.net/pdf?id=eWtMdr6yCmL>Trading Information between Latents in Hierarchical Variational Autoencoders</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tim Z. Xiao, Robert Bamler</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eWtMdr6yCmL>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We generalize the rate/distortion theory of VAEs and analyze both theoeretically and analytically how manipulating each individual layer&#39;s rate affects performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eWtMdr6yCmL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Variational Autoencoders (VAEs) were originally motivated as probabilistic generative models in which one performs approximate Bayesian inference. The proposal of $\beta$-VAEs breaks this interpretation and generalizes VAEs to application domains beyond generative modeling (e.g., representation learning, clustering, or lossy data compression) by introducing an objective function that allows practitioners to trade off between the information content (&#34;bit rate&#34;) of the latent representation and the distortion of reconstructed data. In this paper, we reconsider this rate/distortion trade-off in the context of hierarchical VAEs, i.e., VAEs with more than one layer of latent variables. We propose a method to control each layer&#39;s contribution to the rate independently. We identify the most general class of inference models to which our proposed method is applicable, and we derive theoretical bounds on the performance of downstream tasks as functions of the individual layers&#39; rates. Our experiments demonstrate that the proposed method allows us to better tune hierarchical VAEs for a diverse set of practical use cases.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1044. <h><a href=https://openreview.net/pdf?id=6PIrhAx1j4i>Understanding DDPM Latent Codes Through Optimal Transport</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Valentin Khrulkov, Gleb Ryzhakov, Andrei Chertkov, Ivan Oseledets</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6PIrhAx1j4i>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> ddim encoder is almost equal to optimal transport
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6PIrhAx1j4i.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Diffusion models have recently outperformed alternative approaches to model the distribution of natural images. Such diffusion models allow for deterministic sampling via the probability flow ODE, giving rise to a latent space and an encoder map. While having important practical applications, such as the estimation of the likelihood, the theoretical properties of this map are not yet fully understood. In the present work, we partially address this question for the popular case of the VP-SDE (DDPM) approach. We show that, perhaps surprisingly, the DDPM encoder map coincides with the optimal transport map for common distributions; we support this claim by extensive numerical experiments using advanced tensor train solver for multidimensional Fokker-Planck equation. We provide additional theoretical evidence for the case of multivariate normal distributions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1045. <h><a href=https://openreview.net/pdf?id=5tKXUZil3X>TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bairu Hou, Jinghan Jia, Yihua Zhang, Guanhua Zhang, Yang Zhang, Sijia Liu, Shiyu Chang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5tKXUZil3X>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5tKXUZil3X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Robustness evaluation against adversarial examples has become increasingly important to unveil the trustworthiness of the prevailing deep models in natural language processing (NLP). However, in contrast to the computer vision domain where the first-order projected gradient descent (PGD) is used as the benchmark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TextGrad, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified optimization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TextGrad can be baked into adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TextGrad not only in attack generation for robustness evaluation but also in adversarial defense. From the attack perspective, we show that TextGrad achieves remarkable improvements in both the attack success rate and the perplexity score over five state-of-the-art baselines. From the defense perspective, TextGrad-enabled adversarial training yields the most robust NLP model against a wide spectrum of NLP attacks. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1046. <h><a href=https://openreview.net/pdf?id=TatRHT_1cK>Quantifying Memorization Across Neural Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TatRHT_1cK>6.25 - [6, 8, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Model size, duplication, and context length all impact how easy it is to extract training data from large language models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TatRHT_1cK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others).
We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1047. <h><a href=https://openreview.net/pdf?id=ZhuXksSJYWn>Masked Vision and Language Modeling for Multi-modal Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gukyeong Kwon, Zhaowei Cai, Avinash Ravichandran, Erhan Bas, Rahul Bhotika, Stefano Soatto</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZhuXksSJYWn>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZhuXksSJYWn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, not only achieves state-of-the-art performance by using a large amount of data but also outperforms the other competitors by a significant margin in the regimes of limited training data. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1048. <h><a href=https://openreview.net/pdf?id=Pgtn4l6eKjv>DySR: Adaptive Super-Resolution via Algorithm and System Co-design</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Pgtn4l6eKjv>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present DySR, an algorithm and system co-design approach to maintain super-resolution streaming task QoS on mobile devices via fast model adaptation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Pgtn4l6eKjv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Super resolution (SR) is a promising approach for improving the quality of low resolution steaming services on mobile devices.
On mobile devices, the available computing and memory resources change dynamically depending on other running applications.
Due to the high computation and memory demands of SR models, it is essential to adapt the model according to available resources to harvest the best possible model performance while maintaining quality of service (QoS), such as meeting a minimum framerate and avoiding interruptions.  Nevertheless, there is no SR model or machine learning system that supports adaptive SR, and enabling adaptive SR model on mobile devices is challenging because adapting model can cause significant framerate drop or even service interruption. To address this challenge, we take an algorithm and system co-design approach and propose DySR that maintains QoS while maximizing the model performance.  During the training stage, DySR employs an adaption-aware one-shot Neural Architecture Search to produce sub-graphs that share kernel operation weights for low model adaption overhead while striking a balance between performance and framerate. During the inference stage, an incremental model adaption method is developed for further reducing the model adaption overhead. We evaluate on a diverse set of hardware and datasets to show that DySR can generate models close to the Pareto frontier while maintaining a steady framerate throughput with a memory footprint of around 40\% less compared to baseline methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1049. <h><a href=https://openreview.net/pdf?id=FYZCHEtt6H0>ERL-Re$^2$: Efficient Evolutionary Reinforcement Learning with Shared State Representation and Individual Policy Representation </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianye HAO, Pengyi Li, Hongyao Tang, YAN ZHENG, Xian Fu, Zhaopeng Meng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FYZCHEtt6H0>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel and effective framework to fuse Reinforcement Learning and Evolutionary Algorithm for policy optimization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FYZCHEtt6H0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep Reinforcement Learning (Deep RL) and Evolutionary Algorithm (EA) are two major paradigms of policy optimization with distinct learning principles, i.e., gradient-based v.s. gradient-free. An appealing research direction is integrating Deep RL and EA to devise new methods by fusing their complementary advantages. However, existing works on combining Deep RL and EA have two common drawbacks:1) the RL agent and EA agents learn their policies individually, neglecting efficient sharing of useful common knowledge; 2) parameter-level policy optimization guarantees no semantic level of behavior evolution for the EA side. In this paper, we propose Evolutionary Reinforcement Learning with Two-scale State Representation and Policy Representation (ERL-Re$^2$), a novel solution to the aforementioned two drawbacks. The key idea of ERL-Re$^2$ is two-scale representation: all EA and RL policies share the same nonlinear state representation while maintaining individual linear policy representations. The state representation conveys expressive common features of the environment learned by all the agents collectively; the linear policy representation provides a favorable space for efficient policy optimization, where novel behavior-level crossover and mutation operations can be performed. Moreover, the linear policy representation allows convenient generalization of policy fitness with the help of Policy-extended Value Function Approximator (PeVFA), further improving the sample efficiency of fitness estimation. The experiments on a range of continuous control tasks show that ERL-Re$^2$ consistently outperforms advanced baselines and achieves the State Of The Art (SOTA). Our code is available on  https://github.com/yeshenpy/ERL-Re2.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1050. <h><a href=https://openreview.net/pdf?id=cxCEOSF99f>Understanding Influence Functions and Datamodels via Harmonic Analysis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nikunj Saunshi, Arushi Gupta, Mark Braverman, Sanjeev Arora</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cxCEOSF99f>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper establishes connections between datamodels, influence functions and Fourier coefficients using theoretical tools from harmonic analysis of Boolean functions
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cxCEOSF99f.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Influence functions estimate effect of individual data points on predictions of the model on test data and were adapted to deep learning in \cite{koh2017understanding}. They have been used for detecting data poisoning, detecting helpful and harmful examples, influence of groups of datapoints, etc. Recently, \cite{ilyas2022datamodels} introduced a linear regression method they termed {\em datamodels} to predict the effect of training points on outputs on test data. The current paper seeks to provide a better theoretical understanding of such interesting empirical phenomena. The primary tool is harmonic analysis and the idea of {\em noise stability}. Contributions include: (a) Exact characterization of the learnt datamodel in terms of Fourier coefficients. (b) An efficient method to estimate the residual error and quality of the optimum linear datamodel without having to train the datamodel. (c) New insights into when influences of groups of datapoints may or may not add up linearly.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1051. <h><a href=https://openreview.net/pdf?id=Zeb5mTuqT5>Confidence-Conditioned Value Functions for Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Joey Hong, Aviral Kumar, Sergey Levine</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Zeb5mTuqT5>6.25 - [5, 6, 8, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new offline reinforcement learning algorithm that adapts how conservative its behavior will be.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Zeb5mTuqT5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of OOD actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Bellman backup that simultaneously learns Q-values for any degree of confidence with high probability. By conditioning on confidence, our value functions enable adaptive strategies during online evaluation by controlling for confidence level using the history of observations thus far. This approach can be implemented in practice by conditioning the Q-function from existing conservative algorithms on the confidence. We theoretically show that our learned value functions produce conservative estimates of the true value at any desired confidence. Finally, we empirically show that our algorithm outperforms existing conservative offline RL algorithms on multiple discrete control domains. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1052. <h><a href=https://openreview.net/pdf?id=AP0iZoaRaS>Interactive Portrait Harmonization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeya Maria Jose Valanarasu, HE Zhang, Jianming Zhang, Yilin Wang, Zhe Lin, Jose Echevarria, Yinglan Ma, Zijun Wei, Kalyan Sunkavalli, Vishal Patel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AP0iZoaRaS>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new flexible framework that allows users to pick certain regions of the background image and use it to guide the harmonization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AP0iZoaRaS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Current image harmonization methods consider the entire background as the guidance for harmonization. However, this may limit the capability for user to choose any specific object/person in the background to guide the harmonization. To enable flexible interaction between user and harmonization, we introduce interactive harmonization, a new setting where the harmonization is performed with respect to a selected region in the reference image instead of the entire background. A new flexible framework that allows users to pick certain regions of the background image and use it to guide the harmonization is proposed. Inspired by professional portrait harmonization users, we also introduce a new luminance matching loss to optimally match the color/luminance conditions between the composite foreground and select reference region. This framework provides more control to the image harmonization pipeline achieving visually pleasing portrait edits. Furthermore, we also introduce a new dataset carefully curated for validating portrait harmonization. Extensive experiments on both synthetic and real-world datasets show that the proposed approach is efficient and robust compared to previous harmonization baselines, especially for portraits.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1053. <h><a href=https://openreview.net/pdf?id=UvmDCdSPDOW>Information-Theoretic Diffusion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xianghao Kong, Rob Brekelmans, Greg Ver Steeg</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UvmDCdSPDOW>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new information-theoretic foundation for diffusion models leads to simpler and more computationally efficient density modeling. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UvmDCdSPDOW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Denoising diffusion models have spurred significant gains in density modeling and image generation, precipitating an industrial revolution in text-guided AI art generation.  We introduce a new mathematical foundation for diffusion models inspired by classic results in information theory that connect Information with Minimum Mean Square Error regression, the so-called I-MMSE relations. We generalize the I-MMSE relations to \emph{exactly} relate the data distribution to an optimal denoising regression problem, leading to an elegant refinement of existing diffusion bounds.  This new insight leads to several improvements for probability distribution estimation, including a theoretical justification for diffusion model ensembling. Remarkably, our framework shows how continuous and discrete probabilities can be learned with the same regression objective, avoiding domain-specific generative models used in variational methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1054. <h><a href=https://openreview.net/pdf?id=2r6YMqz4Mml>ROCO: A General Framework for Evaluating Robustness of Combinatorial Optimization Solvers on Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Han Lu, Zenan Li, Runzhong Wang, Qibing Ren, Xijun Li, Mingxuan Yuan, Jia Zeng, Xiaokang Yang, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2r6YMqz4Mml>6.25 - [8, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2r6YMqz4Mml.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Solving combinatorial optimization (CO) on graphs has been attracting increasing interests from the machine learning community whereby data-driven approaches were recently devised to go beyond traditional manually-designated algorithms. In this paper, we study the robustness of a combinatorial solver as a blackbox regardless it is classic or learning-based though the latter can often be more interesting to the ML community. Specifically, we develop a practically feasible robustness metric for general CO solvers. A no-worse optimal cost guarantee is developed as such the optimal solutions are not required to achieve for solvers, and we tackle the non-differentiable challenge in input instance disturbance by resorting to black-box adversarial attack methods. Extensive experiments are conducted on 14 unique combinations of solvers and CO problems, and we demonstrate that the performance of state-of-the-art solvers like Gurobi can degenerate by over 20% under the given time limit bound on the hard instances discovered by our robustness metric, raising concerns about the robustness of combinatorial optimization solvers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1055. <h><a href=https://openreview.net/pdf?id=OgbtSLESnI>TabCaps: A Capsule Neural Network for Tabular Data Classification with BoW Routing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jintai Chen, KuanLun Liao, Yanwen Fang, Danny Chen, Jian Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OgbtSLESnI>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed a capsule neural network for tabular data classification.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OgbtSLESnI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Records in a table are represented by a collection of heterogeneous scalar features. Previous work often made predictions for records in a paradigm that processed each feature as an operating unit, which requires to well cope with the heterogeneity. In this paper, we propose to encapsulate all feature values of a record into vectorial features and process them collectively rather than have to deal with individual ones, which directly captures the representations at the data level and benefits robust performances. Specifically, we adopt the concept of &#34;capsules&#34; to organize features into vectorial features, and devise a novel capsule neural network called &#34;TabCaps&#34; to process the vectorial features for classification. In TabCaps, a record is encoded into several vectorial features by some optimizable multivariate Gaussian kernels in the primary capsule layer, where each vectorial feature represents a specific &#34;profile&#34; of the input record and is transformed into senior capsule layer under the guidance of a new straightforward routing algorithm. The design of routing algorithm is motivated by the Bag-of-Words (BoW) model, which performs capsule feature grouping straightforwardly and efficiently, in lieu of the computationally complex clustering of previous routing algorithms. Comprehensive experiments show that TabCaps achieves competitive and robust performances in tabular data classification tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1056. <h><a href=https://openreview.net/pdf?id=TTLLGx3eet>Sequential Attention for Feature Selection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Taisuke Yasuda, Mohammadhossein Bateni, Lin Chen, Matthew Fahrbach, Gang Fu, Vahab Mirrokni</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TTLLGx3eet>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Sequential feature selection using the attention mechanism, with provable guarantees.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TTLLGx3eet.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, and thus inherits all of its provable guarantees. Our theoretical and empirical analyses offer new explanations towards the effectiveness of attention and its connections to overparameterization, which may be of independent interest.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1057. <h><a href=https://openreview.net/pdf?id=4dZeBJ83oxk>3D Segmenter: 3D Transformer based Semantic Segmentation via 2D Panoramic Distillation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>ZHENNAN WU, YANG LI, Yifei Huang, Lin Gu, Tatsuya Harada, Hiroyuki Sato</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4dZeBJ83oxk>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Distill knowledge from 2D strong model to enhance 3D semantic segmentation
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4dZeBJ83oxk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, 2D semantic segmentation has witnessed a significant advancement thanks to the huge amount of 2D image datasets available. Therefore, in this work, we propose the first 2D-to-3D knowledge distillation strategy to enhance 3D semantic segmentation model with knowledge embedded in the latent space of powerful 2D models. Specifically, unlike standard knowledge distillation, where teacher and student models take the same data as input, we use 2D panoramas properly aligned with corresponding 3D rooms to train the teacher network and use the learned knowledge from 2D teacher to guide 3D student. To facilitate our research, we create a large-scale, fine-annotated 3D semantic segmentation benchmark, containing voxel-wise semantic labels and aligned panoramas of 5175 scenes. Based on this benchmark, we propose a 3D volumetric semantic segmentation network, which adapts Video Swin Transformer as backbone and introduces a skip connected linear decoder.  Achieving a state-of-the-art performance, our 3D Segmenter is computationally efficient and only requires $3.8\%$ of the parameters compared to the prior art. Our code and data will be released upon acceptance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1058. <h><a href=https://openreview.net/pdf?id=HaHCoGcpV9>Sound Randomized Smoothing in Floating-Point Arithmetic</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Vaclav Voracek, Matthias Hein</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HaHCoGcpV9>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We construct classifiers producing wrong randomized smoothing certificates on images and propose a method to overcome this at a negligible cost.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HaHCoGcpV9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Randomized smoothing is sound when using infinite precision. However, we show that randomized smoothing is no longer sound for limited floating-point precision. We present a simple example where randomized smoothing certifies a radius of $1.26$ around a point, even though there is an adversarial example in the distance $0.8$ and show how this can be abused to give false certificates for CIFAR10. We discuss the implicit assumptions of randomized smoothing and show that they do not apply to generic image classification models whose smoothed versions are commonly  certified.  In order to overcome this problem, we propose a sound approach to randomized smoothing when using floating-point precision with essentially  equal speed for quantized input. It yields sound certificates or image classifiers which for the ones tested so far are very similar to the unsound practice of randomized smoothing.  Our only assumption is that we have  access to a fair coin.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1059. <h><a href=https://openreview.net/pdf?id=tJbbQfw-5wv>A critical look at the evaluation of GNNs under heterophily: Are we really making progress?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, Liudmila Prokhorenkova</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tJbbQfw-5wv>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that popular heterophilous datasets for node classification have serious drawbacks, propose several new ones, and show that, at this moment, standard GNNs outperform most of the specialized models on these datasets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tJbbQfw-5wv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Node classification is a classical graph representation learning task on which Graph Neural Networks (GNNs) have recently achieved strong results. However, it is often believed that standard GNNs only work well for homophilous graphs, i.e., graphs where edges tend to connect nodes of the same class. Graphs without this property are called heterophilous, and it is typically assumed that specialized methods are required to achieve strong performance on such graphs. In this work, we challenge this assumption. First, we show that the standard datasets used for evaluating heterophily-specific models have serious drawbacks, making results obtained by using them unreliable. The most significant of these drawbacks is the presence of a large number of duplicate nodes in the datasets Squirrel and Chameleon, which leads to train-test data leakage. We show that removing duplicate nodes strongly affects GNN performance on these datasets. Then, we propose a set of heterophilous graphs of varying properties that we believe can serve as a better benchmark for evaluating the performance of GNNs under heterophily. We show that standard GNNs achieve strong results on these heterophilous graphs, almost always outperforming specialized models. Our datasets and the code for reproducing our experiments are available at https://github.com/yandex-research/heterophilous-graphs
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1060. <h><a href=https://openreview.net/pdf?id=o7koEEMA1bR>Deep Generative Symbolic Regression</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samuel Holt, Zhaozhi Qian, Mihaela van der Schaar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=o7koEEMA1bR>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/o7koEEMA1bR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Symbolic regression (SR) aims to discover concise closed-form mathematical equations from data, a task fundamental to scientific discovery. However, the problem is highly challenging because closed-form equations lie in a complex combinatorial search space. Existing methods, ranging from heuristic search to reinforcement learning, fail to scale with the number of input variables. We make the observation that closed-form equations often have structural characteristics and invariances (e.g. the commutative law) that could be further exploited to build more effective symbolic regression solutions. Motivated by this observation, our key contribution is to leverage pre-trained deep generative models to capture the intrinsic regularities of equations, thereby providing a solid foundation for subsequent optimization steps. We show that our novel formalism unifies several prominent approaches of symbolic regression and offers a new perspective to justify and improve on the previous ad hoc designs, such as the usage of cross-entropy loss during pre-training. Specifically, we propose an instantiation of our framework, Deep Generative Symbolic Regression (DGSR). In our experiments, we show that DGSR achieves a higher recovery rate of true equations in the setting of a larger number of input variables, and it is more computationally efficient at inference time than state-of-the-art RL symbolic regression solutions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1061. <h><a href=https://openreview.net/pdf?id=HtoA0oT30jC>Novel View Synthesis with Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, Mohammad Norouzi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HtoA0oT30jC>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Novel View Synthesis with diffusion models from as few a single image
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HtoA0oT30jC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present 3DiM (pronounced &#34;three-dim&#34;), a diffusion model for 3D novel view synthesis from as few as a single image. The core of 3DiM is an image-to-image diffusion model -- 3DiM takes a single reference view and their poses as inputs, and generates a novel view via diffusion. 3DiM can then generate a full 3D consistent scene following our novel stochastic conditioning sampler: the output frames of the scene are generated autoregressively, and during the reverse diffusion process of each individual frame, we select a random conditioning frame from the set of previous frames at each denoising step. We demonstrate that stochastic conditioning yields much more 3D consistent results compared to the naive sampling process which only conditions on a single previous frame. We compare 3DiMs to prior work on the SRN ShapeNet dataset, demonstrating that 3DiM&#39;s generated videos from a single view achieve much higher fidelity while being approximately 3D consistent. We also introduce a new evaluation methodology, 3D consistency scoring, to measure the 3D consistency of a generated object by training a neural field on the model&#39;s output views. 3DiMs are geometry free, do not rely on hyper-networks or test-time optimization for novel view synthesis, and allow a single model to easily scale to a large number of scenes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1062. <h><a href=https://openreview.net/pdf?id=9EAQVEINuum>Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sheng Zhang, Hao Cheng, Jianfeng Gao, Hoifung Poon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9EAQVEINuum>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9EAQVEINuum.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a bi-encoder framework for named entity recognition (NER), which applies contrastive learning to map candidate text spans and entity types into the same vector representation space. Prior work predominantly approaches NER as sequence labeling or span classification. We instead frame NER as a representation learning problem that maximizes the similarity between the vector representations of an entity mention and its type. This makes it easy to handle nested and flat NER alike, and can better leverage noisy self-supervision signals. A major challenge to this bi-encoder formulation for NER lies in separating non-entity spans from entity mentions. Instead of explicitly labeling all non-entity spans as the same class $\texttt{Outside}$ ($\texttt{O}$) as in most prior methods, we introduce a novel dynamic thresholding loss, learned in conjunction with the standard contrastive loss. Experiments show that our method performs well in both supervised and distantly supervised settings, for nested and flat NER alike, establishing new state of the art across standard datasets in the general domain (e.g., ACE2004, ACE2005, CoNLL2003) and high-value verticals such as biomedicine (e.g., GENIA, NCBI, BC5CDR, JNLPBA). We release the code at https://github.com/microsoft/binder.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1063. <h><a href=https://openreview.net/pdf?id=FJXf1FXN8C>Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jun-Kun Wang, Andre Wibisono</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FJXf1FXN8C>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FJXf1FXN8C.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider a setting that a model needs to adapt to a new domain under distribution shifts, given that only unlabeled test samples from the new domain are accessible at test time. A common idea in most of the related works is constructing pseudo-labels for the unlabeled test samples and applying gradient descent (GD) to a loss function with the pseudo-labels. Recently, Goyal et al. (2022) propose conjugate labels, which is a new kind of pseudo-labels for self-training at test time. They empirically show that the conjugate label outperforms other ways of pseudo-labeling on many domain adaptation benchmarks. However, provably showing that GD with conjugate labels learns a good classifier for test-time adaptation remains open. In this work, we aim at theoretically understanding GD with hard and conjugate labels for a binary classification problem. We show that for square loss, GD with conjugate labels converges to an $\epsilon$-optimal predictor under a Gaussian model for any arbitrarily small $\epsilon$, while GD with hard pseudo-labels fails in this task. We also analyze them under different loss functions for the update. Our results shed lights on understanding when and why GD with hard labels or conjugate labels works in test-time adaptation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1064. <h><a href=https://openreview.net/pdf?id=XSRSWxyJIC>Parameter-Efficient Fine-Tuning Design Spaces</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, Diyi Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XSRSWxyJIC>6.25 - [6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XSRSWxyJIC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Parameter-efficient fine-tuning aims to achieve comparable performances of fine-tuning with much fewer trainable parameters. Recently, various tuning strategies (e.g., Adapters, Prefix Tuning, BitFit, and LoRA) have been proposed. However, their designs are hand-crafted separately, and it remains unclear whether certain design patterns exist for parameter-efficient fine-tuning. Thus, we present a parameter-efficient fine-tuning design paradigm and discover design patterns that are applicable to different experimental settings. Instead of focusing on designing another individual tuning strategy, we introduce parameter-efficient fine-tuning design spaces that parameterize tuning structures and tuning strategies. Specifically, any design space is characterized by four components: layer grouping, trainable parameter allocation, tunable groups, and strategy assignment. Our comprehensive empirical study leads to the discovery of design patterns: (i) grouping layers in a spindle pattern, (ii) uniformly allocating the number of trainable parameters to layers, (ii) tuning all the groups, and (iv) tuning different groups with proper strategies. Our discovered design patterns result in new parameter-efficient fine-tuning methods. Experiments show that these methods consistently outperform investigated parameter-efficient fine-tuning strategies across different backbone models and different tasks in natural language processing.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1065. <h><a href=https://openreview.net/pdf?id=_01dDd3f78>Concept Gradient: Concept-based Interpretation Without Linear Assumption</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrew Bai, Chih-Kuan Yeh, Neil Y.C. Lin, Pradeep Kumar Ravikumar, Cho-Jui Hsieh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_01dDd3f78>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Extending concept-based gradient interpretation to non-linear concept functions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_01dDd3f78.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Concept-based interpretations of black-box models are often more intuitive for humans to understand. The most widely adopted approach for concept-based, gradient interpretation is Concept Activation Vector (CAV). CAV relies on learning a linear relation between some latent representation of a given model and concepts. The premise of meaningful concepts lying in a linear subspace of model layers is usually implicitly assumed but does not hold true in general. In this work we proposed Concept Gradient (CG), which extends concept-based, gradient interpretation methods to non-linear concept functions. We showed that for a general (potentially non-linear) concept, we can mathematically measure how a small change of concept affects the model’s prediction, which is an extension of gradient-based interpretation to the concept space. We demonstrated empirically that CG outperforms CAV in attributing concept importance on real world datasets and performed case study on a medical dataset. The code is available at github.com/jybai/concept-gradients.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1066. <h><a href=https://openreview.net/pdf?id=TjEzIsyEsQ6>Multi-Objective Reinforcement Learning: Convexity, Stationarity and Pareto Optimality</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoye Lu, Daniel Herman, Yaoliang Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TjEzIsyEsQ6>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a linear scalarization based algorithm that has the potential to find the entire Pareto front.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TjEzIsyEsQ6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In recent years, single-objective reinforcement learning (SORL) algorithms have received a significant amount of attention and seen some strong results. However, it is generally recognized that many practical problems have intrinsic multi-objective properties that cannot be easily handled by SORL algorithms. Although there have been many multi-objective reinforcement learning (MORL) algorithms proposed, there has been little recent exploration of the fundamental properties of the spaces we are learning in. In this paper, we perform a rigorous analysis of policy induced value functions and use the insights to distinguish three views of Pareto optimality. The results imply the convexity of the induced value function&#39;s range for stationary policies and suggest that any point of its Pareto front can be achieved by training a policy using linear scalarization (LS). We show the problem that leads to the suboptimal performance of LS can be solved by adding strongly concave terms to the immediate rewards, which motivates us to propose a new vector reward-based Q-learning algorithm, CAPQL. Combined with an actor-critic formulation, our algorithm achieves state-of-the-art performance on multiple MuJoCo tasks in the preference agnostic setting. Furthermore, we empirically show that, in contrast to other LS-based algorithms, our approach is significantly more stable, achieving similar results across various random seeds. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1067. <h><a href=https://openreview.net/pdf?id=T-qVtA3pAxG>Serving Graph Compression for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Si Si, Felix Yu, Ankit Singh Rawat, Cho-Jui Hsieh, Sanjiv Kumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=T-qVtA3pAxG>6.25 - [8, 8, 3, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Compressing the graph for graph neural networks inference
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/T-qVtA3pAxG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Serving a GNN model online is challenging --- in many applications when testing nodes are connected to training nodes, one has to propagate information from training nodes to testing nodes to achieve the best performance, and storing the whole training set (including training graph and node features) during inference stage is prohibitive for large-scale problems. In this paper, we study graph compression to reduce the storage requirement for GNN in serving. Given a GNN model to be served, we propose to construct a compressed graph with  a smaller number of nodes. In serving time, one just needs to replace the original training set graph by this compressed graph, without the need of changing the actual GNN model and the forward pass. We carefully analyze the error in the forward pass and derive simple ways to construct the compressed graph to minimize the approximation error. Experimental results on semi-supervised node classification demonstrate that the proposed method can significantly reduce the serving space requirement for GNN inference.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1068. <h><a href=https://openreview.net/pdf?id=Lr8cOOtYbfL>Planning with Large Language Models for Code Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, Chuang Gan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Lr8cOOtYbfL>6.25 - [3, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a novel framework for code generation by combining the advantages of a large language model and a planning algorithm.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Lr8cOOtYbfL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing large language model-based code generation pipelines typically use beam search or sampling algorithms during the decoding process. Although the programs they generate achieve high token-matching-based scores, they often fail to compile or generate incorrect outputs. The main reason is that conventional Transformer decoding algorithms may not be the best choice for code generation. In this work, we propose a novel Transformer decoding algorithm, Planning-Guided Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs. Specifically, instead of simply optimizing the likelihood of the generated sequences, the Transformer makes use of a planner that generates candidate programs and tests them on public test cases. The Transformer can therefore make more informed decisions and generate tokens that will eventually lead to higher-quality programs. We also design a mechanism that shares information between the Transformer and the planner to make our algorithm computationally efficient. We empirically evaluate our framework with several large language models as backbones on public coding challenge benchmarks, showing that 1) it can generate programs that consistently achieve higher performance compared with competing baseline methods; 2) it enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objective.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1069. <h><a href=https://openreview.net/pdf?id=XHc5zRPxqV9>DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Donghan Yu, Sheng Zhang, Patrick Ng, Henghui Zhu, Alexander Hanbo Li, Jun Wang, Yiqun Hu, William Yang Wang, Zhiguo Wang, Bing Xiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XHc5zRPxqV9>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel KBQA framework that jointly generates both direct answers and logical forms, and then combines them to obtain the final answers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XHc5zRPxqV9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Question answering over knowledge bases (KBs) aims to answer natural language questions with factual information such as entities and relations in KBs. Previous methods either generate logical forms that can be executed over KBs to obtain final answers or predict answers directly. Empirical results show that the former often produces more accurate answers, but it suffers from  non-execution issues due to potential syntactic and semantic errors in the generated logical forms. In this work, we propose a novel framework DecAF that jointly generates both logical forms and direct answers, and then combines the merits of them to get the final answers. Moreover, different from most of the previous methods, DecAF is based on simple free-text retrieval without relying on any entity linking tools --- this simplification eases its adaptation to different datasets. DecAF achieves new state-of-the-art accuracy on WebQSP, FreebaseQA, and GrailQA benchmarks, while getting competitive results on the ComplexWebQuestions benchmark.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1070. <h><a href=https://openreview.net/pdf?id=Pia70sP2Oi1>Planckian Jitter: countering the color-crippling effects of color jitter on self-supervised training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Simone Zini, Alex Gomez-Villa, Marco Buzzelli, Bartłomiej Twardowski, Andrew D. Bagdanov, Joost van de weijer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Pia70sP2Oi1>6.25 - [8, 6, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Pia70sP2Oi1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Several recent works on self-supervised learning are trained by mapping different augmentations of the same image to the same feature representation. The data augmentations used are of crucial importance to the quality of learned feature representations. In this paper, we analyze how the color jitter traditionally used in data augmentation negatively impacts the quality of the color features in learned feature representations. To address this problem, we propose a more realistic, physics-based color data augmentation - which we call Planckian Jitter - that creates realistic variations in chromaticity and produces a model robust to illumination changes that can be commonly observed in real life, while maintaining the ability to discriminate image content based on color information.
Experiments confirm that such a representation is complementary to the representations learned with the currently-used color jitter augmentation and that a simple concatenation leads to significant performance gains on a wide range of downstream datasets. 
In addition, we present a color sensitivity analysis that documents the impact of different training methods on model neurons and shows that the performance of the learned features is robust with respect to illuminant variations.
Official code available at: https://github.com/TheZino/PlanckianJitter
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1071. <h><a href=https://openreview.net/pdf?id=iLMgk2IGNyv>GAMR: A Guided Attention Model for (visual) Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mohit Vaishnav, Thomas Serre</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iLMgk2IGNyv>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A framework for a memory and attention based architecture demonstrating the capability of sample efficient learning and generalization capability on complex visual reasoning tasks aligned with the theory of visual routines. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iLMgk2IGNyv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Humans continue to outperform modern AI systems in their ability to flexibly parse and understand complex visual scenes. Here, we present a novel module for visual reasoning, the Guided Attention Model for (visual) Reasoning ($\textit{GAMR}$), which instantiates an active vision theory -- positing that the brain solves complex visual reasoning problems dynamically -- via sequences of attention shifts to select and route task-relevant visual information into memory. Experiments on an array of visual reasoning tasks and datasets demonstrate GAMR&#39;s ability to learn visual routines in a robust and sample-efficient manner. In addition, GAMR is shown to be capable of zero-shot generalization on completely novel reasoning tasks. Overall, our work provides computational support for cognitive theories that postulate the need for a critical interplay between attention and memory to dynamically maintain and manipulate task-relevant visual information to solve complex visual reasoning tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1072. <h><a href=https://openreview.net/pdf?id=C1ns08q9jZ>MetaGL: Evaluation-Free Selection of Graph Learning Models via Meta-Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Namyong Park, Ryan A. Rossi, Nesreen Ahmed, Christos Faloutsos</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=C1ns08q9jZ>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a meta-learning based framework that tackles the new problem of selecting a graph learning model without any evaluation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/C1ns08q9jZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Given a graph learning task, such as link prediction, on a new graph, how can we select the best method as well as its hyperparameters (collectively called a model) without having to train or evaluate any model on the new graph? Model selection for graph learning has been largely ad hoc. A typical approach has been to apply popular methods to new datasets, but this is often suboptimal. On the other hand, systematically comparing models on the new graph quickly becomes too costly, or even impractical. In this work, we develop the first meta-learning approach for evaluation-free graph learning model selection, called MetaGL, which utilizes the prior performances of existing methods on various benchmark graph datasets to automatically select an effective model for the new graph, without any model training or evaluations. To quantify similarities across a wide variety of graphs, we introduce specialized meta-graph features that capture the structural characteristics of a graph. Then we design G-M network, which represents the relations among graphs and models, and develop a graph-based meta-learner operating on this G-M network, which estimates the relevance of each model to different graphs. Extensive experiments show that using MetaGL to select a model for the new graph greatly outperforms several existing meta-learning techniques tailed for graph learning model selection (up to 47% better), while being extremely fast at test time (∼1 sec).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1073. <h><a href=https://openreview.net/pdf?id=SaRj2ka1XZ3>Language Models Can Teach Themselves to Program Better</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Patrick Haluptzok, Matthew Bowers, Adam Tauman Kalai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SaRj2ka1XZ3>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Language Models can be used to generate Programming Puzzles and Solutions, which can be filtered for correctness and used to finetune the LLM to improve its performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SaRj2ka1XZ3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM’s performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model “improves itself” using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al. , 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1074. <h><a href=https://openreview.net/pdf?id=5NTt8GFjUHkr>Automatic Chain of Thought Prompting in Large Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5NTt8GFjUHkr>6.25 - [8, 6, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an automatic prompting method (Auto-CoT) to elicit chain-of-thought reasoning in large language models without needing manually-designed demonstrations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5NTt8GFjUHkr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large Language Models (LLMs) can carry out complex reasoning tasks by generating intermediate reasoning steps. These steps are triggered by what is called chain-of-thought (CoT) prompting, which comes in two flavors: one leverages a simple prompt like &#34;Let’s think step by step&#34; to facilitate step-by-step reasoning before answering a question (Zero-Shot-CoT). The other uses manual demonstrations, each composed of a question and a reasoning chain that leads to an answer (Manual-CoT). Unfortunately, the superior performance of the latter strategy crucially hinges on manually generating task-specific demonstrations. This makes it far less scalable and more dependent on the talent of the CoT engineer. We show that such manual efforts may be eliminated by leveraging LLMs to generate the reasoning chains on its own. Since these generated chains often come with mistakes we propose a number of mitigation strategies. Our proposed Auto-CoT method automaticaly samples diverse questions and we perform post-processing quality control to generate usable reasoning chains from Zero-Shot-CoT. On ten public benchmark reasoning tasks, Auto-CoT performs on par with Manual-CoT without the need for human intervention. Code is available at https://github.com/amazon-research/auto-cot.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1075. <h><a href=https://openreview.net/pdf?id=NyR8OZFHw6i>FIGARO: Controllable Music Generation using Learned and Expert Features</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dimitri von Rütte, Luca Biggio, Yannic Kilcher, Thomas Hofmann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NyR8OZFHw6i>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We achieve state-of-the-art results in symbolic music style transfer by enabling human-interpretable control over the generation process while improving sample quality at the same time.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NyR8OZFHw6i.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent symbolic music generative models have achieved significant improvements in the quality of the generated samples. Nevertheless, it remains hard for users to control the output in such a way that it matches their expectation. To address this limitation, high-level, human-interpretable conditioning is essential. In this work, we release FIGARO, a Transformer-based conditional model trained to generate symbolic music based on a sequence of high-level control codes. To this end, we propose description-to-sequence learning, which consists of automatically extracting fine-grained, human-interpretable features (the description) and training a sequence-to-sequence model to reconstruct the original sequence given only the description as input. FIGARO achieves state-of-the-art performance in multi-track symbolic music generation both in terms of style transfer and sample quality. We show that performance can be further improved by combining human-interpretable with learned features. Our extensive experimental evaluation shows that FIGARO is able to generate samples that closely adhere to the content of the input descriptions, even when they deviate significantly from the training distribution.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1076. <h><a href=https://openreview.net/pdf?id=oLIZ2jGTiv>Tuning Frequency Bias in Neural Network Training with Nonuniform Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Annan Yu, Yunan Yang, Alex Townsend</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oLIZ2jGTiv>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oLIZ2jGTiv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Small generalization errors of over-parameterized neural networks (NNs) can be partially explained by the frequency biasing phenomenon, where gradient-based algorithms minimize the low-frequency misfit before reducing the high-frequency residuals. Using the Neural Tangent Kernel (NTK), one can provide a theoretically rigorous analysis for training where data are drawn from constant or piecewise-constant probability densities. Since most training data sets are not drawn from such distributions, we use the NTK model and a data-dependent quadrature rule to theoretically quantify the frequency biasing of NN training given fully nonuniform data. By replacing the loss function with a carefully selected Sobolev norm, we can further amplify, dampen, counterbalance, or reverse the intrinsic frequency biasing in NN training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1077. <h><a href=https://openreview.net/pdf?id=ZccFLU-Yk65>DBQ-SSD: Dynamic Ball Query for Efficient 3D Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinrong Yang, Lin Song, Songtao Liu, Weixin Mao, Zeming Li, Xiaoping Li, Hongbin Sun, Jian Sun, Nanning Zheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZccFLU-Yk65>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZccFLU-Yk65.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many point-based 3D detectors adopt point-feature sampling strategies to drop some points for efficient inference. These strategies are typically based on fixed and handcrafted rules, making it difficult to handle complicated scenes. Different from them, we propose a Dynamic Ball Query (DBQ) network to adaptively select a subset of input points according to the input features, and assign the feature transform with a suitable receptive field for each selected point. It can be embedded into some state-of-the-art 3D detectors and trained in an end-to-end manner, which significantly reduces the computational cost. Extensive experiments demonstrate that our method can reduce latency by 30%-100% on KITTI, Waymo, and ONCE datasets. Specifically, the inference speed of our detector can reach 162 FPS on KITTI scene, and 30 FPS on Waymo and ONCE scenes without performance degradation. Due to skipping the redundant points, some evaluation metrics show significant improvements.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1078. <h><a href=https://openreview.net/pdf?id=EnrY5TOrbQ>Agnostic Learning of General ReLU Activation Using Gradient Descent</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pranjal Awasthi, Alex Tang, Aravindan Vijayaraghavan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EnrY5TOrbQ>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a convergence analysis of gradient descent for the problem of agnostically learning a single ReLU function under Gaussian distributions that achieves loss of O(OPT). 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EnrY5TOrbQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We provide a convergence analysis of gradient descent for the problem of agnostically learning a single ReLU function under Gaussian distributions. Unlike prior work that studies the setting of zero bias, we consider the more challenging scenario when the bias of the ReLU function is non-zero. Our main result establishes that starting from random initialization, in a polynomial number of iterations gradient descent outputs, with high probability, a ReLU function that achieves an error that is within a constant factor of the optimal i.e., it is guaranteed to achieve an error of $O(OPT)$, where $OPT$ is the error of the best ReLU function. This is a significant improvement over existing guarantees for gradient descent, which only guarantee  error of $O(\sqrt{d \cdot OPT})$ even in the zero-bias case  (Frei et al., 2020). We also provide finite sample guarantees, and obtain similar guarantees for a broader class of marginal distributions beyond Gaussians. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1079. <h><a href=https://openreview.net/pdf?id=2L9gzS80tA4>Does Learning from Decentralized Non-IID Unlabeled Data Benefit from Self Supervision?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lirui Wang, Kaiqing Zhang, Yunzhu Li, Yonglong Tian, Russ Tedrake</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2L9gzS80tA4>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study decentralized learning with non-IID unlabeled data, and try to understand the robustness and communication efficiency of decentralized self-supervised learning, through extensive experiments and theoretical analysis.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2L9gzS80tA4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The success of machine learning relies heavily on massive amounts of data, which are usually generated and stored across a range of diverse and distributed data sources. Decentralized learning has thus been advocated and widely deployed to make efficient use of distributed datasets, with an extensive focus on supervised learning (SL) problems. Unfortunately, the majority of real-world data are unlabeled and can be highly heterogeneous across sources. In this work, we carefully study decentralized learning with unlabeled data through the lens of self-supervised learning (SSL), specifically contrastive visual representation learning. We study the effectiveness of a range of contrastive learning algorithms under a decentralized learning setting, on relatively large-scale datasets including ImageNet-100, MS-COCO, and a new real-world robotic warehouse dataset. Our experiments show that the decentralized SSL (Dec-SSL) approach is robust to the heterogeneity of decentralized datasets, and learns useful representation for object classification, detection, and segmentation tasks, even when combined with the simple and standard decentralized learning algorithm of Federated Averaging (FedAvg). This robustness makes it possible to significantly reduce communication and to reduce the participation ratio of data sources with only minimal drops in performance. Interestingly, using the same amount of data, the representation learned by Dec-SSL can not only perform on par with that learned by centralized SSL which requires communication and excessive data storage costs, but also sometimes outperform representations extracted from decentralized SL which requires extra knowledge about the data labels. Finally, we provide theoretical insights into understanding why data heterogeneity is less of a concern for Dec-SSL objectives, and introduce feature alignment and clustering techniques to develop a new Dec-SSL algorithm that further improves the performance, in the face of highly non-IID data. Our study presents positive evidence to embrace unlabeled data in decentralized learning, and we hope to provide new insights into whether and why decentralized SSL is effective and/or even advantageous.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1080. <h><a href=https://openreview.net/pdf?id=OIe3kpwl40D>SMART: Sentences as Basic Units for Text Evaluation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Reinald Kim Amplayo, Peter J Liu, Yao Zhao, Shashi Narayan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OIe3kpwl40D>6.25 - [6, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OIe3kpwl40D.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Widely used evaluation metrics for text generation either do not work well with longer texts or fail to evaluate all aspects of text quality. In this paper, we introduce a new metric called SMART to mitigate such limitations. Specifically, we treat sentences as basic units of matching instead of tokens, and use a sentence matching function to soft-match candidate and reference sentences. Candidate sentences are also compared to sentences in the source documents to allow grounding (e.g., factuality) evaluation. Our results show that system-level correlations of our proposed metric with a model-based matching function outperforms all competing metrics on the SummEval summarization meta-evaluation dataset, while the same metric with a string-based matching function is competitive with current model-based metrics. The latter does not use any neural model, which is useful during model development phases where resources can be limited and fast evaluation is required. SMART also outperforms all factuality evaluation metrics on the TRUE benchmark. Finally, we also conducted extensive analyses showing that our proposed metrics work well with longer  summaries and are less biased towards specific models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1081. <h><a href=https://openreview.net/pdf?id=-iADdfa4GKH>Monocular Scene Reconstruction with 3D SDF Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weihao Yuan, Xiaodong Gu, Heng Li, Zilong Dong, Siyu Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-iADdfa4GKH>6.25 - [6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-iADdfa4GKH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Monocular scene reconstruction from posed images is challenging due to the complexity of a large environment. Recent volumetric methods learn to directly predict the TSDF volume and have demonstrated promising results in this task. However, most methods focus on how to extract and fuse the 2D features to a 3D feature volume, but none of them improve the way how the 3D volume is aggregated. In this work, we propose an SDF transformer network, which replaces the role of 3D CNN for better 3D feature aggregation. To reduce the explosive computation complexity of the 3D multi-head attention, we propose a sparse window attention module, where the attention is only calculated between the non-empty voxels within a local window. Then a top-down-bottom-up 3D attention network is built for 3D feature aggregation, where a dilate-attention structure is proposed to prevent geometry degeneration, and two global modules are employed to equip with global receptive fields. The experiments on multiple datasets show that this 3D transformer network generates a more accurate and complete reconstruction, which outperforms previous methods by a large margin. Remarkably, the mesh accuracy is improved by 41.8%, and the mesh completeness is improved by 25.3% on the ScanNet dataset. The code of our method will be made public.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1082. <h><a href=https://openreview.net/pdf?id=ICYasJBlZNs>Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yulun Wu, Rob Barton, Zichen Wang, Vassilis N. Ioannidis, Carlo De Donno, Layne C Price, Luis F. Voloch, George Karypis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ICYasJBlZNs>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We predict single-cell perturbation responses using a graph variational Bayesian causal inference framework with distilled gene regulatory networks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ICYasJBlZNs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Predicting the responses of a cell under perturbations may bring important benefits to drug discovery and personalized therapeutics. In this work, we propose a novel graph variational Bayesian causal inference framework to predict a cell&#39;s gene expressions under counterfactual perturbations (perturbations that this cell did not factually receive), leveraging information representing biological knowledge in the form of gene regulatory networks (GRNs) to aid individualized cellular response predictions. Aiming at a data-adaptive GRN, we also developed an adjacency matrix updating technique for graph convolutional networks and used it to refine GRNs during pre-training, which generated more insights on gene relations and enhanced model performance. Additionally, we propose a robust estimator within our framework for the asymptotically efficient estimation of marginal perturbation effect, which is yet to be carried out in previous works. With extensive experiments, we exhibited the advantage of our approach over state-of-the-art deep learning models for individual response prediction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1083. <h><a href=https://openreview.net/pdf?id=8JqINxA-2a>Unified Discrete Diffusion for Simultaneous Vision-Language Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minghui Hu, Chuanxia Zheng, Zuopeng Yang, Tat-Jen Cham, Heliang Zheng, Chaoyue Wang, Dacheng Tao, Ponnuthurai N. Suganthan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8JqINxA-2a>6.25 - [6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed Unified Discrete Denoising Diffusion model, which allows us to construct a joint vision-language probability distribution, leading to a capability of simultaneously generating cross-domain results. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8JqINxA-2a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The recently developed discrete diffusion model performs extraordinarily well in generation tasks, especially in the text-to-image task, showing great potential for modeling multimodal signals. In this paper, we leverage these properties and present a unified multimodal generation model, which can perform text-based, image-based, and even vision-language simultaneous generation using a single model. Specifically, we unify the discrete diffusion process for multimodal signals by proposing a unified Markov transition matrix and a unified objective. Moreover, we design a multimodal mutual attention module to highlight the inter-modal linkages, which is vital for multimodal generation. Extensive experiments indicate that our proposed method can perform comparably to the state-of-the-art solutions in various generation tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1084. <h><a href=https://openreview.net/pdf?id=XUxad2Gj40n>EVC: Towards Real-Time Neural Image Compression with Mask Decay</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wang Guo-Hua, Jiahao Li, Bin Li, Yan Lu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XUxad2Gj40n>6.25 - [8, 8, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XUxad2Gj40n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural image compression has surpassed state-of-the-art traditional codecs (H.266/VVC) for rate-distortion (RD) performance, but suffers from large complexity and separate models for different rate-distortion trade-offs. In this paper, we propose an Efficient single-model Variable-bit-rate Codec (EVC), which is able to run at 30 FPS with 768x512 input images and still outperforms VVC for the RD performance. By further reducing both encoder and decoder complexities, our small model even achieves 30 FPS with 1920x1080 input images. To bridge the performance gap between our different capacities models, we meticulously design the mask decay, which transforms the large model&#39;s parameters into the small model automatically. And a novel sparsity regularization loss is proposed to mitigate shortcomings of $L_p$ regularization. Our algorithm significantly narrows the performance gap by 50% and 30% for our medium and small models, respectively. At last, we advocate the scalable encoder for neural image compression. The encoding complexity is dynamic to meet different latency requirements. We propose decaying the large encoder multiple times to reduce the residual representation progressively. Both mask decay and residual representation learning greatly improve the RD performance of our scalable encoder. Our code is at https://github.com/microsoft/DCVC.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1085. <h><a href=https://openreview.net/pdf?id=ddad0PNUvV>Images as Weight Matrices: Sequential Image Generation Through Synaptic Learning Rules</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kazuki Irie, Jürgen Schmidhuber</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ddad0PNUvV>6.25 - [8, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train neural nets to execute sequences of synaptic learning rules to sequentially generate natural images (instead of weight matrices).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ddad0PNUvV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Work on fast weight programmers has demonstrated the effectiveness of key/value outer product-based learning rules for sequentially generating a weight matrix (WM) of a neural net (NN) by another NN or itself. However, the weight generation steps are typically not visually interpretable by humans, because the contents stored in the WM of an NN are not. Here we apply the same principle to generate natural images. The resulting fast weight painters (FPAs) learn to execute sequences of delta learning rules to sequentially generate images as sums of outer products of self-invented keys and values, one rank at a time, as if each image was a WM of an NN. We train our FPAs in the generative adversarial networks framework, and evaluate on various image datasets. We show how these generic learning rules can generate images with respectable visual quality without any explicit inductive bias for images. While the performance largely lags behind the one of specialised state-of-the-art image generators, our approach allows for visualising how synaptic learning rules iteratively produce complex connection patterns, yielding human-interpretable meaningful images. Finally, we also show that an additional convolutional U-Net (now popular in diffusion models) at the output of an FPA can learn one-step &#34;denoising&#34; of FPA-generated images to enhance their quality.
Our code is public.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1086. <h><a href=https://openreview.net/pdf?id=5IND3TXJRb->Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mohit Sharma, Claudio Fantacci, Yuxiang Zhou, Skanda Koppula, Nicolas Heess, Jon Scholz, Yusuf Aytar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5IND3TXJRb->6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5IND3TXJRb-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent works have shown that large models pretrained on common visual learning tasks can provide useful representations for a wide range of specialized perception problems, as well as a variety of robotic manipulation tasks.  While prior work on robotic manipulation has predominantly used frozen pretrained features, we demonstrate that in robotics this approach can fail to reach optimal performance, and that fine-tuning of the full model can lead to significantly better results. Unfortunately, fine-tuning disrupts the pretrained visual representation, and causes representational drift towards the fine-tuned task thus leading to a loss of the versatility of the original model. We introduce a method for lossless adaptation to address this shortcoming of classical fine-tuning. We demonstrate that appropriate placement of our parameter efficient adapters can significantly reduce the performance gap between frozen pretrained representations and full end-to-end fine-tuning without changes to the original representation and thus preserving original capabilities of the pretrained model. We perform a comprehensive investigation across three major model architectures (ViTs, NFNets, and ResNets), supervised (ImageNet-1K classification) and self-supervised pretrained weights (CLIP, BYOL, Visual MAE) in three manipulation task domains and 35 individual tasks, and demonstrate that our claims are strongly validated in various settings. Please see real world videos at https://sites.google.com/view/robo-adapters
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1087. <h><a href=https://openreview.net/pdf?id=p66AzKi6Xim>What Can we Learn From The Selective Prediction And Uncertainty Estimation Performance Of 523 Imagenet Classifiers?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ido Galil, Mohammed Dabbah, Ran El-Yaniv</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p66AzKi6Xim>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> What are the best DNNs and training regimes for eliciting superior uncertainty estimation? Analyzing 523 DNNs in order to provide insights that practitioners and researchers can use to maximize the potential of current methods and discover new ones
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p66AzKi6Xim.jpg loading="lazy">
                </div>
                <div class="abstract">
                    When deployed for risk-sensitive tasks, deep neural networks must include an uncertainty estimation mechanism.
Here we examine the relationship between deep architectures and their respective training regimes, with their corresponding selective prediction and uncertainty estimation performance. We consider some of the most popular estimation performance metrics previously proposed including AUROC, ECE, AURC as well as coverage for selective accuracy constraint. 
We present a novel and comprehensive study of selective prediction and the uncertainty estimation performance of 523 existing pretrained deep ImageNet classifiers that are available in popular repositories.
We identify numerous and previously unknown factors that affect uncertainty estimation and examine the relationships between the different metrics. We find that distillation-based training regimes consistently yield better uncertainty estimations than other training schemes such as vanilla training, pretraining on a larger dataset and adversarial training.
Moreover, we find a subset of ViT models that outperform any other models in terms of uncertainty estimation performance.
For example, we discovered an unprecedented 99% top-1 selective accuracy on ImageNet at 47% coverage
(and 95% top-1 accuracy at 80%) for a ViT model, whereas a competing EfficientNet-V2-XL cannot obtain these accuracy constraints at any level of coverage. 
Our companion paper, also published in ICLR 2023 (A framework for benchmarking class-out-of-distribution detection and its application to ImageNet), examines the performance of these classifiers in a class-out-of-distribution setting.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1088. <h><a href=https://openreview.net/pdf?id=OXP9Ns0gnIq>Differentiable Gaussianization Layers for Inverse Problems Regularized by Deep Generative Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dongzhuo Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OXP9Ns0gnIq>6.25 - [6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OXP9Ns0gnIq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep generative models such as GANs, normalizing flows, and diffusion models are powerful regularizers for inverse problems. They exhibit great potential for helping reduce ill-posedness and attain high-quality results. However, the latent tensors of such deep generative models can fall out of the desired high-dimensional standard Gaussian distribution during inversion, particularly in the presence of data noise and inaccurate forward models, leading to low-fidelity solutions. To address this issue, we propose to reparameterize and Gaussianize the latent tensors using novel differentiable data-dependent layers wherein custom operators are defined by solving optimization problems. These proposed layers constrain inverse problems to obtain high-fidelity in-distribution solutions. We validate our technique on three inversion tasks: compressive-sensing MRI, image deblurring, and eikonal tomography (a nonlinear PDE-constrained inverse problem) using two representative deep generative models: StyleGAN2 and Glow. Our approach achieves state-of-the-art performance in terms of accuracy and consistency.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1089. <h><a href=https://openreview.net/pdf?id=oX3tGygjW1q>Minimum Description Length Control</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ted Moskovitz, Ta-Chu Kao, Maneesh Sahani, Matthew Botvinick</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=oX3tGygjW1q>6.25 - [6, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel framework for multitask reinforcement learning which seeks to distill shared structure among tasks into a low-complexity representation, which is then leveraged to accelerate convergence on new tasks. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/oX3tGygjW1q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a novel framework for multitask reinforcement learning based on the minimum description length (MDL) principle. In this approach, which we term MDL-control (MDL-C), the agent learns the common structure among the tasks with which it is faced and then distills it into a simpler representation which facilitates faster convergence and generalization to new tasks. In doing so, MDL-C naturally balances adaptation to each task with epistemic uncertainty about the task distribution. We motivate MDL-C via formal connections between the MDL principle and Bayesian inference, derive theoretical performance guarantees, and demonstrate MDL-C&#39;s empirical effectiveness on both discrete and high-dimensional continuous control tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1090. <h><a href=https://openreview.net/pdf?id=8sSnD78NqTN>Learning Soft Constraints From Constrained Expert Demonstrations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ashish Gaurav, Kasra Rezaee, Guiliang Liu, Pascal Poupart</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8sSnD78NqTN>6.25 - [8, 5, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8sSnD78NqTN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving scenarios.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1091. <h><a href=https://openreview.net/pdf?id=uBKBoix9NXa>Understanding weight-magnitude hyperparameters in training binary networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Joris Quist, Yunqiang Li, Jan van Gemert</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uBKBoix9NXa>6.25 - [6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We analysed the effects of hyperparameters in BNN optimization and propose an optimizer that is based upon Infinite Impulse Response Filters
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uBKBoix9NXa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Binary Neural Networks (BNNs) are compact and efficient by using binary weights instead of real-valued weights. Current BNNs use latent real-valued weights during training, where several training hyper-parameters are inherited from real-valued networks. The interpretation of several of these hyperparameters is based on the magnitude of the real-valued weights. For BNNs, however, the magnitude of binary weights is not meaningful, and thus it is unclear what these hyperparameters actually do. One example is weight-decay, which aims to keep the magnitude of real-valued weights small. Other examples are latent weight initialization, the learning rate, and learning rate decay, which influence the magnitude of the real-valued weights. The magnitude is interpretable for real-valued weights, but loses its meaning for binary weights. In this paper we offer a new interpretation of these magnitude-based hyperparameters based on higher-order gradient filtering during network optimization. Our analysis makes it possible to understand how magnitude-based hyperparameters influence the training of binary networks which allows for new optimization filters specifically designed for binary neural networks that are independent of their real-valued interpretation. Moreover, our improved understanding reduces the number of hyperparameters, which in turn eases the hyperparameter tuning effort which may lead to better hyperparameter values for improved accuracy. Code is available at https://github.com/jorisquist/Understanding-WM-HP-in-BNNs
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1092. <h><a href=https://openreview.net/pdf?id=z0_V5O9cmNw>Learning in temporally structured environments</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matt Jones, Tyler R. Scott, Mengye Ren, Gamaleldin Fathy Elsayed, Katherine Hermann, David Mayo, Michael Curtis Mozer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=z0_V5O9cmNw>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Models that learn at multiple timescales perform well in tasks with complex temporal structure
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/z0_V5O9cmNw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Natural environments have temporal structure at multiple timescales. This property is reflected in biological learning and memory but typically not in machine learning systems. We advance a multiscale learning method in which each weight in a neural network is decomposed as a sum of subweights with different learning and decay rates. Thus knowledge becomes distributed across different timescales, enabling rapid adaptation to task changes while avoiding catastrophic interference. First, we prove previous models that learn at multiple timescales, but with complex coupling between timescales, are equivalent to multiscale learning via a reparameterization that eliminates this coupling. The same analysis yields a new characterization of momentum learning, as a fast weight with a negative learning rate. Second, we derive a model of Bayesian inference over $1/f$ noise, a common temporal pattern in many online learning domains that involves long-range (power law) autocorrelations. The generative side of the model expresses $1/f$ noise as a sum of diffusion processes at different timescales, and the inferential side tracks these latent processes using a Kalman filter. We then derive a variational approximation to the Bayesian model and show how it is an extension of the multiscale learner. The result is an optimizer that can be used as a drop-in replacement in an arbitrary neural network architecture. Third, we evaluate the ability of these methods to handle nonstationarity by testing them in online prediction tasks characterized by $1/f$ noise in the latent parameters. We find that the Bayesian model significantly outperforms online stochastic gradient descent and two batch heuristics that rely preferentially or exclusively on more recent data. Moreover, the variational approximation performs nearly as well as the full Bayesian model, and with memory requirements that are linear in the size of the network.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1093. <h><a href=https://openreview.net/pdf?id=nQai_B1Zrt> Decompose to Generalize: Species-Generalized Animal Pose Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guangrui Li, Yifan Sun, Zongxin Yang, Yi Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nQai_B1Zrt>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nQai_B1Zrt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper challenges the cross-species generalization problem for animal pose estimation, aiming to learn a pose estimator that can be well generalized to novel species. We find the relation between different joints is important with two-fold impact: 1) on the one hand, some relation is consistent across all the species and may help two joints mutually confirm each other, e.g., the eyes help confirm the nose and vice versa because they are close in all species. 2) on the other hand, some relation is inconsistent for different species due to the species variation and may bring severe distraction rather than benefit. With these two insights, we propose a Decompose-to-Generalize (D-Gen) pose estimation method to break the inconsistent relations while preserving the consistent ones. Specifically, D-Gen first decomposes the body joints into several joint concepts so that each concept contains multiple closely-related joints. Given these joint concepts, D-Gen 1) promotes the interaction between intra-concept joints to enhance their reliable mutual confirmation, and 2) suppresses the interaction between inter-concept joints to prohibit their mutual distraction.  Importantly, we explore various decomposition approaches, i.e., heuristic, geometric and attention-based approaches. Experimental results show that all these decomposition manners yield reasonable joint concepts and substantially improve cross-species generalization (and the attention-based approach is the best). 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1094. <h><a href=https://openreview.net/pdf?id=0f-0I6RFAch>Improving Out-of-distribution Generalization with Indirection Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kha Pham, Hung Le, Man Ngo, Truyen Tran</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0f-0I6RFAch>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0f-0I6RFAch.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a generic module named Indirection Layer (InLay), which leverages indirection and data internal relationships to effectively construct symbolic indirect representations to improve out-of-distribution generalization capabilities of various neural architectures. InLay receives data input in the form of a sequence of objects, treats it as a complete weighted graph whose vertices are the objects and edge weights are scalars representing relationships between vertices. The input is first mapped via indirection to a symbolic graph with data-independent and trainable vertices. This symbolic graph is then propagated, resulting in new vertex features whose indirection will be used for prediction steps afterward. Theoretically, we show that the distances between indirection representations are bounded by the distances between corresponding graphs, implying that unseen samples with very different surface statistics can still be close in the representation space to the seen samples if they share similar internal relationships. We demonstrate that InLay is consistently effective in improving out-of-distribution generalization throughout a comprehensive suite of experiments, including IQ problems, distorted image classification, and few-shot domain adaptation NLP classification. We also conduct ablation studies to verify different design choices of InLay.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1095. <h><a href=https://openreview.net/pdf?id=inU2quhGdNU>Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weiyang Liu, Longhui Yu, Adrian Weller, Bernhard Schölkopf</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=inU2quhGdNU>6.25 - [6, 8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/inU2quhGdNU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC&#39;s key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity. HUG not only provably converges to GNC, but also decouples GNC into two separate objectives. Unlike cross-entropy loss that couples intra-class compactness and inter-class separability, HUG enjoys more flexibility and serves as a good alternative loss function. Empirical results show that HUG works well in terms of generalization and robustness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1096. <h><a href=https://openreview.net/pdf?id=x5mtJD2ovc>kNN-Diffusion: Image Generation via Large-Scale Retrieval</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, Yaniv Taigman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=x5mtJD2ovc>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/x5mtJD2ovc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent text-to-image models have achieved impressive results. However, since they require large-scale datasets of text-image pairs, it is impractical to train them on new domains where data is scarce or not labeled.
In this work, we propose using large-scale retrieval methods, in particular, efficient k-Nearest-Neighbors (kNN), which offers novel capabilities: (1) training a substantially small and efficient text-to-image diffusion model using only pre-trained multi-modal embeddings, but without an explicit text-image dataset, (2) generating out-of-distribution images by simply swapping the retrieval database at inference time, and (3) performing text-driven local semantic manipulations while preserving object identity. To demonstrate the robustness of our method, we apply our kNN approach on two state-of-the-art diffusion backbones, and show results on several different datasets. As evaluated by human studies and automatic metrics, our method achieves state-of-the-art results compared to existing approaches that train text-to-image generation models using images-only dataset.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1097. <h><a href=https://openreview.net/pdf?id=ORp91sAbzI>Leveraging Unlabeled Data to Track Memorization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mahsa Forouzesh, Hanie Sedghi, Patrick Thiran</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ORp91sAbzI>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a practical metric to track memorization for neural networks, which together with the overall training accuracy can distinguish models with low label noise memorization on the training set and high generalization to unseen data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ORp91sAbzI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks may easily memorize noisy labels present in real-world data, which degrades their ability to generalize. It is therefore important to track and evaluate the robustness of models against noisy label memorization. We propose a metric, called $\textit{susceptibility}$, to gauge such memorization for neural networks. Susceptibility is simple and easy to compute during training. Moreover, it does not require access to ground-truth labels and it only uses unlabeled data. We empirically show the effectiveness of our metric in tracking memorization on various architectures and datasets and provide theoretical insights into the design of the susceptibility metric. Finally, we show through extensive experiments on datasets with synthetic and real-world label noise that one can utilize susceptibility and the overall training accuracy to distinguish models that maintain a low memorization on the training set and generalize well to unseen clean data. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1098. <h><a href=https://openreview.net/pdf?id=6ruVLB727MC>UL2: Unifying Language Learning Paradigms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6ruVLB727MC>6.25 - [6, 8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> How to train a language model properly
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6ruVLB727MC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We release Flax-based T5X model checkpoints for the 20B model publicly.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1099. <h><a href=https://openreview.net/pdf?id=EzLtB4M1SbM>Test-Time Adaptation via Self-Training with Nearest Neighbor Information</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minguk Jang, Sae-Young Chung, Hye Won Chung</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EzLtB4M1SbM>6.25 - [6, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work presents a simple and efficient test-time adaptation method to adapt trained classifiers by utilizing an ensemble of adaptation modules and self-training with nearest neighbor information.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EzLtB4M1SbM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Test-time adaptation (TTA) aims to adapt a trained classifier using online unlabeled test data only, without any information related to the training procedure. Most existing TTA methods adapt the trained classifier using the classifier&#39;s prediction on the test data as pseudo-label.
However, under test-time domain shift, accuracy of the pseudo labels cannot be guaranteed, and thus the TTA methods often encounter performance degradation at the adapted classifier. To overcome this limitation, we propose a novel test-time adaptation method, called Test-time Adaptation via Self-Training with nearest neighbor information (TAST), which is composed of the following procedures: (1) adds trainable adaptation modules on top of the trained feature extractor; (2) newly defines a pseudo-label distribution for the test data by using the nearest neighbor information; (3) trains these modules only a few times during test time to match the nearest neighbor-based pseudo label distribution and a prototype-based class distribution for the test data; and (4) predicts the label of test data using the average predicted class distribution from these modules. The pseudo-label generation is based on the basic intuition that a test data and its nearest neighbor in the embedding space are likely to share the same label under the domain shift. By utilizing multiple randomly initialized adaptation modules, TAST extracts useful information for the classification of the test data under the domain shift, using the nearest neighbor information. TAST showed better performance than the state-of-the-art TTA methods on two standard benchmark tasks, domain generalization, namely VLCS, PACS, OfficeHome, and TerraIncognita, and image corruption, particularly CIFAR-10/100C.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1100. <h><a href=https://openreview.net/pdf?id=Ki4ocDm364>Scaling Pareto-Efficient Decision Making via Offline Multi-Objective RL</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Baiting Zhu, Meihua Dang, Aditya Grover</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Ki4ocDm364>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce new dataset &amp; benchmarks and propose new algorithms for offline Multi-Objective Reinforcement Learning (MORL)
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Ki4ocDm364.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent&#39;s preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Transformers via a novel preference-and-return-conditioned policy. Empirically, we show that PEDA closely approximates the behavioral policy on the D4MORL benchmark and provides an excellent approximation of the Pareto-front with appropriate conditioning, as measured by the hypervolume and sparsity metrics. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1101. <h><a href=https://openreview.net/pdf?id=IVESH65r0Ar>A Simple Yet Powerful Deep Active Learning With Snapshots Ensembles</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Seohyeon Jung, Sanghyun Kim, Juho Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IVESH65r0Ar>6.25 - [3, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IVESH65r0Ar.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Given an unlabeled pool of data and the experts who can label them, active learning aims to build an agent that can effectively acquire data to be queried to the experts, maximizing the gain in performance when trained with them. While there are several principles for active learning, a prevailing approach is to estimate uncertainties of predictions for unlabeled samples and use them to define acquisition functions. Active learning with the uncertainty principle works well for deep learning, especially for large-scale image classification tasks with deep neural networks. Still, it is often overlooked how the uncertainty of predictions is estimated, despite the common findings on the difficulty of accurately estimating uncertainties of deep neural networks. In this paper, we highlight the effectiveness of snapshot ensembles for deep active learning. Compared to the previous approaches based on Monte-Carlo dropout or deep ensembles, we show that a simple acquisition strategy based on uncertainties estimated from parameter snapshots gathered from a single optimization path significantly improves the quality of the acquired samples. Based on this observation, we further propose an efficient active learning algorithm that maintains a single learning trajectory throughout the entire active learning episodes, unlike the existing algorithms training models from scratch for every active learning episode. Through the extensive empirical comparison, we demonstrate the effectiveness of snapshot ensembles for deep active learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1102. <h><a href=https://openreview.net/pdf?id=2QzNuaRHn4Z>Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Amrith Setlur, Don Dennis, Benjamin Eysenbach, Aditi Raghunathan, Chelsea Finn, Virginia Smith, Sergey Levine</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2QzNuaRHn4Z>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Robustness to group shifts without training group annotations can be achieved with a constrained form of DRO.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2QzNuaRHn4Z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g.,  image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need not spend valuable model capacity achieving high accuracy on contrived groups of examples. Based on this, we consider the two-player game formulation of DRO where the adversary&#39;s capacity is bitrate-constrained. Our resulting practical algorithm, Bitrate-Constrained DRO (\bdro), does not require group information on training samples yet matches the performance of Group DRO on datasets that have training group annotations and that of CVaR DRO on long-tailed distributions. Our theoretical analysis reveals that in some settings \bdro objective can provably yield statistically efficient and less conservative solutions than unconstrained CVaR DRO.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1103. <h><a href=https://openreview.net/pdf?id=CUOaVn6mYEj>Hierarchical Sliced Wasserstein Distance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Khai Nguyen, Tongzheng Ren, Huy Nguyen, Litu Rout, Tan Minh Nguyen, Nhat Ho</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CUOaVn6mYEj>6.25 - [6, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The paper proposes hierarchical sliced Wasserstein distance which is faster than the conventional sliced Wasserstein distance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CUOaVn6mYEj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sliced Wasserstein (SW) distance has been widely used in different application scenarios since it can be scaled to a large number of supports without suffering from the curse of dimensionality. The value of sliced Wasserstein distance is the average of transportation cost between one-dimensional representations (projections) of original measures that are obtained by Radon Transform (RT). Despite its efficiency in the number of supports, estimating the sliced Wasserstein requires a relatively large number of projections in high-dimensional settings. Therefore, for applications where the number of supports is relatively small compared with the dimension, e.g., several deep learning applications where the mini-batch approaches are utilized, the complexities from matrix multiplication of Radon Transform become the main computational bottleneck. To address this issue, we propose to derive projections by linearly and randomly combining a smaller number of projections which are named bottleneck projections. We explain the usage of these projections by introducing Hierarchical Radon Transform (HRT) which is constructed by applying  Radon Transform variants recursively. We then formulate the approach into a new metric between measures, named Hierarchical Sliced Wasserstein (HSW) distance. By proving the injectivity of HRT, we derive the metricity of HSW. Moreover, we investigate the theoretical properties of HSW including its connection to SW variants and its computational and sample complexities. Finally, we compare the computational cost and generative quality of HSW with the conventional SW on the task of deep generative modeling using various benchmark datasets including CIFAR10, CelebA, and Tiny ImageNet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1104. <h><a href=https://openreview.net/pdf?id=nUsP9lFADUF>Prototypical Calibration for Few-shot Learning of Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, Furu Wei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nUsP9lFADUF>6.25 - [6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nUsP9lFADUF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations. In this work, we propose prototypical calibration to adaptively learn a more robust decision boundary for zero- and few-shot classification, instead of greedy decoding. Concretely, our method first adopts Gaussian mixture distribution to estimate the prototypical clusters for all categories. Then we assign each cluster to the corresponding label by solving a weighted bipartite matching problem. Given an example, its prediction is calibrated by the likelihood of prototypical clusters. Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks. Extensive analysis across different scales also indicates that our method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1105. <h><a href=https://openreview.net/pdf?id=8wbnpOJY-f>Trainable Weight Averaging: Efficient Training by Optimizing Historical Solutions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tao Li, Zhehao Huang, Qinghua Tao, Yingwen Wu, Xiaolin Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8wbnpOJY-f>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose trainable weight averaging (TWA) to optimize historical solutions in DNNs&#39; training to achieve efficiency and better performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8wbnpOJY-f.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Stochastic gradient descent (SGD) and its variants are considered as the de-facto methods to train deep neural networks (DNNs). While recent improvements to SGD mainly focus on the descent algorithm itself, few works pay attention to utilizing the historical solutions---as an iterative method, SGD has gone through substantial explorations before convergence. Recently, an interesting attempt is stochastic weight averaging (SWA), which significantly improves the generalization by simply averaging the solutions at the tail stage of training. In this paper, we realize that the averaging coefficients could be determined in a trainable manner and propose Trainable Weight Averaging (TWA), a novel optimization method in the reduced subspace spanned by historical solutions. TWA has much greater flexibility and can be applied to the head stage of training to achieve training efficiency while preserving good generalization capability. Further, we propose a distributed training scheme to resolve the memory burden of large-scale training with efficient parallel computation. In the extensive numerical experiments, (i) TWA achieves consistent improvements over SWA with less sensitivity to learning rate; (ii) applying TWA in the head stage of training largely speeds up the convergence, resulting in over $40\%$ time saving on CIFAR and $30\%$ on ImageNet with improved generalization compared with regular training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1106. <h><a href=https://openreview.net/pdf?id=ltWade-cpK>Optimal Activation Functions for the Random Features Regression Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianxin Wang, José Bento</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ltWade-cpK>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ltWade-cpK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The asymptotic mean squared test error and sensitivity of the Random Features Regression model (RFR) have been recently studied. We build on this work and identify in closed-form the family of Activation Functions (AFs) that minimize a combination of the test error and sensitivity of the RFR under different notions of functional parsimony. We find scenarios under which the optimal AFs are linear, saturated linear functions, or expressible in terms of Hermite polynomials. Finally, we show how using optimal AFs impacts well established properties of the RFR model, such as its double descent curve, and the dependency of its optimal regularization parameter on the observation noise level.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1107. <h><a href=https://openreview.net/pdf?id=4C8ChYvMYBn>The Curious Case of Benign Memorization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sotiris Anagnostidis, Gregor Bachmann, Lorenzo Noci, Thomas Hofmann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4C8ChYvMYBn>6.25 - [8, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4C8ChYvMYBn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the empirical advances of deep learning across a variety of learning tasks, our theoretical understanding of its success is still very restricted. One of the key challenges is the overparametrized nature of modern models, enabling complete overfitting of the data even if the labels are randomized, i.e. networks can completely \textit{memorize} all given patterns. While such a memorization capacity seems worrisome, in this work we show that under training protocols that include \textit{data augmentation}, neural networks learn to memorize entirely random labels in a benign way, i.e. they learn embeddings that lead to highly non-trivial performance under nearest neighbour probing. We demonstrate that deep models have the surprising ability to separate noise from signal by distributing the task of memorization and feature learning to different layers. As a result, only the very last layers are used for memorization, while preceding layers encode performant features which remain largely unaffected by the label noise. We explore the intricate role of the augmentations used for training and identify a memorization-generalization trade-off in terms of their diversity, marking a clear distinction to all previous works. Finally, we give a first explanation for the emergence of benign memorization by showing that \textit{malign} memorization under data augmentation is infeasible due to the insufficient capacity of the model for the increased sample size. As a consequence, the network is forced to leverage the correlated nature of the augmentations and as a result learns meaningful features. To complete the picture, a better theory of feature learning in deep neural networks is required to fully understand the origins of this phenomenon.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1108. <h><a href=https://openreview.net/pdf?id=yYEb8v65X8>Light Sampling Field and BRDF Representation for Physically-based Neural Rendering</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jing Yang, Hanyuan Xiao, Wenbin Teng, Yunxuan Cai, Yajie Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yYEb8v65X8>6.25 - [3, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yYEb8v65X8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Physically-based rendering (PBR) is key for immersive rendering effects used widely in the industry to showcase detailed realistic scenes from computer graphics assets. A well-known caveat is that producing the same is computationally heavy and relies on complex capture devices. Inspired by the success in quality and efficiency of recent volumetric neural rendering, we want to develop a physically-based neural shader to eliminate device dependency and significantly boost performance. However, no existing lighting and material models in the current neural rendering approaches can accurately represent the comprehensive lighting models and BRDFs properties required by the PBR process. Thus, this paper proposes a novel lighting representation that models direct and indirect light locally through a light sampling strategy in a learned light sampling field. We also propose BRDF models to separately represent surface/subsurface scattering details to enable complex objects such as translucent material (i.e., skin, jade). We then implement our proposed representations with an end-to-end physically-based neural face skin shader, which takes a standard face asset (i.e., geometry, albedo map, and normal map) and an HDRI for illumination as inputs and generates a photo-realistic rendering as output. Extensive experiments showcase the quality and efficiency of our PBR face skin shader, indicating the effectiveness of our proposed lighting and material representations. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1109. <h><a href=https://openreview.net/pdf?id=ivwZO-HnzG_>Recon: Reducing Conflicting Gradients From the Root For Multi-Task Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guangyuan SHI, Qimai Li, Wenlong Zhang, Jiaxin Chen, Xiao-Ming Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ivwZO-HnzG_>6.25 - [3, 8, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a very simple yet effective approach to reduce the occurrence of conflicting gradients for multi-task learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ivwZO-HnzG_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A fundamental challenge for multi-task learning is that different tasks may conflict with each other when they are solved jointly, and a cause of this phenomenon is conflicting gradients during optimization. Recent works attempt to mitigate the influence of conflicting gradients by directly altering the gradients based on some criteria. However, our empirical study shows that ``gradient surgery&#39;&#39; cannot effectively reduce the occurrence of conflicting gradients. In this paper, we take a different approach to reduce conflicting gradients from the root. In essence, we investigate the task gradients w.r.t. each shared network layer, select the layers with high conflict scores, and turn them to task-specific layers. Our experiments show that such a simple approach can greatly reduce the occurrence of conflicting gradients in the remaining shared layers and achieve better performance, with only a slight increase in model parameters in many cases. Our approach can be easily applied to improve various state-of-the-art methods including gradient manipulation methods and branched architecture search methods. Given a network architecture (e.g., ResNet18), it only needs to search for the conflict layers once, and the network can be modified to be used with different methods on the same or even different datasets to gain performance improvement. The source code is available at https://github.com/moukamisama/Recon.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1110. <h><a href=https://openreview.net/pdf?id=4PJUBT9f2Ol>Generative Modelling with Inverse Heat Dissipation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Severi Rissanen, Markus Heinonen, Arno Solin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4PJUBT9f2Ol>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a generative model that iteratively reverses the heat equation, increasing the effective resolution of the image
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4PJUBT9f2Ol.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While diffusion models have shown great success in image generation, their noise-inverting generative process does not explicitly consider the structure of images, such as their inherent multi-scale nature. Inspired by diffusion models and the empirical success of coarse-to-fine modelling, we propose a new diffusion-like model that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image. We interpret the solution of the forward heat equation with constant additive noise as a variational approximation in the diffusion latent variable model. Our new model shows emergent qualitative properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images. Spectral analysis on natural images highlights connections to diffusion models and reveals an implicit coarse-to-fine inductive bias in them.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1111. <h><a href=https://openreview.net/pdf?id=sZI1Oj9KBKy>TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chaitanya Murti, Tanay Narshana, Chiranjib Bhattacharyya</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sZI1Oj9KBKy>6.25 - [8, 6, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use the total variation distance between the class conditional distributions of filter outputs for structured pruning of neural networks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sZI1Oj9KBKy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Achieving structured, data-free sparsity of deep neural networks (DNNs) remains an open area of research.  In this work, we address the challenge of pruning filters without access to the original training set or loss function. We propose the discriminative filters hypothesis, that well-trained models possess discriminative filters, and any non-discriminative filters can be pruned without impacting the predictive performance of the classifier. Based on this hypothesis, we propose a new paradigm for pruning neural networks: distributional pruning, wherein we only require access to the distributions that generated the original datasets. Our approach to solving the problem of formalising and quantifying the discriminating ability of filters is through the total variation (TV) distance between the class-conditional distributions of the filter outputs. We present empirical results that, using this definition of discriminability, support our hypothesis on a variety of datasets and architectures. Next, we define the LDIFF score, a heuristic to quantify the extent to which a layer possesses a mixture of discriminative and non-discriminative filters. We empirically demonstrate that the LDIFF score is indicative of the performance of random pruning for a given layer, and thereby indicates the extent to which a layer may be pruned. Our main contribution is a novel one-shot pruning algorithm, called TVSPrune, that identifies non-discriminative filters for pruning. We extend this algorithm to IterTVSPrune, wherein we iteratively apply TVSPrune, thereby enabling us to achieve greater sparsity. Last, we demonstrate the efficacy of the TVSPrune on a variety of datasets, and show that in some cases, we can prune up to 60% of parameters with only a 2% loss of accuracy without any fine-tuning of the model, beating the nearest baseline by almost 10%.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1112. <h><a href=https://openreview.net/pdf?id=Sy-o2N0hF4f>Become a Proficient Player with Limited Data through Watching Pure Videos</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weirui Ye, Yunsheng Zhang, Pieter Abbeel, Yang Gao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Sy-o2N0hF4f>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Sy-o2N0hF4f.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, RL has shown its strong ability for visually complex tasks. However, it suffers from the low sample efficiency and poor generalization ability, which prevent RL from being useful in real-world scenarios. Inspired by the huge success of unsupervised pre-training methods on language and vision domains, we propose to improve the sample efficiency via a novel pre-training method for model-based RL. 
Instead of using pre-recorded agent trajectories that come with their own actions, we consider the setting where the pre-training data are action-free videos, which are more common and available in the real world. We introduce a two-phase training pipeline as follows: for the pre-training phase, we implicitly extract the hidden action embedding from videos and pre-train the visual representation and the environment dynamics network through a novel \Changes{forward-inverse} cycle consistency \Changes{(FICC)} objective based on vector quantization; for down-stream tasks, we finetune with small amount of task data based on the learned models. Our framework can significantly improve the sample efficiency on Atari Games with data of only one hour of game playing. We achieve 118.4\% mean human performance and 36.0\% median performance with only 50k environment steps, which is 85.6\% and 65.1\% better than the scratch EfficientZero model. We believe such pre-training approach can provide an option for solving real-world RL problems. The code is available at \url{https://github.com/YeWR/FICC.git}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1113. <h><a href=https://openreview.net/pdf?id=c5tbxWXU9-y>Information-Theoretic Analysis of Unsupervised Domain Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziqiao Wang, Yongyi Mao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=c5tbxWXU9-y>6.25 - [3, 8, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We derived new information-theoretic generalization bounds for the unsupervised domain adaptation problem.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/c5tbxWXU9-y.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper uses information-theoretic tools to analyze the generalization error in unsupervised domain adaptation (UDA). We present novel upper bounds for two notions of generalization errors. The first notion measures the gap between the population risk in the target domain and that in the source domain, and the second measures the gap between the population risk in the target domain and the empirical risk in the source domain. While our bounds for the first kind of error are in line with the traditional analysis and give similar insights, our bounds on the second kind of error are algorithm-dependent, which also provide insights into algorithm designs. Specifically, we present two simple techniques for improving generalization in UDA and validate them experimentally.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1114. <h><a href=https://openreview.net/pdf?id=P4bXCawRi5J>Understanding Zero-shot Adversarial Robustness for Large-Scale Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, Carl Vondrick</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=P4bXCawRi5J>6.25 - [6, 8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/P4bXCawRi5J.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP&#39;s performance on new tasks. In this work, we identify and explore the problem of adapting large-scale models for zero-shot adversarial robustness. We first identify two key factors during model adaption--training losses and adaptation methods--that affect the model&#39;s zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1115. <h><a href=https://openreview.net/pdf?id=x-mXzBgCX3a>FairGBM: Gradient Boosting with Fairness Constraints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>André Cruz, Catarina G Belém, João Bravo, Pedro Saleiro, Pedro Bizarro</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=x-mXzBgCX3a>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel fairness-aware method based on constrained optimization for Gradient Boosting models, that can match state-of-the-art fairness and performance while training 10x faster.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/x-mXzBgCX3a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Tabular data is prevalent in many high-stakes domains, such as financial services or public policy. Gradient Boosted Decision Trees (GBDT) are popular in these settings due to their scalability, performance, and low training cost. While fairness in these domains is a foremost concern, existing in-processing Fair ML methods are either incompatible with GBDT, or incur in significant performance losses while taking considerably longer to train. We present FairGBM, a dual ascent learning framework for training GBDT under fairness constraints, with little to no impact on predictive performance when compared to unconstrained GBDT. Since observational fairness metrics are non-differentiable, we propose smooth convex error rate proxies for common fairness criteria, enabling gradient-based optimization using a ``proxy-Lagrangian&#39;&#39; formulation. Our implementation shows an order of magnitude speedup in training time relative to related work, a pivotal aspect to foster the widespread adoption of FairGBM by real-world practitioners.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1116. <h><a href=https://openreview.net/pdf?id=_BoPed4tYww>Timing is Everything: Learning to Act Selectively with Costly Actions and Budgetary Constraints</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>David Henry Mguni, Aivar Sootla, Juliusz Krzysztof Ziomek, Oliver Slumbers, Zipeng Dai, Kun Shao, Jun Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_BoPed4tYww>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_BoPed4tYww.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many real-world settings involve costs for performing actions; transaction costs
in financial systems and fuel costs being common examples. In these settings,
performing actions at each time step quickly accumulates costs leading to vastly
suboptimal outcomes. Additionally, repeatedly acting produces wear and tear and
ultimately, damage. Determining when to act is crucial for achieving successful
outcomes and yet, the challenge of efficiently learning to behave optimally when
actions incur minimally bounded costs remains unresolved. In this paper, we intro-
duce a reinforcement learning (RL) framework named Learnable Impulse Control
Reinforcement Algorithm (LICRA), for learning to optimally select both when
to act and which actions to take when actions incur costs. At the core of LICRA
is a nested structure that combines RL and a form of policy known as impulse
control which learns to maximise objectives when actions incur costs. We prove
that LICRA, which seamlessly adopts any RL method, converges to policies that
optimally select when to perform actions and their optimal magnitudes. We then
augment LICRA to handle problems in which the agent can perform at most k &lt; ∞
actions and more generally, faces a budget constraint. We show LICRA learns the
optimal value function and ensures budget constraints are satisfied almost surely.
We demonstrate empirically LICRA’s superior performance against benchmark
RL methods in OpenAI gym’s Lunar Lander and in Highway environments and a
variant of the Merton portfolio problem within finance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1117. <h><a href=https://openreview.net/pdf?id=sKHqgFOaFXI>How Informative is the Approximation Error from Tensor Decomposition for Neural Network Compression?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jetze Schuurmans, kim batselier, Julian Kooij</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sKHqgFOaFXI>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show empirically an approximation error resulting from compressing a network layer with tensor decomposition is correlated with the classification error, enabling the choice of layer, decomposition and rank to be based on the approximation error.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sKHqgFOaFXI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Tensor decompositions have been successfully applied to compress neural networks. The compression algorithms using tensor decompositions commonly minimize the approximation error on the weights. Recent work assumes the approximation error on the weights is a proxy for the performance of the model to compress multiple layers and fine-tune the compressed model. Surprisingly, little research has systematically evaluated which approximation errors can be used to make choices regarding the layer, tensor decomposition method, and level of compression. To close this gap, we perform an experimental study to test if this assumption holds across different layers and types of decompositions, and what the effect of fine-tuning is. We include the approximation error on the features resulting from a compressed layer in our analysis to test if this provides a better proxy, as it explicitly takes the data into account. We find the approximation error on the weights has a positive correlation with the performance error, before as well as after fine-tuning. Basing the approximation error on the features does not improve the correlation significantly. While scaling the approximation error commonly is used to account for the different sizes of layers, the average correlation across layers is smaller than across all choices (i.e. layers, decompositions, and level of compression) before fine-tuning. When calculating the correlation across the different decompositions, the average rank correlation is larger than across all choices. This means multiple decompositions can be considered for compression and the approximation error can be used to choose between them.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1118. <h><a href=https://openreview.net/pdf?id=pvgEL1yS3Ql>Cross-Layer Retrospective Retrieving via Layer Attention</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yanwen Fang, Yuxi CAI, Jintai Chen, Jingyu Zhao, Guangjian Tian, Guodong Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pvgEL1yS3Ql>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A multi-head recurrent layer attention mechanism is proposed to retrieve query-related information from previous layers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pvgEL1yS3Ql.jpg loading="lazy">
                </div>
                <div class="abstract">
                    More and more evidence has shown that strengthening layer interactions can enhance the representation power of a deep neural network, while self-attention excels at learning interdependencies by retrieving query-activated information. Motivated by this, we devise a cross-layer attention mechanism, called multi-head recurrent layer attention (MRLA), that sends a query representation of the current layer to all previous layers to retrieve query-related information from different levels of receptive fields. A light-weighted version of MRLA is also proposed to reduce the quadratic computation cost. The proposed layer attention mechanism can enrich the representation power of many state-of-the-art vision networks, including CNNs and vision transformers. Its effectiveness has been extensively evaluated in image classification, object detection and instance segmentation tasks, where improvements can be consistently observed. For example, our MRLA can improve 1.6% Top-1 accuracy on ResNet-50, while only introducing 0.16M parameters and 0.07B FLOPs. Surprisingly, it can boost the performances by a large margin of 3-4% box AP and mask AP in dense prediction tasks. Our code is available at https://github.com/joyfang1106/MRLA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1119. <h><a href=https://openreview.net/pdf?id=AwWaBXLIJE>Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wei Hung, Bo Kai Huang, Ping-Chun Hsieh, Xi Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AwWaBXLIJE>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We boost the sample efficiency of multi-objective RL by using Q snapshots 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AwWaBXLIJE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many real-world continuous control problems are in the dilemma of weighing the pros and cons, multi-objective reinforcement learning (MORL) serves as a generic framework of learning control policies for different preferences over objectives. However, the existing MORL methods either rely on multiple passes of explicit search for finding the Pareto front and therefore are not sample-efficient, or utilizes a shared policy network for coarse knowledge sharing among policies. To boost the sample efficiency of MORL, we propose $Q$-Pensieve, a policy improvement scheme that stores a collection of $Q$-snapshots to jointly determine the policy update direction and thereby enables data sharing at the policy level. We show that $Q$-Pensieve can be naturally integrated with soft policy iteration with convergence guarantee. To substantiate this concept, we propose the technique of $Q$ replay buffer, which stores the learned $Q$-networks from the past iterations, and arrive at a practical actor-critic implementation. Through extensive experiments and an ablation study, we demonstrate that with much fewer samples, the proposed algorithm can outperform the benchmark MORL methods on a variety of MORL benchmark tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1120. <h><a href=https://openreview.net/pdf?id=V_06QV-kZX>Analyzing Tree Architectures in Ensembles via Neural Tangent Kernel</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ryuichi Kanoh, Mahito Sugiyama</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=V_06QV-kZX>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We formulate and analyze the Neural Tangent Kernel (NTK) induced by soft tree ensembles for arbitrary tree architectures
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/V_06QV-kZX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A soft tree is an actively studied variant of a decision tree that updates splitting rules using the gradient method. Although soft trees can take various architectures, their impact is not theoretically well known. In this paper, we formulate and analyze the Neural Tangent Kernel (NTK) induced by soft tree ensembles for arbitrary tree architectures. This kernel leads to the remarkable finding that only the number of leaves at each depth is relevant for the tree architecture in ensemble learning with an infinite number of trees. In other words, if the number of leaves at each depth is fixed, the training behavior in function space and the generalization performance are exactly the same across different tree architectures, even if they are not isomorphic. We also show that the NTK of asymmetric trees like decision lists does not degenerate when they get infinitely deep. This is in contrast to the perfect binary trees, whose NTK is known to degenerate and leads to worse generalization performance for deeper trees.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1121. <h><a href=https://openreview.net/pdf?id=CGuvK3U09LH>Suppressing the Heterogeneity: A Strong Feature Extractor for Few-shot Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhengdong Hu, Yifan Sun, Yi Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CGuvK3U09LH>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CGuvK3U09LH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper tackles the Few-shot Semantic Segmentation (FSS) task with focus on learning the feature extractor. Somehow the feature extractor has been overlooked by recent state-of-the-art methods, which directly use a deep model pretrained on ImageNet for feature extraction (without further fine-tuning). Under this background, we think the FSS feature extractor deserves exploration and observe the heterogeneity (i.e., the intra-class diversity in the raw images) as a critical challenge hindering the intra-class feature compactness. The heterogeneity has three levels from coarse to fine: 1) Sample-level: the inevitable distribution gap between the support and query images makes them heterogeneous from each other. 2) Region-level: the background in FSS actually contains multiple regions with different semantics. 3) Patch-level: some neighboring patches belonging to a same class may appear quite different from each other. Motivated by these observations, we propose a feature extractor with Multi-level Heterogeneity Suppressing (MuHS). MuHS leverages the attention mechanism in transformer backbone to effectively suppress all these three-level heterogeneity. Concretely, MuHS reinforces the attention / interaction between different samples (query and support), different regions and neighboring patches by constructing cross-sample attention, cross-region interaction and a novel masked image segmentation (inspired by the recent masked image modeling), respectively. We empirically show that 1) MuHS brings consistent improvement for various FSS heads and 2) using a simple linear classification head, MuHS sets new states of the art on multiple FSS datasets, validating the importance of FSS feature learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1122. <h><a href=https://openreview.net/pdf?id=cddbeL1HWaD>Cheap Talk Discovery and Utilization in Multi-Agent Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yat Long Lo, Christian Schroeder de Witt, Samuel Sokota, Jakob Nicolaus Foerster, Shimon Whiteson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cddbeL1HWaD>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel problem formulation and methodology in MARL on learning where to communicate and where best to communicate.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cddbeL1HWaD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    By enabling agents to communicate, recent cooperative multi-agent reinforcement learning (MARL) methods have demonstrated better task performance and more coordinated behavior. Most existing approaches facilitate inter-agent communication by allowing agents to send messages to each other through free communication channels, i.e., \emph{cheap talk channels}. Current methods require these channels to be constantly accessible and known to the agents a priori. In this work, we lift these requirements such that the agents must discover the cheap talk channels and learn how to use them. Hence, the problem has two main parts: \emph{cheap talk discovery} (CTD) and \emph{cheap talk utilization} (CTU). We introduce a novel conceptual framework for both parts and develop a new algorithm based on mutual information maximization that outperforms existing algorithms in CTD/CTU settings. We also release a novel benchmark suite to stimulate future research in CTD/CTU.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1123. <h><a href=https://openreview.net/pdf?id=xPkJYRsQGM>Contrastive Learning for Unsupervised Domain Adaptation of Time Series</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yilmazcan Ozyurt, Stefan Feuerriegel, Ce Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xPkJYRsQGM>6.25 - [6, 3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a novel framework for UDA of time series data, called CLUDA, through contrastive learning framework to learn domain-invariant contextual representations in multivariate time series.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xPkJYRsQGM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised domain adaptation (UDA) aims at learning a machine learning model using a labeled source domain that performs well on a similar yet different, unlabeled target domain. UDA is important in many applications such as medicine, where it is used to adapt risk scores across different patient cohorts. In this paper, we develop a novel framework for UDA of time series data, called CLUDA. Specifically, we propose a contrastive learning framework to learn contextual representations in multivariate time series, so that these preserve label information for the prediction task. In our framework, we further capture the variation in the contextual representations between source and target domain via a custom nearest-neighbor contrastive learning. To the best of our knowledge, ours is the first framework to learn domain-invariant, contextual representation for UDA of time series data. We evaluate our framework using a wide range of time series datasets to demonstrate its effectiveness and show that it achieves state-of-the-art performance for time series UDA. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1124. <h><a href=https://openreview.net/pdf?id=TuHkVOjSAR>Strategic Classification with Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Itay Eilat, Ben Finkelshtein, Chaim Baskin, Nir Rosenfeld</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TuHkVOjSAR>6.25 - [8, 8, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the problem of learning graph neural networks under strategic user behavior, and propose an efficient method for learning strategy-robust graph-based classifiers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TuHkVOjSAR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Strategic classification studies learning in settings where users can modify their features to obtain favorable predictions. Most current works focus on simple classifiers that trigger independent user responses. Here we examine the implications of learning with more elaborate models that break the independence assumption. Motivated by the idea that applications of strategic classification are often social in nature, we focus on graph neural networks, which make use of social relations between users to improve predictions. Using a graph for learning introduces inter-user dependencies in prediction; our key point is that strategic users can exploit these to promote their goals. As we show through analysis and simulation, this can work either against the system---or for it. Based on this, we propose a differentiable framework for strategically-robust learning of graph-based classifiers. Experiments on several real networked datasets demonstrate the utility of our approach.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1125. <h><a href=https://openreview.net/pdf?id=b39dQt_uffW>Safe Reinforcement Learning From Pixels Using a Stochastic Latent Representation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yannick Hogewind, Thiago D. Simão, Tal Kachman, Nils Jansen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b39dQt_uffW>6.25 - [5, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes Safe SLAC, a safety-constrained RL approach for partially observable settings, which uses a stochastic latent variable model combined with a safety critic.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b39dQt_uffW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We address the problem of safe reinforcement learning from pixel observations. Inherent challenges in such settings are (1) a trade-off between reward optimization and adhering to safety constraints, (2) partial observability, and (3) high-dimensional observations. We formalize the problem in a constrained, partially observable Markov decision process framework, where an agent obtains distinct reward and safety signals. To address the curse of dimensionality, we employ a novel safety critic using the stochastic latent actor-critic (SLAC) approach. The latent variable model predicts rewards and safety violations, and we use the safety critic to train safe policies. Using well-known benchmark environments, we demonstrate competitive performance over existing approaches regarding computational requirements, final reward return, and satisfying the safety constraints. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1126. <h><a href=https://openreview.net/pdf?id=Xo2E217_M4n>FLIP: A Provable Defense Framework for Backdoor Mitigation in Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaiyuan Zhang, Guanhong Tao, Qiuling Xu, Siyuan Cheng, Shengwei An, Yingqi Liu, Shiwei Feng, Guangyu Shen, Pin-Yu Chen, Shiqing Ma, Xiangyu Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Xo2E217_M4n>6.25 - [8, 6, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Xo2E217_M4n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated Learning (FL) is a distributed learning paradigm that enables different parties to train a model together for high quality and strong privacy protection. In this scenario, individual participants may get compromised and perform backdoor attacks by poisoning the data (or gradients). Existing work on robust aggregation and certified FL robustness does not study how hardening benign clients can affect the global model (and the malicious clients). In this work, we theoretically analyze the connection among cross-entropy loss, attack success rate, and clean accuracy in this setting. Moreover, we propose a trigger reverse engineering based defense and show that our method can achieve robustness improvement with guarantee (i.e., reducing the attack success rate) without affecting benign accuracy. We conduct comprehensive experiments across different datasets and attack settings. Our results on nine competing SOTA defense methods show the empirical superiority of our method on both single-shot and continuous FL backdoor attacks. Code is available at https://github.com/KaiyuanZh/FLIP.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1127. <h><a href=https://openreview.net/pdf?id=Peot1SFDX0>Preference Transformer: Modeling Human Preferences using Transformers for RL</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, Kimin Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Peot1SFDX0>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a transformer-based architecture for preference-based RL considering non-Markovian rewards.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Peot1SFDX0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1128. <h><a href=https://openreview.net/pdf?id=b9tUk-f_aG>Bridging the Gap to Real-World Object-Centric Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Schölkopf, Thomas Brox, Francesco Locatello</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b9tUk-f_aG>6.25 - [6, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our method uses slot attention with self-supervised DINO features to discover objects on real-world data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b9tUk-f_aG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Humans naturally decompose their environment into entities at the appropriate level of abstraction to act in the world. Allowing machine learning algorithms to derive this decomposition in an unsupervised way has become an important line of research. However, current methods are restricted to simulated data or require additional information in the form of motion or depth in order to successfully discover objects. In this work, we overcome this limitation by showing that reconstructing features from models trained in a self-supervised manner is a sufficient training signal for object-centric representations to arise in a fully unsupervised way. Our approach, DINOSAUR, significantly out-performs existing object-centric learning models on simulated data and is the first unsupervised object-centric model that scales to real world-datasets such as COCO and PASCAL VOC. DINOSAUR is conceptually simple and shows competitive performance compared to more involved pipelines from the computer vision literature. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1129. <h><a href=https://openreview.net/pdf?id=bRwBpKrNzF7>Faster Last-iterate Convergence of Policy Optimization in Zero-Sum Markov Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shicong Cen, Yuejie Chi, Simon Shaolei Du, Lin Xiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bRwBpKrNzF7>6.25 - [8, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We achieve better last-iterate convergence result of policy optimization for two-player zero-sum Markov games, with single-loop and symmetric update rules.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bRwBpKrNzF7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multi-Agent Reinforcement Learning (MARL)---where multiple agents learn to interact in a shared dynamic environment---permeates across a wide range of critical applications. While there has been substantial progress on understanding the global convergence of policy optimization methods in single-agent RL, designing and analysis of efficient policy optimization algorithms in the MARL setting present significant challenges and new desiderata, which unfortunately, remain highly inadequately addressed by existing theory. In this paper, we focus on the most basic setting of competitive multi-agent RL, namely two-player zero-sum Markov games, and study equilibrium finding algorithms in both the infinite-horizon discounted setting and the finite-horizon episodic setting. We propose a single-loop policy optimization method with symmetric updates from both agents, where the policy is updated via the entropy-regularized optimistic multiplicative weights update (OMWU) method and the value is updated on a slower timescale. We show that, in the full-information tabular setting, the proposed method achieves a finite-time last-iterate linear convergence to the quantal response equilibrium of the regularized problem, which translates to a sublinear convergence to the Nash equilibrium by controlling the amount of regularization. Our convergence results improve upon the best known iteration complexities, and lead to a better understanding of policy optimization in competitive Markov games.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1130. <h><a href=https://openreview.net/pdf?id=rB3zRN0lBYr>Memorization Capacity of Neural Networks with Conditional Computation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Erdem Koyuncu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rB3zRN0lBYr>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In classical &#34;unconditional&#34; ReLU nets, one needs $O(\sqrt{n})$ arithmetic operations to recall any one of $n$ stored patterns. Conditional computation reduces this to $O(\log n)$ and this is the best possible. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rB3zRN0lBYr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many empirical studies have demonstrated the performance benefits of conditional computation in neural networks, including reduced inference time and power consumption. We study the fundamental limits of neural conditional computation from the perspective of memorization capacity. For Rectified Linear Unit (ReLU) networks without conditional computation, it is known that memorizing a collection of $n$ input-output relationships can be accomplished via a neural network with $O(\sqrt{n})$ neurons. Calculating the output of this neural network can be accomplished using $O(\sqrt{n})$ elementary arithmetic operations of additions, multiplications and comparisons for each input. Using a conditional ReLU network, we show that the same task can be accomplished using only $O(\log n)$ operations per input. This represents an almost exponential improvement as compared to networks without conditional computation. We also show that the $\Theta(\log n)$ rate is the best possible. Our achievability result utilizes a general methodology to synthesize a conditional network out of an unconditional network in a computationally-efficient manner, bridging the gap between unconditional and conditional architectures. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1131. <h><a href=https://openreview.net/pdf?id=Zuc_MHtUma4>Kernel Neural Optimal Transport</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alexander Korotin, Daniil Selikhanovych, Evgeny Burnaev</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Zuc_MHtUma4>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Zuc_MHtUma4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the Neural Optimal Transport (NOT) algorithm which uses the general optimal transport formulation and learns stochastic transport plans. We show that NOT with the weak quadratic cost may learn fake plans which are not optimal. To resolve this issue, we introduce kernel weak quadratic costs. We show that they provide improved theoretical guarantees and practical performance. We test NOT with kernel costs on the unpaired image-to-image translation task.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1132. <h><a href=https://openreview.net/pdf?id=-ng-FXFlzgK>Neural Image-based Avatars: Generalizable Radiance Fields for Human Avatar Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>YoungJoong Kwon, Dahun Kim, Duygu Ceylan, Henry Fuchs</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-ng-FXFlzgK>6.25 - [8, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-ng-FXFlzgK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a method that enables synthesizing novel views and novel poses of arbitrary human performers from sparse multi-view images. A key ingredient of our method is a hybrid appearance blending module that combines the advantages of the implicit body NeRF representation and image-based rendering. Existing generalizable human NeRF methods that are conditioned on the body model have shown robustness against the geometric variation of arbitrary human performers. Yet they often exhibit blurry results when generalized onto unseen identities. Meanwhile, image-based rendering shows high-quality results when sufficient observations are available, whereas it suffers artifacts in sparse-view settings. We propose Neural Image-based Avatars (NIA) that exploits the best of those two methods: to maintain robustness under new articulations and self-occlusions while directly leveraging the available (sparse) source view colors to preserve appearance details of new subject identities. Our hybrid design outperforms recent methods on both in-domain identity generalization as well as challenging cross-dataset generalization settings. Also, in terms of the pose generalization, our method outperforms even the per-subject optimized animatable NeRF methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1133. <h><a href=https://openreview.net/pdf?id=-z9hdsyUwVQ>Linear Convergence of Natural Policy Gradient Methods with Log-Linear Policies</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Rui Yuan, Simon Shaolei Du, Robert M. Gower, Alessandro Lazaric, Lin Xiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-z9hdsyUwVQ>6.25 - [8, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show linear convergence of natural policy gradient methods with log-linear policies without any regularization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-z9hdsyUwVQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider infinite-horizon discounted Markov decision processes and study the convergence rates of the natural policy gradient (NPG) and the Q-NPG methods with the log-linear policy class. Using the compatible function approximation framework, both methods with log-linear policies can be written as approximate versions of the policy mirror descent (PMD) method. We show that both methods attain linear convergence rates and $\tilde{\mathcal{O}}(1/\epsilon^2)$ sample complexities using a simple, non-adaptive geometrically increasing step size, without resorting to entropy or other strongly convex regularization. Lastly, as a byproduct, we obtain sublinear convergence rates for both methods with arbitrary constant step size.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1134. <h><a href=https://openreview.net/pdf?id=i-DleYh34BM>Pruning Deep Neural Networks from a Sparsity Perspective</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Enmao Diao, Ganghua Wang, Jiawei Zhang, Yuhong Yang, Jie Ding, Vahid Tarokh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=i-DleYh34BM>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work develops PQ Index (PQI) as a new measure of sparsity and proposes a Sparsity-informed Adaptive Pruning (SAP) algorithm. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/i-DleYh34BM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1135. <h><a href=https://openreview.net/pdf?id=X5SUR7g2vVw>Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Penghao Wu, Li Chen, Hongyang Li, Xiaosong Jia, Junchi Yan, Yu Qiao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=X5SUR7g2vVw>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a visuomotor driving policy pre-training paradigm, which leverages self-supervised geometric modeling to learn driving policy representation and achieves superior performance on various downstream driving tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/X5SUR7g2vVw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Witnessing the impressive achievements of pre-training techniques on large-scale data in the field of computer vision and natural language processing, we wonder whether this idea could be adapted in a grab-and-go spirit, and mitigate the sample inefficiency problem for visuomotor driving. Given the highly dynamic and variant nature of the input, the visuomotor driving task inherently lacks the view and translation invariance, and the visual input contains massive irrelevant information for decision making, resulting in predominant pre-training approaches from general vision less suitable for the autonomous driving task. To this end, we propose PPGeo (Policy Pre-training via Geometric modeling), an intuitive and straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving. We aim at learning policy representations as a powerful abstraction by modeling 3D geometric scenes on large-scale unlabeled and uncalibrated YouTube driving videos. The proposed PPGeo is performed in two stages to support effective self-supervised training. In the first stage, the geometric modeling framework generates pose and depth predictions simultaneously, with two consecutive frames as input. In the second stage, the visual encoder learns driving policy representation by predicting the future ego-motion and optimizing with the photometric error based on current visual observation only. As such, the pre-trained visual encoder is equipped with rich driving policy related representations and thereby competent for multiple visuomotor driving tasks. As a side product, the pre-trained geometric modeling networks could bring further improvement to the depth and odometry estimation tasks. Extensive experiments covering a wide span of challenging scenarios have demonstrated the superiority of our proposed approach, where improvements range from 2% to even over 100% with very limited data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1136. <h><a href=https://openreview.net/pdf?id=znLlSgN-4S0>More Centralized Training, Still Decentralized Execution: Multi-Agent Conditional Policy Factorization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiangxing Wang, Deheng Ye, Zongqing Lu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=znLlSgN-4S0>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel multi-agent reinforcement learning algorithm where dependency among agents is explicitly considered
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/znLlSgN-4S0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In cooperative multi-agent reinforcement learning (MARL), combining value decomposition with actor-critic enables agents to learn stochastic policies, which are more suitable for the partially observable environment. Given the goal of learning local policies that enable decentralized execution, agents are commonly assumed to be independent of each other, even in centralized training. However, such an assumption may prohibit agents from learning the optimal joint policy. To address this problem, we explicitly take the dependency among agents into centralized training. Although this leads to the optimal joint policy, it may not be factorized for decentralized execution. Nevertheless, we theoretically show that from such a joint policy, we can always derive another joint policy that achieves the same optimality but can be factorized for decentralized execution. To this end, we propose multi-agent conditional policy factorization (MACPF), which takes more centralized training but still enables decentralized execution. We empirically verify MACPF in various cooperative MARL tasks and demonstrate that MACPF achieves better performance or faster convergence than baselines. Our code is available at https://github.com/PKU-RL/FOP-DMAC-MACPF.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1137. <h><a href=https://openreview.net/pdf?id=wlMDF1jQF86>Which Layer is Learning Faster? A Systematic Exploration of Layer-wise Convergence Rate for Deep Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yixiong Chen, Alan Yuille, Zongwei Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wlMDF1jQF86>6.25 - [5, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We empirically show that the shallower layers converge faster than the deeper layers in neural networks, and provide the theoretical justification and practical value of this finding.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wlMDF1jQF86.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The deeply hierarchical structures enable deep neural networks (DNNs) to fit extremely complex target functions. However, the complex interaction between layers also makes the learning process of a particular layer poorly understood. This work demonstrates that the shallower layers of DNNs tend to converge faster than the deeper layers. We call this phenomenon Layer Convergence Bias. We also uncover the fundamental reason behind this phenomenon: Flatter local minima of shallower layers make their gradients more stable and predictive, allowing for faster training. Another surprising result is that the shallower layers tend to learn the low-frequency components of the target function, while the deeper layers usually learn the high-frequency components. It is consistent with the recent discovery that DNNs learn lower frequency objects faster.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1138. <h><a href=https://openreview.net/pdf?id=pf8RIZTMU58>DepthFL : Depthwise Federated Learning for Heterogeneous Clients</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minjae Kim, Sangyoon Yu, Suhyun Kim, Soo-Mook Moon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pf8RIZTMU58>6.25 - [8, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> DepthFL is a new federated learning framework based on depth scaling to tackle system heterogeneity.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pf8RIZTMU58.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning is for training a global model without collecting private local data from clients. As they repeatedly need to upload locally-updated weights or gradients instead, clients require both computation and communication resources enough to participate in learning, but in reality their resources are heterogeneous. To enable resource-constrained clients to train smaller local models, width scaling techniques have been used, which reduces the channels of a global model. Unfortunately, width scaling suffers from heterogeneity of local models when averaging them, leading to a lower accuracy than when simply excluding resource-constrained clients from training. This paper proposes a new approach based on depth scaling called DepthFL. DepthFL defines local models of different depths by pruning the deepest layers off the global model, and allocates them to clients depending on their available resources. Since many clients do not have enough resources to train deep local models, this would make deep layers partially-trained with insufficient data, unlike shallow layers that are fully trained. DepthFL alleviates this problem by mutual self-distillation of knowledge among the classifiers of various depths within a local model. Our experiments show that depth-scaled local models build a global model better than width-scaled ones, and that self-distillation is highly effective in training data-insufficient deep layers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1139. <h><a href=https://openreview.net/pdf?id=nJfylDvgzlq>Make-A-Video: Text-to-Video Generation without Text-Video Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, Yaniv Taigman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nJfylDvgzlq>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nJfylDvgzlq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose Make-A-Video -- an approach for directly translating the tremendous recent progress in Text-to-Image (T2I) generation to Text-to-Video (T2V). Our intuition is simple: learn what the world looks like and how it is described from paired text-image data, and learn how the world moves from unsupervised video footage. Make-A-Video has three advantages: (1) it accelerates training of the T2V model (it does not need to learn visual and multimodal representations from scratch), (2) it does not require paired text-video data, and (3) the generated videos inherit the vastness (diversity in aesthetic, fantastical depictions, etc.) of today&#39;s image generation models. 
We design a simple yet effective way to build on T2I models with novel and effective spatial-temporal modules. First, we decompose the full temporal U-Net and attention tensors and approximate them in space and time. Second, we design a spatial temporal pipeline to generate high resolution and frame rate videos with a video decoder, interpolation model and two super resolution models that can enable various applications besides T2V. In all aspects, spatial and temporal resolution, faithfulness to text, and quality, Make-A-Video sets the new state-of-the-art in text-to-video generation, as determined by both qualitative and quantitative measures.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1140. <h><a href=https://openreview.net/pdf?id=B92TMCG_7rp>Re-parameterizing Your Optimizers rather than Architectures</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaohan Ding, Honghao Chen, Xiangyu Zhang, Kaiqi Huang, Jungong Han, Guiguang Ding</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=B92TMCG_7rp>6.25 - [6, 8, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Modify gradient flow to incorporate model-specific prior knowledge into the optimizers for training simple and efficient models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/B92TMCG_7rp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The well-designed structures in neural networks reflect the prior knowledge incorporated into the models. However, though different models have various priors, we are used to training them with model-agnostic optimizers such as SGD. In this paper, we propose to incorporate model-specific prior knowledge into optimizers by modifying the gradients according to a set of model-specific hyper-parameters. Such a methodology is referred to as Gradient Re-parameterization, and the optimizers are named RepOptimizers. For the extreme simplicity of model structure, we focus on a VGG-style plain model and showcase that such a simple model trained with a RepOptimizer, which is referred to as RepOpt-VGG, performs on par with or better than the recent well-designed models. From a practical perspective, RepOpt-VGG is a favorable base model because of its simple structure, high inference speed and training efficiency. Compared to Structural Re-parameterization, which adds priors into models via constructing extra training-time structures, RepOptimizers require no extra forward/backward computations and solve the problem of quantization. We hope to spark further research beyond the realms of model structure design. Code and models https://github.com/DingXiaoH/RepOptimizers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1141. <h><a href=https://openreview.net/pdf?id=nWTzIsgrYNN>Composite Slice Transformer: An Efficient Transformer with Composition of Multi-Scale Multi-Range Attentions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingu Lee, Saurabh Pitre, Tianyu Jiang, Pierre-David Letourneau, Matthew J Morse, Kanghwan Jang, Joseph Soriaga, Parham Noorzad, Hsin-Pai Cheng, Christopher Lott</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nWTzIsgrYNN>6.25 - [5, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an efficient Transformer based on composition of multi-scale attention with stacked slice representation and show that it outperforms the state-of-the-art efficient transformers in multiple benchmarks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nWTzIsgrYNN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Since the introduction of Transformers, researchers have tackled the notoriously expensive quadratic complexity problem.  While significant computational efficiency improvements have been achieved, they come at the cost of reduced accuracy trade-offs. In this paper, we propose Composite Slice Transformer (CST), a Transformer-based network equipped with a composition of multi-scale multi-range attentions, boosting both efficiency and modeling capability.
After stacking fixed-length slices of the input sequence, each layer in CST performs a pair of fine-and-coarse-grained attentions with short-long ranges in a sequential manner, coupled with volatile instant positional embedding, enabling efficient token interactions {\em and} improving expressiveness of the model.
In addition to significantly reduced $O(NL+N^2/L^2)$ complexity for sequence length $N$ and slice length $L$, CST achieves superior performance on a variety of tasks.  We show that CST surpasses recently published efficient Transformers on the Long Range Arena benchmark, demonstrating the bidirectional long-range dependency modeling capability of our model. It outperforms the standard Transformer by a margin of $6.9$\% in average accuracy across the five classification tasks of the benchmark, while being of complexity comparable to other efficient transformers. Furthermore, on the word-level autoregressive language modeling task with the WikiText-103 dataset, CST performs competitively against the Transformer model with only $2$\% gap in the test perplexity while outperforming other efficient Transformers.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1142. <h><a href=https://openreview.net/pdf?id=ytZIYmztET>EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Crawshaw, Yajie Bao, Mingrui Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ytZIYmztET>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce EPISODE, an algorithm for federated learning with heterogeneous data under the relaxed smoothness setting for training deep neural networks, and provide state-of-the-art computational and communication complexity guarantees.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ytZIYmztET.jpg loading="lazy">
                </div>
                <div class="abstract">
                     Gradient clipping is an important technique for deep neural networks with exploding gradients, such as recurrent neural networks. Recent studies have shown that the loss functions of these networks do not satisfy the conventional smoothness condition, but instead satisfy a relaxed smoothness condition, i.e., the Lipschitz constant of the gradient scales linearly in terms of the gradient norm. Due to this observation, several gradient clipping algorithms have been developed for nonconvex and relaxed-smooth functions. However, the existing algorithms only apply to the single-machine or multiple-machine setting with homogeneous data across machines. It remains unclear how to design provably efficient gradient clipping algorithms in the general Federated Learning (FL) setting with heterogeneous data and limited communication rounds. In this paper, we design EPISODE, the very first algorithm to solve FL problems with heterogeneous data in the nonconvex and relaxed smoothness setting. The key ingredients of the algorithm are two new techniques called \textit{episodic gradient clipping} and \textit{periodic resampled corrections}. At the beginning of each round, EPISODE resamples stochastic gradients from each client and obtains the global averaged gradient, which is used to (1) determine whether to apply gradient clipping for the entire round and (2) construct local gradient corrections for each client. Notably, our algorithm and analysis provide a unified framework for both homogeneous and heterogeneous data under any noise level of the stochastic gradient, and it achieves state-of-the-art complexity results. In particular, we prove that EPISODE can achieve linear speedup in the number of machines, and it requires significantly fewer communication rounds. Experiments on several heterogeneous datasets, including text classification and image classification, show the superior performance of EPISODE over several strong baselines in FL. The code is available at https://github.com/MingruiLiu-ML-Lab/episode.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1143. <h><a href=https://openreview.net/pdf?id=WBXbRs63oVu>PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>PeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, Xiang Ren</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WBXbRs63oVu>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WBXbRs63oVu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM&#39;s internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO&#39;s reasoning LM is fine-tuned to solve the task using the generated rationale as context, while regularized to output less confident predictions when the rationale is perturbed. Across four datasets, we show that PINTO significantly improves the generalization ability of the reasoning LM, yielding higher performance on both in-distribution and out-of-distribution test sets. Also, we find that PINTO&#39;s rationales are more faithful to its task predictions than those generated by competitive baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1144. <h><a href=https://openreview.net/pdf?id=vZTp1oPV3PC>One Transformer Can Understand Both 2D &amp; 3D Molecular Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, Di He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vZTp1oPV3PC>6.25 - [6, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vZTp1oPV3PC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1145. <h><a href=https://openreview.net/pdf?id=_xlsjehDvlY>STUNT: Few-shot Tabular Learning with Self-generated Tasks from Unlabeled Tables</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jaehyun Nam, Jihoon Tack, Kyungmin Lee, Hankook Lee, Jinwoo Shin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_xlsjehDvlY>6.25 - [6, 8, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a few-shot tabular learning framework that meta-learns over the self-generated tasks from unlabeled tables.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_xlsjehDvlY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning with few labeled tabular samples is often an essential requirement for industrial machine learning applications as varieties of tabular data suffer from high annotation costs or have difficulties in collecting new samples for novel tasks. Despite the utter importance, such a problem is quite under-explored in the field of tabular learning, and existing few-shot learning schemes from other domains are not straightforward to apply, mainly due to the heterogeneous characteristics of tabular data. In this paper, we propose a simple yet effective framework for few-shot semi-supervised tabular learning, coined Self-generated Tasks from UNlabeled Tables (STUNT). Our key idea is to self-generate diverse few-shot tasks by treating randomly chosen columns as a target label. We then employ a meta-learning scheme to learn generalizable knowledge with the constructed tasks. Moreover, we introduce an unsupervised validation scheme for hyperparameter search (and early stopping) by generating a pseudo-validation set using STUNT from unlabeled data. Our experimental results demonstrate that our simple framework brings significant performance gain under various tabular few-shot learning benchmarks, compared to prior semi- and self-supervised baselines. Code is available at https://github.com/jaehyun513/STUNT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1146. <h><a href=https://openreview.net/pdf?id=UG8bQcD3Emv>CUTS: Neural Causal Discovery from Irregular Time-Series Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuxiao Cheng, Runzhao Yang, Tingxiong Xiao, Zongren Li, Jinli Suo, Kunlun He, Qionghai Dai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UG8bQcD3Emv>6.25 - [8, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Discovering causal relations from unstructured time-series data with a mutually boosting iterative method.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UG8bQcD3Emv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Causal discovery from time-series data has been a central task in machine learning. Recently, Granger causality inference is gaining momentum due to its good explainability and high compatibility with emerging deep neural networks. However, most existing methods assume structured input data and degenerate greatly when encountering data with randomly missing entries or non-uniform sampling frequencies, which hampers their applications in real scenarios. To address this issue, here we present CUTS, a neural Granger causal discovery algorithm to jointly impute unobserved data points and build causal graphs, via plugging in two mutually boosting modules in an iterative framework: (i) Latent data prediction stage: designs a Delayed Supervision Graph Neural Network (DSGNN) to hallucinate and register unstructured data which might be of high dimension and with complex distribution; (ii) Causal graph fitting stage: builds a causal adjacency matrix with imputed data under sparse penalty. Experiments show that CUTS effectively infers causal graphs from irregular time-series data, with significantly superior performance to existing methods. Our approach constitutes a promising step towards applying causal discovery to real applications with non-ideal observations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1147. <h><a href=https://openreview.net/pdf?id=C2ulri4duIs>Computational Language Acquisition with Theory of Mind</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andy Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=C2ulri4duIs>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Analyzing the effects of Theory of Mind and environment complexity on language acquisition models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/C2ulri4duIs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack &amp; Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1148. <h><a href=https://openreview.net/pdf?id=yf1icZHC-l9>Complexity-Based Prompting for Multi-step Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yf1icZHC-l9>6.25 - [8, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show using prompts with more reasoning steps can improve language models multi-step reasoning ability 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yf1icZHC-l9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on math word reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority
of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3, our approach substantially improves multi-step reasoning accuracy, with an 8.6% absolute improvement on GSM8K, and 6.4% on MathQA. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1149. <h><a href=https://openreview.net/pdf?id=ZKDUlVMqG_O>Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaifeng Zhang, Yang Fu, Shubhankar Borse, Hong Cai, Fatih Porikli, Xiaolong Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZKDUlVMqG_O>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZKDUlVMqG_O.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other downstream tasks such as keypoint transfer. Surprisingly, our method, without any human annotations or simulators, can achieve on-par or even better performance than previous supervised or semi-supervised methods on in-the-wild images. Code and videos are available at https://kywind.github.io/self-pose.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1150. <h><a href=https://openreview.net/pdf?id=_Mic8V96Voy>Eva: Practical Second-order Optimization with Kronecker-vectorized Approximation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lin Zhang, Shaohuai Shi, Bo Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_Mic8V96Voy>6.25 - [6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an efficient approximation algorithm to accelerate second-order optimization for deep learning models. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_Mic8V96Voy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Second-order optimization algorithms exhibit excellent convergence properties for training deep learning models, but often incur significant computation and memory overheads. This can result in lower training efficiency than the first-order counterparts such as stochastic gradient descent (SGD). In this work, we present a memory- and time-efficient second-order algorithm named Eva with two novel techniques: 1) we construct the second-order information with the Kronecker factorization of small stochastic vectors over a mini-batch of training data to reduce memory consumption, and 2) we derive an efficient update formula without explicitly computing the inverse of matrices using the Sherman-Morrison formula. We further provide a theoretical interpretation of Eva from a trust-region optimization point of view to understand how it works. Extensive experimental results on different models and datasets show that Eva reduces the end-to-end training time up to $2.05\times$ and $2.42\times$ compared to first-order SGD and second-order algorithms (K-FAC and Shampoo), respectively. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1151. <h><a href=https://openreview.net/pdf?id=Hnk1WRMAYqg>Multimodal Federated Learning via Contrastive Representation Ensemble</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qiying Yu, Yang Liu, Yimu Wang, Ke Xu, Jingjing Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Hnk1WRMAYqg>6.25 - [6, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> CreamFL, a multimodal FL framework using contrastive representation-level ensemble to learn a larger server model from heterogeneous clients across multi-modalities.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Hnk1WRMAYqg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose \textit{Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL)}, a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-local cross-modal ensemble strategy to aggregate client representations. To mitigate local model drift caused by two unprecedented heterogeneous factors stemming from multimodal discrepancy (\textit{modality gap} and \textit{task gap}), we further propose two inter-modal and intra-modal contrasts to regularize local training, which complements information of the absent modality for uni-modal clients and regularizes local clients to head towards global consensus. Thorough evaluations and ablation studies on image-text retrieval and visual question answering tasks showcase the superiority of CreamFL over state-of-the-art FL methods and its practical value.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1152. <h><a href=https://openreview.net/pdf?id=m3twGT2bAug>Continuous pseudo-labeling from the start</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dan Berrebbi, Ronan Collobert, Samy Bengio, Navdeep Jaitly, Tatiana Likhomanenko</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=m3twGT2bAug>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show how to perform continuous self-training right from the start without any supervised pre-training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/m3twGT2bAug.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-training (ST), or pseudo-labeling has sparked significant interest in the automatic speech recognition (ASR) community recently because of its success in harnessing unlabeled data. Unlike prior semi-supervised learning approaches that relied on iteratively regenerating pseudo-labels (PLs) from a trained model and using them to train a new model, recent state-of-the-art methods perform `continuous training&#39; where PLs are generated using a very recent version of the model being trained. Nevertheless, these approaches still rely on bootstrapping the ST using an initial supervised learning phase where the model is trained on labeled data alone. We believe this has the potential for over-fitting to the labeled dataset in low resource settings and that ST from the start of training should reduce over-fitting. In this paper we show how we can do this by dynamically controlling the evolution of PLs during the training process in ASR. To the best of our knowledge, this is the first study that shows the feasibility of generating PLs from the very start of the training. We are able to achieve this using two techniques that avoid instabilities which lead to degenerate models that do not generalize. Firstly, we control the evolution of PLs through a curriculum that uses the online changes in PLs to control the membership of the cache of PLs and improve generalization. Secondly, we find that by sampling transcriptions from the predictive distribution, rather than only using the best transcription, we can stabilize training further. With these techniques, our ST models match prior works without an external language model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1153. <h><a href=https://openreview.net/pdf?id=h8T5dZWTZ-Z>Basic Binary Convolution Unit for Binarized Image Restoration Network</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bin Xia, Yulun Zhang, Yitong Wang, Yapeng Tian, Wenming Yang, Radu Timofte, Luc Van Gool</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=h8T5dZWTZ-Z>6.25 - [8, 3, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We reconsider the components in BNNs and design a strong, simple and efficient baisc binary convolution unit.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/h8T5dZWTZ-Z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Lighter and faster image restoration (IR) models are crucial for the deployment on resource-limited devices. Binary neural network (BNN), one of the most promising model compression methods, can dramatically reduce the computations and parameters of full-precision convolutional neural networks (CNN). However, there are different properties between BNN and full-precision CNN, and we can hardly use the experience of designing CNN to develop BNN. In this study, we reconsider components in binary convolution, such as residual connection, BatchNorm, activation function, and structure, for IR tasks. We conduct systematic analyses to explain each component&#39;s role in binary convolution and discuss the pitfalls. Specifically, we find that residual connection can reduce the information loss caused by binarization; BatchNorm can solve the value range gap between residual connection and binary convolution; The position of the activation function dramatically affects the performance of BNN. Based on our findings and analyses, we design a simple yet efficient basic binary convolution unit (BBCU). Furthermore, we divide IR networks into four parts and specially design variants of BBCU for each part to explore the benefit of binarizing these parts. We conduct experiments on different IR tasks, and our BBCU significantly outperforms other BNNs and lightweight models, which shows that BBCU can serve as a basic unit for binarized IR networks. All codes and models will be released.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1154. <h><a href=https://openreview.net/pdf?id=-Y34L45JR6z>Policy Expansion for Bridging Offline-to-Online Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haichao Zhang, Wei Xu, Haonan Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-Y34L45JR6z>6.25 - [6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Bridging offline-to-online RL with Policy Expansion
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-Y34L45JR6z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pre-training with offline data and online fine-tuning using reinforcement learning is a promising strategy for learning control policies by leveraging the best of both worlds in terms of sample efficiency and performance. One natural approach is to initialize the policy for online learning with the one trained offline.
In this work, we introduce a policy expansion scheme for this task. After learning the offline policy, we use it as one candidate policy in a policy set, and further learn another policy that will be responsible for further learning as an expansion to the policy set. The two policies will be composed in an adaptive manner for interacting with the environment. With this approach, the policy previously learned offline is fully retained during online learning, thus mitigating the potential issues such as destroying the useful behaviors of the offline policy in the initial stage of online learning while allowing the offline policy participate in the  exploration  naturally in an adaptive manner. Moreover, new useful behaviors can potentially be captured by the newly added policy through learning. 
Experiments are conducted on a number of tasks and the results demonstrate the effectiveness of the proposed approach.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1155. <h><a href=https://openreview.net/pdf?id=fB4V-2QvCEm>Population-size-Aware Policy Optimization for Mean-Field Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pengdeng Li, Xinrun Wang, Shuxin Li, Hau Chan, Bo An</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fB4V-2QvCEm>6.25 - [8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fB4V-2QvCEm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, we attempt to bridge the two fields of finite-agent and infinite-agent games, by studying how the optimal policies of agents evolve with the number of agents (population size) in mean-field games, an agent-centric perspective in contrast to the existing works focusing typically on the convergence of the empirical distribution of the population. To this end, the premise is to obtain the optimal policies of a set of finite-agent games with different population sizes. However, either deriving the closed-form solution for each game is theoretically intractable, training a distinct policy for each game is computationally intensive, or directly applying the policy trained in a game to other games is sub-optimal. We address these challenges through the \textbf{P}opulation-size-\textbf{A}ware \textbf{P}olicy \textbf{O}ptimization (PAPO). Our contributions are three-fold. First, to efficiently generate efficient policies for games with different population sizes, we propose PAPO, which unifies two natural options (augmentation and hypernetwork) and achieves significantly better performance. PAPO consists of three components: i) the population-size encoding which transforms the original value of population size to an equivalent encoding to avoid training collapse, ii) a hypernetwork to generate a distinct policy for each game conditioned on the population size, and iii) the population size as an additional input to the generated policy. Next, we construct a multi-task-based training procedure to efficiently train the neural networks of PAPO by sampling data from multiple games with different population sizes. Finally, extensive experiments on multiple environments show the significant superiority of PAPO over baselines, and the analysis of the evolution of the generated policies further deepens our understanding of the two fields of finite-agent and infinite-agent games. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1156. <h><a href=https://openreview.net/pdf?id=sE7-XhLxHA>DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pengcheng He, Jianfeng Gao, Weizhu Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sE7-XhLxHA>6.25 - [5, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sE7-XhLxHA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents a new pre-trained language model, NewModel, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the “tug-of-war” dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained NewModel using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the NewModel Large model achieves a 91.37% average score, which is 1.37% over DeBERTa and 1.91% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mNew-Model and observed a larger improvement over strong baselines compared to English models. For example, the mNewModel Base achieves a 79.8% zero-shot cross-lingual accuracy on XNLI and a 3.6% improvement over XLM-R Base, creating a new SOTA on this benchmark. We will make our model and code publicly available.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1157. <h><a href=https://openreview.net/pdf?id=xveTeHVlF7j>A Self-Attention Ansatz for Ab-initio Quantum Chemistry</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ingrid von Glehn, James S Spencer, David Pfau</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xveTeHVlF7j>6.25 - [6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use a novel self-attention neural network to make quantum chemistry calculations from first principles much more accurate.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xveTeHVlF7j.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a novel neural network architecture using self-attention, the Wavefunction Transformer (PsiFormer), which can be used as an approximation (or &#34;Ansatz&#34;) for solving the many-electron Schrödinger equation, the fundamental equation for quantum chemistry and material science. This equation can be solved *from first principles*, requiring no external training data. In recent years, deep neural networks like the FermiNet and PauliNet have been used to significantly improve the accuracy of these first-principle calculations, but they lack an attention-like mechanism for gating interactions between electrons. Here we show that the PsiFormer can be used as a drop-in replacement for these other neural networks, often dramatically improving the accuracy of the calculations. On larger molecules especially, the ground state energy can be improved by dozens of kcal/mol, a qualitative leap over previous methods. This demonstrates that self-attention networks can learn complex quantum mechanical correlations between electrons, and are a promising route to reaching unprecedented accuracy in chemical calculations on larger systems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1158. <h><a href=https://openreview.net/pdf?id=CN223OXgyb5>BALTO: fast tensor program optimization with diversity-based active learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jun Bi, Xiaqing Li, Qi Guo, Rui Zhang, Yuanbo Wen, Xing Hu, Zidong Du, Xinkai Song, Yifan Hao, Yunji Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CN223OXgyb5>6.25 - [6, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CN223OXgyb5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Tensor program optimization (TPO) based on pre-trained models can effectively reduce the computing time of deep neural networks. However, training of such models is prohibitively expensive, which highly depends on a large-scale dataset and thus requires tremendous time-consuming performance measurements (more than 1 million) on target platforms. In this paper, we propose BALTO, a fast TPO approach with biased-diversity-based active learning, aiming at reducing much lower training costs under similar optimization accuracy.The key insight is that random sampling of existing approaches suffers from a heavy redundancy of low-performance programs, which incurs tremendous duplicated time-consuming measurements. Inspired by this, BALTO removes such redundancy by introducing active learning (AL) to TPO for a much lower training cost. However, applying AL with a brute-force way in BALTO can lead to an overestimation problem. To address this, we further propose a biased-diversity-based diversity scheme specially designed for BALTO. We compare BALTO against TenSet on $6$ typical hardware platforms over $2$ learning models. Experimental results show that, on average, BALTO only requires 5% of the total performance measurements of TenSet to achieve the same or higher model accuracy. Moreover, the optimized tensor programs even outperform that of TenSet by 1.06% due to higher model accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1159. <h><a href=https://openreview.net/pdf?id=9Nj_gNdvqYf>Leveraging Importance Weights in Subset Selection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gui Citovsky, Giulia DeSalvo, Sanjiv Kumar, Srikumar Ramalingam, Afshin Rostamizadeh, Yunjuan Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9Nj_gNdvqYf>6.2 - [8, 3, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9Nj_gNdvqYf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a subset selection algorithm designed to work with arbitrary model families in a practical batch setting. In such a setting, an algorithm can sample examples one at a time but, in order to limit overhead costs, is only able to update its state (i.e. further train model weights) once a large enough batch of examples is selected.  Our algorithm, IWeS, selects examples by importance sampling where the sampling probability assigned to each example is based on the entropy of models trained on previously selected batches. IWeS admits significant performance improvement compared to other subset selection algorithms for seven publicly available datasets. Additionally, it is competitive in an active learning setting, where the label information is not available at selection time. We also provide an initial theoretical analysis to support our importance weighting approach, proving generalization and sampling rate bounds.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1160. <h><a href=https://openreview.net/pdf?id=JpRExTbl1->Gradient Gating for Deep Multi-Rate Learning on Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>T. Konstantin Rusch, Benjamin Paul Chamberlain, Michael W. Mahoney, Michael M. Bronstein, Siddhartha Mishra</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JpRExTbl1->6.2 - [6, 8, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JpRExTbl1-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present Gradient Gating (G$^2$), a novel framework for improving the performance of Graph Neural Networks (GNNs). Our framework is based on gating the output of GNN layers with a mechanism for multi-rate flow of message passing information across nodes of the underlying graph. Local gradients are harnessed to further modulate message passing updates. Our framework flexibly allows one to use any basic GNN layer as a wrapper around which the multi-rate gradient gating mechanism is built. We rigorously prove that G$^2$ alleviates the oversmoothing problem and allows the design of deep GNNs. Empirical results are presented to demonstrate that the proposed framework achieves state-of-the-art performance on a variety of graph learning tasks, including on large-scale heterophilic graphs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1161. <h><a href=https://openreview.net/pdf?id=YfwMIDhPccD>GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Jinzheng He, Zhou Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YfwMIDhPccD>6.2 - [6, 6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YfwMIDhPccD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generating photo-realistic video portraits with arbitrary speech audio is a crucial problem in film-making and virtual reality. Recently, several works explore the usage of neural radiance field (NeRF) in this task to improve 3D realness and image fidelity. However, the generalizability of previous NeRF-based methods is limited by the small scale of training data. In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based talking face generation method, which can generate natural results corresponding to various out-of-domain audio. Specifically, we learn a variational motion generator on a large lip-reading corpus, and introduce a domain adaptative post-net to calibrate the result. Moreover, we learn a NeRF-based renderer conditioned on the predicted motion. A head-aware torso-NeRF is proposed to eliminate the head-torso separation problem. Extensive experiments show that our method achieves more generalized and high-fidelity talking face generation compared to previous methods. Video samples and source code are available at https://geneface.github.io .
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1162. <h><a href=https://openreview.net/pdf?id=rB6TpjAuSRy>CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, Jie Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rB6TpjAuSRy>6.2 - [8, 6, 8, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rB6TpjAuSRy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work, we present CogVideo, a 9B-parameter transformer for text-to-video generation. The CogVideo model has been trained by inheriting a pretrained text-to-image model, CogView2, which significantly reduces the training cost and alleviates the problem of scarcity and weak relevance. We also propose a multi-frame-rate training strategy for better aligning text and video clips. CogVideo achieves state-of-the-art performance in machine evaluation and outperforms publicly available models by a large margin in human evaluation. Its codes and model are also publicly available at https://github.com/THUDM/CogVideo.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1163. <h><a href=https://openreview.net/pdf?id=_JScUk9TBUn>Uniform-in-time propagation of chaos for the mean-field gradient Langevin dynamics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Taiji Suzuki, Atsushi Nitanda, Denny Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_JScUk9TBUn>6.2 - [6, 6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_JScUk9TBUn.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The mean-field Langevin dynamics is characterized by a stochastic differential equation that arises from (noisy) gradient descent on an infinite-width two-layer neural network, which can be viewed as an interacting particle system. In this work, we establish a quantitative weak propagation of chaos result for the system, with a finite-particle discretization error of $\mathcal{O}(1/N)$ \textit{uniformly over time}, where $N$ is the width of the neural network. This allows us to directly transfer the optimization guarantee for infinite-width networks to practical finite-width models without excessive overparameterization. On the technical side, our analysis differs from most existing studies on similar mean field dynamics in that we do not require the interaction between particles to be sufficiently weak to obtain a uniform propagation of chaos, because such assumptions may not be satisfied in neural network optimization. Instead, we make use of a logarithmic Sobolev-type condition which can be verified in appropriate regularized risk minimization settings. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1164. <h><a href=https://openreview.net/pdf?id=gVOXZproe-e>How to prepare your task head for finetuning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yi Ren, Shangmin Guo, Wonho Bae, Danica J. Sutherland</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gVOXZproe-e>6.2 - [6, 6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Features need mild adaptation during finetuning, so mildly update your task head and then finetune together.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gVOXZproe-e.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the era of deep learning, transferring information from a pretrained network to a downstream task by finetuning has many benefits. The choice of task head plays an important role in fine-tuning, as the pretrained and downstream tasks are usually different. Although there exist many different designs for finetuning, a full understanding of when and why these algorithms work has been elusive. We analyze how the choice of task head controls feature adaptation and hence influences the downstream performance.  By decomposing the feature&#39;s learning dynamics, we find the key aspect is the training accuracy and loss at the beginning of finetuning, which determines the &#34;energy&#34; available for the feature&#39;s adaptation. We identify a significant trend in the effect of changes in this initial energy on the resulting features after finetuning. Specifically, as the energy increases, the Euclidean and cosine distances between the resulting and original features increase, while their dot product (and the resulting features’ norm) first increases and then decreases. Inspired by this, we give several practical principles that lead to better downstream performance. We analytically prove this trend in an overparamterized linear setting and verify its applicability to different experimental settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1165. <h><a href=https://openreview.net/pdf?id=DHyHRBwJUTN>Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, Ashwin Kalyan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DHyHRBwJUTN>6.2 - [6, 6, 8, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a new tabular math word problem dataset, TabMWP, and we propose a novel approach to it that learns to select in-context examples in few-shot GPT-3 via policy gradient. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DHyHRBwJUTN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples. The data and code are available at https://promptpg.github.io.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1166. <h><a href=https://openreview.net/pdf?id=qFVVBzXxR2V>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Abulhair Saparov, He He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qFVVBzXxR2V>6.2 - [6, 6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a new synthetic QA dataset called PrOntoQA to systematically explore the reasoning ability of language models via formal analysis, and find that while they can produce valid proof steps, they have difficulty with proof planning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qFVVBzXxR2V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1167. <h><a href=https://openreview.net/pdf?id=Ojpb1y8jflw>StyleMorph: Disentangled 3D-Aware Image Synthesis with a 3D Morphable StyleGAN</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eric-Tuan Le, Edward Bartrum, Iasonas Kokkinos</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Ojpb1y8jflw>6.2 - [6, 6, 8, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A deformable 3D-aware photorealistic image generator
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Ojpb1y8jflw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce StyleMorph, a 3D-aware generative model that disentangles 3D shape, camera pose, object appearance, and background appearance for high quality image synthesis. We account for shape variability by morphing a canonical 3D object template, effectively learning a 3D morphable model in an entirely unsupervised manner through backprop. We chain 3D morphable modelling with deferred neural rendering by performing an implicit surface rendering of “Template Object Coordinates” (TOCS), which can be understood as an unsupervised counterpart to UV maps. This provides a detailed 2D TOCS map signal that reflects the compounded geometric effects of non-rigid shape variation, camera pose, and perspective projection. We combine 2D TOCS maps with an independent appearance code to condition a StyleGAN-based deferred neural rendering (DNR) network for foreground image (object) synthesis; we use a separate code for background synthesis and do late fusion to deliver the final result. We show competitive synthesis results on 4 datasets (FFHQ faces, AFHQ Cats, Dogs, Wild), while achieving the joint disentanglement of shape, pose, object and background texture.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1168. <h><a href=https://openreview.net/pdf?id=Lnxl5pr018>Empowering Graph Representation Learning with Test-Time Graph Transformation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, Neil Shah</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Lnxl5pr018>6.2 - [6, 8, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Transforming the test graph data can enhance the generalization and robustness of graph neural networks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Lnxl5pr018.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As powerful tools for representation learning on graphs, graph neural networks (GNNs) have facilitated various applications from drug discovery to recommender systems. Nevertheless, the effectiveness of GNNs is immensely challenged by issues related to data quality, such as distribution shift, abnormal features and adversarial attacks. Recent efforts have been made on tackling these issues from a modeling perspective which requires additional cost of changing model architectures or re-training model parameters. In this work, we provide a data-centric view to tackle these issues and propose a graph transformation framework named GTrans which adapts and refines graph data at test time to achieve better performance. We provide theoretical analysis on the design of the framework and discuss why adapting graph data works better than adapting the model. Extensive experiments have demonstrated the effectiveness of GTrans on three distinct scenarios for eight benchmark datasets where suboptimal data is presented. Remarkably, GTrans performs the best in most cases with improvements up to 2.8%, 8.2% and 3.8% over the best baselines on three experimental settings. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1169. <h><a href=https://openreview.net/pdf?id=4FBUihxz5nm>A Mixture-of-Expert Approach to RL-based Dialogue Management</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yinlam Chow, Azamat Tulepbergenov, Ofir Nachum, Dhawal Gupta, Moonkyung Ryu, Mohammad Ghavamzadeh, Craig Boutilier</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4FBUihxz5nm>6.2 - [8, 6, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A mixture-of-expert based dialogue manager that is amenable to sequential decision making techniques
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4FBUihxz5nm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite recent advancements in language models (LMs), their application to dialogue management (DM) problems and ability to carry on rich conversations remain a challenge. We use reinforcement learning (RL) to develop a dialogue agent that avoids being short-sighted (outputting generic utterances) and maximizes overall user satisfaction. Most existing RL approaches to DM train the agent at the word-level, and thus, have to deal with a combinatorially complex action space even for a medium-size vocabulary. As a result, they struggle to produce a successful and engaging dialogue even if they are warm-started with a pre-trained LM. To address this issue, we develop a RL-based DM using a novel mixture of expert language model (MoE-LM) that consists of (i) a LM capable of learning diverse semantics for conversation histories, (ii) a number of specialized LMs (or experts) capable of generating utterances corresponding to a particular attribute or personality, and (iii) a RL-based DM that performs dialogue planning with the utterances generated by the experts. Our MoE approach provides greater flexibility to generate sensible utterances with different intents and allows RL to focus on conversational-level DM. We compare it with SOTA baselines on open-domain dialogues and demonstrate its effectiveness both in terms of the diversity and sensibility of the generated utterances and the overall DM performance. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1170. <h><a href=https://openreview.net/pdf?id=8E5Yazboyh>Sharper Bounds for Uniformly Stable Algorithms with Stationary Mixing Process</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shi Fu, Yunwen Lei, Qiong Cao, Xinmei Tian, Dacheng Tao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8E5Yazboyh>6.17 - [6, 6, 8, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop stability and generalization bounds for learning with mixing sequences.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8E5Yazboyh.jpg loading="lazy">
                </div>
                <div class="abstract">
                      Generalization analysis of learning algorithms often builds on a critical assumption that training examples are independently and identically distributed, which is often violated in practical problems such as time series prediction. In this paper, we use algorithmic stability to study the generalization performance of learning algorithms with $\psi$-mixing data, where the dependency between observations weakens over time. We show uniformly stable algorithms guarantee high-probability generalization bounds of the order $O(1/\sqrt{n})$ (within a logarithmic factor), where $n$ is the sample size. We apply our general result to specific algorithms including regularization schemes, stochastic gradient descent and localized iterative regularization, and develop excess population risk bounds for learning with $\psi$-mixing data. Our analysis builds on a novel moment bound for weakly-dependent random variables on a $\varphi$-mixing sequence and a novel error decomposition of generalization error.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1171. <h><a href=https://openreview.net/pdf?id=8T4qmZbTkW7>Progressively Compressed Auto-Encoder for Self-supervised Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jin Li, Yaoming Wang, XIAOPENG ZHANG, Yabo Chen, Dongsheng Jiang, Wenrui Dai, Chenglin Li, Hongkai Xiong, Qi Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8T4qmZbTkW7>6.17 - [5, 6, 8, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8T4qmZbTkW7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As a typical self-supervised learning strategy, Masked Image Modeling (MIM) is driven by recovering all masked patches from visible ones. However, patches from the same image are highly correlated and it is redundant to reconstruct all the masked patches. We find that this redundancy is neglected by existing MIM based methods and causes non-negligible overheads in computation that do not necessarily benefit self-supervised representation. In this paper, we present a novel approach named PCAE, short for Progressively Compressed AutoEncoder, to address the redundant reconstruction issue by progressively compacting tokens and only retaining necessary information for forward propagation and reconstruction. In particular, we identify those redundant tokens in an image via a simple yet effective similarity metric between each token with the mean of the token sequence. Those redundant tokens that other ones can probably represent are progressively dropped accordingly during the forward propagation, and importantly, we only focus on reconstructing these retained tokens. As a result, we are able to achieve a better trade-off between performance and efficiency for pre-training. Besides, benefitting from the flexible strategy, PCAE can be also directly employed for downstream fine-tuning tasks and enable scalable deployment. Experiments show that PCAE achieves comparable performance to MAE with only 1/8 GPU days. The code is available at https://github.com/caddyless/PCAE/.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1172. <h><a href=https://openreview.net/pdf?id=lh-HRYxuoRr>This Looks Like It Rather Than That: ProtoKNN For Similarity-Based Classifiers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuki Ukai, Tsubasa Hirakawa, Takayoshi Yamashita, Hironobu Fujiyoshi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lh-HRYxuoRr>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lh-HRYxuoRr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Among research on the interpretability of deep learning models, the &#39;this looks like that&#39; framework with ProtoPNet has attracted significant attention. By combining the strong power of deep learning models with the interpretability of case-based inference, ProtoPNet can achieve high accuracy while keeping its reasoning process interpretable. Many methods based on ProtoPNet have emerged to take advantage of this benefit, but despite their practical usefulness, they run into difficulty when utilizing similarity-based classifiers, e.g., in domains where unknown class samples exist. This is because ProtoPNet and its variants adopt the training process specific to linear classifiers, which allows the prototypes to represent useful image features for class recognition. Due to this difficulty, the effectiveness of similarity-based classifiers (e.g., k-nearest neighbor (KNN)) on the &#39;this looks like that&#39; framework have not been sufficiently examined. To alleviate this problem, we propose ProtoKNN, an extension of ProtoPNet that adopts KNN classifiers. Extensive experiments on multiple open datasets demonstrate that the proposed method can achieve competitive results with a state-of-the-art method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1173. <h><a href=https://openreview.net/pdf?id=AuEgNlEAmed>A theoretical study of inductive biases in contrastive learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeff Z. HaoChen, Tengyu Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AuEgNlEAmed>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide the first theoretical analysis of self-supervised learning that incorporates the effect of inductive biases of model classes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AuEgNlEAmed.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Understanding self-supervised learning is important but challenging. Previous theoretical works study the role of pretraining losses, and view neural networks as general black boxes. However, the recent work of [Saunshi et al.] argues that the model architecture --- a component largely ignored by previous works --- also has significant influences on the downstream performance of self-supervised learning. In this work, we provide the first theoretical analysis of self-supervised learning that incorporates the effect of inductive biases originating from the model class. In particular, we focus on contrastive learning --- a popular self-supervised learning method that is widely used in the vision domain. We show that when the model has limited capacity, contrastive representations would recover certain special clustering structures that are compatible with the model architecture, but ignore many other clustering structures in the data distribution. As a result, our theory can capture the more realistic setting where contrastive representations have much lower dimensionality than the number of clusters in the data distribution. We instantiate our theory on several synthetic data distributions, and provide empirical evidence to support the theory.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1174. <h><a href=https://openreview.net/pdf?id=kIo_C6QmMOM>Coupled Multiwavelet Operator Learning for Coupled Differential Equations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiongye Xiao, Defu Cao, Ruochen Yang, Gaurav Gupta, Gengshuo Liu, Chenzhong Yin, Radu Balan, Paul Bogdan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kIo_C6QmMOM>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel coupled multiwavelet operator learning scheme for efficiently solving coupled differential equations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kIo_C6QmMOM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Coupled partial differential equations (PDEs) are key tasks in modeling the complex dynamics of many physical processes. Recently, neural operators have shown the ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet space, so the difficulty of solving the coupled PDEs depends on dealing with the coupled mappings between the functions. Towards this end, we propose a \textit{coupled multiwavelets neural operator} (CMWNO) learning scheme by decoupling the coupled integral kernels during the multiwavelet decomposition and reconstruction procedures in the Wavelet space. The proposed model achieves significantly higher accuracy compared to previous learning-based solvers in solving the coupled PDEs including Gray-Scott (GS) equations and the non-local mean field game (MFG) problem. According to our experimental results, the proposed model exhibits a $2X-4X$ improvement relative $L$2 error compared to the best results from the state-of-the-art models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1175. <h><a href=https://openreview.net/pdf?id=Mj7K4lglGyj>UNICORN: A Unified Backdoor Trigger Inversion Framework</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhenting Wang, Kai Mei, Juan Zhai, Shiqing Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Mj7K4lglGyj>6.0 - [6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Mj7K4lglGyj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The backdoor attack, where the adversary uses inputs stamped with triggers (e.g., a patch) to activate pre-planted malicious behaviors, is a severe threat to Deep Neural Network (DNN) models. Trigger inversion is an effective way of identifying backdoor models and understanding embedded adversarial behaviors. A challenge of trigger inversion is that there are many ways of constructing the trigger. Existing methods cannot generalize to various types of triggers by making certain assumptions or attack-specific constraints. The fundamental reason is that existing work does not formally define the trigger and the inversion problem. This work formally defines and analyzes the trigger and the inversion problem. Then, it proposes a unified framework to invert backdoor triggers based on the formalization of triggers and the identified inner behaviors of backdoor models from our analysis. Our prototype UNICORN is general and effective in inverting backdoor triggers in DNNs. The code can be found at https://github.com/RU-System-Software-and-Security/UNICORN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1176. <h><a href=https://openreview.net/pdf?id=zlwBI2gQL3K>Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Han Wu, Jie Yin, Bala Rajaratnam, Jianyuan Guo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zlwBI2gQL3K>6.0 - [6, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zlwBI2gQL3K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge graphs (KGs) are powerful in terms of their inference abilities, but are also notorious for their incompleteness and long-tail distribution of relations. To address these challenges and expand the coverage of KGs, few-shot KG completion aims to make predictions for triplets involving novel relations when only a few training triplets are provided as reference. Previous methods have focused on designing local neighbor aggregators to learn entity-level information and/or imposing sequential dependency assumption at the triplet level to learn meta relation information. However, pairwise triplet-level interactions and context-level relational information have been largely overlooked for learning meta representations of few-shot relations. In this paper, we propose a hierarchical relational learning method (HiRe) for few-shot KG completion. By jointly capturing three levels of relational information (entity-level, triplet-level and context-level), HiRe can effectively learn and refine the meta representation of few-shot relations, and consequently generalize well to new unseen relations. Extensive experiments on two benchmark datasets validate the superiority of HiRe over state-of-the-art methods. The code of HiRe can be found in supplementary material and will be released after acceptance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1177. <h><a href=https://openreview.net/pdf?id=ja4Lpp5mqc2>TrojText: Test-time Invisible Textual Trojan Insertion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qian Lou, Yepeng Liu, Bo Feng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ja4Lpp5mqc2>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> TrojText is a more realistic, efficient, test-time invisible textual Trojan Insertion method  against intelligent neuron models
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ja4Lpp5mqc2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In Natural Language Processing (NLP), intelligent neuron models can be susceptible to textual Trojan attacks. Such attacks occur when Trojan models behave normally for standard inputs but generate malicious output for inputs that contain a specific trigger. Syntactic-structure triggers, which are invisible, are becoming more popular for Trojan attacks because they are difficult to detect and defend against. However, these types of attacks require a large corpus of training data to generate poisoned samples with the necessary syntactic structures for Trojan insertion. Obtaining such data can be difficult for attackers, and the process of generating syntactic poisoned triggers and inserting Trojans can be time-consuming. This paper proposes a solution called TrojText, which aims to determine whether invisible textual Trojan attacks can be performed more efficiently and cost-effectively without training data. The proposed approach, called the Representation-Logit Trojan Insertion (RLI) algorithm, uses smaller sampled test data instead of large training data to achieve the desired attack. The paper also introduces two additional techniques, namely the accumulated gradient ranking (AGR) and Trojan Weights Pruning (TWP), to reduce the number of tuned parameters and the attack overhead. The TrojText approach was evaluated on three datasets (AG’s News, SST-2, and OLID) using three NLP models (BERT, XLNet, and DeBERTa). The experiments demonstrated that the TrojText approach achieved a 98.35% classification accuracy for test sentences in the target class on the BERT model for the AG’s News dataset. The source code for TrojText is available at https://github.com/UCF-ML-Research/TrojText.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1178. <h><a href=https://openreview.net/pdf?id=Pza24zf9FpS>Neural Groundplans: Persistent Neural Scene Representations from a Single Image</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Prafull Sharma, Ayush Tewari, Yilun Du, Sergey Zakharov, Rares Andrei Ambrus, Adrien Gaidon, William T. Freeman, Fredo Durand, Joshua B. Tenenbaum, Vincent Sitzmann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Pza24zf9FpS>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We train a self-supervised model that learns to map a single image to a 3D representation of the scene, with separate components for the immovable and movable 3D regions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Pza24zf9FpS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a method to map 2D image observations of a scene to a persistent 3D scene representation, enabling novel view synthesis and disentangled representation of the movable and immovable components of the scene. Motivated by the bird’s-eye-view (BEV) representation commonly used in vision and robotics, we propose conditional neural groundplans, ground-aligned 2D feature grids, as persistent and memory-efficient scene representations. Our method is trained self-supervised from unlabeled multi-view observations using differentiable rendering, and learns to complete geometry and appearance of occluded regions. In addition, we show that we can leverage multi-view videos at training time to learn to separately reconstruct static and movable components of the scene from a single image at test time. The ability to separately reconstruct movable objects enables a variety of downstream tasks using simple heuristics, such as extraction of object-centric 3D representations, novel view synthesis, instance-level segmentation, 3D bounding box prediction, and scene editing. This highlights the value of neural groundplans as a backbone for efficient 3D scene understanding models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1179. <h><a href=https://openreview.net/pdf?id=uR6x8Be7o_M>Learning to reason over visual objects</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shanka Subhra Mondal, Taylor Whittington Webb, Jonathan Cohen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uR6x8Be7o_M>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uR6x8Be7o_M.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A core component of human intelligence is the ability to identify abstract patterns inherent in complex, high-dimensional perceptual data, as exemplified by visual reasoning tasks such as Raven’s Progressive Matrices (RPM). Motivated by the goal of designing AI systems with this capacity, recent work has focused on evaluating whether neural networks can learn to solve RPM-like problems. Previous work has generally found that strong performance on these problems requires the incorporation of inductive biases that are specific to the RPM problem format, raising the question of whether such models might be more broadly useful. Here, we investigated the extent to which a general-purpose mechanism for processing visual scenes in terms of objects might help promote abstract visual reasoning. We found that a simple model, consisting only of an object-centric encoder and a transformer reasoning module, achieved state-of-the-art results on both of two challenging RPM-like benchmarks (PGM and I-RAVEN), as well as a novel benchmark with greater visual complexity (CLEVR-Matrices). These results suggest that an inductive bias for object-centric processing may be a key component of abstract visual reasoning, obviating the need for problem-specific inductive biases.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1180. <h><a href=https://openreview.net/pdf?id=pOyi9KqE56b>Beyond Lipschitz: Sharp Generalization and Excess Risk Bounds for Full-Batch GD</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Konstantinos Nikolakakis, Farzin Haddadpour, Amin Karbasi, Dionysios Kalogerias</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pOyi9KqE56b>6.0 - [5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show generalization and excess risk guarantees for the full-batch Gradient Descent (GD) algorithm
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pOyi9KqE56b.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We provide sharp path-dependent generalization and excess risk guarantees for the full-batch Gradient Descent (GD) algorithm on smooth losses (possibly non-Lipschitz, possibly nonconvex). At the heart of our analysis is an upper bound on the generalization error, which implies that average output stability and a bounded expected optimization error at termination lead to generalization. This result shows that a small generalization error occurs along the optimization path, and allows us to bypass Lipschitz or sub-Gaussian assumptions on the loss prevalent in previous works. For nonconvex, convex, and strongly convex losses, we show the explicit dependence of the generalization error in terms of the accumulated path-dependent optimization error, terminal optimization error, number of samples, and number of iterations. For nonconvex smooth losses, we prove that full-batch GD efficiently generalizes close to any stationary point at termination, and recovers the generalization error guarantees of stochastic algorithms with fewer assumptions. For smooth convex losses, we show that the generalization error is tighter than existing bounds for SGD (up to one order of error magnitude). Consequently the excess risk matches that of SGD for quadratically less iterations. Lastly, for strongly convex smooth losses, we show that full-batch GD achieves essentially the same excess risk rate as compared with the state of the art on SGD, but with an exponentially smaller number of iterations (logarithmic in the dataset size).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1181. <h><a href=https://openreview.net/pdf?id=dZrQR7OR11>Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Han Guo, Philip Greengard, Hongyi Wang, Andrew Gelman, Yoon Kim, Eric Xing</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dZrQR7OR11>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work introduces a probabilistic message-passing algorithm for federated learning based on expectation propagation (FedEP) and studies algorithmic considerations to scale up classic expectation propagation to modern federated learning settings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dZrQR7OR11.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1182. <h><a href=https://openreview.net/pdf?id=esFxSb_0pSL>Pareto Invariant Risk Minimization: Towards Mitigating the Optimization Dilemma in Out-of-Distribution Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Bingzhe Wu, Yonggang Zhang, MA KAILI, Han Yang, Peilin Zhao, Bo Han, James Cheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=esFxSb_0pSL>6.0 - [5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a novel Multi-Objective Optimization perspective to understand and allieviate the optimization delimma in Out-of-Distribution generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/esFxSb_0pSL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, there has been a growing surge of interest in enabling machine learning systems to generalize well to Out-of-Distribution (OOD) data. Most efforts are devoted to advancing optimization objectives that regularize models to capture the underlying invariance; however, there often are compromises in the optimization process of these OOD objectives: i) Many OOD objectives have to be relaxed as penalty terms of Empirical Risk Minimization (ERM) for the ease of optimization, while the relaxed forms can weaken the robustness of the original objective; ii) The penalty terms also require careful tuning of the penalty weights due to the intrinsic conflicts between ERM and OOD objectives. Consequently, these compromises could easily lead to suboptimal performance of either the ERM or OOD objective. To address these issues, we introduce a multi-objective optimization (MOO) perspective to understand the OOD optimization process, and propose a new optimization scheme called PAreto Invariant Risk Minimization (PAIR). PAIR improves the robustness of OOD objectives by cooperatively optimizing with other OOD objectives, thereby bridging the gaps caused by the relaxations. Then PAIR approaches a Pareto optimal solution that trades off the ERM and OOD objectives properly. Extensive experiments on challenging benchmarks, WILDS, show that PAIR alleviates the compromises and yields top OOD performances.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1183. <h><a href=https://openreview.net/pdf?id=d8mr8lKIZ3n>Arbitrary Virtual Try-on Network: Characteristics Representation and Trade-off between Body and Clothing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yu Liu, Mingbo Zhao, Zhao Zhang, Jicong Fan, Yang Lou, Shuicheng Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=d8mr8lKIZ3n>6.0 - [5, 8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a special 2D virtual try-on network for cross-category try on task, e.g. long sleeves&lt;-&gt;short sleeves or long pants&lt;-&gt;skirts, since the limb may be exposed or hidden in such case.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/d8mr8lKIZ3n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning based virtual try-on system has achieved some encouraging progress recently, but there still remain several big challenges that need to be solved, such as trying on arbitrary clothes of all types, trying on the clothes from one category to another and generating image-realistic results with few artifacts. To handle this issue, we propose the Arbitrary Virtual Try-On Network (AVTON) that is utilized for all-type clothes, which can synthesize realistic try-on images by preserving and trading off characteristics of the target clothes and the reference person. Our approach includes three modules: 1) Limbs Prediction Module, which is utilized for predicting the human body parts by preserving the characteristics of the reference person. This is especially good for handling cross-category try-on task (e.g., long sleeves \(\leftrightarrow\) short sleeves or long pants \(\leftrightarrow\) skirts, etc.), where the exposed arms or legs with the skin colors and details can be reasonably predicted; 2) Improved Geometric Matching Module, which is designed to warp clothes according to the geometry of the target person. We improve the TPS-based warping method with a compactly supported radial function (Wendland&#39;s \(\Psi\)-function); 3) Trade-Off Fusion Module, which is to trade off the characteristics of the warped clothes and the reference person. This module is to make the generated try-on images look more natural and realistic based on a fine-tuning symmetry of the network structure. Extensive simulations are conducted and our approach can achieve better performance compared with the state-of-the-art virtual try-on methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1184. <h><a href=https://openreview.net/pdf?id=yQdBtFfleh6>Rethinking skip connection model as a learnable Markov chain</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chen Dengsheng, Jie Hu, Wenwen Qiang, Xiaoming Wei, Enhua Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yQdBtFfleh6>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Penal connection only introduces negligible computational burden and can be implemented with one line of code under most popular deep learning frameworks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yQdBtFfleh6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Over the past few years afterward the birth of ResNet, skip connection has become the defacto standard for the design of modern architectures due to its widespread adoption, easy optimization, and proven performance.
Prior work has explained the effectiveness of the skip connection mechanism from different perspectives.
In this work, we deep dive into the model&#39;s behaviors with skip connections which can be formulated as a learnable Markov chain.
An efficient Markov chain is preferred as it always maps the input data to the target domain in a better way.
However, while a model is explained as a Markov chain, it is not guaranteed to be optimized following an efficient Markov chain by existing SGD-based optimizers prone to getting trapped in local optimal points.
In order to move towards a more efficient Markov chain, we propose a simple routine of penal connection to make any residual-like model become a learnable Markov chain.
Aside from that, the penal connection can also be viewed as a particular model regularization and can be easily implemented with one line of code in the most popular deep learning frameworks. 
The encouraging experimental results in multi-modal translation and image recognition empirically confirm our conjecture of the learnable Markov chain view and demonstrate the superiority of the proposed penal connection.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1185. <h><a href=https://openreview.net/pdf?id=PUIqjT4rzq7>Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, William Yang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PUIqjT4rzq7>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a training-free approach to incorporate language structured for compositional text-to-image synthesis
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PUIqjT4rzq7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large-scale diffusion models have achieved state-of-the-art results on text-to-image synthesis (T2I) tasks. Despite their ability to generate high-quality yet creative images, we observe that attribution-binding and compositional capabilities are still considered major challenging issues, especially when involving multiple objects. Attribute-binding requires the model to associate objects with the correct attribute descriptions, and compositional skills require the model to combine and generate multiple concepts into a single image. In this work, we improve these two aspects of T2I models to achieve more accurate image compositions. To do this, we incorporate linguistic structures with the diffusion guidance process based on the controllable properties of manipulating cross-attention layers in diffusion-based T2I models. We observe that keys and values in cross-attention layers have strong semantic meanings associated with object layouts and content. Therefore, by manipulating the cross-attention representations based on linguistic insights, we can better preserve the compositional semantics in the generated image. Built upon Stable Diffusion, a SOTA T2I model, our structured cross-attention design is efficient that requires no additional training samples. We achieve better compositional skills in qualitative and quantitative results, leading to a significant 5-8\% advantage in head-to-head user comparison studies. Lastly, we conduct an in-depth analysis to reveal potential causes of incorrect image compositions and justify the properties of cross-attention layers in the generation process. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1186. <h><a href=https://openreview.net/pdf?id=ipflrGaf7ry>Can Agents Run Relay Race with Strangers? Generalization of RL to Out-of-Distribution Trajectories</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Li-Cheng Lan, Huan Zhang, Cho-Jui Hsieh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ipflrGaf7ry>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ipflrGaf7ry.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we evaluate and improve the generalization performance for reinforcement learning (RL) agents on the set of ``controllable&#39;&#39; states, where good policies exist on these states to achieve the goal. An RL agent that generally masters a task should reach its goal starting from any controllable state of the environment instead of memorizing a small set of trajectories. To practically evaluate this type of generalization, we propose relay evaluation, which starts the test agent from the middle of other independently well-trained stranger agents&#39; trajectories. With extensive experimental evaluation, we show the prevalence of generalization failure on controllable states from stranger agents. For example, in the Humanoid environment, we observed that a well-trained Proximal Policy Optimization (PPO) agent, with only 3.9\% failure rate during regular testing, failed on 81.6\% of the states generated by well-trained stranger PPO agents. To improve &#34;relay generalization,&#34; we propose a novel method called Self-Trajectory Augmentation (STA), which will reset the environment to the agent&#39;s old states according to the Q function during training. After applying STA to the Soft Actor Critic&#39;s (SAC) training procedure, we reduced the failure rate of SAC under relay-evaluation by more than three times in most settings without impacting agent performance and increasing the needed number of environment interactions. Our code is available at https://github.com/lan-lc/STA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1187. <h><a href=https://openreview.net/pdf?id=qY1hlv7gwg>Selective Annotation Makes Language Models Better Few-Shot Learners</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongjin SU, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Tao Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qY1hlv7gwg>6.0 - [8, 6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a select-then-annotate framework to make large language models better few-shot learners. Our method, vote-k, greatly improves the task performance over classification, commonsense reasoning, dialogue, and text/code generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qY1hlv7gwg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1188. <h><a href=https://openreview.net/pdf?id=LiXDW7CF94J>How robust is unsupervised representation learning to distribution shift?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuge Shi, Imant Daunhawer, Julia E Vogt, Philip Torr, Amartya Sanyal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LiXDW7CF94J>6.0 - [6, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Representations learned from self-supervised learning and auto-encoder based algorithms are surprisingly robust to distribution shift.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LiXDW7CF94J.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The robustness of machine learning algorithms to distributions shift is primarily discussed in the context of supervised learning (SL). As such, there is a lack of insight on the robustness of the representations learned from unsupervised methods, such as self-supervised learning (SSL) and auto-encoder based algorithms (AE), to distribution shift. We posit that the input-driven objectives of unsupervised algorithms lead to representations that are more robust to distribution shift than the target-driven objective of SL. We verify this by extensively evaluating the performance of SSL and AE on both synthetic and realistic distribution shift datasets. Following observations that the linear layer used for classification itself can be susceptible to spurious correlations, we evaluate the representations using a linear
head trained on a small amount of out-of-distribution (OOD) data, to isolate the robustness of the learned representations from that of the linear head. We also develop “controllable” versions of existing realistic domain generalisation datasets with adjustable degrees of distribution shifts. This allows us to study the robustness of different learning algorithms under versatile yet realistic distribution shift
conditions. Our experiments show that representations learned from unsupervised learning algorithms generalise better than SL under a wide variety of extreme as well as realistic distribution shifts.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1189. <h><a href=https://openreview.net/pdf?id=GMRodZ8OlVr>ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhengzhe Liu, Peng Dai, Ruihui Li, XIAOJUAN QI, Chi-Wing Fu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GMRodZ8OlVr>6.0 - [6, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An efficient text-guided 3D shape generation framework without needing paired text and shape. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GMRodZ8OlVr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Text-guided 3D shape generation remains challenging due to the absence of large paired text-shape dataset, the substantial semantic gap between these two modalities, and the structural complexity of 3D shapes. This paper presents a new framework called Image as Stepping Stone (ISS) for the task by introducing 2D image as a stepping stone to connect the two modalities and to eliminate the need for paired text-shape data. Our key contribution is a two-stage feature-space-alignment approach that maps CLIP features to shapes by harnessing a pre-trained single-view reconstruction (SVR) model with multi-view supervisions: first map the CLIP image feature to the detail-rich shape space in the SVR model, then map the CLIP text feature to the shape space and optimize the mapping by encouraging CLIP consistency between the input text and the rendered images. Further, we formulate a textguided shape stylization module to dress up the output shapes with novel structures and textures. Beyond existing works on 3D shape generation from text, our new approach is general for creating shapes in a broad range of categories, without requiring paired text-shape data. Experimental results manifest that our approach outperforms the state-of-the-arts and our baselines in terms of fidelity and consistency with text. Further, our approach can stylize the generated shapes with both realistic and fantasy structures and textures. Codes are available at https://github.com/liuzhengzhe/ISS-Image-as-Stepping-Stone-for-Text-Guided-3D-Shape-Generation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1190. <h><a href=https://openreview.net/pdf?id=JmkjrlVE-DG>Over-Training with Mixup May Hurt Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zixuan Liu, Ziqiao Wang, Hongyu Guo, Yongyi Mao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JmkjrlVE-DG>6.0 - [6, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We empirically discovered a U-shaped generalization curve of Mixup training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JmkjrlVE-DG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Mixup, which creates synthetic training instances by linearly interpolating random sample pairs, is a simple and yet effective regularization technique to boost the performance of deep models trained with SGD. In this work, we report a previously unobserved phenomenon in Mixup raining: on a number of standard datasets, the performance of Mixup-trained models starts to decay after training for a large number of epochs, giving rise to a  U-shaped generalization curve. This behavior is further aggravated when the size of original dataset is reduced. To help understand such a behavior of Mixup, we show theoretically that Mixup training may introduce undesired data-dependent label noises to the synthesized data. Via analyzing a least-square regression problem with a random feature model, we explain why noisy labels may cause the U-shaped curve to occur: Mixup improves generalization through fitting the clean patterns at the early training stage,  but as training progresses, Mixup becomes over-fitting to the noise in the synthetic data. Extensive experiments are performed on a variety of benchmark datasets, validating this explanation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1191. <h><a href=https://openreview.net/pdf?id=WumysvcMvV6>Mind the Gap: Offline Policy Optimization for Imperfect Rewards</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianxiong Li, Xiao Hu, Haoran Xu, Jingjing Liu, Xianyuan Zhan, Qing-Shan Jia, Ya-Qin Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WumysvcMvV6>6.0 - [5, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes an offline policy optimization approach for imperfect rewards.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WumysvcMvV6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reward function is essential in reinforcement learning (RL), serving as the guiding signal to incentivize agents to solve given tasks, however, is also notoriously difficult to design. In many cases, only imperfect rewards are available, which inflicts substantial performance loss for RL agents. In this study, we propose a unified offline policy optimization approach, \textit{RGM (Reward Gap Minimization)}, which can smartly handle diverse types of imperfect rewards. RGM is formulated as a bi-level optimization problem: the upper layer optimizes a reward correction term that performs visitation distribution matching w.r.t. some expert data; the lower layer solves a pessimistic RL problem with the corrected rewards. By exploiting the duality of the lower layer, we derive a tractable algorithm that enables sampled-based learning without any online interactions.  Comprehensive experiments demonstrate that RGM achieves superior performance to existing methods under diverse settings of imperfect rewards. Further, RGM can effectively correct wrong or inconsistent rewards against expert preference and retrieve useful information from biased rewards. Code is available at https://github.com/Facebear-ljx/RGM.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1192. <h><a href=https://openreview.net/pdf?id=SVl1w1u3InX>CASR: Generating Complex Sequences with Autoregressive Self-Boost Refinement</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongwei Han, Mengyu Zhou, Shi Han, Xiu Li, Dongmei Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SVl1w1u3InX>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> CASR improves left-to-right autoregressive generation without heuristic intermediate sequences for complex answers via self-boost refinement
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SVl1w1u3InX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There are sequence generation tasks where the best order to generate the target sequence is not left-to-right. For example, an answer to the Sudoku game, a structured code like s-expression, and even a logical natural language answer where the analysis may be generated after the decision. We define the target sequences of those tasks as complex sequences. Obviously, a complex sequence should be constructed with multiple logical steps, and has dependencies among each part of itself (e.g. decisions depend on analyses). It&#39;s a great challenge for the classic left-to-right autoregressive generation system to generate complex sequences. Current approaches improve one-pass left-to-right generation on NLG tasks by generating different heuristic intermediate sequences in multiple stages. However, for complex sequences, the heuristic rules to break down them may hurt performance, and increase additional exposure bias. To tackle these challenges, we propose a PLM-friendly autoregressive self-boost refinement framework, CASR. When training, CASR inputs the predictions generated by the model itself at the previous refinement step (instead of those produced by heuristic rules). To find an optimal design, we also discuss model architecture, parameter efficiency and initialization strategy. By evaluating CASR on Sudoku, WebQSP, MTOP and KVRET through controlled experiments and empirical studies, we find that CASR produces high-quality outputs. CASR also improves Accuracy on Sudoku (70.93% --&gt; 97.28%) and achieves state-of-the-art performance on KVRET with Micro F1 score (67.88% --&gt; 70.00%).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1193. <h><a href=https://openreview.net/pdf?id=eGm22rqG93>DELTA: DEGRADATION-FREE FULLY TEST-TIME ADAPTATION</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bowen Zhao, Chen Chen, Shu-Tao Xia</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eGm22rqG93>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eGm22rqG93.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Fully test-time adaptation aims at adapting a pre-trained model to the test stream during real-time inference, which is urgently required when the test distribution differs from the training distribution. Several efforts have been devoted to improving adaptation performance. However, we find that two unfavorable defects are concealed in the prevalent adaptation methodologies like test-time batch normalization (BN) and self-learning. First, we reveal that the normalization statistics in test-time BN are completely affected by the currently received test samples, resulting in inaccurate estimates. Second, we show that during test-time adaptation, the parameter update is biased towards some dominant classes. In addition to the extensively studied test stream with independent and class-balanced samples, we further observe that the defects can be exacerbated in more complicated test environments, such as (time) dependent or class-imbalanced data. We observe that previous approaches work well in certain scenarios while show performance degradation in others due to their faults. In this paper, we provide a plug-in solution called DELTA for Degradation-freE fuLly Test-time Adaptation, which consists of two components: (i) Test-time Batch Renormalization (TBR), introduced to improve the estimated normalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to address the class bias within optimization. We investigate various test-time adaptation methods on three commonly used datasets with four scenarios, and a newly introduced real-world dataset. DELTA can help them deal with all scenarios simultaneously, leading to SOTA performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1194. <h><a href=https://openreview.net/pdf?id=6iVJOtr2zL2>Contrastive Meta-Learning for Partially Observable Few-Shot Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Adam Jelley, Amos Storkey, Antreas Antoniou, Sam Devlin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6iVJOtr2zL2>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An approach for meta-learning contrastive representations under partial observability.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6iVJOtr2zL2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many contrastive and meta-learning approaches learn representations by identifying common features in multiple views. However, the formalism for these approaches generally assumes features to be shared across views to be captured coherently. We consider the problem of learning a unified representation from partial observations, where useful features may be present in only some of the views. We approach this through a probabilistic formalism enabling views to map to representations with different levels of uncertainty in different components; these views can then be integrated with one another through marginalisation over that uncertainty. Our approach, Partial Observation Experts Modelling (POEM), then enables us to meta-learn consistent representations from partial observations. We evaluate our approach on an adaptation of a comprehensive few-shot learning benchmark, Meta-Dataset, and demonstrate the benefits of POEM over other meta-learning methods at representation learning from partial observations. We further demonstrate the utility of POEM by meta-learning to represent an environment from partial views observed by an agent exploring the environment.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1195. <h><a href=https://openreview.net/pdf?id=1ROAstc9jv>ChiroDiff: Modelling chirographic data with Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang, Yi-Zhe Song</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1ROAstc9jv>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Learning diffusion model for continuous-time chirographic data (e.g. handwriting, sketch etc.)
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1ROAstc9jv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generative modelling over continuous-time geometric constructs, a.k.a $chirographic\ data$ such as handwriting, sketches, drawings etc., have been accomplished through autoregressive distributions. Such strictly-ordered discrete factorization however falls short of capturing key properties of chirographic data -- it fails to build holistic understanding of the temporal concept due to one-way visibility (causality). Consequently, temporal data has been modelled as discrete token sequences of fixed sampling rate instead of capturing the true underlying concept. In this paper, we introduce a powerful model-class namely Denoising\ Diffusion\ Probabilistic\ Models or DDPMs for chirographic data that specifically addresses these flaws. Our model named &#34;ChiroDiff&#34;, being non-autoregressive, learns to capture holistic concepts and therefore remains resilient to higher temporal sampling rate up to a good extent. Moreover, we show that many important downstream utilities (e.g. conditional sampling, creative mixing) can be flexibly implemented using ChiroDiff. We further show some unique use-cases like stochastic vectorization, de-noising/healing, abstraction are also possible with this model-class. We perform quantitative and qualitative evaluation of our framework on relevant datasets and found it to be better or on par with competing approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1196. <h><a href=https://openreview.net/pdf?id=A3sgyt4HWp>Contextual Image Masking Modeling via Synergized Contrasting without View Augmentation for Faster and Better Visual Pretraining</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shaofeng Zhang, Feng Zhu, Rui Zhao, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=A3sgyt4HWp>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel framework for synergizing MIM and contrastive learning in a close-loop.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/A3sgyt4HWp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a new contextual masking image modeling (MIM) approach called contrasting-aided contextual MIM (ccMIM), under the MIM paradigm for visual pretraining. Specifically, we adopt importance sampling to select the masked patches with richer semantic information for reconstruction, instead of random sampling as done in previous MIM works. As such, the resulting patch reconstruction task from the remaining less semantic patches could be more difficult and helps to learn. To speed up the possibly slowed convergence due to our more difficult reconstruction task, we further propose a new contrastive loss that aligns the tokens of the vision transformer extracted from the selected masked patches and the remaining ones, respectively. The hope is that it serves as a regularizer for patch feature learning such that the image-level global information could be captured in both masked and unmasked patches, and notably such a single-view contrasting avoids the tedious image augmentation step required in recent efforts of introducing contrastive learning to MIM (to speedup convergence and discriminative ability). Meanwhile, the attention score from the contrastive global feature can also carry effective semantic clues to in turn guide our above masking patch selection scheme. In consequence, our contextual MIM and contrastive learning are synergetically performed in a loop (semantic patch selection-token alignment contrasting) to boost the best of the two worlds: fast convergence and strong performance on downstream tasks without ad-hoc augmentations, which are verified by empirical results on ImageNet-1K for both classification and dense vision tasks. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1197. <h><a href=https://openreview.net/pdf?id=42zs3qa2kpy>Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, Jun Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=42zs3qa2kpy>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/42zs3qa2kpy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In offline reinforcement learning, weighted regression is a common method to ensure the learned policy stays close to the behavior policy and to prevent selecting out-of-sample actions. In this work, we show that due to the limited distributional expressivity of policy models, previous methods might still select unseen actions during training, which deviates from their initial motivation. To address this problem, we adopt a generative approach by decoupling the learned policy into two parts: an expressive generative behavior model and an action evaluation model. The key insight is that such decoupling avoids learning an explicitly parameterized policy model with a closed-form expression. Directly learning the behavior policy allows us to leverage existing advances in generative modeling, such as diffusion-based methods, to model diverse behaviors. As for action evaluation, we combine our method with an in-sample planning technique to further avoid selecting out-of-sample actions and increase computational efficiency. Experimental results on D4RL datasets show that our proposed method achieves competitive or superior performance compared with state-of-the-art offline RL methods, especially in complex tasks such as AntMaze. We also empirically demonstrate that our method can successfully learn from a heterogeneous dataset containing multiple distinctive but similarly successful strategies, whereas previous unimodal policies fail.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1198. <h><a href=https://openreview.net/pdf?id=BqrPeZ_e5P>SIMPLE: Specialized Model-Sample Matching for Domain Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziyue Li, Kan Ren, XINYANG JIANG, Yifei Shen, Haipeng Zhang, Dongsheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=BqrPeZ_e5P>6.0 - [5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/BqrPeZ_e5P.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In domain generalization (DG), most existing methods aspire to fine-tune a specific pretrained model through novel DG algorithms. In this paper, we propose an alternative direction, i.e., to efficiently leverage a pool of pretrained models without fine-tuning. Through extensive empirical and theoretical evidence, we demonstrate that (1) pretrained models have possessed generalization to some extent while there is no single best pretrained model across all distribution shifts, and (2) out-of-distribution (OOD) generalization error depends on the fitness between the pretrained model and unseen test distributions. This analysis motivates us to incorporate diverse pretrained models and to dispatch the best matched models for each OOD sample by means of recommendation techniques. To this end, we propose SIMPLE, a specialized model-sample matching method for domain generalization. First, the predictions of pretrained models are adapted to the target domain by a linear label space transformation. A matching network aware of model specialty is then proposed to dynamically recommend proper pretrained models to predict each test sample. The experiments on DomainBed show that our method achieves significant performance improvements (up to 12.2% for individual dataset and 3.9% on average) compared to state-of-the-art (SOTA) methods and further achieves 6.1% gain via enlarging the pretrained model pool. Moreover, our method is highly efficient and achieves more than 1000 times training speedup compared to the conventional DG methods with fine-tuning a pretrained model. Code and supplemental materials are available at https://seqml.github.io/simple.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1199. <h><a href=https://openreview.net/pdf?id=htL4UZ344nF>CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Nadezhda Chirkova, Sergey Troshin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=htL4UZ344nF>6.0 - [5, 3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/htL4UZ344nF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account source code specifics. We propose subtokenziation that reduces average length by 17--40% without downstream performance drop, and show that a carefully chosen subtokenization may improve  quality by 0.5-2%, possibly with some length increase.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1200. <h><a href=https://openreview.net/pdf?id=PbkBDQ5_UbV>Pessimism in the Face of Confounders: Provably Efficient Offline Reinforcement Learning in Partially Observable Markov Decision Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Miao Lu, Yifei Min, Zhaoran Wang, Zhuoran Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PbkBDQ5_UbV>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PbkBDQ5_UbV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study offline reinforcement learning (RL) in partially observable Markov decision processes. In particular, we aim to learn an optimal policy from a dataset collected by a behavior policy which possibly depends on the latent state. Such a dataset is confounded in the sense that the latent state simultaneously affects the action and the observation, which is prohibitive for existing offline RL algorithms. To this end, we propose the \underline{P}roxy variable \underline{P}essimistic \underline{P}olicy \underline{O}ptimization (\texttt{P3O}) algorithm, which addresses the confounding bias and the distributional shift between the optimal and behavior policies in the context of general function approximation. At the core of \texttt{P3O} is a coupled sequence of pessimistic confidence regions  constructed via proximal causal inference, which is  formulated as  minimax estimation. Under a partial coverage assumption on the confounded dataset, we prove that \texttt{P3O} achieves a $n^{-1/2}$-suboptimality, where $n$ is the number of trajectories in the dataset. To our best knowledge, \texttt{P3O} is the first provably efficient offline RL algorithm for POMDPs with a confounded dataset.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1201. <h><a href=https://openreview.net/pdf?id=_q7A0m3vXH0>Geometrically regularized autoencoders for non-Euclidean data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Cheongjae Jang, Yonghyeon Lee, Yung-Kyun Noh, Frank C. Park</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_q7A0m3vXH0>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose geometrically regularized autoencoders for non-Euclidean data and discuss their various use cases.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_q7A0m3vXH0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Regularization is almost {\it de rigueur} when designing autoencoders that are sparse and robust to noise. Given the recent surge of interest in machine learning problems involving non-Euclidean data, in this paper we address the regularization of autoencoders on curved spaces. We show that by ignoring the underlying geometry of the data and applying standard vector space regularization techniques, autoencoder performance can be severely degraded, or worse, training can fail to converge. Assuming that both the data space and latent space can be modeled as Riemannian manifolds, we show how to construct regularization terms in a coordinate-invariant way, and develop geometric generalizations of the denoising autoencoder and reconstruction contractive autoencoder such that the essential properties that enable the estimation of the derivative of the log-probability density are preserved. Drawing upon various non-Euclidean data sets, we show that our geometric autoencoder regularization techniques can have important performance advantages over vector-spaced methods while avoiding other breakdowns that can result from failing to account for the underlying geometry.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1202. <h><a href=https://openreview.net/pdf?id=YV8tP7bW6Kt>Can We Faithfully Represent Absence States to Compute Shapley Values on a DNN?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jie Ren, Zhanpeng Zhou, Qirui Chen, Quanshi Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YV8tP7bW6Kt>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a method to examine and learn baseline values for Shapley values, which ensures that the absent variables do not introduce information to the model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YV8tP7bW6Kt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Masking some input variables of a deep neural network (DNN) and computing output changes on the masked input sample represent a typical way to compute attributions of input variables in the sample. People usually mask an input variable using its baseline value. However, there is no theory to examine whether baseline value faithfully represents the absence of an input variable, i.e., removing all signals from the input variable. Fortunately, recent studies (Ren et al., 2023a; Deng et al., 2022a) show that the inference score of a DNN can be strictly disentangled into a set of causal patterns (or concepts) encoded by the DNN. Therefore, we propose to use causal patterns to examine the faithfulness of baseline values. More crucially, it is proven that causal patterns can be explained as the elementary rationale of the Shapley value. Furthermore, we propose a method to learn optimal baseline values, and experimental results have demonstrated its effectiveness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1203. <h><a href=https://openreview.net/pdf?id=q-PbpHD3EOk>Learning Fast and Slow for Online Time Series Forecasting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Quang Pham, Chenghao Liu, Doyen Sahoo, Steven Hoi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=q-PbpHD3EOk>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel time series forecasting framework based on continual learning and the Complementary Learning System theory
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/q-PbpHD3EOk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the recent success of deep learning for time series forecasting, these methods are not scalable for many real-world applications where data arrives sequentially. Training deep neural forecasters on the fly is notoriously challenging because of their limited ability to adapt to non-stationary environments and remember old knowledge. We argue that the fast adaptation capability of deep neural networks is critical and successful solutions require handling changes to both new and recurring patterns effectively. In this work, inspired by the Complementary Learning Systems (CLS) theory, we propose Fast and Slow learning Network (FSNet) as a novel framework to address the challenges of online forecasting. Particularly, FSNet improves the slowly-learned backbone by dynamically balancing fast adaptation to recent changes and retrieving similar old knowledge. FSNet achieves this mechanism via an interaction between two novel complementary components: (i) a per-layer adapter to support fast learning from individual layers, and (ii) an associative memory to support remembering, updating, and recalling repeating events. Extensive experiments on real and synthetic datasets validate FSNet&#39;s efficacy and robustness to both new and recurring patterns. Our code is publicly available at: \url{https://github.com/salesforce/fsnet/}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1204. <h><a href=https://openreview.net/pdf?id=piIsx-G3Gux>On Explaining Neural Network Robustness with Activation Path</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziping Jiang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=piIsx-G3Gux>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/piIsx-G3Gux.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite their verified performance, neural networks are prone to be misled by maliciously designed adversarial examples. This work investigates the robustness of neural networks from the activation pattern perspective. We find that despite the complex structure of the deep neural network, most of the neurons provide locally stable contributions to the output, while the minority, which we refer to as float neurons, can greatly affect the prediction. We decompose the computational graph of the neural network into the fixed path and float path and investigate their role in generating adversarial examples. Based on our analysis, we categorize the vulnerable examples into Lipschitz vulnerability and float neuron vulnerability. We show that the boost of robust accuracy from randomized smoothing is the result of correcting the latter. We then propose an SC-RFP (smoothed classifier with repressed float path) to further reduce the instability of the float neurons and show that our result can provide a higher certified radius as well as accuracy. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1205. <h><a href=https://openreview.net/pdf?id=5O2uzDusEN5>DFlow: Learning to Synthesize Better Optical Flow Datasets via a Differentiable Pipeline</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kwon Byung-Ki, Nam Hyeon-Woo, Ji-Yun Kim, Tae-Hyun Oh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5O2uzDusEN5>6.0 - [5, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Differentiable and efficient optical flow data generation pipeline
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5O2uzDusEN5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Comprehensive studies of synthetic optical flow datasets have attempted to reveal what properties lead to accuracy improvement in learning-based optical flow estimation. However, manually identifying and verifying the properties that contribute to accurate optical flow estimation require large-scale trial-and-error experiments with iteratively generating whole synthetic datasets and training on them, \ie, impractical. To address this challenge, we propose a differentiable optical flow data generation pipeline and a loss function to drive the pipeline, called DFlow. DFlow efficiently synthesizes a dataset effective for a target domain without the need for cumbersome try-and-errors.  This favorable property is achieved by proposing an efficient dataset comparison method that uses neural networks to approximately encode each dataset and compares the proxy networks instead of explicitly comparing datasets in a pairwise way. Our experiments show the competitive performance of our DFlow against the prior arts in pre-training. Furthermore, compared to competing datasets, DFlow achieves the best fine-tuning performance on the Sintel public benchmark with RAFT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1206. <h><a href=https://openreview.net/pdf?id=fSa5IjNMmmi>Multi-objective optimization via equivariant deep hypervolume approximation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jim Boelrijk, Bernd Ensing, Patrick Forré</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fSa5IjNMmmi>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Hypervolume approximation using permutation invariant, scaling equivariant neural network
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fSa5IjNMmmi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Optimizing multiple competing objectives is a common problem across science and industry. The inherent inextricable trade-off between those objectives leads one to the task of exploring their Pareto front. A meaningful quantity for the purpose of the latter is the hypervolume indicator, which is used in Bayesian Optimization (BO) and Evolutionary Algorithms (EAs). However, the computational complexity for the calculation of the hypervolume scales unfavorably with increasing number of objectives and data points, which restricts its use in those common multi-objective optimization frameworks. 
To overcome these restrictions, previous work has focused on approximating the hypervolume using deep learning. In this work, we propose a novel deep learning architecture to approximate the hypervolume function, which we call DeepHV. For better sample efficiency and generalization, we exploit the fact that the hypervolume is scale equivariant in each of the objectives as well as permutation invariant w.r.t. both the objectives and the samples, by using a deep neural network that is equivariant w.r.t. the combined group of scalings and permutations. We show through an ablation study that including these symmetries leads to significantly improved model accuracy. 
We evaluate our method against exact, and approximate hypervolume methods in terms of accuracy, computation time, and generalization. We also apply and compare our methods to state-of-the-art multi-objective BO methods and EAs on a range of synthetic and real-world benchmark test cases. The results show that our methods are promising for such multi-objective optimization tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1207. <h><a href=https://openreview.net/pdf?id=mE91GkXYipg>Compositional Prompt Tuning with Motion Cues for Open-vocabulary Video Relation Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaifeng Gao, Long Chen, Hanwang Zhang, Jun Xiao, Qianru Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mE91GkXYipg>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mE91GkXYipg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Prompt tuning with large-scale pretrained vision-language models empowers open-vocabulary prediction trained on limited base categories, e.g., object classification and detection. In this paper, we propose compositional prompt tuning with motion cues: an extended prompt tuning paradigm for compositional predictions of video data. In particular, we present Relation Prompt (RePro) for Open-vocabulary Video Visual Relation Detection (Open-VidVRD), where conventional prompt tuning is easily biased to certain subject-object combinations and motion patterns. To this end, RePro addresses the two technical challenges of Open-VidVRD: 1) the prompt tokens should respect the two different semantic roles of subject and object, and 2) the tuning should account for the diverse spatiotemporal motion patterns of the subject-object compositions. Our RePro achieves a new state-of-the-art performance on two VidVRD benchmarks of not only the base training object and predicate categories, but also the unseen ones. Extensive ablations also demonstrate the effectiveness of the proposed compositional and multi-mode design of prompt. Code is available at https://github.com/Dawn-LX/OpenVoc-VidVRD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1208. <h><a href=https://openreview.net/pdf?id=JdgO-ht1uTN>Logical Entity Representation in Knowledge-Graphs for Differentiable Rule Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chi Han, Qizheng He, Charles Yu, Xinya Du, Hanghang Tong, Heng Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JdgO-ht1uTN>6.0 - [5, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose logical entity representation (LERP) to incorporate contextual information of entities into logical rule learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JdgO-ht1uTN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Probabilistic logical rule learning has shown great strength in logical rule mining and knowledge graph completion. It learns logical rules to predict missing edges by reasoning on existing edges in the knowledge graph. However, previous efforts have largely been limited to only modeling chain-like Horn clauses such as R1(x; z) ^ R2(z; y) ) H(x; y). This formulation overlooks additional contextual information from neighboring sub-graphs of entity variables x, y and z. Intuitively, there is a large gap here, as local sub-graphs have been found to provide important information for knowledge graph completion. Inspired by these observations, we propose Logical Entity RePresentation (LERP) to encode contextual information of entities in the knowledge graph. A LERP is designed as a vector of probabilistic logical functions on the entity’s neighboring sub-graph. It is an interpretable representation while allowing for differentiable optimization. We can then incorporate LERP into probabilistic logical rule learning to learn more expressive rules. Empirical results demonstrate that with LERP, our model outperforms other rule learning methods in knowledge graph completion and is comparable or even superior to state-of-the-art black-box methods. Moreover, we find that our model can discover a more expressive family of logical rules. LERP can also be further combined with embedding learning methods like TransE to make it more interpretable.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1209. <h><a href=https://openreview.net/pdf?id=D-zfUK7BR6c>Diagnosing and Rectifying Vision Models using Language</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuhui Zhang, Jeff Z. HaoChen, Shih-Cheng Huang, Kuan-Chieh Wang, James Zou, Serena Yeung</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=D-zfUK7BR6c>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our work highlights a distinct advantage of multi-modal embedding space: the ability to diagnose vision classifiers through natural language.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/D-zfUK7BR6c.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent multi-modal contrastive learning models have demonstrated the ability to learn an embedding space suitable for building strong vision classifiers, by leveraging the rich information in large-scale image-caption datasets. Our work highlights a distinct advantage of this multi-modal embedding space: the ability to diagnose vision classifiers through natural language. The traditional process of diagnosing model behaviors in deployment settings involves labor-intensive data acquisition and annotation. Our proposed method can discover high-error data slices, identify influential attributes and further rectify undesirable model behaviors, without requiring any visual data. Through a combination of theoretical explanation and empirical verification, we present conditions under which classifiers trained on embeddings from one modality can be equivalently applied to embeddings from another modality. On a range of image datasets with known error slices, we demonstrate that our method can effectively identify the error slices and influential attributes, and can further use language to rectify failure modes of the classifier.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1210. <h><a href=https://openreview.net/pdf?id=_hb4vM3jspB>Data-Free One-Shot Federated Learning Under Very High Statistical Heterogeneity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Clare Elizabeth Heinbaugh, Emilio Luz-Ricca, Huajie Shao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_hb4vM3jspB>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We vastly improve on one-shot federated learning performance under very high statistical heterogeneity by reframing the local learning task with a conditional variational autoencoder.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_hb4vM3jspB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning (FL) is an emerging distributed learning framework that collaboratively trains a shared model without transferring the local clients&#39; data to a centralized server. Motivated by concerns stemming from extended communication and potential attacks, one-shot FL limits communication to a single round while attempting to retain performance. However, one-shot FL methods often degrade under high statistical heterogeneity, fail to promote pipeline security, or require an auxiliary public dataset. To address these limitations, we propose two novel data-free one-shot FL methods: FedCVAE-Ens and its extension FedCVAE-KD. Both approaches reframe the local learning task using a conditional variational autoencoder (CVAE) to address high statistical heterogeneity. Furthermore, FedCVAE-KD leverages knowledge distillation to compress the ensemble of client decoders into a single decoder. We propose a method that shifts the center of the CVAE prior distribution and experimentally demonstrate that this promotes security, and show how either method can incorporate heterogeneous local models. We confirm the efficacy of the proposed methods over baselines under high statistical heterogeneity using multiple benchmark datasets. In particular, at the highest levels of statistical heterogeneity, both FedCVAE-Ens and FedCVAE-KD typically more than double the accuracy of the baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1211. <h><a href=https://openreview.net/pdf?id=AZFvpnnewr>Trainability Preserving Neural Pruning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huan Wang, Yun Fu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AZFvpnnewr>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a new filter pruning approach that effectively preserves trainability during pruning with encouraging performance. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AZFvpnnewr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many recent works have shown trainability plays a central role in neural network pruning -- unattended broken trainability can lead to severe under-performance and unintentionally amplify the effect of retraining learning rate, resulting in biased (or even misinterpreted) benchmark results. This paper introduces trainability preserving pruning (TPP), a scalable method to preserve network trainability against pruning, aiming for improved pruning performance and being more robust to retraining hyper-parameters (e.g., learning rate). Specifically, we propose to penalize the gram matrix of convolutional filters to decorrelate the pruned filters from the retained filters. In addition to the convolutional layers, per the spirit of preserving the trainability of the whole network, we also propose to regularize the batch normalization parameters (scale and bias). Empirical studies on linear MLP networks show that TPP can perform on par with the oracle trainability recovery scheme. On nonlinear ConvNets (ResNet56/VGG19) on CIFAR10/100, TPP outperforms the other counterpart approaches by an obvious margin. Moreover, results on ImageNet-1K with ResNets suggest that TPP consistently performs more favorably against other top-performing structured pruning approaches. Code: https://github.com/MingSun-Tse/TPP.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1212. <h><a href=https://openreview.net/pdf?id=CROlOA9Nd8C>Copy is All You Need</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tian Lan, Deng Cai, Yan Wang, Heyan Huang, Xian-Ling Mao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CROlOA9Nd8C>6.0 - [8, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CROlOA9Nd8C.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to domain-specific text collection without extra training. Finally, we observe that our approach attains additional performance gains by simply scaling up to larger text collections, again without further training.\footnote{Our source codes are publicly available at \url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1213. <h><a href=https://openreview.net/pdf?id=rmoMvptXK7M>Gray-Box Gaussian Processes for Automated Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gresa Shala, André Biedenkapp, Frank Hutter, Josif Grabocka</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rmoMvptXK7M>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rmoMvptXK7M.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite having achieved spectacular milestones in an array of important real-world applications, most Reinforcement Learning (RL) methods are very brittle concerning their hyperparameters. Notwithstanding the crucial importance of setting the hyperparameters in training state-of-the-art agents, the task of hyperparameter optimization (HPO) in RL is understudied. In this paper, we propose a novel gray-box Bayesian Optimization technique for HPO in RL, that enriches Gaussian Processes with reward curve estimations based on generalized logistic functions. In a very large-scale experimental protocol, comprising 5 popular RL methods (DDPG, A2C, PPO, SAC, TD3), dozens of environments (Atari, Mujoco), and 7 HPO baselines, we demonstrate that our method significantly outperforms current HPO practices in RL.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1214. <h><a href=https://openreview.net/pdf?id=pRCMXcfdihq>Protein Sequence and Structure Co-Design with Equivariant Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chence Shi, Chuanrui Wang, Jiarui Lu, Bozitao Zhong, Jian Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pRCMXcfdihq>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel framework for protein sequence and structure co-design, which translates proteins in the joint sequence-structure space in an iterative and end-to-end manner.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pRCMXcfdihq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Proteins are macromolecules that perform essential functions in all living organisms. Designing novel proteins with specific structures and desired functions has been a long-standing challenge in the field of bioengineering. Existing approaches generate both protein sequence and structure using either autoregressive models or diffusion models, both of which suffer from high inference costs. In this paper, we propose a new approach capable of protein sequence and structure co-design, which iteratively translates both protein sequence and structure into the desired state from random initialization, based on context features given a priori. Our model consists of a trigonometry-aware encoder that reasons geometrical constraints and interactions from context features, and a roto-translation equivariant decoder that translates protein sequence and structure interdependently. Notably, all protein amino acids are updated in one shot in each translation step, which significantly accelerates the inference process. Experimental results across multiple tasks show that our model outperforms previous state-of-the-art baselines by a large margin, and is able to design proteins of high fidelity as regards both sequence and structure, with running time orders of magnitude less than sampling-based methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1215. <h><a href=https://openreview.net/pdf?id=NI7StoWHJPT>Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guande He, Jianfei Chen, Jun Zhu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NI7StoWHJPT>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NI7StoWHJPT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1216. <h><a href=https://openreview.net/pdf?id=cIbjyd2Vcy>Towards a Unified Theoretical Understanding of Non-contrastive Learning via Rank Differential Mechanism</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhijian Zhuo, Yifei Wang, Jinwen Ma, Yisen Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cIbjyd2Vcy>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cIbjyd2Vcy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, a variety of methods under the name of non-contrastive learning (like BYOL, SimSiam, SwAV, DINO) show that when equipped with some asymmetric architectural designs, aligning positive pairs alone is sufficient to attain good performance in self-supervised visual learning. Despite some understandings of some specific modules (like the predictor in BYOL), there is yet no unified theoretical understanding of how these seemingly different asymmetric designs can all avoid feature collapse, particularly considering  methods that also work without the predictor (like DINO). In this work, we propose a unified theoretical understanding for existing variants of non-contrastive learning. Our theory named Rank Differential Mechanism (RDM) shows that all these asymmetric designs create a consistent rank difference in their dual-branch output features. This rank difference will provably lead to an improvement of effective dimensionality and alleviate either complete or dimensional feature collapse. Different from previous theories, our RDM theory is applicable to different asymmetric designs (with and without the predictor), and thus can serve as a unified understanding of existing non-contrastive learning methods. Besides, our RDM theory also provides practical guidelines for designing many new non-contrastive variants. We show that these variants indeed achieve comparable performance to existing methods on benchmark datasets, and some of them even outperform the baselines. Our code is available at \url{https://github.com/PKU-ML/Rank-Differential-Mechanism}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1217. <h><a href=https://openreview.net/pdf?id=nJ3Vx78Nf7p>Neural Bregman Divergences for Distance Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fred Lu, Edward Raff, Francis Ferraro</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nJ3Vx78Nf7p>6.0 - [8, 3, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop the first effective deep learning tooling for learning arbitrary Bregman divergences
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nJ3Vx78Nf7p.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many metric learning tasks, such as triplet learning, nearest neighbor retrieval, and visualization, are treated primarily as embedding tasks where the ultimate metric is some variant of the Euclidean distance (e.g., cosine or Mahalanobis), and the algorithm must learn to embed points into the pre-chosen space. The study of non-Euclidean geometries is often not explored, which we believe is due to a lack of tools for learning non-Euclidean measures of distance. Recent work has shown that Bregman divergences can be learned from data, opening a promising approach to learning asymmetric distances. We propose a new approach to learning arbitrary Bergman divergences in a differentiable manner via input convex neural networks and show that it overcomes significant limitations of previous works. We also demonstrate that our method more faithfully learns divergences over a set of both new and previously studied tasks, including asymmetric regression, ranking, and clustering. Our tests further extend to known asymmetric, but non-Bregman tasks, where our method still performs competitively despite misspecification, showing the general utility of our approach for asymmetric learning. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1218. <h><a href=https://openreview.net/pdf?id=sPgP6aISLTD>Temporal Disentanglement of Representations for Improved Generalisation in Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mhairi Dunion, Trevor McInroe, Kevin Sebastian Luck, Josiah P. Hanna, Stefano V Albrecht</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sPgP6aISLTD>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce Temporal Disentanglement (TED) to learn disentangled representations for Reinforcement Learning, improving generalisation to unseen environment variables.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sPgP6aISLTD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reinforcement Learning (RL) agents are often unable to generalise well to environment variations in the state space that were not observed during training. This issue is especially problematic for image-based RL, where a change in just one variable, such as the background colour, can change many pixels in the image. The changed pixels can lead to drastic changes in the agent&#39;s latent representation of the image, causing the learned policy to fail. To learn more robust representations, we introduce TEmporal Disentanglement (TED), a self-supervised auxiliary task that leads to disentangled image representations exploiting the sequential nature of RL observations. We find empirically that RL algorithms utilising TED as an auxiliary task adapt more quickly to changes in environment variables with continued training compared to state-of-the-art representation learning methods. Since TED enforces a disentangled structure of the representation, our experiments also show that policies trained with TED generalise better to unseen values of variables irrelevant to the task (e.g. background colour) as well as unseen values of variables that affect the optimal policy (e.g. goal positions).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1219. <h><a href=https://openreview.net/pdf?id=R0Xxvr_X3ZA>Causal Confusion and Reward Misidentification in Preference-Based Reward Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca Dragan, Daniel S. Brown</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=R0Xxvr_X3ZA>6.0 - [5, 6, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We identify and analyze important factors that influence causal confusion when learning rewards from human preference labels.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/R0Xxvr_X3ZA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Learning policies via preference-based reward learning is an increasingly popular method for customizing agent behavior, but has been shown anecdotally to be prone to spurious correlations and reward hacking behaviors. While much prior work focuses on causal confusion in reinforcement learning and behavioral cloning, we focus on a systematic study of causal confusion and reward misidentification when learning from preferences. In particular, we perform a series of sensitivity and ablation analyses on several benchmark domains where rewards learned from preferences achieve minimal test error but fail to generalize to out-of-distribution states---resulting in poor policy performance when optimized. We find that the presence of non-causal distractor features, noise in the stated preferences, and partial state observability can all exacerbate reward misidentification. We also identify a set of methods with which to interpret misidentified learned rewards. In general, we observe that optimizing misidentified rewards drives the policy off the reward&#39;s training distribution, resulting in high predicted (learned) rewards but low true rewards. These findings illuminate the susceptibility of preference learning to reward misidentification and causal confusion---failure to consider even one of many factors can result in unexpected, undesirable behavior. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1220. <h><a href=https://openreview.net/pdf?id=ashPce_W8F->Understanding Why Generalized Reweighting Does Not Improve Over ERM</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Runtian Zhai, Chen Dan, J Zico Kolter, Pradeep Kumar Ravikumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ashPce_W8F->6.0 - [8, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We theoretically prove that the broad family of GRW algorithms which covers many popular methods has an implicit bias almost equivalent to ERM, and thus cannot improve over ERM.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ashPce_W8F-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Empirical risk minimization (ERM) is known to be non-robust in practice to distributional shift where the training and the test distributions are different. A suite of approaches, such as importance weighting, and variants of distributionally robust optimization (DRO), have been proposed to solve this problem. But a line of recent work has empirically shown that these approaches do not significantly improve over ERM in real applications with distribution shift. The goal of this work is to obtain a comprehensive theoretical understanding of this intriguing phenomenon. We first posit the class of Generalized Reweighting (GRW) algorithms, as a broad category of approaches that iteratively update model parameters based on iterative reweighting of the training samples. We show that when overparameterized models are trained under GRW, the resulting models are close to that obtained by ERM. We also show that adding small regularization which does not greatly affect the empirical training accuracy does not help. Together, our results show that a broad category of what we term GRW approaches are not able to achieve distributionally robust generalization. Our work thus has the following sobering takeaway: to make progress towards distributionally robust generalization, we either have to develop non-GRW approaches, or perhaps devise novel classification/regression loss functions that are adapted to GRW approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1221. <h><a href=https://openreview.net/pdf?id=01KmhBsEPFO>Exploring Low-Rank Property in Multiple Instance Learning for Whole Slide Image Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinxi Xiang, Jun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=01KmhBsEPFO>6.0 - [5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/01KmhBsEPFO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The classification of gigapixel-sized whole slide images (WSIs) with slide-level labels can be formulated as a multiple-instance-learning (MIL) problem. State-of-the-art models often consist of two decoupled parts: local feature embedding with a pre-trained model followed by a global feature aggregation network for classification. We leverage the properties of the apparent similarity in high-resolution WSIs, which essentially exhibit \textit{low-rank} structures in the data manifold, to develop a novel MIL with a boost in both feature embedding and feature aggregation. We extend the contrastive learning with a pathology-specific Low-Rank Constraint  (LRC) for feature embedding to pull together samples (i.e., patches) belonging to the same pathological tissue in the low-rank subspace and simultaneously push apart those from different latent subspaces. At the feature aggregation stage, we introduce an iterative low-rank attention MIL (ILRA-MIL) model to aggregate features with low-rank learnable latent vectors to model global interactions among all instances. We highlight the importance of instance correlation modeling but refrain from directly using the transformer encoder considering the $O(n^2)$ complexity. ILRA-MIL with LRC pre-trained features achieves strong empirical results across various benchmarks, including (i) 96.49\% AUC on the CAMELYON16 for binary metastasis classification, (ii) 97.63\% AUC on the TCGA-NSCLC for lung cancer subtyping, and (iii) 0.6562 kappa on the large-scale PANDA dataset for prostate cancer classification. The code is available at https://github.com/jinxixiang/low_rank_wsi.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1222. <h><a href=https://openreview.net/pdf?id=IrzkT99fDJH>AGRO: Adversarial discovery of error-prone Groups for Robust Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bhargavi Paranjape, Pradeep Dasigi, Vivek Srikumar, Luke Zettlemoyer, Hannaneh Hajishirzi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=IrzkT99fDJH>6.0 - [8, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> AGRO is an end-to-end robust optimization technique that discovers error-prone groups and optimizes for their accuracy, resulting in improved robustness to test-time distributional shifts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/IrzkT99fDJH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Models trained via empirical risk minimization (ERM) are known to rely on spurious correlations between labels and task-independent input features, resulting in poor generalization to distributional shifts. Group distributionally robust optimization (G-DRO) can alleviate this problem by minimizing the worst-case loss over a set of pre-defined groups over training data. G-DRO successfully improves performance of the worst group, where the correlation does not hold. However, G-DRO assumes that the spurious correlations and associated worst groups are known in advance,  making it challenging to apply them to new tasks with potentially multiple unknown correlations. We propose AGRO---Adversarial Group discovery for Distributionally Robust Optimization---an end-to-end approach that jointly identifies error-prone groups and improves accuracy on them. AGRO equips G-DRO with an adversarial slicing model to find a group assignment for training examples which maximizes worst-case loss over the discovered groups. On the WILDS benchmark, AGRO results in 8\% higher model performance on average on known worst-groups, compared to prior group discovery approaches used with G-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and MS-COCO---datasets where potential spurious correlations are as yet uncharacterized. Human evaluation of ARGO groups shows that they contain well-defined, yet previously unstudied spurious correlations that lead to model errors.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1223. <h><a href=https://openreview.net/pdf?id=FbRY1XVfwK>Accelerating Hamiltonian Monte Carlo via Chebyshev Integration Time</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jun-Kun Wang, Andre Wibisono</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FbRY1XVfwK>6.0 - [5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FbRY1XVfwK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Hamiltonian Monte Carlo (HMC) is a popular method in sampling. While there are quite a few works of studying this method on various aspects, an interesting question is how to choose its integration time to achieve acceleration. In this work, we consider accelerating the process of sampling from a distribution $\pi(x) \propto \exp(-f(x))$ via HMC via time-varying integration time. When the potential $f$ is $L$-smooth and $m$-strongly convex, i.e. for sampling from a log-smooth and strongly log-concave target distribution $\pi$, it is known that under a constant integration time, the number of iterations that ideal HMC takes to get an $\epsilon$ Wasserstein-2 distance to the target $\pi$ is $O( \kappa \log \frac{1}{\epsilon} )$, where $\kappa := \frac{L}{m}$ is the condition number. We propose a scheme of time-varying integration time based on the roots of Chebyshev polynomials. We show that in the case of quadratic potential $f$, i.e. when the target $\pi$ is a Gaussian distribution, ideal HMC with this choice of integration time only takes $O( \sqrt{\kappa} \log \frac{1}{\epsilon} )$ number of iterations to reach Wasserstein-2 distance less than $\epsilon$; this improvement on the dependence on condition number is akin to acceleration in optimization. The design and analysis of HMC with the proposed integration time is built on the tools of Chebyshev polynomials. Experiments find the advantage of adopting our scheme of time-varying integration time even for sampling from distributions with smooth strongly convex potentials that are not quadratic. 

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1224. <h><a href=https://openreview.net/pdf?id=a6rCdfABJXg>Equivariance-aware Architectural Optimization of Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaitlin Maile, Dennis George Wilson, Patrick Forré</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=a6rCdfABJXg>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Towards architectural optimization over subgroups of equivariant neural networks, we present two mechanisms for approximate equivariance over subgroups and two equivariance-aware neural architecture search algorithms that utilize them.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/a6rCdfABJXg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Incorporating equivariance to symmetry groups as a constraint during neural network training can improve performance and generalization for tasks exhibiting those symmetries, but such symmetries are often not perfectly nor explicitly present. This motivates algorithmically optimizing the architectural constraints imposed by equivariance. We propose the equivariance relaxation morphism, which preserves functionality while reparameterizing a group equivariant layer to operate with equivariance constraints on a subgroup, as well as the $[G]$-mixed equivariant layer, which mixes layers constrained to different groups to enable within-layer equivariance optimization. We further present evolutionary and differentiable neural architecture search (NAS) algorithms that utilize these mechanisms respectively for equivariance-aware architectural optimization. Experiments across a variety of datasets show the benefit of dynamically constrained equivariance to find effective architectures with approximate equivariance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1225. <h><a href=https://openreview.net/pdf?id=tXc-riXhmx>Revisit Finetuning strategy for Few-Shot Learning to Transfer the Emdeddings</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Heng Wang, Tan Yue, Xiang Ye, Zihang He, Bohan Li, Yong Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tXc-riXhmx>6.0 - [6, 6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design a novel finetuning strategy to finetune the feature extractor with unbiased estimation in Few-Shot Learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tXc-riXhmx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Few-Shot Learning (FSL) aims to learn a simple and effective bias on limited novel samples. Recently, many methods have been focused on re-training a randomly initialized linear classifier to adapt it to the novel features extracted by the pre-trained feature extractor (called Linear-Probing-based methods). These methods typically assumed the pre-trained feature extractor was robust enough, i.e., finetuning was not needed, and hence the pre-trained feature extractor does not change on the novel samples. However, the unchanged pre-trained feature extractor will distort the features of novel samples because the robustness assumption may not hold, especially on the out-of-distribution samples. To extract the undistorted features, we designed Linear-Probing-Finetuning with Firth-Bias (LP-FT-FB) to yield an accurate bias on the limited samples for better finetuning the pre-trained feature extractor, providing stronger transferring ability. In LP-FT-FB, we further proposed inverse Firth Bias Reduction (i-FBR) to regularize the over-parameterized feature extractor on which FBR does not work well.	The proposed i-FBR effectively alleviates the over-fitting problem of the feature extractor in the process of finetuning and helps extract undistorted novel features. To show the effectiveness of the designed LP-FT-FB, we conducted a lot of experiments on the commonly used FSL datasets under different backbones, including in-domain and cross-domain FSL tasks. The experimental results show that the proposed FT-LP-FB outperforms the SOTA FSL methods. The code is available at https://github.com/whzyf951620/LinearProbingFinetuningFirthBias.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1226. <h><a href=https://openreview.net/pdf?id=wC98X1qpDBA> Cycle-consistent Masked AutoEncoder for Unsupervised Domain Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haiyang Yang, Xiaotong Li, SHIXIANG TANG, Feng Zhu, Yizhou Wang, Meilin Chen, LEI BAI, Rui Zhao, Wanli Ouyang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wC98X1qpDBA>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wC98X1qpDBA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-supervised learning methods undergo undesirable performance drops when there exists a significant domain gap between training and testing scenarios. Therefore, unsupervised domain generalization (UDG) is proposed to tackle the problem, which requires the model to be trained on several different domains without supervision and generalize well on unseen test domains. Existing methods either rely on a cross-domain and semantically consistent image pair in contrastive methods or the reconstruction pair in generative methods, while the precious image pairs are not available without semantic labels. In this paper, we propose a cycle cross-domain reconstruction task for unsupervised domain generalization in the absence of paired images. The cycle cross-domain reconstruction task converts a masked image from one domain to another domain and then reconstructs the original image from the converted images. To preserve the divergent domain knowledge of decoders in the cycle reconstruction task, we propose a novel domain-contrastive loss to regularize the domain information in reconstructed images encoded with the desirable domain style. Qualitative results on extensive datasets illustrate our method improves the state-of-the-art unsupervised domain generalization methods by average $\textbf{+5.59\%}, \textbf{+4.52\%}, \textbf{+4.22\%}, \textbf{+7.02\%}$ on $1\%, 5\%, 10\%, 100\%$ PACS, and $\textbf{+5.08\%}, \textbf{+6.49\%}, \textbf{+1.79\%}, \textbf{+0.53\%}$ on $1\%, 5\%, 10\%, 100\%$ DomainNet, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1227. <h><a href=https://openreview.net/pdf?id=35QyoZv8cKO>ESCHER: Eschewing Importance Sampling in Games by Computing a History Value Function to Estimate Regret</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Stephen Marcus McAleer, Gabriele Farina, Marc Lanctot, Tuomas Sandholm</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=35QyoZv8cKO>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a principled deep CFR algorithm that can scale to large games by removing importance sampling
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/35QyoZv8cKO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent techniques for approximating Nash equilibria in very large games leverage neural networks to learn approximately optimal policies (strategies). One promis- ing line of research uses neural networks to approximate counterfactual regret minimization (CFR) or its modern variants. DREAM, the only current CFR-based neural method that is model free and therefore scalable to very large games, trains a neural network on an estimated regret target that can have extremely high variance due to an importance sampling term inherited from Monte Carlo CFR (MCCFR). In this paper we propose an unbiased model-free method that does not require any importance sampling. Our method, ESCHER, is principled and is guaranteed to converge to an approximate Nash equilibrium with high probability. We show that the variance of the estimated regret of ESCHER is orders of magnitude lower than DREAM and other baselines. We then show that ESCHER outperforms the prior state of the art—DREAM and neural fictitious self play (NFSP)—on a number of games and the difference becomes dramatic as game size increases. In the very large game of dark chess, ESCHER is able to beat DREAM and NFSP in a head-to-head competition over 90% of the time.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1228. <h><a href=https://openreview.net/pdf?id=SoyOsp7i_l>Logical Message Passing Networks with One-hop Inference on Atomic Formulas</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zihao Wang, Yangqiu Song, Ginny Wong, Simon See</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SoyOsp7i_l>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SoyOsp7i_l.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct one-hop inferences on atomic formulas, the results of which are regarded as the messages passed in LMPNN. The reasoning process over the overall logical formulas is turned into the forward pass of LMPNN that incrementally aggregates local information to finally predict the answers&#39; embeddings. The complex logical inference across different types of queries will then be learned from training examples based on the LMPNN architecture. Theoretically, our query-graph represenation is more general than the prevailing operator-tree formulation, so our approach applies to a broader range of complex KG queries. Empirically, our approach yields the new state-of-the-art neural CQA model. Our research bridges the gap between complex KG query answering tasks and the long-standing achievements of knowledge graph representation learning. Our implementation can be found at https://github.com/HKUST-KnowComp/LMPNN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1229. <h><a href=https://openreview.net/pdf?id=CIFOsnhZvON>TempCLR: Temporal Alignment Representation with Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuncong Yang, Jiawei Ma, Shiyuan Huang, Long Chen, Xudong Lin, Guangxing Han, Shih-Fu Chang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CIFOsnhZvON>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Global sequence matching under temporal order consistency matters in contrastive-based video-paragraph/text learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CIFOsnhZvON.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Video representation learning has been successful in video-text pre-training for zero-shot transfer, where each sentence is trained to be close to the paired video clips in a common feature space. For long videos, given a paragraph of description where the sentences describe different segments of the video, by matching all sentence-clip pairs,  the paragraph and the full video are aligned implicitly. However, such unit-level similarity measure may ignore the global temporal context over a long time span, which inevitably limits the generalization ability. In this paper, we propose a contrastive learning framework TempCLR to compare the full video and the paragraph explicitly. As the video/paragraph is formulated as a sequence of clips/sentences, under the constraint of their temporal order, we use dynamic time warping to compute the minimum cumulative cost over sentence-clip pairs as the sequence-level distance. To explore the temporal dynamics, we break the consistency of temporal order by shuffling the video clips or sentences according to the temporal granularity. In this way, we obtain the representations for clips/sentences, which perceive the temporal information and thus facilitate the sequence alignment. In addition to pre-training on the video and paragraph, our approach can also generalize on the matching between different video instances. We evaluate our approach on video retrieval, action step localization, and few-shot action recognition, and achieve consistent performance gain over all three tasks. Detailed ablation studies are provided to justify the approach design. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1230. <h><a href=https://openreview.net/pdf?id=h1o7Ry9Zctm>Revisiting Robustness in Graph Machine Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lukas Gosch, Daniel Sturm, Simon Geisler, Stephan Günnemann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=h1o7Ry9Zctm>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> GNNs suffer from over-robustness, that is robustness beyond the point of semantic change, which can be combated by including the known label-structure at inference time.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/h1o7Ry9Zctm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results suggest: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the inference process of GNNs significantly reduces over-robustness, while having a positive effect on test accuracy and adversarial robustness. Theoretically, leveraging our new semantics-aware notion of robustness, we prove that there is no robustness-accuracy tradeoff for inductively classifying a newly added node. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1231. <h><a href=https://openreview.net/pdf?id=sSt9fROSZRO>Investigating Multi-task Pretraining and Generalization in Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Adrien Ali Taiga, Rishabh Agarwal, Jesse Farebrother, Aaron Courville, Marc G Bellemare</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sSt9fROSZRO>6.0 - [3, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Multi-task training and generalization on Atari game variants, showing benefits from fine-tuning over zero shot and scaling data size and model capacity.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sSt9fROSZRO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep reinforcement learning~(RL) has achieved remarkable successes in complex single-task settings. However, designing RL agents that can learn multiple tasks and leverage prior experience to quickly adapt to a related new task remains challenging. Despite previous attempts to improve on these areas, our understanding of multi-task training and generalization in RL remains limited. To fill this gap, we investigate the generalization capabilities of a popular actor-critic method, IMPALA. Specifically, we build on previous work that has advocated for the use of modes and difficulties of Atari 2600 games as a challenging benchmark for transfer learning in RL. We do so by pretraining an agent on multiple variants of the same Atari game before fine-tuning on the remaining never-before-seen variants. This protocol simplifies the multi-task pretraining phase by limiting negative interference between tasks and allows us to better understand the dynamics of multi-task training and generalization. We find that, given a fixed amount of pretraining data, agents trained with more variations are able to generalize better. Surprisingly, we also observe that this advantage can still be present after fine-tuning for 200M environment frames than when doing zero-shot transfer. This highlights the potential effect of a good learned representation. We also find that, even though small networks have remained popular to solve Atari 2600 games, increasing the capacity of the value and policy network is critical to achieve good performance as we increase the number of pretraining modes and difficulties. Overall, our findings emphasize key points that are essential for efficient multi-task training and generalization in reinforcement learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1232. <h><a href=https://openreview.net/pdf?id=wGvzQWFyUB>Personalized Reward Learning with Interaction-Grounded Learning (IGL)</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jessica Maghakian, Paul Mineiro, Kishan Panaganti, Mark Rucker, Akanksha Saran, Cheng Tan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wGvzQWFyUB>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Eliminating reward engineering for recommendation systems
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wGvzQWFyUB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In an era of countless content offerings, recommender systems alleviate information overload by providing users with personalized content suggestions. Due to the scarcity of explicit user feedback, modern recommender systems typically optimize for the same fixed combination of implicit feedback signals across all users. However, this approach disregards a growing body of work highlighting that (i) implicit signals can be used by users in diverse ways, signaling anything from satisfaction to active dislike, and (ii) different users communicate preferences in different ways. We propose applying the recent Interaction Grounded Learning (IGL) paradigm to address the challenge of learning representations of diverse user communication modalities. Rather than requiring a fixed, human-designed reward function, IGL is able to learn personalized reward functions for different users and then optimize directly for the latent user satisfaction. We demonstrate the success of IGL with experiments using simulations as well as with real-world production traces.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1233. <h><a href=https://openreview.net/pdf?id=iN3Lh-Vy2TH>Phase transition for detecting a small community in a large network</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiashun Jin, Tracy Ke, Paxton Turner, Anru Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iN3Lh-Vy2TH>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Signed-quadrilateral is optimal among computationally efficient tests for detecting a small community in the degree-corrected stochastic block model.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iN3Lh-Vy2TH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    How to detect a small community in a large network is an interesting problem, including clique detection as a special case, where a naive degree-based $\chi^2$-test was shown to be powerful in the presence of an Erdös-Renyi (ER) background. Using Sinkhorn&#39;s theorem, we show that the signal captured by the $\chi^2$-test may be a modeling artifact, and it may disappear once we replace the Erdös-Renyi model by a broader network model.  We show that the recent SgnQ test is more appropriate for such a setting. The test is optimal in detecting communities with sizes comparable to the whole network, but has never been studied for our setting, which is substantially different and more challenging.  Using a degree-corrected block model (DCBM), we establish phase transitions of this testing problem concerning the size of the small community and the edge densities in small and large communities.  When the size of the small community is larger than $\sqrt{n}$, the SgnQ test is optimal  for it attains the computational lower bound (CLB), the information lower bound for methods allowing polynomial computation time. When the size of the small community is smaller than $\sqrt{n}$, we establish the parameter regime where the SgnQ test has full power and make some conjectures of the CLB. We also study the classical information lower bound (LB) and show that there is always a gap between the CLB and LB in our range of interest. 	
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1234. <h><a href=https://openreview.net/pdf?id=v3y68gz-WEz>Riemannian Metric Learning via Optimal Transport</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Christopher Scarvelis, Justin Solomon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=v3y68gz-WEz>6.0 - [8, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/v3y68gz-WEz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce an optimal transport-based model for learning a metric tensor from cross-sectional samples of evolving probability measures on a common Riemannian manifold. We neurally parametrize the metric as a spatially-varying matrix field and efficiently optimize our model&#39;s objective using a simple alternating scheme. Using this learned metric, we can non-linearly interpolate between probability measures and compute geodesics on the manifold. We show that metrics learned using our method improve the quality of trajectory inference on scRNA and bird migration data at the cost of little additional cross-sectional data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1235. <h><a href=https://openreview.net/pdf?id=9-umxtNPx5E>Masked Frequency Modeling for Self-Supervised Visual Pre-Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiahao Xie, Wei Li, Xiaohang Zhan, Ziwei Liu, Yew-Soon Ong, Chen Change Loy</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9-umxtNPx5E>6.0 - [8, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9-umxtNPx5E.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present Masked Frequency Modeling (MFM), a unified frequency-domain-based approach for self-supervised pre-training of visual models. Instead of randomly inserting mask tokens to the input embeddings in the spatial domain, in this paper, we shift the perspective to the frequency domain. Specifically, MFM first masks out a portion of frequency components of the input image and then predicts the missing frequencies on the frequency spectrum. Our key insight is that predicting masked components in the frequency domain is more ideal to reveal underlying image patterns rather than predicting masked patches in the spatial domain, due to the heavy spatial redundancy. Our findings suggest that with the right configuration of mask-and-predict strategy, both the structural information within high-frequency components and the low-level statistics among low-frequency counterparts are useful in learning good representations. For the first time, MFM demonstrates that, for both ViT and CNN, a simple non-Siamese framework can learn meaningful representations even using none of the following: (i) extra data, (ii) extra model, (iii) mask token. Experimental results on image classification and semantic segmentation, as well as several robustness benchmarks show the competitive performance and advanced robustness of MFM compared with recent masked image modeling approaches. Furthermore, we also comprehensively investigate the effectiveness of classical image restoration tasks for representation learning from a unified frequency perspective and reveal their intriguing relations with our MFM approach. Project page: https://www.mmlab-ntu.com/project/mfm/index.html.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1236. <h><a href=https://openreview.net/pdf?id=NmZXv4467ai>Decision Transformer under Random Frame Dropping</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaizhe Hu, Ray Chen Zheng, Yang Gao, Huazhe Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NmZXv4467ai>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Learning to control against random frame dropping through three original modifications to the Decision Transformer.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NmZXv4467ai.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Controlling agents remotely with deep reinforcement learning~(DRL) in the real world is yet to come. One crucial stepping stone is to devise RL algorithms that are robust in the face of dropped information from corrupted communication or malfunctioning sensors. Typical RL methods usually require considerable online interaction data that are costly and unsafe to collect in the real world. Furthermore, when applying to the frame dropping scenarios, they perform unsatisfactorily even with moderate drop rates. To address these issues, we propose  Decision Transformer under Random Frame Dropping~(DeFog), an offline RL algorithm that enables agents to act robustly in frame dropping scenarios without online interaction. DeFog first randomly masks out data in the offline datasets and explicitly adds the time span of frame dropping as inputs. After that, a finetuning stage on the same offline dataset with a higher mask rate would further boost the performance. Empirical results show that DeFog outperforms strong baselines under severe frame drop rates like 90\%, while maintaining similar returns under non-frame-dropping conditions in the regular MuJoCo control benchmarks and the Atari environments. Our approach offers a robust and deployable solution for controlling agents in real-world environments with limited or unreliable data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1237. <h><a href=https://openreview.net/pdf?id=QWQM0ZwZdRS>Fake It Until You Make It : Towards Accurate Near-Distribution Novelty Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hossein Mirzaei, Mohammadreza Salehi, Sajjad Shahabi, Efstratios Gavves, Cees G. M. Snoek, Mohammad Sabokrou, Mohammad Hossein Rohban</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QWQM0ZwZdRS>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QWQM0ZwZdRS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We aim for image-based novelty detection. Despite considerable progress, existing models either fail or face dramatic drop under the so-called ``near-distribution&#34; setup, where the differences between normal and anomalous samples are subtle. We first demonstrate existing methods could experience up to 20\% decrease in their AUCs in the near-distribution setting. Next, we propose to exploit a score-based generative model to produce synthetic near-distribution anomalous data. Our model is then fine-tuned to distinguish such data from the normal samples. We make quantitative as well as qualitative evaluation of this strategy, and compare the results with a variety of GAN-based models.  Effectiveness of our method for both near-distribution and standard novelty detection is assessed through extensive experiments on datasets in diverse applications such as medical images, object classification, and quality control. This reveals that our method significantly improves upon existing models, and consistently decreases the gap between the near-distribution and standard novelty detection AUCs by a considerable amount.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1238. <h><a href=https://openreview.net/pdf?id=dcN0CaXQhT>Causal Reasoning in the Presence of Latent Confounders via Neural ADMG Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthew Ashman, Chao Ma, Agrin Hilmkil, Joel Jennings, Cheng Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dcN0CaXQhT>6.0 - [5, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dcN0CaXQhT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Latent confounding has been a long-standing obstacle for causal reasoning from observational data. One popular approach is to model the data using acyclic directed mixed graphs (ADMGs), which describe ancestral relations between variables using directed and bidirected edges. However, existing methods using ADMGs are based on either linear functional assumptions or a discrete search that is complicated to use and lacks computational tractability for large datasets. In this work, we further extend the existing body of work and develop a novel gradient-based approach to learning an ADMG with nonlinear functional relations from observational data. We first show that the presence of latent confounding is identifiable under the assumptions of bow-free ADMGs with nonlinear additive noise models. With this insight, we propose a novel neural causal model based on autoregressive flows. This not only enables us to model complex causal relationships behind the data, but also estimate their functional relationships (hence treatment effects) simultaneously. We further validate our approach via experiments on both synthetic and real-world datasets, and demonstrate the competitive performance against relevant baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1239. <h><a href=https://openreview.net/pdf?id=ULsuEVQbV-9>Estimating individual treatment effects under unobserved confounding using binary instruments</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dennis Frauen, Stefan Feuerriegel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ULsuEVQbV-9>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a multiple robust machine learning framework for individual treatment effect estimation using binary instrumental variables.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ULsuEVQbV-9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Estimating conditional average treatment effects (CATEs) from observational data is relevant in many fields such as personalized medicine. However, in practice, the treatment assignment is usually confounded by unobserved variables and thus introduces bias. A remedy to remove the bias is the use of instrumental variables (IVs). Such settings are widespread in medicine (e.g., trials where the treatment assignment is used as binary IV). In this paper, we propose a novel, multiply robust machine learning framework, called MRIV, for estimating CATEs using binary IVs and thus yield an unbiased CATE estimator. Different from previous work for binary IVs, our framework estimates the CATE directly via a pseudo-outcome regression. (1)~We provide a theoretical analysis where we show that our framework yields multiple robust convergence rates: our CATE estimator achieves fast convergence even if several nuisance estimators converge slowly. (2)~We further show that our framework asymptotically outperforms state-of-the-art plug-in IV methods for CATE estimation, in the sense that it achieves a faster rate of convergence if the CATE is smoother than the individual outcome surfaces. (3)~We build upon our theoretical results and propose a tailored deep neural network architecture called MRIV-Net for CATE estimation using binary IVs. Across various computational experiments, we demonstrate empirically that our MRIV-Net achieves state-of-the-art performance. To the best of our knowledge, our MRIV is the first multiply robust machine learning framework tailored to estimating CATEs in the binary IV setting. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1240. <h><a href=https://openreview.net/pdf?id=m9LCdYgN8-6>DAG Learning on the Permutahedron</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Valentina Zantedeschi, Luca Franceschi, Jean Kaddour, Matt Kusner, Vlad Niculae</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=m9LCdYgN8-6>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a continuous optimization framework over the Permutahedron for discovering (DAGs) from observations that does not relax acyclicity and accommodates any edge-optimization procedure, edge structural parameterization, and optimization loss
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/m9LCdYgN8-6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a continuous optimization framework for discovering a latent directed acyclic graph (DAG) from observational data. Our approach optimizes over the polytope of permutation vectors, the so-called Permutahedron, to learn a topological ordering. Edges can be optimized jointly, or learned conditional on the ordering via a non-differentiable subroutine. Compared to existing continuous optimization approaches our formulation has a number of advantages including: 1. validity: optimizes over exact DAGs as opposed to other relaxations optimizing approximate DAGs; 2. modularity: accommodates any edge-optimization procedure, edge structural parameterization, and optimization loss; 3. end-to-end: either alternately iterates between node-ordering and edge-optimization, or optimizes them jointly; We demonstrate, on real-world data problems in protein-signaling and transcriptional network discovery, that our approach lies on the Pareto frontier of two key metrics, the SID and SHD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1241. <h><a href=https://openreview.net/pdf?id=zDjtZZBZtqK>Denoising Masked Autoencoders Help Robust Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>QuanLin Wu, Hang Ye, Yuntian Gu, Huishuai Zhang, Liwei Wang, Di He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zDjtZZBZtqK>6.0 - [5, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we propose a new self-supervised method for learning certified robust classifiers of images.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zDjtZZBZtqK.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we propose a new self-supervised method, which is called denoising masked autoencoders (DMAE), for learning certified robust classifiers of images. In DMAE, we corrupt each image by adding Gaussian noises to each pixel value and randomly masking several patches. A Transformer-based encoder-decoder model is then trained to reconstruct the original image from the corrupted one. In this learning paradigm, the encoder will learn to capture relevant semantics for the downstream tasks, which is also robust to Gaussian additive noises. We show that the pre-trained encoder can naturally be used as the base classifier in Gaussian smoothed models, where we can analytically compute the certified radius for any data point. Although the proposed method is simple, it yields significant performance improvement in downstream classification tasks. We show that the DMAE ViT-Base model, which just uses 1/10 parameters of the model developed in recent work (Carlini et al., 2022), achieves competitive or better certified accuracy in various settings. The DMAE ViT-Large model significantly surpasses all previous results, establishing a new state-of-the-art on ImageNet dataset. We further demonstrate that the pre-trained model has good transferability to the CIFAR-10 dataset, suggesting its wide adaptability. Models and code are available at
https://github.com/quanlin-wu/dmae.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1242. <h><a href=https://openreview.net/pdf?id=VWqiPBB_EM>$O(T^{-1})$ Convergence of Optimistic-Follow-the-Regularized-Leader in Two-Player Zero-Sum Markov Games </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuepeng Yang, Cong Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VWqiPBB_EM>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VWqiPBB_EM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We prove that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with smooth value updates, finds an $O(T^{−1})$ approximate Nash equilibrium in $T$ iterations for two-player zero-sum Markov games with full information. This improves the $\tilde{O}(T^{−5/6})$ convergence rate recently shown by Zhang et al (2022). The refined analysis hinges on two essential ingredients. First, the sum of the regrets of the two players, though not necessarily non-negative as in normal-form games, is approximately non-negative in Markov games. This property allows us to bound the second-order path lengths of the learning dynamics. Second, we prove a tighter algebraic inequality regarding the weights deployed by OFTRL that shaves an extra $\log T$ factor. This crucial improvement enables the inductive analysis that leads to the final $O(T^{−1})$ rate.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1243. <h><a href=https://openreview.net/pdf?id=7J-30ilaUZM>Learning Structured Representations by Embedding Class Hierarchy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Siqi Zeng, Remi Tachet des Combes, Han Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7J-30ilaUZM>6.0 - [5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose to learn structured representations that preserve the hierarchy between label classes by using CPCC as a regularizer.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7J-30ilaUZM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing models for learning representations in supervised classification problems are permutation invariant with respect to class labels. However, structured knowledge about the classes, such as hierarchical label structures, widely exists in many real-world datasets, e.g., the ImageNet and CIFAR benchmarks. How to learn representations that can preserve such structures among the classes remains an open problem. To approach this problem, given a tree of class hierarchy, we first define a tree metric between any pair of nodes in the tree to be the length of the shortest path connecting them. We then provide a method to learn the hierarchical relationship of class labels by approximately embedding the tree metric in the Euclidean space of features. More concretely, during supervised training, we propose to use the Cophenetic Correlation Coefficient (CPCC) as a regularizer for the cross-entropy loss to correlate the tree metric of classes and the Euclidean distance in the class-conditioned representations. Our proposed regularizer is computationally lightweight and easy to implement. Empirically, we demonstrate that this approach can help to learn more interpretable representations due to the preservation of the tree metric, and leads to better in-distribution generalization as well as under sub-population shifts over six real-world datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1244. <h><a href=https://openreview.net/pdf?id=4cOfD2qL6T>Exploring perceptual straightness in learned visual representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anne Harrington, Vasha DuTell, Ayush Tewari, Mark Hamilton, Simon Stent, Ruth Rosenholtz, William T. Freeman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4cOfD2qL6T>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4cOfD2qL6T.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Humans have been shown to use a &#39;&#39;straightened&#39;&#39; encoding to represent the natural visual world as it evolves in time (Henaff et al. 2019). In the context of discrete video sequences, &#39;&#39;straightened&#39;&#39; means that changes between frames follow a more linear path in representation space at progressively deeper levels of processing. While deep convolutional networks are often proposed as models of human visual processing, many do not straighten natural videos. In this paper, we explore the relationship between network architecture, differing types of robustness, biologically-inspired filtering mechanisms, and representational straightness in response to time-varying input; we identify strengths and limitations of straightness as a useful way of evaluating neural network representations. We find that (1) adversarial training leads to straighter representations in both CNN and transformer-based architectures but (2) this effect is task-dependent, not generalizing to tasks such as segmentation and frame-prediction, where straight representations are not favorable for predictions; and nor to other types of robustness. In addition, (3) straighter representations impart temporal stability to class predictions, even for out-of-distribution data. Finally, (4) biologically-inspired elements increase straightness in the early stages of a network, but do not guarantee increased straightness in downstream layers of CNNs. We show that straightness is an easily computed measure of representational robustness and stability, as well as a hallmark of human representations with benefits for computer vision models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1245. <h><a href=https://openreview.net/pdf?id=op-ceGueqc4>Scaling Laws For Deep Learning Based Image Reconstruction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tobit Klug, Reinhard Heckel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=op-ceGueqc4>6.0 - [8, 6, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The performance improvement of deep learning based image reconstruction methods as a function of the training set size slows already at moderate training set sizes, indicating that only marginal gains are expected beyond a few thousand examples.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/op-ceGueqc4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks trained end-to-end to map a measurement of a (noisy) image to a clean image perform excellent for a variety of linear inverse problems. 
Current methods are only trained on a few hundreds or thousands of images as opposed to the millions of examples deep networks are trained on in other domains. 
In this work, we study whether major performance gains are expected from scaling up the training set size.
We consider image denoising, accelerated magnetic resonance imaging, and super-resolution and empirically determine the reconstruction quality as a function of training set size, while simultaneously scaling the network size.  
For all three tasks we find that an initially steep power-law scaling slows significantly already at moderate training set sizes. 
Interpolating those scaling laws suggests that even training on millions of images would not significantly improve performance. 
To understand the expected behavior, we analytically characterize the performance of a linear estimator learned with early stopped gradient descent. 
The result formalizes the intuition that once the error induced by learning the signal model is small relative to the error floor, more training examples do not improve performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1246. <h><a href=https://openreview.net/pdf?id=dCSFiAl_VO3>Improved Learning-augmented Algorithms for k-means and k-medians Clustering</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thy Dinh Nguyen, Anamay Chaturvedi, Huy Nguyen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dCSFiAl_VO3>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dCSFiAl_VO3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the problem of clustering in the learning-augmented setting. We are given a data set in $d$-dimensional Euclidean space, and a label for each data point given by a predictor indicating what subsets of points should be clustered together. This setting captures situations where we have access to some auxiliary information about the data set relevant for our clustering objective, for instance the labels output by a neural network. Following prior work, we assume that there are at most an $\alpha \in (0,c)$ for some $c&lt;1$ fraction of false positives and false negatives in each  predicted cluster, in the absence of which the labels would attain the optimal clustering cost $\mathrm{OPT}$. For a dataset of size $m$, we propose a deterministic $k$-means algorithm that produces centers with an improved bound on the clustering cost compared to the previous randomized state-of-the-art algorithm while preserving the $O( d m \log m)$ runtime. Furthermore, our algorithm works even when the predictions are not very accurate, i.e., our cost bound holds for $\alpha$ up to $1/2$, an improvement from $\alpha$ being at most $1/7$ in previous work. For the $k$-medians problem we again improve upon prior work by achieving a biquadratic improvement in the dependence of the approximation factor on the accuracy parameter $\alpha$ to get a cost of $(1+O(\alpha))\mathrm{OPT}$, while requiring essentially just $O(md \log^3 m/\alpha)$ runtime.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1247. <h><a href=https://openreview.net/pdf?id=UP_GHHPw7rP>Nearly Minimax Optimal Offline Reinforcement Learning with Linear Function Approximation: Single-Agent MDP and Markov Game</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Liwei Wang, Tong Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UP_GHHPw7rP>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UP_GHHPw7rP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning (RL) aims at learning an optimal strategy using a pre-collected dataset without further interactions with the environment. While various algorithms have been proposed for offline RL in the previous literature, the minimax optimality has only been (nearly) established for tabular Markov decision processes (MDPs). In this paper, we focus on offline RL with linear function approximation and propose a new pessimism-based algorithm for offline linear MDP. At the core of our algorithm is the uncertainty decomposition via a reference function, which is new in the literature of offline RL under linear function approximation. Theoretical analysis demonstrates that our algorithm can match the performance lower bound up to logarithmic factors. We also extend our techniques to the two-player zero-sum Markov games (MGs), and establish a new performance lower bound for MGs, which tightens the existing result, and verifies the nearly minimax optimality of the proposed algorithm. To the best of our knowledge, these are the first computationally efficient and nearly minimax optimal algorithms for offline single-agent MDPs and MGs with linear function approximation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1248. <h><a href=https://openreview.net/pdf?id=5BaqCFVh5qL>Avoiding spurious correlations via logit correction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sheng Liu, Xu Zhang, Nitesh Sekhar, Yue Wu, Prateek Singhal, Carlos Fernandez-Granda</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5BaqCFVh5qL>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b>  We propose the logit correction (LC) loss, a simple yet effective improvement on the softmax cross-entropy loss, to mitigate spurious correlations
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5BaqCFVh5qL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Empirical studies suggest that machine learning models trained with empirical risk minimization (ERM) often rely on attributes that may be spuriously correlated with the class labels. Such models typically lead to poor performance during inference for data lacking such correlations. In this work, we explicitly consider a situation where potential spurious correlations are present in the majority of training data. In contrast with existing approaches, which use the ERM model outputs to detect the samples without spurious correlations and either heuristically upweight or upsample those samples, we propose the logit correction (LC) loss, a simple yet effective improvement on the softmax cross-entropy loss, to correct the sample logit. We demonstrate that minimizing the LC loss is equivalent to maximizing the group-balanced accuracy, so the proposed LC could mitigate the negative impacts of spurious correlations. Our extensive experimental results further reveal that the proposed LC loss outperforms state-of-the-art solutions on multiple popular benchmarks by a large margin, an average 5.5% absolute improvement, without access to spurious attribute labels. LC is also competitive with oracle methods that make use of the attribute labels.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1249. <h><a href=https://openreview.net/pdf?id=3zSn48RUO8M>What shapes the loss landscape of self supervised learning?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Liu Ziyin, Ekdeep Singh Lubana, Masahito Ueda, Hidenori Tanaka</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3zSn48RUO8M>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We analytically solve the loss landscape of self-supervised learning and identify the causes of complete and dimensional collapse
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3zSn48RUO8M.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1250. <h><a href=https://openreview.net/pdf?id=bH-kCY6LdKg>A new characterization of the edge of stability based on a sharpness measure aware of batch gradient distribution</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sungyoon Lee, Cheongjae Jang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bH-kCY6LdKg>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bH-kCY6LdKg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    For full-batch gradient descent (GD), it has been empirically shown that the sharpness, the top eigenvalue of the Hessian, increases and then hovers above $2/\text{(learning rate)}$, and this is called ``the edge of stability&#39;&#39; phenomenon. However, it is unclear why the sharpness is somewhat larger than $2/\text{(learning rate)}$ and how this can be extended to general mini-batch stochastic gradient descent (SGD). We propose a new sharpness measure (interaction-aware-sharpness) aware of the \emph{interaction} between the batch gradient distribution and the loss landscape geometry. This leads to a more refined and general characterization of the edge of stability for SGD. Moreover, based on the analysis of a concentration measure of the batch gradient, we propose a more accurate scaling rule, Linear and Saturation Scaling Rule (LSSR), between batch size and learning rate.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1251. <h><a href=https://openreview.net/pdf?id=7Cb7Faxa1OB>Understanding The Robustness of Self-supervised Learning Through Topic Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeping Luo, Shiyou Wu, Cindy Weng, Mo Zhou, Rong Ge</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7Cb7Faxa1OB>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7Cb7Faxa1OB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-supervised learning has significantly improved the performance of many NLP tasks. However, how can self-supervised learning discover useful features, and why is it better than traditional approaches such as probabilistic models are still largely unknown. In this paper, we focus on the context of topic modeling and highlight a key advantage of self-supervised learning - when applied to data generated by topic models, self-supervised learning can be oblivious to the specific model, and hence is less susceptible to model misspecification. In particular, we prove that commonly used self-supervised objectives based on reconstruction or contrastive samples can both recover useful posterior information for general topic models. Empirically, we show that the same objectives can perform on par with posterior inference using the correct model, while outperforming posterior inference using misspecified models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1252. <h><a href=https://openreview.net/pdf?id=-qg8MQNrxZw>SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qiang Wan, Zilong Huang, Jiachen Lu, Gang YU, Li Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-qg8MQNrxZw>6.0 - [5, 8, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-qg8MQNrxZw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Since the introduction of Vision Transformers, the landscape of many computer vision tasks (e.g., semantic segmentation), which has been overwhelmingly dominated by CNNs, recently has significantly revolutionized. However, the computational cost and memory requirement render these methods unsuitable on the mobile device, especially for the high resolution per-pixel semantic segmentation task. In this paper, we introduce a new method squeeze-enhanced Axial Transformer (SeaFormer) for mobile semantic segmentation. Specifically, we design a generic attention block characterized by the formulation of squeeze Axial and spatial enhancement. It can be further used to create a family of backbone architectures with superior cost-effectiveness. Coupled with a light segmentation head, we demonstrate state-of-the-art results on the ADE20K, Pascal Context and COCO-stuff datasets. Critically, we beat both the mobile-friendly rivals and Transformer-based counterparts with better performance and lower latency without bells and whistles. Beyond semantic segmentation, we further apply the proposed SeaFormer architecture to image classification problem, demonstrating the potentials of serving as a versatile mobile-friendly backbone.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1253. <h><a href=https://openreview.net/pdf?id=4UldFtZ_CVF>Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuai Zhang, Meng Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miao Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4UldFtZ_CVF>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Encouraged by the empirical success of sparse learners in accelerating GNN training, this paper characterizes the impact of graph sampling and neuron pruning on the sample complexity and convergence rate for a desirable test accuracy quantitatively.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4UldFtZ_CVF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include graph sparsification that samples a subgraph to reduce the amount of data aggregation and model sparsification that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1254. <h><a href=https://openreview.net/pdf?id=NRHajbzg8y0P>Multimodal Analogical Reasoning over Knowledge Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ningyu Zhang, Lei Li, Xiang Chen, Xiaozhuan Liang, Shumin Deng, Huajun Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NRHajbzg8y0P>6.0 - [8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Multimodal analogical reasoning over knowledge graphs with a new dataset MARS and a new framework MarT.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NRHajbzg8y0P.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. We hope our work can deliver benefits and inspire future research. Code and datasets are available in https://github.com/zjunlp/MKG_Analogy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1255. <h><a href=https://openreview.net/pdf?id=7h5KSs2PCRi>Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyuan Allen-Zhu, Yuanzhi Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7h5KSs2PCRi>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a theory to study how generative adversarial networks (GANs) can efficiently learn certain hierarchically generated distributions that are close to the distribution of images in practice.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7h5KSs2PCRi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generative adversarial networks (GANs) are among the most successful models for learning high-complexity, real-world distributions. However, in theory, due to the highly non-convex, non-concave landscape of the minmax training objective, GAN remains one of the least understood deep learning models. In this work, we formally study how GANs can efficiently learn certain hierarchically generated distributions that are close to the distribution of real-life images. We prove that when a distribution has a structure that we refer to as \emph{forward super-resolution}, then simply training generative adversarial networks using stochastic gradient descent ascent (SGDA) can learn this distribution efficiently, both in sample and time complexities.
We also provide empirical evidence that our assumption ``forward super-resolution&#39;&#39; is very natural in practice, and the underlying learning mechanisms that we study in this paper (to allow us efficiently train GAN via GDA in theory) simulates the actual learning process of GANs on real-world problems.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1256. <h><a href=https://openreview.net/pdf?id=zVrw4OH1Lch>FIFA: Making Fairness More Generalizable in Classifiers Trained on Imbalanced Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhun Deng, Jiayao Zhang, Linjun Zhang, Ting Ye, Yates Coley, Weijie J Su, James Zou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zVrw4OH1Lch>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zVrw4OH1Lch.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Algorithmic fairness plays an important role in machine learning and imposing fairness constraints during learning is a common approach. However, many datasets are imbalanced in certain label classes (e.g. &#34;healthy&#34;) and sensitive subgroups (e.g. &#34;older patients&#34;). Empirically, this imbalance leads to a lack of generalizability not only of classification but also of fairness properties, especially in over-parameterized models. For example, fairness-aware training may 
ensure equalized odds (EO) on the training data, but EO is far from being satisfied on new users. In this paper, we propose a theoretically-principled, yet {\bf F}lexible approach that is {\bf I}mbalance-{\bf F}airness-{\bf A}ware ({\bf FIFA}). Specifically, FIFA encourages both classification and fairness generalization and can be flexibly combined with many existing fair learning methods with logits-based losses. While our main focus is on EO, FIFA can be directly applied to achieve equalized opportunity (EqOpt); and under certain conditions, it can also be applied to other fairness notions. We demonstrate the power of FIFA by combining it with a popular fair classification algorithm, and the resulting algorithm achieves significantly better fairness generalization on several real-world datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1257. <h><a href=https://openreview.net/pdf?id=Fg3mYW8owg>Knowledge Distillation based Degradation Estimation for Blind Super-Resolution</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bin Xia, Yulun Zhang, Yitong Wang, Yapeng Tian, Wenming Yang, Radu Timofte, Luc Van Gool</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Fg3mYW8owg>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a knowledge distillation based blind super-resolution network, which can generalize to all degradation processes and achieve SOTA performance efficiently.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Fg3mYW8owg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Blind image super-resolution (Blind-SR) aims to recover a high-resolution (HR) image from its corresponding low-resolution (LR) input image with unknown degradations. Most of the existing works design an explicit degradation estimator for each degradation to guide SR. However, it is infeasible to provide concrete labels of multiple degradation combinations (\eg, blur, noise, jpeg compression) to supervise the degradation estimator training. In addition, these special designs for certain degradation, such as blur, impedes the models from being generalized to handle different degradations. To this end, it is necessary to design an implicit degradation estimator that can extract discriminative degradation representation for all degradations without relying on the supervision of degradation ground-truth. In this paper, we propose a Knowledge Distillation based Blind-SR network (KDSR). It consists of a knowledge distillation based implicit degradation estimator network (KD-IDE) and an efficient SR network. To learn the KDSR model, we first train a teacher network: KD-IDE$_{T}$. It takes paired HR and LR patches as inputs and is optimized with the SR network jointly. Then, we further train a student network KD-IDE$_{S}$, which only takes LR images as input and learns to extract the same implicit degradation representation (IDR) as KD-IDE$_{T}$. In addition, to fully use extracted IDR, we design a simple, strong, and efficient IDR based dynamic convolution residual block (IDR-DCRB) to build an SR network. We conduct extensive experiments under classic and real-world degradation settings. The results show that KDSR achieves SOTA performance and can generalize to various degradation processes. The source codes and pre-trained models will be released.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1258. <h><a href=https://openreview.net/pdf?id=li4GQCQWkv>Towards Inferential Reproducibility of Machine Learning Research</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Hagmann, Philipp Meier, Stefan Riezler</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=li4GQCQWkv>6.0 - [5, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Methods for inferential reproducibility of machine learning, using signficance testing under meta-parameter variation, variance components,  and reliability coefficients.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/li4GQCQWkv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reliability of machine learning evaluation --- the consistency of observed evaluation scores across replicated model training runs --- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized likelihood ratio test (GLRT). This allows us to incorporate arbitrary sources of noise like meta-parameter variations into statistical significance testing, and to assess performance differences conditional on data properties. Furthermore, a variance component analysis (VCA) enables the analysis of the  contribution of noise sources to overall variance and the computation of a reliability coefficient by the ratio of substantial to total variance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1259. <h><a href=https://openreview.net/pdf?id=VE1s3e5xriA>Dual Student Networks for Data-Free Model Stealing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>James Beetham, Navid Kardan, Ajmal Saeed Mian, Mubarak Shah</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VE1s3e5xriA>6.0 - [8, 5, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VE1s3e5xriA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Data-free model stealing aims to replicate a target model without direct access to either the training data or the target model. To accomplish this, existing methods use a generator to produce samples in order to train a student model to match the target model outputs. To this end, the two main challenges are estimating gradients of the target model without access to its parameters, and generating a diverse set of training samples that thoroughly explores the input space. We propose a Dual Student method where two students are symmetrically trained in order to provide the generator a criterion to generate samples that the two students disagree on. On one hand, disagreement on a sample implies at least one student has classified the sample incorrectly when compared to the target model. This incentive towards disagreement implicitly encourages the generator to explore more diverse regions of the input space. On the other hand, our method utilizes gradients of student models to indirectly estimate gradients of the target model. We show that this novel training objective for the generator network is equivalent to optimizing a lower bound on the generator&#39;s loss if we had access to the target model gradients. In other words, our method alters the standard data-free model stealing paradigm by substituting the target model with a separate student model, thereby creating a lower bound which can be directly optimized without additional target model queries or separate synthetic datasets. We show that our new optimization framework provides more accurate gradient estimation of the target model and better accuracies on benchmark classification datasets. Additionally, our approach balances improved query efficiency with training computation cost. Finally, we demonstrate that our method serves as a better proxy model for transfer-based adversarial attacks than existing data-free model stealing methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1260. <h><a href=https://openreview.net/pdf?id=9MO7bjoAfIA>Self-Ensemble Protection: Training Checkpoints Are Good Data Protectors</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sizhe Chen, Geng Yuan, Xinwen Cheng, Yifan Gong, Minghai Qin, Yanzhi Wang, Xiaolin Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9MO7bjoAfIA>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We protect proprietary datasets by using intermediate checkpoints in a self-ensemble way, which more than halves the testing accuracy in unauthorized training compared to the best baselines.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9MO7bjoAfIA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As data becomes increasingly vital, a company would be very cautious about releasing data, because the competitors could use it to train high-performance models, thereby posing a tremendous threat to the company&#39;s commercial competence. To prevent training good models on the data, we could add imperceptible perturbations to it. Since such perturbations aim at hurting the entire training process, they should reflect the vulnerability of DNN training, rather than that of a single model. Based on this new idea, we seek perturbed examples that are always unrecognized (never correctly classified) in training. In this paper, we uncover them by model checkpoints&#39; gradients, forming the proposed self-ensemble protection (SEP), which is very effective because (1) learning on examples ignored during normal training tends to yield DNNs ignoring normal examples; (2) checkpoints&#39; cross-model gradients are close to orthogonal, meaning that they are as diverse as DNNs with different architectures. That is, our amazing performance of ensemble only requires the computation of training one model. By extensive experiments with 9 baselines on 3 datasets and 5 architectures, SEP is verified to be a new state-of-the-art, e.g., our small $\ell_\infty=2/255$ perturbations reduce the accuracy of a CIFAR-10 ResNet18 from 94.56% to 14.68%, compared to 41.35% by the best-known method. Code is available at https://github.com/Sizhe-Chen/SEP.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1261. <h><a href=https://openreview.net/pdf?id=Jbdc0vTOcol>A Time Series is Worth 64 Words:  Long-term Forecasting with Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, Jayant Kalagnanam</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Jbdc0vTOcol>6.0 - [6, 5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Channel-independent patch time series transformer works very well for long-term forecasting and representation learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Jbdc0vTOcol.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-training performed on one dataset to other datasets also produces SOTA forecasting accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1262. <h><a href=https://openreview.net/pdf?id=tPrRs6YB2P>Scenario-based Question Answering with Interacting Contextual Properties</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haitian Sun, William W. Cohen, Ruslan Salakhutdinov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tPrRs6YB2P>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed a model for scenario-based QA which requires reasoning over multiple contextual properties in user scenarios to find answers that are consistent with the scenarios and to identify necessary information which is missing from the scenarios.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tPrRs6YB2P.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In the scenario-based Question Answering (QA) task, models are asked to find answers that are appropriate to the user scenarios associated with the question and identify information that is missing from the scenarios but is necessary for the answers to hold. Scenarios commonly include multiple properties of users, such as age, employment status, and income level for the question “How much can I claim from this benefit”. The properties relevant to a potential answer are given in a document, which will state conditions necessary for the answer to hold. Documents also may specify how conditions interact with each other, e.g. with text like “one of the conditions below must apply”. Although understanding the relationship between conditions is crucial for solving this challenging QA task, limited work has been done so far in modeling this. In this paper, we propose the T-Reasoner model, which solves this problem with three jointly learned modules: an entailment module which checks whether a condition has been satisfied by the scenario, a decoding module which locates eligible answers from documents, and a reasoning module which infers the relationship between conditions and performs a reasoning step to determine the logically consistent answers and identify missing conditions. T-Reasoner outperforms strong baselines on a synthetic scenario-based QA dataset and achieves a new state-of-the-art on two scenario-based QA benchmarks, outperforming the prior best models by 3-10 points.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1263. <h><a href=https://openreview.net/pdf?id=bQB6qozaBw>Information Plane Analysis for Dropout Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Linara Adilova, Bernhard C Geiger, Asja Fischer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bQB6qozaBw>6.0 - [3, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Information plane analysis is a promising tool for neural networks analysis, for which mutual information can be measured more reliable with continuous dropout
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bQB6qozaBw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The information-theoretic framework promises to explain the predictive power of neural networks. In particular, the information plane analysis, which measures mutual information (MI) between input and representation as well as representation and output, should give rich insights into the training process. This approach, however, was shown to strongly depend on the choice of estimator of the MI. The problem is amplified for deterministic networks if the MI between input and representation is infinite. Thus, the estimated values are defined by the different approaches for estimation, but do not adequately represent the training process from an information-theoretic perspective. In this work, we show that dropout with continuously distributed noise ensures that MI is finite. We demonstrate in a range of experiments that this enables a meaningful information plane analysis for a class of dropout neural networks that is widely used in practice.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1264. <h><a href=https://openreview.net/pdf?id=fiB2RjmgwQ6>Filter-Recovery Network for Multi-Speaker Audio-Visual Speech Separation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoyue Cheng, Zhaoyang Liu, Wayne Wu, Limin Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fiB2RjmgwQ6>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fiB2RjmgwQ6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we systematically study the audio-visual speech separation task in a multi-speaker scenario. Given the facial information of each speaker, the goal of this task is to separate the corresponding speech from the mixed speech. The existing works are designed for speech separation in a controlled setting with a fixed number of speakers (mostly 2 or 3 speakers), which seems to be impractical for real applications. As a result, we try to utilize a single model to separate the voices with a variable number of speakers. Based on the observation, there are two prominent issues for multi-speaker separation: 1) There are some noisy voice pieces belonging to other speakers in the separation results; 2) Part of the target speech is missing after separation. Accordingly, we propose \textbf{BFRNet}, including a {\bf B}asic audio-visual speech separator and a Filter-Recovery Network (\textbf{FRNet}). FRNet can refine the coarse audio separated by basic audio-visual speech separator. To have fair comparisons, we build a comprehensive benchmark for multi-speaker audio-visual speech separation to verify the performance of various methods. Experimental results show that our method is able to achieve the state-of-the-art performance. Furthermore, we also find that FRNet can boost the performance of other off-the-shelf speech separators, which exhibits its ability of generalization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1265. <h><a href=https://openreview.net/pdf?id=HJFVrpCaGE>Provable Robustness against Wasserstein Distribution Shifts via Input Randomization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aounon Kumar, Alexander Levine, Tom Goldstein, Soheil Feizi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HJFVrpCaGE>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present provable robustness guarantees on the accuracy of a model under Wasserstein shifts of the input distribution.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HJFVrpCaGE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Certified robustness in machine learning has primarily focused on adversarial perturbations with a fixed attack budget for each sample in the input distribution. In this work, we present provable robustness guarantees on the accuracy of a model under bounded Wasserstein shifts of the data distribution. We show that a simple procedure that randomizes the input of the model within a transformation space is provably robust to distributional shifts under that transformation. Our framework allows the datum-specific perturbation size to vary across different points in the input distribution and is general enough to include fixed-sized perturbations as well. Our certificates produce guaranteed lower bounds on the performance of the model for any shift (natural or adversarial) of the input distribution within a Wasserstein ball around the original distribution. We apply our technique to certify robustness against natural (non-adversarial) transformations of images such as color shifts, hue shifts, and changes in brightness and saturation. We obtain strong performance guarantees for the robust model under clearly visible shifts in the input images. Our experiments establish the non-vacuousness of our certificates by showing that the certified lower bound on a robust model&#39;s accuracy is higher than the empirical accuracy of an undefended model under a distribution shift. Moreover, our results also imply guaranteed lower bounds (hardness result) on the performance of models trained on so-called &#34;unlearnable&#34; datasets that have been poisoned to interfere with model training. We show that the performance of a robust model is guaranteed to remain above a certain threshold on the test distribution even when the base model is trained on the poisoned dataset.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1266. <h><a href=https://openreview.net/pdf?id=-5EWhW_4qWP>NTK-SAP: Improving neural network pruning by aligning training dynamics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yite Wang, Dawei Li, Ruoyu Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-5EWhW_4qWP>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a pruning-at-initialization method by aligning the eigenspectrum of NTK to that of the dense network.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-5EWhW_4qWP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pruning neural networks before training has received increasing interest due to its potential to reduce training time and memory. One popular method is to prune the connections based on a  certain metric, but it is not entirely clear what metric is the best choice. Recent advances in neural tangent kernel (NTK) theory suggest that the training dynamics of large enough neural networks is closely related to the spectrum of the NTK. Motivated by this finding, we propose to prune the connections that have the least influence on the spectrum of the NTK. This method can help maintain the NTK spectrum, which may help align the training dynamics to that of its dense counterpart. However, one possible issue is that the fixed-weight-NTK corresponding to a given initial point can be very different from the NTK corresponding to later iterates during the training phase. We further propose to sample multiple realizations of random weights to estimate the NTK spectrum. Note that our approach is weight-agnostic, which is different from most existing methods that are weight-dependent. In addition, we use random inputs to compute the fixed-weight-NTK, making our method data-agnostic as well. We name our foresight pruning algorithm Neural Tangent Kernel Spectrum-Aware Pruning (NTK-SAP). Empirically, our method achieves better performance than all baselines on multiple datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1267. <h><a href=https://openreview.net/pdf?id=h21yJhdzbwz>Towards One-shot Neural Combinatorial Solvers: Theoretical and Empirical Notes on the Cardinality-Constrained Case</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Runzhong Wang, Li Shen, Yiting Chen, Xiaokang Yang, Dacheng Tao, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=h21yJhdzbwz>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a Gumbel-Sinkhorn network for cardinality-constrained combinatorial optimization with theoretical and empirical notes. We surpass Erdos Goes Neural on optimization problems, and present an application on predictive portfolio optimization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/h21yJhdzbwz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    One-shot non-autoregressive neural networks, different from RL-based ones, have been actively adopted for solving combinatorial optimization (CO) problems, which can be trained by the objective score in a self-supervised manner. Such methods have shown their superiority in efficiency (e.g. by parallelization) and potential for tackling predictive CO problems for decision-making under uncertainty. While the discrete constraints often become a bottleneck for gradient-based neural solvers, as currently handled in three typical ways: 1) adding a soft penalty in the objective, where a bounded violation of the constraints cannot be guaranteed, being critical to many constraint-sensitive scenarios; 2) perturbing the input to generate an approximate gradient in a black-box manner, though the constraints are exactly obeyed while the approximate gradients can hurt the performance on the objective score; 3) a compromise by developing soft algorithms whereby the output of neural networks obeys a relaxed constraint, and there can still occur an arbitrary degree of constraint-violation. Towards the ultimate goal of establishing a general framework for neural CO solver with the ability to control an arbitrary-small degree of constraint violation, in this paper, we focus on a more achievable and common setting: the cardinality constraints, which in fact can be readily encoded by a differentiable optimal transport (OT) layer. Based on this observation, we propose OT-based cardinality constraint encoding for end-to-end CO problem learning with two variants: Sinkhorn and Gumbel-Sinkhorn, whereby their violation of the constraints can be exactly characterized and bounded by our theoretical results. On synthetic and real-world CO problem instances, our methods surpass the state-of-the-art CO network and are comparable to (if not superior to) the commercial solver Gurobi. In particular, we further showcase a case study of applying our approach to the predictive portfolio optimization task on real-world asset price data, improving the Sharpe ratio from 1.1 to 2.0 of a strong LSTM+Gurobi baseline under the classic predict-then-optimize paradigm.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1268. <h><a href=https://openreview.net/pdf?id=rFQfjDC9Mt>Clean-image Backdoor: Attacking Multi-label Models with Poisoned Labels Only</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kangjie Chen, Xiaoxuan Lou, Guowen Xu, Jiwei Li, Tianwei Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rFQfjDC9Mt>6.0 - [6, 6, 6, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rFQfjDC9Mt.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multi-label models have been widely used in various applications including image annotation and object detection. The fly in the ointment is its inherent vulnerability to backdoor attacks due to the adoption of deep learning techniques. However, all existing backdoor attacks exclusively require to modify training inputs (e.g., images), which may be impractical in real-world applications. In this paper, we aim to break this wall and propose the first clean-image backdoor attack, which only poisons the training labels without touching the training samples. Our key insight is that in a multi-label learning task, the adversary can just manipulate the annotations of training samples consisting of a specific set of classes to activate the backdoor. We design a novel trigger exploration method to find convert and effective triggers to enhance the attack performance. We also propose three target label selection strategies to achieve different goals. Experimental results indicate that our clean-image backdoor can achieve a 98% attack success rate while preserving the model&#39;s functionality on the benign inputs. Besides, the proposed clean-image backdoor can evade existing state-of-the-art defenses.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1269. <h><a href=https://openreview.net/pdf?id=1UCaQYUdE_o>Understanding Neural Coding on Latent Manifolds by Sharing Features and Dividing Ensembles</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Martin Bjerke, Lukas Schott, Kristopher T Jensen, Claudia Battistin, David A. Klindt, Benjamin Adric Dunn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1UCaQYUdE_o>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose neural latent variable models with feature sharing and ensemble detection.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1UCaQYUdE_o.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Systems neuroscience relies on two complementary views of neural data, characterized by single neuron tuning curves and analysis of population activity. These two perspectives combine elegantly in neural latent variable models that constrain the relationship between latent variables and neural activity, modeled by simple tuning curve functions. This has recently been demonstrated using Gaussian processes, with applications to realistic and topologically relevant latent manifolds. Those and previous models, however, missed crucial shared coding properties of neural populations. We propose $\textit{feature sharing}$ across neural tuning curves which significantly improves performance and helps optimization. We also propose a solution to the $\textit{ensemble detection}$ problem, where different groups of neurons, i.e., ensembles, can be modulated by different latent manifolds. Achieved through a soft clustering of neurons during training, this allows for the separation of mixed neural populations in an unsupervised manner. These innovations lead to more interpretable models of neural population activity that train well and perform better even on mixtures of complex latent manifolds. Finally, we apply our method on a recently published grid cell dataset, and recover distinct ensembles, infer toroidal latents and predict neural tuning curves in a single integrated modeling framework.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1270. <h><a href=https://openreview.net/pdf?id=3YFDsSRSxB->Unicom: Universal and Compact Representation Learning for Image Retrieval</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiang An, Jiankang Deng, Kaicheng Yang, Jaiwei Li, Ziyong Feng, Jia Guo, Jing Yang, Tongliang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3YFDsSRSxB->6.0 - [6, 5, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3YFDsSRSxB-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern image retrieval methods typically rely on fine-tuning pre-trained encoders to extract image-level descriptors.
However, the most widely used models are pre-trained on ImageNet-1K with limited classes. The pre-trained feature representation is therefore not universal enough to generalize well to the diverse open-world classes. 
In this paper, we first cluster the large-scale \laion{} into one million pseudo classes based on the joint textual and visual features extracted by the CLIP model. Due to the confusion of label granularity, the automatically clustered dataset inevitably contains heavy inter-class conflict. To alleviate such conflict, we randomly select partial inter-class prototypes to construct the margin-based softmax loss. To further enhance the low-dimensional feature representation, we randomly select partial feature dimensions when calculating the similarities between embeddings and class-wise prototypes. The dual random partial selections are with respect to the class dimension and the feature dimension of the prototype matrix, making the classification conflict-robust and the feature embedding compact. Our method significantly outperforms state-of-the-art unsupervised and supervised image retrieval approaches on multiple benchmarks. The code and pre-trained models are released to facilitate future research \url{https://github.com/deepglint/unicom}. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1271. <h><a href=https://openreview.net/pdf?id=0cpM2ApF9p6>MeshDiffusion: Score-based Generative 3D Mesh Modeling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, Weiyang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0cpM2ApF9p6>6.0 - [6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Diffusion model on 3D meshes of arbitrary topology by direct parametrizing meshes with tetrahedral grids
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0cpM2ApF9p6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the task of generating realistic 3D shapes, which is useful for a variety of applications such as automatic scene generation and physical simulation. Compared to other 3D representations like voxels and point clouds, meshes are more desirable in practice, because (1) they enable easy and arbitrary manipulation of shapes for relighting and simulation, and (2) they can fully leverage the power of modern graphics pipelines which are mostly optimized for meshes. Previous scalable methods for generating meshes typically rely on sub-optimal post-processing, and they tend to produce overly-smooth or noisy surfaces without fine-grained geometric details. To overcome these shortcomings, we take advantage of the graph structure of meshes and use a simple yet very effective generative modeling method to generate 3D meshes. Specifically, we represent meshes with deformable tetrahedral grids, and then train a diffusion model on this direct parameterization. We demonstrate the effectiveness of our model on multiple generative tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1272. <h><a href=https://openreview.net/pdf?id=jwdqNwyREyh>Layer Grafted Pre-training: Bridging Contrastive Learning And Masked Image Modeling For Label-Efficient Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziyu Jiang, Yinpeng Chen, Mengchen Liu, Dongdong Chen, Xiyang Dai, Lu Yuan, Zicheng Liu, Zhangyang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jwdqNwyREyh>6.0 - [8, 5, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed a simple yet principled combination for MIM and CL that can merge both merits of them.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jwdqNwyREyh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, both Contrastive Learning (CL) and Mask Image Modeling (MIM) demonstrate that self-supervision is powerful to learn good representations. However, naively combining them is far from success. In this paper, we start by making the empirical observation that a naive joint optimization of CL and MIM losses leads to conflicting gradient directions - more severe as the layers go deeper. This motivates us to shift the paradigm from combining loss at the end, to choosing the proper learning method per network layer. Inspired by experimental observations, we find that MIM and CL are suitable to lower and higher layers, respectively. We hence propose to combine them in a surprisingly simple, ``sequential cascade&#39;&#39; fashion: early layers are first trained under one MIM loss, on top of which latter layers continue to be trained under another CL loss. The proposed Layer Grafted Pre-training learns good visual representations that demonstrate superior label efficiency in downstream applications, in particular yielding strong few-shot performance besides linear evaluation. For instance, on ImageNet-1k, Layer Grafted Pre-training yields 65.5% Top-1 accuracy in terms of 1% few-shot learning with ViT-B/16, which improves MIM and CL baselines by 14.4% and 2.1% with no bells and whistles. The code is available at https://github.com/VITA-Group/layerGraftedPretraining_ICLR23.git.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1273. <h><a href=https://openreview.net/pdf?id=TkQ1sxd9P4>Interpretable Debiasing of Vectorized Language Representations with Iterative Orthogonalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Prince Osei Aboagye, Yan Zheng, Jack Shunn, Chin-Chia Michael Yeh, Junpeng Wang, Zhongfang Zhuang, Huiyuan Chen, Liang Wang, Wei Zhang, Jeff Phillips</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TkQ1sxd9P4>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our proposed debiasing technique significantly improves the amount of debiasing while retaining relevant information in the embedding representation. It can also be extended to multiple subspace debiasing.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TkQ1sxd9P4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a new mechanism to augment a word vector embedding representation that offers improved bias removal while retaining the key information—resulting in improved interpretability of the representation. Rather than removing the information associated with a concept that may induce bias, our proposed method identifies two concept subspaces and makes them orthogonal. The resulting representation has these two concepts uncorrelated. Moreover, because they are orthogonal, one can simply apply a rotation on the basis of the representation so that the resulting subspace corresponds with coordinates. This explicit encoding of concepts to coordinates works because they have been made fully orthogonal, which previous approaches do not achieve. Furthermore, we show that this can be extended to multiple subspaces. As a result, one can choose a subset of concepts to be represented transparently and explicitly, while the others are retained in the mixed but extremely expressive format of the representation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1274. <h><a href=https://openreview.net/pdf?id=Qc_OopMEBnC>Learning to Segment from Noisy Annotations: A Spatial Correction Approach</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiachen Yao, Yikai Zhang, Songzhu Zheng, Mayank Goswami, Prateek Prasanna, Chao Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Qc_OopMEBnC>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Qc_OopMEBnC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Noisy labels can significantly affect the performance of deep neural networks (DNNs). In medical image segmentation tasks, annotations are error-prone due to the high demand in annotation time and in the annotators&#39; expertise. Existing methods mostly tackle label noise in classification tasks. Their independent-noise assumptions do not fit label noise in segmentation task. In this paper, we propose a novel noise model for segmentation problems that encodes spatial correlation and bias, which are prominent in segmentation annotations. Further, to mitigate such label noise, we propose a label correction method to recover true label progressively. We provide theoretical guarantees of the correctness of the proposed method. Experiments show that our approach outperforms current state-of-the-art methods on both synthetic and real-world noisy annotations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1275. <h><a href=https://openreview.net/pdf?id=xYWqSjBcGMl>Anamnesic Neural Differential Equations with Orthogonal Polynomial Projections</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Edward De Brouwer, Rahul G Krishnan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xYWqSjBcGMl>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Long-term memory Neural ODEs archictecture using orthogonal polynomials projections.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xYWqSjBcGMl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural ordinary differential equations (Neural ODEs) are an effective framework for learning dynamical systems from irregularly sampled time series data. These models provide a continuous-time latent representation of the underlying dynamical system where new observations at arbitrary time points can be used to update the latent representation of the dynamical system. Existing parameterizations for the dynamics functions of Neural ODEs limit the ability of the model to retain global information about the time series; specifically, a piece-wise integration of the latent process between observations can result in a loss of memory on the dynamic patterns of previously observed data points. We propose PolyODE, a Neural ODE that models the latent continuous-time process as a projection onto a basis of orthogonal polynomials. This formulation enforces long-range memory and preserves a global representation of the underlying dynamical system. Our construction is backed by favourable theoretical guarantees and in a series of experiments, we demonstrate that it outperforms previous works in the reconstruction of past and future data, and in downstream prediction tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1276. <h><a href=https://openreview.net/pdf?id=sciA_xgYofB>Impossibly Good Experts and How to Follow Them</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aaron Walsman, Muru Zhang, Sanjiban Choudhury, Dieter Fox, Ali Farhadi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sciA_xgYofB>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sciA_xgYofB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the sequential decision making problem of learning from an expert that has access to more information than the learner.  For many problems this extra information will enable the expert to achieve greater long term reward than any policy without this privileged information access.  We call these experts ``Impossibly Good&#39;&#39; because no learning algorithm will be able to reproduce their behavior.  However, in these settings it is reasonable to attempt to recover the best policy possible given the agent&#39;s restricted access to information.  We provide a set of necessary criteria on the expert that will allow a learner to recover the optimal policy in the reduced information space from the expert&#39;s advice alone.  We also provide a new approach called Elf Distillation (Explorer Learning from Follower) that can be used in cases where these criteria are not met and environmental rewards must be taken into account.  We show that this algorithm performs better than a variety of strong baselines on a challenging suite of Minigrid and Vizdoom environments.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1277. <h><a href=https://openreview.net/pdf?id=_X12NmQKvX>TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Siheng Xiong, Yuan Yang, Faramarz Fekri, James Clayton Kerce</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_X12NmQKvX>6.0 - [8, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_X12NmQKvX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while providing interpretable results. In particular, we consider various scenarios in which training samples are limited, data is biased, and the time range between training and inference are different. In all these cases, TILP works much better than the state-of-the-art methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1278. <h><a href=https://openreview.net/pdf?id=n70oyIlS4g>An Extensible Multi-modal Multi-task Object Dataset with Materials</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Trevor Scott Standley, Ruohan Gao, Dawn Chen, Jiajun Wu, Silvio Savarese</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n70oyIlS4g>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a dataset of Amazon product listings. The dataset includes images, text, price, mass, materials, and categories + others. We also show how to quickly add custom binary attributes to the dataset.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n70oyIlS4g.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present EMMa, an Extensible, Multimodal dataset of Amazon product listings that contains rich Material annotations. It contains more than 2.8 million objects, each with image(s), listing text, mass, price, product ratings, and position in Amazon’s product-category taxonomy. We also design a comprehensive taxonomy of 182 physical materials (e.g., Plastic → Thermoplastic → Acrylic). Objects areannotated with one or more materials from this taxonomy. With the numerous attributes available for each object, we develop a Smart Labeling framework to quickly add new binary labels to all objects with very little manual labeling effort, making the dataset extensible. Each object attribute in our dataset can be included in either the model inputs or outputs, leading to combinatorial possibilities in task configurations. For example, we can train a model to predict the object category from the listing text, or the mass and price from the product listing image. EMMa offers a new benchmark for multi-task learning in computer vision and NLP, and allows practitioners to efficiently add new tasks and object attributes at scale.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1279. <h><a href=https://openreview.net/pdf?id=vPXp7K_Yhre>Asynchronous Gradient Play in Zero-Sum Multi-agent Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruicheng Ao, Shicong Cen, Yuejie Chi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vPXp7K_Yhre>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work provides the first set of algorithms and analyses on understanding asynchronous gradient play in zero-sum multi-agent polymatrix games under a wide range of delay assumptions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vPXp7K_Yhre.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Finding equilibria via gradient play in competitive multi-agent games has been attracting a growing amount of attention in recent years, with emphasis on designing efficient strategies where the agents operate in a decentralized and symmetric manner with guaranteed convergence. While significant efforts have been made in understanding zero-sum two-player matrix games, the performance in zero-sum multi-agent games remains inadequately explored, especially in the presence of delayed feedbacks, leaving the scalability and resiliency of gradient play open to questions. In this paper, we make progress by studying asynchronous gradient plays in zero-sum polymatrix games under delayed feedbacks. We first establish that the last iterate of entropy-regularized optimistic multiplicative weight updates (OMWU) method converges linearly to the quantal response equilibrium (QRE), the solution concept under bounded rationality, in the absence of delays. The linear convergence continues to hold even when the feedbacks are randomly delayed under mild statistical assumptions, albeit at a slower rate. Moving beyond random delays, we further demonstrate entropy-regularized OMWU with two-timescale learning rates enjoys faster last-iterate convergence under fixed delays, and continues to converge provably even when the delays are arbitrarily bounded. Our methods also lead to finite-time guarantees to approximate the Nash equilibrium (NE) by moderating the amount of regularization. To the best of our knowledge, this work is the first that aims to understand asynchronous gradient play in zero-sum polymatrix games under a wide range of delay assumptions.
 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1280. <h><a href=https://openreview.net/pdf?id=u2Pd6x794I>When Source-Free Domain Adaptation Meets Learning with Noisy Labels</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Li Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, Ian McLeod, Boyu Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=u2Pd6x794I>6.0 - [6, 6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/u2Pd6x794I.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent state-of-the-art source-free domain adaptation (SFDA) methods have focused on learning meaningful cluster structures in the feature space, which have succeeded in adapting the knowledge from source domain to unlabeled target domain without accessing the private source data. However, existing methods rely on the pseudo-labels generated by source models that can be noisy due to domain shift. In this paper, we study SFDA from the perspective of learning with label noise (LLN). Unlike the label noise in the conventional LLN scenario, we prove that the label noise in SFDA follows a different distribution assumption. We also prove that such a difference makes existing LLN methods that rely on their distribution assumptions unable to address the label noise in SFDA. Empirical evidence suggests that only marginal improvements are achieved when applying the existing LLN methods to solve the SFDA problem. On the other hand, although there exists a fundamental difference between the label noise in the two scenarios, we demonstrate theoretically that the early-time training phenomenon (ETP), which has been previously observed in conventional label noise settings, can also be observed in the SFDA problem. Extensive experiments demonstrate significant improvements to existing SFDA algorithms by leveraging ETP to address the label noise in SFDA.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1281. <h><a href=https://openreview.net/pdf?id=r0xte-t40I>Learning Human-Compatible Representations for Case-Based Decision Support</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Han Liu, Yizhou Tian, Chacha Chen, Shi Feng, Yuxin Chen, Chenhao Tan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=r0xte-t40I>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We combine metric learning and supervised classification to learn human-compatible decision-focused representation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/r0xte-t40I.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Algorithmic case-based decision support provides examples to help human make sense of predicted labels and aid human in decision-making tasks. Despite the promising performance of supervised learning, representations learned by supervised models may not align well with human intuitions: what models consider as similar examples can be perceived as distinct by humans. As a result, they have limited effectiveness in case-based decision support. In this work, we incorporate ideas from metric learning with supervised learning to examine the importance of alignment for effective decision support. In addition to instance-level labels, we use human-provided triplet judgments to learn human-compatible decision-focused representations. Using both synthetic data and human subject experiments in multiple classification tasks, we demonstrate that such representation is better aligned with human perception than representation solely optimized for classification. Human-compatible representations identify nearest neighbors that are perceived as more similar by humans and allow humans to make more accurate predictions, leading to substantial improvements in human decision accuracies (17.8% in butterfly vs. moth classification and 13.2% in pneumonia classification).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1282. <h><a href=https://openreview.net/pdf?id=S-h1oFv-mq>Long-Tailed Learning Requires Feature Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thomas Laurent, James von Brecht, Xavier Bresson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=S-h1oFv-mq>6.0 - [5, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the importance of learning features in order to achieve good generalization when the data distribution has a long tail. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/S-h1oFv-mq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a simple data model inspired from natural data such as text or images, and use it to study the importance of learning features in order to achieve good generalization. Our data model follows a long-tailed distribution in the sense that some rare and uncommon subcategories have few representatives in the training set. In this context we provide evidence that a learner succeeds if and only if it identifies the correct features, and moreover derive non-asymptotic generalization error bounds that precisely quantify the penalty that one must pay for not learning features.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1283. <h><a href=https://openreview.net/pdf?id=Idusfje4-Wq>Diffusion Models for Causal Discovery via Topological Ordering</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pedro Sanchez, Xiao Liu, Alison Q O&#39;Neil, Sotirios A. Tsaftaris</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Idusfje4-Wq>6.0 - [8, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We use diffusion models for causal discovery by iteratively finding and removing leaves in causal graph, resulting in a efficient topological ordering algorithm for high-dimensional graphs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Idusfje4-Wq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Discovering causal relations from observational data becomes possible with additional assumptions such as considering the functional relations to be constrained as nonlinear with additive noise (ANM). Even with strong assumptions, causal discovery involves an expensive search problem over the space of directed acyclic graphs (DAGs). \emph{Topological ordering} approaches reduce the optimisation space of causal discovery by searching over a permutation rather than graph space.
For ANMs, the \emph{Hessian} of the data log-likelihood can be used for finding leaf nodes in a causal graph, allowing its topological ordering. However, existing computational methods for obtaining the Hessian still do not scale as the number of variables and the number of samples are increased. Therefore, inspired by recent innovations in diffusion probabilistic models (DPMs), we propose \emph{DiffAN}, a topological ordering algorithm that leverages DPMs for learning a Hessian function. We introduce theory for updating the learned Hessian without re-training the neural network, and we show that computing with a subset of samples gives an accurate approximation of the ordering, which allows scaling to datasets with more samples and variables. We show empirically that our method scales exceptionally well to datasets with up to $500$ nodes and up to $10^5$ samples while still performing on par over small datasets with state-of-the-art causal discovery methods.
Implementation is available at \url{https://github.com/vios-s/DiffAN} .
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1284. <h><a href=https://openreview.net/pdf?id=4MbGnp4iPQ>Equivariant Shape-Conditioned Generation of 3D Molecules for Ligand-Based Drug Design</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Keir Adams, Connor W. Coley</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4MbGnp4iPQ>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop a shape-conditioned 3D generative model for ligand-based drug design
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4MbGnp4iPQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Shape-based virtual screening is widely used in ligand-based drug design to search chemical libraries for molecules with similar 3D shapes yet novel 2D graph structures compared to known ligands. 3D deep generative models can potentially automate this exploration of shape-conditioned 3D chemical space; however, no existing models can reliably generate geometrically realistic drug-like molecules in conformations with a specific shape. We introduce a new multimodal 3D generative model that enables shape-conditioned 3D molecular design by equivariantly encoding molecular shape and variationally encoding chemical identity. We ensure local geometric and chemical validity of generated molecules by using autoregressive fragment-based generation with heuristic bonding geometries, allowing the model to prioritize the scoring of rotatable bonds to best align the growing conformation to the target shape. We evaluate our 3D generative model in tasks relevant to drug design including shape-conditioned generation of chemically diverse molecular structures and shape-constrained molecular property optimization, demonstrating its utility over virtual screening of enumerated libraries.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1285. <h><a href=https://openreview.net/pdf?id=WL8FlAugqQ>Neural DAG Scheduling via One-Shot Priority Sampling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wonseok Jeon, Mukul Gagrani, Burak Bartan, Weiliang Will Zeng, Harris Teague, Piero Zappi, Christopher Lott</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=WL8FlAugqQ>6.0 - [5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel ML scheduler that uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/WL8FlAugqQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the problem of scheduling operations/nodes, the dependency among which is characterized by a Directed Acyclic Graph (DAG). Due to its NP-hard nature, heuristic algorithms were traditionally used to acquire reasonably good solutions, and more recent works have proposed Machine Learning (ML) heuristics that can generalize to unseen graphs and outperform the non-ML heuristics. However, it is computationally costly to generate solutions using existing ML schedulers since they adopt the episodic reinforcement learning framework that necessitates multi-round neural network processing. We propose a novel ML scheduler that uses a one-shot neural network encoder to sample node priorities which are converted by list scheduling to the final schedules. Since the one-shot encoder can efficiently sample the priorities in parallel, our algorithm runs significantly faster than existing ML baselines and has comparable run time with the fast traditional heuristics. We empirically show that our algorithm generates better schedules than both non-neural and neural baselines across various real-world and synthetic scheduling tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1286. <h><a href=https://openreview.net/pdf?id=jClGv3Qjhb>A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongkang Li, Meng Wang, Sijia Liu, Pin-Yu Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jClGv3Qjhb>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A theoretical characterization of generalization and sample complexity of training three-layer Vision Transformers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jClGv3Qjhb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, the theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a three-layer ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover,  this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1287. <h><a href=https://openreview.net/pdf?id=Qd0p0bl-A9t>Provably Efficient Lifelong Reinforcement Learning with Linear Representation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sanae Amani, Lin Yang, Ching-An Cheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Qd0p0bl-A9t>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study lifelong RL, where the agent needs to solve a streaming sequence of tasks. We propose an algorithm with provable sublinear regret using sublinear number of planning calls for any sequence of tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Qd0p0bl-A9t.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We theoretically study lifelong reinforcement learning (RL) with linear representation in a regret minimization setting. The goal of the agent is to learn a multi-task policy based on a linear representation while solving a sequence of tasks that may be adaptively chosen based on the agent&#39;s past behaviors. We frame the problem as a linearly parameterized contextual Markov decision process (MDP), where each task is specified by a context and the transition dynamics is context-independent, and we introduce a new completeness-style assumption on the representation which is sufficient to ensure the optimal multi-task policy is realizable under the linear representation. Under this assumption, we propose an algorithm, called UCB Lifelong Value Distillation (UCBlvd), that provably achieves sublinear regret for any sequence of tasks while using only sublinear planning calls. Specifically, for $K$ task episodes of horizon $H$, our algorithm has a regret bound $\tilde{\mathcal{O}}(\sqrt{(d^3+d^\prime d)H^4K})$ using $\mathcal{O}(dH\log(K))$ number of planning calls, where $d$ and $d^\prime$ are the feature dimensions of the dynamics and rewards, respectively. This theoretical guarantee implies that our algorithm can enable a lifelong learning agent to learn to internalize experiences into a multi-task policy and rapidly solve new tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1288. <h><a href=https://openreview.net/pdf?id=R1U5G2spbLd>Federated Nearest Neighbor Machine Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yichao Du, Zhirui Zhang, Bingzhe Wu, Lemao Liu, Tong Xu, Enhong Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=R1U5G2spbLd>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel federated nearest neighbor machine translation framework to build low-overhead privacy-preserving MT systems in FL settings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/R1U5G2spbLd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    To protect user privacy and meet legal regulations, federated learning (FL) is attracting significant attention. Training neural machine translation (NMT) models with traditional FL algorithm (e.g., FedAvg) typically relies on multi-round model-based interactions. However, it is impractical and inefficient for machine translation tasks due to the vast communication overheads and heavy synchronization. In this paper, we propose a novel federated nearest neighbor (FedNN) machine translation framework that, instead of multi-round model-based interactions, leverages one-round memorization-based interaction to share knowledge across different clients to build low-overhead privacy-preserving systems. The whole approach equips the public NMT model trained on large-scale accessible data with a $k$-nearest-neighbor ($k$NN) classifier and integrates the external datastore constructed by private text data in all clients to form the final FL model.  A two-phase datastore encryption strategy is introduced to achieve privacy-preserving during this process.  Extensive experiments show that FedNN significantly reduces computational and communication costs compared with FedAvg, while maintaining promising performance in different FL settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1289. <h><a href=https://openreview.net/pdf?id=5gDz_yTcst>Towards Better Selective Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Leo Feng, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, Amir H. Abdi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5gDz_yTcst>6.0 - [8, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5gDz_yTcst.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We tackle the problem of Selective Classification where the objective is to achieve the best performance on a predetermined ratio (coverage) of the dataset. Recent state-of-the-art selective methods come with architectural changes either via introducing a separate selection head or an extra abstention logit. In this paper, we challenge the aforementioned methods. The results suggest that the superior performance of state-of-the-art methods is owed to training a more generalizable classifier rather than their proposed selection mechanisms. We argue that the best performing selection mechanism should instead be rooted in the classifier itself. Our proposed selection strategy uses the classification scores and achieves better results by a significant margin, consistently, across all coverages and all datasets, without any added compute cost. Furthermore, inspired by semi-supervised learning, we propose an entropy-based regularizer that improves the performance of selective classification methods. Our proposed selection mechanism with the proposed entropy-based regularizer achieves new state-of-the-art results.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1290. <h><a href=https://openreview.net/pdf?id=8oJHwb3Sgp>Represent to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lingxiao Wang, Qi Cai, Zhuoran Yang, Zhaoran Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8oJHwb3Sgp>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8oJHwb3Sgp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reinforcement learning in partially observed Markov decision processes (POMDPs) faces two challenges. (i) It often takes the full history to predict the future, which induces a sample complexity that scales exponentially with the horizon. (ii) The observation and state spaces are often continuous, which induces a sample complexity that scales exponentially with the extrinsic dimension. Addressing such challenges requires learning a minimal but sufficient representation of the observation and state histories by exploiting the structure of the POMDP.

To this end, we propose a reinforcement learning algorithm named Represent to Control (RTC), which learns the representation at two levels while optimizing the policy.~(i) For each step, RTC learns to represent the state with a low-dimensional feature, which factorizes the transition kernel. (ii) Across multiple steps, RTC learns to represent the full history with a low-dimensional embedding, which assembles the per-step feature. We integrate (i) and (ii) in a unified framework that allows a variety of estimators (including maximum likelihood estimators and generative adversarial networks). For a class of POMDPs with a low-rank structure in the transition kernel, RTC attains an $O(1/\epsilon^2)$ sample complexity that scales polynomially with the horizon and the intrinsic dimension (that is, the rank). Here $\epsilon$ is the optimality gap. To our best knowledge, RTC is the first sample-efficient algorithm that bridges representation learning and policy optimization in POMDPs with infinite observation and state spaces.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1291. <h><a href=https://openreview.net/pdf?id=3VKiaagxw1S>Gradient Boosting Performs Gaussian Process Inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aleksei Ustimenko, Artem Beliakov, Liudmila Prokhorenkova</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3VKiaagxw1S>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove that gradient boosting converges to a Gaussian process&#39; posterior mean and can be transformed into a sampler from the posterior, which leads to improved knowledge uncertainty estimates.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3VKiaagxw1S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence to a Gaussian Process&#39; posterior mean, which, in turn, allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance. We show that the proposed sampler allows for better knowledge uncertainty estimates leading to improved out-of-domain detection.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1292. <h><a href=https://openreview.net/pdf?id=TrwE8l9aJzs>Learning Zero-Shot Cooperation with Humans, Assuming Humans Are Biased</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, Yi Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TrwE8l9aJzs>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TrwE8l9aJzs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    There is a recent trend of applying multi-agent reinforcement learning (MARL) to train an agent that can cooperate with humans in a zero-shot fashion without using any human data. The typical workflow is to first repeatedly run self-play (SP) to build a policy pool and then train the final adaptive policy against this pool. A crucial limitation of this framework is that every policy in the pool is optimized w.r.t. the environment reward function, which implicitly assumes that the testing partners of the adaptive policy will be precisely optimizing the same reward function as well. However, human objectives are often substantially biased according to their own preferences, which can differ greatly from the environment reward. We propose a more general framework, Hidden-Utility Self-Play (HSP), which explicitly models human biases as hidden reward functions in the self-play objective. By approximating the reward space as linear functions, HSP adopts an effective technique to generate an augmented policy pool with biased policies. We evaluate HSP on the Overcooked benchmark. Empirical results show that our HSP method produces higher rewards than baselines when cooperating with learned human models, manually scripted policies, and real humans. The HSP policy is also rated as the most assistive policy based on human feedback.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1293. <h><a href=https://openreview.net/pdf?id=B8a1FcY0vi>From $t$-SNE to UMAP with contrastive learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sebastian Damrich, Niklas Böhm, Fred A Hamprecht, Dmitry Kobak</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=B8a1FcY0vi>6.0 - [6, 3, 8, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that UMAP is effectively negative sampling applied to the t-SNE loss function.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/B8a1FcY0vi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neighbor embedding methods $t$-SNE and UMAP are the de facto standard for visualizing high-dimensional datasets. Motivated from entirely different viewpoints, their loss functions appear to be unrelated. In practice, they yield strongly differing embeddings and can suggest conflicting interpretations of the same data. The fundamental reasons for this and, more generally, the exact relationship between $t$-SNE and UMAP have remained unclear. In this work, we uncover their conceptual connection via a new insight into contrastive learning methods. Noise-contrastive estimation can be used to optimize $t$-SNE, while UMAP relies on negative sampling, another contrastive method. We find the precise relationship between these two contrastive methods, and provide a mathematical characterization of the distortion introduced by negative sampling. Visually, this distortion results in UMAP generating more compact embeddings with tighter clusters compared to $t$-SNE. We exploit this new conceptual connection to propose and implement a generalization of negative sampling, allowing us to interpolate between (and even extrapolate beyond) $t$-SNE and UMAP and their respective embeddings. Moving along this spectrum of embeddings leads to a trade-off between discrete / local and continuous / global structures, mitigating the risk of over-interpreting ostensible features of any single embedding. We provide a PyTorch implementation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1294. <h><a href=https://openreview.net/pdf?id=zS9sRyaPFlJ>PD-MORL: Preference-Driven Multi-Objective Reinforcement Learning Algorithm</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Toygun Basaklar, Suat Gumussoy, Umit Ogras</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zS9sRyaPFlJ>6.0 - [3, 5, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel approach that obtains a single policy network optimizing multiple objectives using multi-objective reinforcement learning on challenging continuous control tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zS9sRyaPFlJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multi-objective reinforcement learning (MORL) approaches have emerged to tackle many real-world problems with multiple conflicting objectives by maximizing a joint objective function weighted by a preference vector. These approaches find fixed customized policies corresponding to preference vectors specified during training. However, the design constraints and objectives typically change dynamically in real-life scenarios. Furthermore, storing a policy for each potential preference is not scalable. Hence, obtaining a set of Pareto front solutions for the entire preference space in a given domain with a single training is critical. To this end, we propose a novel MORL algorithm that trains a single universal network to cover the entire preference space scalable to continuous robotic tasks. The proposed approach, Preference-Driven MORL (PD-MORL), utilizes the preferences as guidance to update the network parameters. It also employs a novel parallelization approach to increase sample efficiency. We show that PD-MORL achieves up to 25% larger hypervolume for challenging continuous control tasks and uses an order of magnitude fewer trainable parameters compared to prior approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1295. <h><a href=https://openreview.net/pdf?id=Ha2MnQM9Ph>Causal Estimation for Text Data with (Apparent) Overlap Violations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lin Gui, Victor Veitch</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Ha2MnQM9Ph>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Ha2MnQM9Ph.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Consider the problem of estimating the causal effect of some attribute of a text document; for example: what effect does writing a polite vs. rude email have on response time? To estimate a causal effect from observational data, we need to adjust for confounding aspects of the text that affect both the treatment and outcome---e.g., the topic or writing level of the text. These confounding aspects are unknown a priori, so it seems natural to adjust for the entirety of the text (e.g., using a transformer). However, causal identification and estimation procedures rely on the assumption of overlap: for all levels of the adjustment variables, there is randomness leftover so that every unit could have (not) received treatment. Since the treatment here is itself an attribute of the text, it is perfectly determined, and overlap is apparently violated. The purpose of this paper is to show how to handle causal identification and obtain robust causal estimation in the presence of apparent overlap violations. In brief, the idea is to use supervised representation learning to produce a data representation that preserves confounding information while eliminating information that is only predictive of the treatment. This representation then suffices for adjustment and satisfies overlap. Adapting results on non-parametric estimation, we show that this procedure shows robustness with respect to conditional outcome misestimation and yields a low-bias estimator that admits valid uncertainty quantification under weak conditions. Empirical results show reductions in bias and strong improvements in uncertainty quantification relative to the natural (transformer-based) baseline.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1296. <h><a href=https://openreview.net/pdf?id=DBMttEEoLbw>Understanding new tasks through the lens of training data via exponential tilting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Subha Maity, Mikhail Yurochkin, Moulinath Banerjee, Yuekai Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DBMttEEoLbw>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DBMttEEoLbw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deploying machine learning models on new tasks is a major challenge due to differences in distributions of the train (source) data and the new (target) data. However, the training data likely captures some of the properties of the new task.  We consider the problem of reweighing the training samples to gain insights into the distribution of the target task. Specifically, we formulate a distribution shift model based on the exponential tilt assumption and learn train data importance weights minimizing the KL divergence between labeled train and unlabeled target datasets. The learned train data weights can then be used for downstream tasks such as target performance evaluation, fine-tuning, and model selection. We demonstrate the efficacy of our method on Waterbirds and Breeds benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1297. <h><a href=https://openreview.net/pdf?id=ETKGuby0hcs>Discovering Latent Knowledge in Language Models Without Supervision</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Collin Burns, Haotian Ye, Dan Klein, Jacob Steinhardt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ETKGuby0hcs>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ETKGuby0hcs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can&#39;t detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don&#39;t have access to explicit ground truth labels.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1298. <h><a href=https://openreview.net/pdf?id=H0gdPxSwkPb>Diffusion Adversarial Representation Learning for Self-supervised Vessel Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Boah Kim, Yujin Oh, Jong Chul Ye</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=H0gdPxSwkPb>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/H0gdPxSwkPb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Vessel segmentation in medical images is one of the important tasks in the diagnosis of vascular diseases and therapy planning. Although learning-based segmentation approaches have been extensively studied, a large amount of ground-truth labels are required in supervised methods and confusing background structures make neural networks hard to segment vessels in an unsupervised manner. To address this, here we introduce a novel diffusion adversarial representation learning (DARL) model that leverages a denoising diffusion probabilistic model with adversarial learning, and apply it to vessel segmentation. In particular, for self-supervised vessel segmentation, DARL learns the background signal using a diffusion module, which lets a generation module effectively provide vessel representations. Also, by adversarial learning based on the proposed switchable spatially-adaptive denormalization, our model estimates synthetic fake vessel images as well as vessel segmentation masks, which further makes the model capture vessel-relevant semantic information. Once the proposed model is trained, the model generates segmentation masks in a single step and can be applied to general vascular structure segmentation of coronary angiography and retinal images. Experimental results on various datasets show that our method significantly outperforms existing unsupervised and self-supervised vessel segmentation methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1299. <h><a href=https://openreview.net/pdf?id=8WTAh0tj2jC>Agent-based Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Karolis Martinkus, Pál András Papp, Benedikt Schesch, Roger Wattenhofer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8WTAh0tj2jC>6.0 - [5, 6, 5, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a new agent-based sublinear and expressive GNN architecture for graph-level tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8WTAh0tj2jC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a novel graph neural network we call AgentNet, which is designed specifically for graph-level tasks. AgentNet is inspired by sublinear algorithms, featuring a computational complexity that is independent of the graph size. The architecture of AgentNet differs fundamentally from the architectures of traditional graph neural networks. In AgentNet, some trained \textit{neural agents} intelligently walk the graph, and then collectively decide on the output. We provide an extensive theoretical analysis of AgentNet: We show that the agents can learn to systematically explore their neighborhood and that AgentNet can distinguish some structures that are even indistinguishable by 2-WL. Moreover, AgentNet is able to separate any two graphs which are sufficiently different in terms of subgraphs. We confirm these theoretical results with synthetic experiments on hard-to-distinguish graphs and real-world graph classification tasks. In both cases, we compare favorably not only to standard GNNs but also to computationally more expensive GNN extensions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1300. <h><a href=https://openreview.net/pdf?id=s1KljJpAukm>PowerQuant: Automorphism Search for Non-Uniform Quantization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Edouard YVINEC, Arnaud Dapogny, Matthieu Cord, Kevin Bailly</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=s1KljJpAukm>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/s1KljJpAukm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks (DNNs) are nowadays ubiquitous in many domains such as computer vision. However, due to their high latency, the deployment of DNNs hinges on the development of compression techniques such as quantization which consists in lowering the number of bits used to encode the weights and activations. Growing concerns for privacy and security have motivated the development of data-free techniques, at the expanse of accuracy. In this paper, we identity the uniformity of the quantization operator as a limitation of existing approaches, and propose a data-free non-uniform method. More specifically, we argue that to be readily usable without dedicated hardware and implementation, non-uniform quantization shall not change the nature of the mathematical operations performed by the DNN. This leads to search among the continuous automorphisms of $(\mathbb{R}_+^*,\times)$, which boils down to the power functions defined by their exponent. To find this parameter, we propose to optimize the reconstruction error of each layer: in particular, we show that this procedure is locally convex and admits a unique solution. At inference time, we show that our approach, dubbed PowerQuant, only require simple modifications in the quantized DNN activation functions. As such, with only negligible overhead, it significantly outperforms existing methods in a variety of configurations.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1301. <h><a href=https://openreview.net/pdf?id=UrEwJebCxk>Benign Overfitting in Classification: Provably Counter Label Noise with Larger Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaiyue Wen, Jiaye Teng, Jingzhao Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UrEwJebCxk>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Provide a theoretical analysis for the phenomenon that ResNet model overfits benignly on Cifar10 but not benignly on ImageNet
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UrEwJebCxk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Studies on benign overfitting provide insights for the success of overparameterized deep learning models. In this work, we examine whether overfitting is truly benign in real-world classification tasks. We start with the observation that a ResNet model overfits benignly on Cifar10 but not benignly on ImageNet. To understand why benign overfitting fails in the ImageNet experiment, we theoretically analyze benign overfitting under a more restrictive setup where the number of parameters is not significantly larger than the number of data points. Under this mild overparameterization setup, our analysis identifies a phase change: unlike in the previous heavy overparameterization settings, benign overfitting can now fail in the presence of label noise. Our analysis explains our empirical observations, and is validated by a set of control experiments with ResNets. Our work highlights the importance of understanding implicit bias in underfitting regimes as a future direction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1302. <h><a href=https://openreview.net/pdf?id=-vKlt84fHs>Towards Lightweight, Model-Agnostic and Diversity-Aware Active Anomaly Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xu Zhang, Yuan Zhao, Ziang Cui, Liqun Li, Shilin He, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-vKlt84fHs>6.0 - [6, 5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-vKlt84fHs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Active Anomaly Discovery (AAD) is flourishing in the anomaly detection research area, which aims to incorporate analysts’ feedback into unsupervised anomaly detectors. However, existing AAD approaches usually prioritize the samples with the highest anomaly scores for user labeling, which hinders the exploration of anomalies that were initially ranked lower. Besides, most existing AAD approaches are specially tailored for a certain unsupervised detector, making it difficult to extend to other detection models. To tackle these problems, we propose a lightweight, model-agnostic and diversity-aware AAD method, named LMADA. In LMADA, we design a diversity-aware sample selector powered by Determinantal Point Process (DPP). It considers the diversity of samples in addition to their anomaly scores for feedback querying. Furthermore, we propose a model-agnostic tuner. It approximates diverse unsupervised detectors with a unified proxy model, based on which the feedback information is incorporated by a lightweight non-linear representation adjuster. Through extensive experiments on 8 public datasets, LMADA achieved 74% F1-Score improvement on average, outperforming other comparative AAD approaches. Besides, LMADA can also achieve significant performance boosting under any unsupervised detectors.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1303. <h><a href=https://openreview.net/pdf?id=fxC7kJYwA_a>New Insights for the Stability-Plasticity Dilemma in Online Continual Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dahuin Jung, Dongjin Lee, Sunwon Hong, Hyemi Jang, Ho Bae, Sungroh Yoon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fxC7kJYwA_a>6.0 - [6, 5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel online continual learning framework that utilizes multi-scale feature maps in addition to a structure-wise distillation loss and a stability-plasticity normalization module to maintain high stability and plasticity simultaneously.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fxC7kJYwA_a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The aim of continual learning is to learn new tasks continuously (i.e., plasticity) without forgetting previously learned knowledge from old tasks (i.e., stability). In the scenario of online continual learning, wherein data comes strictly in a streaming manner, the plasticity of online continual learning is more vulnerable than offline continual learning because the training signal that can be obtained from a single data point is limited. To overcome the stability-plasticity dilemma in online continual learning, we propose an online continual learning framework named multi-scale feature adaptation network (MuFAN) that utilizes a richer context encoding extracted from different levels of a pre-trained network. Additionally, we introduce a novel structure-wise distillation loss and replace the commonly used batch normalization layer with a newly proposed stability-plasticity normalization module to train MuFAN that simultaneously maintains high plasticity and stability. MuFAN outperforms other state-of-the-art continual learning methods on the SVHN, CIFAR100, miniImageNet, and CORe50 datasets. Extensive experiments and ablation studies validate the significance and scalability of each proposed component: 1) multi-scale feature maps from a pre-trained encoder, 2) the structure-wise distillation loss, and 3) the stability-plasticity normalization module in MuFAN. Code is publicly available at https://github.com/whitesnowdrop/MuFAN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1304. <h><a href=https://openreview.net/pdf?id=dfDv0WU853R>In-sample Actor Critic for Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongchang Zhang, Yixiu Mao, Boyuan Wang, Shuncheng He, Yi Xu, Xiangyang Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dfDv0WU853R>6.0 - [5, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dfDv0WU853R.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning suffers from out-of-distribution issue and extrapolation error. Most methods penalize the out-of-distribution state-action pairs or regularize the trained policy towards the behavior policy but cannot guarantee to get rid of  extrapolation  error. We propose In-sample Actor Critic (IAC) which utilizes sampling-importance resampling to execute in-sample policy evaluation. IAC only uses the target Q-values of the actions in the dataset to evaluate the trained policy, thus avoiding extrapolation  error. The proposed method performs unbiased policy evaluation and has a lower variance than importance sampling in many cases. Empirical results show that IAC obtains competitive performance compared to the state-of-the-art methods on Gym-MuJoCo locomotion domains and much more challenging AntMaze domains.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1305. <h><a href=https://openreview.net/pdf?id=OjDkC57x5sz>Blurring Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Emiel Hoogeboom, Tim Salimans</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OjDkC57x5sz>6.0 - [8, 6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise, bridging the gap between inverse heat dissipation and denoising diffusion
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OjDkC57x5sz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, Rissanen et al., (2022) have presented a new type of diffusion process for generative modeling based on heat dissipation, or blurring, as an alternative to isotropic Gaussian diffusion. Here, we show that blurring can equivalently be defined through a Gaussian diffusion process with non-isotropic noise. In making this connection, we bridge the gap between inverse heat dissipation and denoising diffusion, and we shed light on the inductive bias that results from this modeling choice. Finally, we propose a generalized class of diffusion models that offers the best of both standard Gaussian denoising diffusion and inverse heat dissipation, which we call Blurring Diffusion Models. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1306. <h><a href=https://openreview.net/pdf?id=xQAjSr64PTc>EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yifu Yuan, Jianye HAO, Fei Ni, Yao Mu, YAN ZHENG, Yujing Hu, Jinyi Liu, Yingfeng Chen, Changjie Fan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xQAjSr64PTc>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel model-fused paradigm for Unsupervised Reinforcement Learning to jointly pre-train the dynamics model and unsupervised exploration policy in the pre-training phase, thus improving the downstream task sampling efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xQAjSr64PTc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised reinforcement learning (URL) poses a promising paradigm to learn useful behaviors in a task-agnostic environment without the guidance of extrinsic rewards to facilitate the fast adaptation of various downstream tasks. Previous works focused on the pre-training in a model-free manner while lacking the study of transition dynamics modeling that leaves a large space for the improvement of sample efficiency in downstream tasks. To this end, we propose an Efficient Unsupervised Reinforcement Learning Framework with Multi-choice Dynamics model (EUCLID), which introduces a novel model-fused paradigm to jointly pre-train the dynamics model and unsupervised exploration policy in the pre-training phase, thus better leveraging the environmental samples and improving the downstream task sampling efficiency. However, constructing a generalizable model which captures the local dynamics under different behaviors remains a challenging problem. We introduce the multi-choice dynamics model that covers different local dynamics under different behaviors concurrently, which uses different heads to learn the state transition under different behaviors during unsupervised pre-training and selects the most appropriate head for prediction in the downstream task. Experimental results in the manipulation and locomotion domains demonstrate that EUCLID achieves state-of-the-art performance with high sample efficiency, basically solving the state-based URLB benchmark and reaching a mean normalized score of 104.0±1.2% in downstream tasks with 100k fine-tuning steps, which is equivalent to DDPG’s performance at 2M interactive steps with 20× more data. More visualization videos are released on our homepage.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1307. <h><a href=https://openreview.net/pdf?id=F0KTk2plQzO>Accelerating Guided Diffusion Sampling with Splitting Numerical Methods</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Suttisak Wizadwongsa, Supasorn Suwajanakorn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=F0KTk2plQzO>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We accelerate guided diffusion sampling using splitting numerical methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/F0KTk2plQzO.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Guided diffusion is a technique for conditioning the output of a diffusion model at sampling time without retraining the network for each specific task. However, one drawback of diffusion models, whether they are guided or unguided, is their slow sampling process. 
Recent techniques can accelerate unguided sampling by applying high-order numerical methods to the sampling process when viewed as differential equations. On the contrary, we discover that the same techniques do not work for guided sampling, and little has been explored about its acceleration. This paper explores the culprit of this problem and provides a solution based on operator splitting methods, motivated by our key finding that classical high-order numerical methods are unsuitable for the conditional function. Our proposed method can re-utilize the high-order methods for guided sampling and can generate images with the same quality as a 250-step DDIM baseline using 32-58% less sampling time on ImageNet256. 
We also demonstrate usage on a wide variety of conditional generation tasks, such as text-to-image generation, colorization, inpainting, and super-resolution.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1308. <h><a href=https://openreview.net/pdf?id=b0UksKFcTOL>Latent State Marginalization as a Low-cost Approach for Improving Exploration</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dinghuai Zhang, Aaron Courville, Yoshua Bengio, Qinqing Zheng, Amy Zhang, Ricky T. Q. Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=b0UksKFcTOL>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show how to efficiently marginalize latent variable policies for MaxEnt RL to enable better exploration and more robust training at very minimal additional cost.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/b0UksKFcTOL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While the maximum entropy (MaxEnt) reinforcement learning (RL) framework -- often touted for its exploration and robustness capabilities -- is usually motivated from a probabilistic perspective, the use of deep probabilistic models have not gained much traction in practice due to their inherent complexity. In this work, we propose the adoption of latent variable policies within the MaxEnt framework, which we can provably approximate any policy distribution, and additionally, naturally emerges under the use of world models with a latent belief state. 
We discuss why latent variable policies are difficult to train, how naive approaches can fail, and subsequently introduce a series of improvements centered around low-cost marginalization of the latent state, allowing us to make full use of the latent state at minimal additional cost. We instantiate our method under the actor-critic framework, marginalizing both the actor and critic. The resulting algorithm, referred to as Stochastic Marginal Actor-Critic (SMAC), is simple yet effective. We experimentally validate our method on continuous control tasks, showing that effective marginalization can lead to better exploration and more robust training. Our implementation is open sourced at https://github.com/zdhNarsil/Stochastic-Marginal-Actor-Critic.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1309. <h><a href=https://openreview.net/pdf?id=kjkdzBW3b8p>Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tom Zahavy, Yannick Schroecker, Feryal Behbahani, Kate Baumli, Sebastian Flennerhag, Shaobo Hou, Satinder Singh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kjkdzBW3b8p>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kjkdzBW3b8p.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this work we propose a Reinforcement Learning (RL) agent that can discover complex behaviours in a rich environment with a simple reward function. We define diversity in terms of state-action occupancy measures, since policies with different occupancy measures visit different states on average. More importantly, defining diversity in this way allows us to derive an intrinsic reward function for maximizing the diversity directly. Our agent, DOMiNO, stands for Diversity Optimization Maintaining Near Optimally. It is based on maximizing a reward function with two components: the extrinsic reward and the diversity intrinsic reward, which are combined with Lagrange multipliers to balance the quality-diversity trade-off. Any RL algorithm can be used to maximize this reward and no other changes are needed. We demonstrate that given a simple reward functions in various control domains, like height (stand) and forward velocity (walk), DOMiNO discovers diverse and meaningful behaviours. We also perform extensive analysis of our approach, compare it with other multi-objective baselines, demonstrate that we can control both the quality and the diversity of the set via interpretable hyperparameters, and show that the set is robust to perturbations of the environment.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1310. <h><a href=https://openreview.net/pdf?id=KB1sc5pNKFv>On the Feasibility of Cross-Task Transfer with Model-Based Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yifan Xu, Nicklas Hansen, Zirui Wang, Yung-Chieh Chan, Hao Su, Zhuowen Tu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KB1sc5pNKFv>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We investigate the feasibility of pretraining and cross-task transfer in model-based RL, and improve sample-efficiency substantially over baselines on the Atari100k benchmark.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KB1sc5pNKFv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reinforcement Learning (RL) algorithms can solve challenging control problems directly from image observations, but they often require millions of environment interactions to do so. Recently, model-based RL algorithms have greatly improved sample-efficiency by concurrently learning an internal model of the world, and supplementing real environment interactions with imagined rollouts for policy improvement. However, learning an effective model of the world from scratch is challenging, and in stark contrast to humans that rely heavily on world understanding and visual cues for learning new skills. In this work, we investigate whether internal models learned by modern model-based RL algorithms can be leveraged to solve new, distinctly different tasks faster. We propose Model-Based Cross-Task Transfer (XTRA), a framework for sample-efficient online RL with scalable pretraining and finetuning of learned world models. By offline multi-task pretraining and online cross-task finetuning, we achieve substantial improvements over a baseline trained from scratch; we improve mean performance of model-based algorithm EfficientZero by 23%, and by as much as 71% in some instances. Project page: https://nicklashansen.github.io/xtra
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1311. <h><a href=https://openreview.net/pdf?id=jh1nCir1R3d>SWIFT: Rapid Decentralized Federated Learning via Wait-Free Model Communication</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marco Bornstein, Tahseen Rabbani, Evan Z Wang, Amrit Bedi, Furong Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jh1nCir1R3d>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a wait-free decentralized Federated Learning algorithm which achieves SOTA results while massively reducing communications costs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jh1nCir1R3d.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The decentralized Federated Learning (FL) setting avoids the role of a potentially unreliable or untrustworthy central host by utilizing groups of clients to collaboratively train a model via localized training and model/gradient sharing. Most existing decentralized FL algorithms require synchronization of client models where the speed of synchronization depends upon the slowest client. In this work, we propose SWIFT: a novel wait-free decentralized FL algorithm that allows clients to conduct training at their own speed. Theoretically, we prove that SWIFT matches the gold-standard iteration convergence rate $\mathcal{O}(1/\sqrt{T})$ of parallel stochastic gradient descent for convex and non-convex smooth optimization (total iterations $T$). Furthermore, we provide theoretical results for IID and non-IID settings without any bounded-delay assumption for slow clients which is required by other asynchronous decentralized FL algorithms. Although SWIFT achieves the same iteration convergence rate with respect to $T$ as other state-of-the-art (SOTA) parallel stochastic algorithms, it converges faster with respect to runtime due to its wait-free structure. Our experimental results demonstrate that SWIFT&#39;s runtime is reduced due to a large reduction in communication time per epoch, which falls by an order of magnitude compared to synchronous counterparts. Furthermore, SWIFT produces loss levels for image classification, over IID and non-IID data settings, upwards of 50\% faster than existing SOTA algorithms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1312. <h><a href=https://openreview.net/pdf?id=OM7doLjQbOQ>ILA-DA: Improving Transferability of Intermediate Level Attack with Data Augmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chiu Wai Yan, Tsz-Him Cheung, Dit-Yan Yeung</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OM7doLjQbOQ>6.0 - [5, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed ILA-DA, a method that employs 3 novel augmentation techniques to improve the transferability of adversarial attacks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OM7doLjQbOQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Adversarial attack aims to generate deceptive inputs to fool a machine learning model. In deep learning, an adversarial input created for a specific neural network can also trick other neural networks. This intriguing property is known as black-box transferability of adversarial examples. To improve black-box transferability, a previously proposed method called Intermediate Level Attack (ILA) fine-tunes an adversarial example by maximizing its perturbation on an intermediate layer of the source model. Meanwhile, it has been shown that simple image transformations can also enhance attack transferability. Based on these two observations, we propose ILA-DA, which employs three novel augmentation techniques to enhance ILA. Specifically, we propose (1) an automated way to apply effective image transformations, (2) an efficient reverse adversarial update technique, and (3) an attack interpolation method to create more transferable adversarial examples. Shown by extensive experiments, ILA-DA greatly outperforms ILA and other state-of-the-art attacks by a large margin. On ImageNet, we attain an average attack success rate of 84.5%, which is 19.5% better than ILA and 4.7% better than the previous state-of-the-art across nine undefended models. For defended models, ILA-DA also leads existing attacks and provides further gains when incorporated into more advanced attack methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1313. <h><a href=https://openreview.net/pdf?id=JHklpEZqduQ>Non-parametric Outlier Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Leitian Tao, Xuefeng Du, Jerry Zhu, Yixuan Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JHklpEZqduQ>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JHklpEZqduQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Out-of-distribution (OOD) detection is indispensable for safely deploying machine learning models in the wild. One of the key challenges is that models lack supervision signals from unknown data, and as a result, can produce overconfident predictions on OOD data. Recent work on outlier synthesis modeled the feature space as parametric Gaussian distribution, a strong and restrictive assumption that might not hold in reality. In this paper, we propose a novel framework, non-parametric outlier synthesis (NPOS), which generates artificial OOD training data and facilitates learning a reliable decision boundary between ID and OOD data. Importantly, our proposed synthesis approach does not make any distributional assumption on the ID embeddings, thereby offering strong flexibility and generality. We show that our synthesis approach can be mathematically interpreted as a rejection sampling framework. Extensive experiments show that NPOS can achieve superior OOD detection performance, outperforming the competitive rivals by a significant margin. Code is publicly available at https://github.com/deeplearning-wisc/npos.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1314. <h><a href=https://openreview.net/pdf?id=-t4D61w4zvQ>Temporal Coherent Test Time Optimization for Robust Video Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenyu Yi, SIYUAN YANG, Yufei Wang, Haoliang Li, Yap-peng Tan, Alex Kot</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-t4D61w4zvQ>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-t4D61w4zvQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks are likely to fail when the test data is corrupted in real-world deployment (e.g., blur, weather, etc.). Test-time optimization is an effective way that adapts models to generalize to corrupted data during testing, which has been shown in the image domain. However, the techniques for improving video classification corruption robustness remain few. In this work, we propose a Temporal Coherent Test-time Optimization framework (TeCo) to utilize spatio-temporal information in test-time optimization for robust video classification. To exploit information in video with self-supervised learning, TeCo minimizes the entropy of the prediction based on the global content from video clips. Meanwhile, it also feeds local content to regularize the temporal coherence at the feature level. TeCo retains the generalization ability of various video classification models and achieves significant improvements in corruption robustness across Mini Kinetics-C and Mini SSV2-C. Furthermore, TeCo sets a new baseline in video classification corruption robustness via test-time optimization. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1315. <h><a href=https://openreview.net/pdf?id=3yJ-hcJBqe>Adaptive Robust Evidential Optimization For Open Set Detection from Imbalanced Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hitesh Sapkota, Qi Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3yJ-hcJBqe>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose adaptive robust uncertainty mass quantification for effective open set detection from imbalanced data. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3yJ-hcJBqe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Open set detection (OSD) aims at identifying data samples of an unknown class ($i.e.$, open set) from those of known classes ($i.e.$, closed set) based on a model trained from closed set samples. However, a closed set may involve a highly imbalanced class distribution. Accurately differentiating open set samples and those from a minority class in the closed set poses a fundamental challenge as the model may be equally uncertain when recognizing samples from the minority class. In this paper, we propose Adaptive Robust Evidential Optimization (AREO) that offers a principled way to quantify sample uncertainty through evidential learning while optimally balancing the model training over all classes in the closed set through adaptive distributively robust optimization (DRO). To avoid the model to primarily focus on the most difficult samples by following the standard DRO, adaptive DRO training is performed, which is governed by a novel multi-scheduler learning mechanism to ensure an optimal model training behavior that gives sufficient attention to the difficult samples and the minority class while capable of learning common patterns from the majority classes. Our experimental results on multiple real-world datasets demonstrate that the proposed model outputs uncertainty scores that can clearly separate samples from closed and open sets, respectively, and the detection results outperform the competitive baselines. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1316. <h><a href=https://openreview.net/pdf?id=RiTjKoscnNd>Equivariant Hypergraph Diffusion Neural Operators</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, Pan Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=RiTjKoscnNd>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this work, we are inspired by hypergraph diffusion algorithms and design a novel HNN architecture that holds provable expressiveness while keeping efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/RiTjKoscnNd.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Hypergraph neural networks (HNNs) using neural networks to encode hypergraphs provide a promising way to model higher-order relations in data and further solve relevant prediction tasks built upon such higher-order relations. However, higher-order relations in practice contain complex patterns and are often highly irregular. So, it is often challenging to design an HNN that suffices to express those relations while keeping computational efficiency. Inspired by hypergraph diffusion algorithms, this work proposes a new HNN architecture named ED-HNN, which provably approximates any continuous equivariant hypergraph diffusion operators that can model a wide range of higher-order relations. ED-HNN can be implemented efficiently by combining star expansions of hypergraphs with standard message passing neural networks. ED-HNN further shows great superiority in processing heterophilic hypergraphs and constructing deep models. We evaluate ED-HNN for node classification on nine real-world hypergraph datasets. ED-HNN uniformly outperforms the best baselines over these nine datasets and achieves more than 2%$\uparrow$ in prediction accuracy over four datasets therein. Our code is available at: https://github.com/Graph-COM/ED-HNN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1317. <h><a href=https://openreview.net/pdf?id=4fZc_79Lrqs>ACMP: Allen-Cahn Message Passing with Attractive and Repulsive Forces for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuelin Wang, Kai Yi, Xinliang Liu, Yu Guang Wang, Shi Jin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4fZc_79Lrqs>6.0 - [6, 6, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4fZc_79Lrqs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural message passing is a basic feature extraction unit for graph-structured data considering neighboring node features in network propagation from one layer to the next. We model such process by an interacting particle system with attractive and repulsive forces and the Allen-Cahn force arising in the modeling of phase transition. The dynamics of the system is a reaction-diffusion process which can separate particles without blowing up. This induces an Allen-Cahn message passing (ACMP) for graph neural networks where the numerical iteration for the particle system solution constitutes the message passing propagation. ACMP which has a simple implementation with a neural ODE solver can propel the network depth up to one hundred of layers with theoretically proven strictly positive lower bound of the Dirichlet energy. It thus provides a deep model of GNNs circumventing the common GNN problem of oversmoothing. GNNs with ACMP achieve state of the art performance for real-world node classification tasks on both homophilic and heterophilic datasets. Codes are available at https://github.com/ykiiiiii/ACMP
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1318. <h><a href=https://openreview.net/pdf?id=2WklawyeI08>Hebbian and Gradient-based Plasticity Enables Robust Memory and Rapid Learning in RNNs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yu Duan, Zhongfan Jia, Qian Li, Yi Zhong, Kaisheng Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2WklawyeI08>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2WklawyeI08.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Rapidly learning from ongoing experiences and remembering past events with a flexible memory system are two core capacities of biological intelligence. While the underlying neural mechanisms are not fully understood, various evidence supports that synaptic plasticity plays a critical role in memory formation and fast learning. Inspired by these results, we equip Recurrent Neural Networks (RNNs) with plasticity rules to enable them to adapt their parameters according to ongoing experiences. In addition to the traditional local Hebbian plasticity, we propose a global, gradient-based plasticity rule, which allows the model to evolve towards its self-determined target. Our models show promising results on sequential and associative memory tasks, illustrating their ability to robustly form and retain memories. In the meantime, these models can cope with many challenging few-shot learning problems. Comparing different plasticity rules under the same framework shows that Hebbian plasticity is well-suited for several memory and associative learning tasks; however, it is outperformed by gradient-based plasticity on few-shot regression tasks which require the model to infer the underlying mapping.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1319. <h><a href=https://openreview.net/pdf?id=ndYXTEL6cZz>Extremely Simple Activation Shaping for Out-of-Distribution Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, Rosanne Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ndYXTEL6cZz>6.0 - [3, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop an extremely simple, post hoc, on-the-fly, and plug-and-play activation shaping method for out-of-distribution detection.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ndYXTEL6cZz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The separation between training and deployment of machine learning models implies that not all scenarios encountered in deployment can be anticipated during training, and therefore relying solely on advancements in training has its limits. Out-of-distribution (OOD) detection is an important area that stress-tests a model’s ability to handle unseen situations: Do models know when they don’t know? Existing OOD detection methods either incur extra training steps, additional data or make nontrivial modifications to the trained network. In contrast, in this work, we propose an extremely simple, post-hoc, on-the-fly activation shaping method, ASH, where a large portion (e.g. 90%) of a sample’s activation at a late layer is removed, and the rest (e.g. 10%) simplified or lightly adjusted. The shaping is applied at inference time, and does not require any statistics calculated from training data. Experiments show that such a simple treatment enhances in-distribution and out- of-distribution sample distinction so as to allow state-of-the-art OOD detection on ImageNet, and does not noticeably deteriorate the in-distribution accuracy. Video, animation and code can be found at: https://andrijazz.github.io/ash.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1320. <h><a href=https://openreview.net/pdf?id=yLzLfM-Esnu>Constructive TT-representation of the tensors given as index interaction functions with applications</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gleb Ryzhakov, Ivan Oseledets</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yLzLfM-Esnu>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method to build tensor train representation for a wide class of tensors for which an analytical dependence on the indices is given.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yLzLfM-Esnu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents a method to build explicit tensor-train (TT) representations. We show that a wide class of tensors can be explicitly represented with sparse TT-cores, obtaining, in many cases, optimal TT-ranks. Numerical experiments show that our method outperforms the existing ones in several practical applications, including game theory problems. Theoretical estimations of the number of operations show that in some problems, such as permanent calculation, our methods are close to the known optimal asymptotics, which are obtained by a completely different type of methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1321. <h><a href=https://openreview.net/pdf?id=H0HGljkxQFN>MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan, Yukun Zhu, Alan Yuille, Hartwig Adam, Liang-Chieh Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=H0HGljkxQFN>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/H0HGljkxQFN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1% / 81.5% top-1 accuracy on ImageNet-1K / ImageNet-1K-V2 with ImageNet-22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2% AP$^{\text{box}}$ with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale inference). Finally, the tiny-MOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. The tiny-MOAT family is also benchmarked on downstream tasks, serving as a baseline for the community. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is publicly available.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1322. <h><a href=https://openreview.net/pdf?id=DpE5UYUQzZH>A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two-Player Zero-Sum Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samuel Sokota, Ryan D&#39;Orazio, J Zico Kolter, Nicolas Loizou, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, Christian Kroer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DpE5UYUQzZH>6.0 - [3, 8, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A single algorithm for both single-agent reinforcement learning and approximating quantal response and Nash equilibria in two-player zero-sum games.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DpE5UYUQzZH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work studies an algorithm, which we call magnetic mirror descent, that is inspired by mirror descent and the non-Euclidean proximal gradient algorithm. Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games. These virtues include: 1) Being the first quantal response equilibria solver to achieve linear convergence for extensive-form games with first order feedback; 2) Being the first standard reinforcement learning algorithm to achieve empirically competitive results with CFR in tabular settings; 3) Achieving favorable performance in 3x3 Dark Hex and Phantom Tic-Tac-Toe as a self-play deep reinforcement learning algorithm.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1323. <h><a href=https://openreview.net/pdf?id=cJWxqmmDL2b>Memorization-Dilation: Modeling Neural Collapse Under Noise</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Duc Anh Nguyen, Ron Levie, Julian Lienen, Eyke Hüllermeier, Gitta Kutyniok</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cJWxqmmDL2b>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cJWxqmmDL2b.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The notion of neural collapse refers to several emergent phenomena that have been empirically observed across various canonical classification problems.  During the terminal phase of training a deep neural network, the feature embedding of all examples of the same class tend to collapse to a single representation, and the features of different classes tend to separate as much as possible. Neural collapse is often studied through a simplified model, called the layer-peeled model, in which the network is assumed to have ``infinite expressivity&#39;&#39; and can map each data point to any arbitrary representation. In this work we study a more realistic variant of the layer-peeled model, which takes the positivity of the features into account. Furthermore, we extend this model to also incorporate the limited expressivity of the network. Empirical evidence suggests that the memorization of noisy data points leads to a degradation (dilation) of the neural collapse. Using a model of the memorization-dilation (M-D) phenomenon, we show one mechanism by which different losses lead to different performances of the trained network on noisy data. Our proofs reveal why label smoothing, a modification of cross-entropy empirically observed to produce a regularization effect, leads to improved generalization in classification tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1324. <h><a href=https://openreview.net/pdf?id=P5Z-Zl9XJ7>Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hehe Fan, Zhangyang Wang, Yi Yang, Mohan Kankanhalli</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=P5Z-Zl9XJ7>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes a Continuous-Discrete Convolution (CDConv) for the (3+1)D geometry-sequence strutuere modeling in proteins.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/P5Z-Zl9XJ7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The structure of proteins involves 3D geometry of amino acid coordinates and 1D sequence of peptide chains. The 3D structure exhibits irregularity because amino acids are distributed unevenly in Euclidean space and their coordinates are continuous variables. In contrast, the 1D structure is regular because amino acids are arranged uniformly in the chains and their sequential positions (orders) are discrete variables. Moreover, geometric coordinates and sequential orders are in two types of spaces and their units of length are incompatible. These inconsistencies make it challenging to capture the 3D and 1D structures while avoiding the impact of sequence and geometry modeling on each other. This paper proposes a Continuous-Discrete Convolution (CDConv) that uses irregular and regular approaches to model the geometry and sequence structures, respectively. Specifically, CDConv employs independent learnable weights for different regular sequential displacements but directly encodes geometric displacements due to their irregularity. In this way, CDConv significantly improves protein modeling by reducing the impact of geometric irregularity on sequence modeling. Extensive experiments on a range of tasks, including protein fold classification, enzyme reaction classification, gene ontology term prediction and enzyme commission number prediction, demonstrate the effectiveness of the proposed CDConv. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1325. <h><a href=https://openreview.net/pdf?id=OTbRTIY4YS>Global Explainability of GNNs via Logic Combination of Learned Concepts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Steve Azzolin, Antonio Longa, Pietro Barbiero, Pietro Lio, Andrea Passerini</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OTbRTIY4YS>6.0 - [5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose GLGExplainer, the first Global Explainer for GNNs capable of generating explanations as arbitrary Boolean combinations of graphical concepts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OTbRTIY4YS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While instance-level explanation of GNN is a well-studied problem with plenty of approaches being developed, providing a global explanation for the behaviour of a GNN is much less explored, despite its potential in interpretability and debugging. Existing solutions either simply list local explanations for a given class, or generate a synthetic prototypical graph with maximal score for a given class, completely missing any combinatorial aspect that the GNN could have learned.
In this work, we propose GLGExplainer (Global Logic-based GNN Explainer), the first Global Explainer capable of generating explanations as arbitrary Boolean combinations of learned graphical concepts. GLGExplainer is a fully differentiable architecture that takes local explanations as inputs and combines them into a logic formula over graphical concepts, represented as clusters of local explanations. 
Contrary to existing solutions, GLGExplainer provides accurate and human-interpretable global explanations that are perfectly aligned with ground-truth explanations (on synthetic data) or match existing domain knowledge (on real-world data). Extracted formulas are faithful to the model predictions, to the point of providing insights into some occasionally incorrect rules learned by the model, making GLGExplainer a promising diagnostic tool for learned GNNs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1326. <h><a href=https://openreview.net/pdf?id=mX56bKDybu5>Neural Radiance Field Codebooks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthew Wallingford, Aditya Kusupati, Alex Fang, Vivek Ramanujan, Aniruddha Kembhavi, Roozbeh Mottaghi, Ali Farhadi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mX56bKDybu5>6.0 - [6, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Learning geometrically-aware, object-centric representations through elastic bottlenecks and a differentiable renderer for downstream tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mX56bKDybu5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1\% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (.101 ARI). Finally, we show that NRC improves on the task of depth ordering by 5.5% accuracy in THOR.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1327. <h><a href=https://openreview.net/pdf?id=MnEjsw-vj-X>Active Learning for Object Detection with Evidential Deep Learning and Hierarchical Uncertainty Aggregation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Younghyun Park, Wonjeong Choi, Soyeong Kim, Dong-Jun Han, Jaekyun Moon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MnEjsw-vj-X>6.0 - [6, 6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an active learning method for object detection using evidential deep learning and novel uncertainty aggregation method.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MnEjsw-vj-X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the huge success of object detection, the training process still requires an immense amount of labeled data. Although various active learning solutions for object detection have been proposed, most existing works do not take advantage of epistemic uncertainty, which is an important metric for capturing the usefulness of the sample. Also, previous works pay little attention to the attributes of each bounding box (e.g., nearest object, box size) when computing the informativeness of an image. In this paper, we propose a new active learning strategy for object detection that overcomes the shortcomings of prior works. To make use of epistemic uncertainty, we adopt evidential deep learning (EDL) and propose a new module termed model evidence head (MEH), that makes EDL highly compatible with object detection. Based on the computed epistemic uncertainty of each bounding box, we propose hierarchical uncertainty aggregation (HUA) for obtaining the informativeness of an image. HUA realigns all bounding boxes into multiple levels based on the attributes and aggregates uncertainties in a bottom-up order, to effectively capture the context within the image. Experimental results show that our method outperforms existing state-of-the-art methods by a considerable margin.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1328. <h><a href=https://openreview.net/pdf?id=dSYoPjM5J_W>Revisiting Graph Adversarial Attack and Defense From a Data Distribution Perspective</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kuan Li, Yang Liu, Xiang Ao, Qing He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dSYoPjM5J_W>6.0 - [5, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We revisit graph adversarial attack and defense from a data distribution perspective.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dSYoPjM5J_W.jpg loading="lazy">
                </div>
                <div class="abstract">
                     Recent studies have shown that structural perturbations are significantly effective in degrading the accuracy of Graph Neural Networks (GNNs) in the semi-supervised node classification (SSNC) task. However, why the gradient-based methods are so destructive is rarely explored. In this work, we discover an interesting phenomenon: the adversarial edges are not uniformly distributed on the graph. Nearly all perturbations are generated around the training nodes in poisoning attack. Combined with this phenomenon, we provide an explanation for the effectiveness of the gradient-based attack method from a data distribution perspective and revisit both poisoning attack and evasion attack in SSNC. From this new perspective, we empirically and theoretically discuss some other attack tendencies. Based on the analysis, we provide nine practical tips on both attack and defense and meanwhile leverage them to improve existing attack and defense methods. Moreover, we design a fast attack method and a self-training defense method, which outperform the state-of-the-art methods and can effectively scale to large graphs like ogbn-arxiv. We conduct extensive experiments on four benchmark datasets to verify our claims.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1329. <h><a href=https://openreview.net/pdf?id=04K3PMtMckp>The hidden uniform cluster prior in self-supervised learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mido Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Nicolas Ballas</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=04K3PMtMckp>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Many common self-supervised learning frameworks provably impose a hidden uniform prior, which is detrimental when pretraining with real-world class-imbalanced data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/04K3PMtMckp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A successful paradigm in representation learning is to perform self-supervised pretraining using tasks based on mini-batch statistics; (e.g., SimCLR, VICReg, SwAV, MSN). We show that in the formulation of all these methods is an overlooked prior to learn features that enable uniform clustering of the data. While this prior has led to remarkably semantic representations when pretraining on class-balanced data, such as ImageNet, we demonstrate that it can hamper performance when pretraining on class-imbalanced data. By moving away from conventional uniformity priors and instead preferring power-law distributed feature clusters, we show that one can improve the quality of the learned representations on real-world class-imbalanced datasets. To demonstrate this, we develop an extension of the Masked Siamese Networks (MSN) method to support the use of arbitrary features priors.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1330. <h><a href=https://openreview.net/pdf?id=CW6KmU5wPh>DAVA: Disentangling Adversarial Variational Autoencoder</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Benjamin Estermann, Roger Wattenhofer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CW6KmU5wPh>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an adversarial variational auto-encoder that alleviates the issue of hyperparameter selection for disentanglement learning and propose a new unsupervised disentanglement metric.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CW6KmU5wPh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The use of well-disentangled representations offers many advantages for downstream tasks, e.g. an increased sample efficiency, or better interpretability.
However, the quality of disentangled interpretations is often highly dependent on the choice of dataset-specific hyperparameters, in particular the regularization strength.
To address this issue, we introduce DAVA, a novel training procedure for variational auto-encoders. DAVA completely alleviates the problem of hyperparameter selection.
We compare DAVA to models with optimal hyperparameters.
Without any hyperparameter tuning, DAVA is competitive on a diverse range of commonly used datasets.
Underlying DAVA, we discover a necessary condition for unsupervised disentanglement, which we call PIPE.
We demonstrate the ability of PIPE to positively predict the performance of downstream models in abstract reasoning.
We also thoroughly investigate correlations with existing supervised and unsupervised metrics. The code is available at https://github.com/besterma/dava.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1331. <h><a href=https://openreview.net/pdf?id=iUYpN14qjTF>Understanding the Generalization of Adam in Learning Neural Networks with Proper Regularization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Difan Zou, Yuan Cao, Yuanzhi Li, Quanquan Gu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iUYpN14qjTF>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iUYpN14qjTF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Adaptive gradient methods such as Adam have gained increasing popularity in deep learning optimization. However, it has been observed in many deep learning applications such as image classification, Adam can converge to a different solution with a worse test error compared to (stochastic) gradient descent, even with a fine-tuned regularization.  In this paper, we provide a theoretical explanation for this phenomenon: we show that in the nonconvex setting of learning over-parameterized two-layer convolutional neural networks starting from the same random initialization, for a class of data distributions (inspired from image data), Adam and gradient descent (GD) can converge to different global solutions of the training objective with provably different generalization errors, even with weight decay regularization. In contrast, we show that if the training objective is convex, and the weight decay regularization is employed, any optimization algorithms including Adam and GD will converge to the same solution if the training is successful. This suggests that the generalization gap between Adam and SGD in the presence of weight decay regularization is closely tied to the nonconvex landscape of deep learning optimization, which cannot be covered by the recent neural tangent kernel (NTK) based analysis. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1332. <h><a href=https://openreview.net/pdf?id=47B_ctC4pJ>Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yoonjeon Kim, Hyunsu Kim, Junho Kim, Yunjey Choi, Eunho Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=47B_ctC4pJ>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/47B_ctC4pJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With the advantages of fast inference and human-friendly flexible manipulation, image-agnostic style manipulation via text guidance enables new applications that were not previously available. The state-of-the-art text-guided image-agnostic manipulation method embeds the representation of each channel of StyleGAN independently in the Contrastive Language-Image Pre-training (CLIP) space, and provides it in the form of a Dictionary to quickly find out the channel-wise manipulation direction during inference time. However, in this paper we argue that this dictionary which is constructed by controlling single channel individually is limited to accommodate the versatility of text guidance since the collective and interactive relation among multiple channels are not considered. Indeed, we show that it fails to discover a large portion of manipulation directions that can be found by existing methods, which manually manipulates latent space without texts. To alleviate this issue, we propose a novel method that learns a Dictionary, whose entry corresponds to the representation of a single channel, by taking into account the manipulation effect coming from the interaction with multiple other channels. We demonstrate that our strategy resolves the inability of previous methods in finding diverse known directions from unsupervised methods and unknown directions from random text while maintaining the real-time inference speed and disentanglement ability.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1333. <h><a href=https://openreview.net/pdf?id=KkazG4lgKL>Out-of-Distribution Detection based on In-Distribution Data Patterns Memorization with Modern Hopfield Energy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jinsong Zhang, Qiang Fu, Xu Chen, Lun Du, Zelin Li, Gang Wang, xiaoguang Liu, Shi Han, Dongmei Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KkazG4lgKL>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel out-of-distribution detection method motivated by  Modern Hopfield Energy, and futhur derive a simplified version that is effective, efficient and hyperparameter-free.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KkazG4lgKL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Out-of-Distribution (OOD) detection is essential for safety-critical applications of deep neural networks. OOD detection is challenging since DNN models may produce very high logits value even for OOD samples. Hence, it is of great difficulty to discriminate OOD data by directly adopting Softmax on output logits as the confidence score. Differently, we detect the OOD sample with Hopfield energy in a store-then-compare paradigm. In more detail, penultimate layer outputs on the training set are considered as the representations of in-distribution (ID) data. Thus they can be transformed into stored patterns that serve as anchors to measure the discrepancy of unseen data for OOD detection. Starting from the energy function defined in Modern Hopfield Network for the discrepancy score calculation, we derive a simplified version SHE with theoretical analysis. In SHE, we utilize only one stored pattern to present each class, and these patterns can be obtained by simply averaging the penultimate layer outputs of training samples within this class. SHE has the advantages of hyperparameterfree
and high computational efficiency. The evaluations of nine widely-used OOD datasets show the promising performance of such a simple yet effective approach and its superiority over State-of-the-Art models. Code is available at https://github.com/zjs975584714/SHE ood detection.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1334. <h><a href=https://openreview.net/pdf?id=kkpL4zUXtiw>Bi-level Physics-Informed Neural Networks for PDE Constrained Optimization using Broyden&#39;s Hypergradients</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhongkai Hao, Chengyang Ying, Hang Su, Jun Zhu, Jian Song, Ze Cheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kkpL4zUXtiw>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kkpL4zUXtiw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning based approaches like Physics-informed neural networks (PINNs) and DeepONets have shown promise on solving PDE constrained optimization (PDECO) problems. 
However, existing methods are insufficient to handle those PDE constraints that have a complicated or nonlinear dependency on optimization targets. In this paper, we present a novel bi-level optimization framework to resolve the challenge by decoupling the optimization of the targets and constraints. For the inner loop optimization, we adopt PINNs to solve the PDE constraints only. For the outer loop, we design a novel method by using Broyden&#39;s method based on the Implicit Function Theorem (IFT), which is efficient and accurate for approximating hypergradients. We further present theoretical explanations and error analysis of the hypergradients computation. Extensive experiments on multiple large-scale and nonlinear PDE constrained optimization problems demonstrate that our method achieves state-of-the-art results compared with strong baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1335. <h><a href=https://openreview.net/pdf?id=KE_wJD2RK4>Modeling Multimodal Aleatoric Uncertainty in Segmentation with Mixture of Stochastic Experts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhitong Gao, Yucong Chen, Chuyu Zhang, Xuming He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=KE_wJD2RK4>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel mixture of stochastic experts (MoSE) model training with a Wasserstein-like loss, which produces an efficient two-level representation for the multi-modal aleatoric uncertainty in semantic segmentation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/KE_wJD2RK4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Equipping predicted segmentation with calibrated uncertainty is essential for safety-critical applications. In this work, we focus on capturing the data-inherent uncertainty (aka aleatoric uncertainty) in segmentation, typically when ambiguities exist in input images. Due to the high-dimensional output space and potential multiple modes in segmenting ambiguous images, it remains challenging to predict well-calibrated uncertainty for segmentation. To tackle this problem, we propose a novel mixture of stochastic experts (MoSE) model, where each expert network estimates a distinct mode of the aleatoric uncertainty and a gating network predicts the probabilities of an input image being segmented in those modes. This yields an efficient two-level uncertainty representation. To learn the model, we develop a Wasserstein-like loss that directly minimizes the distribution distance between the MoSE and ground truth annotations. The loss can easily integrate traditional segmentation quality measures and be efficiently optimized via constraint relaxation. We validate our method on the LIDC-IDRI dataset and a modified multimodal Cityscapes dataset. Results demonstrate that our method achieves the state-of-the-art or competitive performance on all metrics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1336. <h><a href=https://openreview.net/pdf?id=sXfWoK4KvSW>Long-Tailed Partial Label Learning via Dynamic Rebalancing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Feng Hong, Jiangchao Yao, Zhihan Zhou, Ya Zhang, Yanfeng Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sXfWoK4KvSW>6.0 - [5, 5, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel method RECORDS for long-tailed partial label learning, which overcomes the drawback of the straightforward combination between long-tailed learning and partial label learning, and significantly improves the performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sXfWoK4KvSW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Real-world data usually couples the label ambiguity and heavy imbalance, challenging the algorithmic robustness of partial label learning (PLL) and long-tailed learning (LT). The straightforward combination of LT and PLL, i.e., LT-PLL, suffers from a fundamental dilemma: LT methods build upon a given class distribution that is unavailable in PLL, and the performance of PLL is severely influenced in long-tailed context. We show that even with the auxiliary of an oracle class prior, the state-of-the-art methods underperform due to an adverse fact that the constant rebalancing in LT is harsh to the label disambiguation in PLL. To overcome this challenge, we thus propose a dynamic rebalancing method, termed as RECORDS, without assuming any prior knowledge about the class distribution. Based on a parametric decomposition of the biased output, our method constructs a dynamic adjustment that is benign to the label disambiguation process and theoretically converges to the oracle class prior. Extensive experiments on three benchmark datasets demonstrate the significant gain of RECORDS compared with a range of baselines. The code is publicly available.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1337. <h><a href=https://openreview.net/pdf?id=1tHAZRqftM>Multi-task Self-supervised Graph Neural Networks Enable Stronger Task Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingxuan Ju, Tong Zhao, Qianlong Wen, Wenhao Yu, Neil Shah, Yanfang Ye, Chuxu Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1tHAZRqftM>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present ParetoGNN, a novel multi-task self-supervised learning framework for graph neural networks, that enhances the task generalization across various downstream tasks and datasets.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1tHAZRqftM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-supervised learning (SSL) for graph neural networks (GNNs) has attracted increasing attention from the graph machine learning community in recent years, owing to its capability to learn performant node embeddings without costly label information. One weakness of conventional SSL frameworks for GNNs is that they learn through a single philosophy, such as mutual information maximization or generative reconstruction. When applied to various downstream tasks, these frameworks rarely perform equally well for every task, because one philosophy may not span the extensive knowledge required for all tasks. To enhance the task generalization across tasks, as an important first step forward in exploring fundamental graph models, we introduce PARETOGNN, a multi-task SSL framework for node representation learning over graphs. Specifically, PARETOGNN is self-supervised by manifold pretext tasks observing multiple philosophies. To reconcile different philosophies, we explore a multiple-gradient descent algorithm, such that PARETOGNN actively learns from every pretext task while minimizing potential conflicts. We conduct comprehensive experiments over four downstream tasks (i.e., node classification, node clustering, link prediction, and partition prediction), and our proposal achieves the best overall performance across tasks on 11 widely adopted benchmark datasets. Besides, we observe that learning from multiple philosophies enhances not only the task generalization but also the single task performances, demonstrating that PARETOGNN achieves better task generalization via the disjoint yet complementary knowledge learned from different philosophies. Our code is publicly available at https://github.com/jumxglhf/ParetoGNN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1338. <h><a href=https://openreview.net/pdf?id=TPiwkItUSu>Behind the Scenes of Gradient Descent: A Trajectory Analysis via Basis Function Decomposition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianhao Ma, Lingjun Guo, Salar Fattahi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=TPiwkItUSu>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/TPiwkItUSu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This work analyzes the solution trajectory of gradient-based algorithms via a novel basis function decomposition. We show that, although solution trajectories of gradient-based algorithms may vary depending on the learning task, they behave almost monotonically when projected onto an appropriate orthonormal function basis. Such projection gives rise to a basis function decomposition of the solution trajectory. Theoretically, we use our proposed basis function decomposition to establish the convergence of gradient descent (GD) on several representative learning tasks. In particular, we improve the convergence of GD on symmetric matrix factorization and provide a completely new convergence result for the orthogonal symmetric tensor decomposition. Empirically, we illustrate the promise of our proposed framework on realistic deep neural networks (DNNs) across different architectures, gradient-based solvers, and datasets. Our key finding is that gradient-based algorithms monotonically learn the coefficients of a particular orthonormal function basis of DNNs defined as the eigenvectors of the conjugate kernel after training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1339. <h><a href=https://openreview.net/pdf?id=iYC5hOMqUg>Bayesian Oracle for bounding information gain in neural encoding models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Konstantin-Klemens Lurz, Mohammad Bashiri, Edgar Y. Walker, Fabian H. Sinz</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iYC5hOMqUg>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a method to obtain upper bounds of information gain in order to evaluate neural encoding models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iYC5hOMqUg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In recent years, deep learning models have set new standards in predicting neural population responses. Most of these models currently focus on predicting the mean response of each neuron for a given input. However, neural variability around this mean is not just noise and plays a central role in several theories on neural computation. To capture this variability, we need models that predict full response distributions for a given stimulus. However, to measure the quality of such models, commonly used correlation-based metrics are not sufficient as they mainly care about the mean of the response distribution. An interpretable alternative evaluation metric for likelihood-based models is \textit{Information Gain} (IG) which evaluates the likelihood of a model relative to a lower and upper bound. However, while a lower bound is usually easy to obtain, constructing an upper bound turns out to be challenging for neural recordings with relatively low numbers of repeated trials, high (shared) variability, and sparse responses. In this work, we generalize the jack-knife oracle estimator for the mean---commonly used for correlation metrics---to a flexible Bayesian oracle estimator for IG based on posterior predictive distributions. We describe and address the challenges that arise when estimating the lower and upper bounds from small datasets. We then show that our upper bound estimate is data-efficient and robust even in the case of sparse responses and low signal-to-noise ratio. We further provide the derivation of the upper bound estimator for a variety of common distributions including the state-of-the-art zero-inflated mixture models, and relate IG to common mean-based metrics. Finally, we use our approach to evaluate such a mixture model resulting in $90\%$ IG performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1340. <h><a href=https://openreview.net/pdf?id=J3Y7cgZOOS>Anti-Symmetric DGN: a stable architecture for Deep Graph Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alessio Gravina, Davide Bacciu, Claudio Gallicchio</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=J3Y7cgZOOS>6.0 - [8, 6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/J3Y7cgZOOS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep Graph Networks (DGNs) currently dominate the research landscape of learning from graphs, due to their efficiency and ability to implement an adaptive message-passing scheme between the nodes. However, DGNs are typically limited in their ability to propagate and preserve long-term dependencies between nodes, i.e., they suffer from the over-squashing phenomena. As a result, 
we can expect them to under-perform, since different problems require to capture interactions at different (and possibly large) radii in order to be effectively solved. In this work, we present Anti-Symmetric Deep Graph Networks (A-DGNs), a framework for stable and non-dissipative DGN design, conceived through the lens of ordinary differential equations. We give theoretical proof that our method is stable and non-dissipative, leading to two key results: long-range information between nodes is preserved, and no gradient vanishing or explosion occurs in training. We empirically validate the proposed approach on several graph benchmarks, showing that A-DGN yields to improved performance and enables to learn effectively even when dozens of layers are used.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1341. <h><a href=https://openreview.net/pdf?id=hxUwnEGxW87>Statistical Theory of Differentially Private Marginal-based Data Synthesis Algorithms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ximing Li, Chendi Wang, Guang Cheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hxUwnEGxW87>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We analyze the differentially private marginal-based data synthesis algorithms in a statistical framework and establish a theoretical guarantee for the accuracy and utility.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hxUwnEGxW87.jpg loading="lazy">
                </div>
                <div class="abstract">
                     Marginal-based methods achieve promising performance in the synthetic data competition hosted by the National Institute of Standards and Technology (NIST).
 To deal with high-dimensional data, the distribution of synthetic data is represented by a probabilistic graphical model (e.g., a Bayesian network), while the raw data distribution is approximated by a collection of low-dimensional marginals.
 Differential privacy (DP) is guaranteed by introducing random noise to each low-dimensional marginal distribution.
 Despite its promising performance in practice, the statistical properties of marginal-based methods are rarely studied in the literature.
 In this paper, we study DP data synthesis algorithms based on Bayesian networks (BN) from a statistical perspective. We establish a rigorous accuracy guarantee for BN-based algorithms, where the errors are measured by the total variation (TV) distance or the $L^2$ distance. 
 Related to downstream machine learning tasks, an upper bound for the utility error of the DP synthetic data is also derived. To complete the picture, we establish a lower bound for TV accuracy that holds for every $\epsilon$-DP synthetic data generator.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1342. <h><a href=https://openreview.net/pdf?id=jbIYfq4Tr->On the Robustness of Safe Reinforcement Learning under Observational Perturbations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, Ding Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jbIYfq4Tr->6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the robustness of safe RL under observational perturbations, and propose two effective adversaries and a defense algorithm to increase the agent&#39;s safety under attacks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jbIYfq4Tr-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Safe reinforcement learning (RL) trains a policy to maximize the task reward while satisfying safety constraints. While prior works focus on the performance optimality, we find that the optimal solutions of many safe RL problems are not robust and safe against carefully designed observational perturbations. We formally analyze the unique properties of designing effective observational adversarial attackers in the safe RL setting.  We show that baseline adversarial attack techniques for standard RL tasks are not always effective for safe RL and propose two new approaches - one maximizes the cost and the other maximizes the reward.  One interesting and counter-intuitive finding is that the maximum reward attack is strong, as it can both induce unsafe behaviors and make the attack stealthy by maintaining the reward. We further propose a robust training framework for safe RL and evaluate it via comprehensive experiments. This paper provides a pioneer work to investigate the safety and robustness of RL under observational attacks for future safe RL studies. Code is available at: \url{https://github.com/liuzuxin/safe-rl-robustness}
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1343. <h><a href=https://openreview.net/pdf?id=y4uc4NtTWaq>Data augmentation alone can improve adversarial training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lin Li, Michael W. Spratling</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=y4uc4NtTWaq>6.0 - [6, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> data augmentation alone can significantly improve adversarial training regarding both accuracy and robustness
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/y4uc4NtTWaq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Adversarial training suffers from the issue of robust overfitting, which seriously impairs its generalization performance. Data augmentation, which is effective at preventing overfitting in standard training, has been observed by many previous works to be ineffective in mitigating overfitting in adversarial training. This work proves that, contrary to previous findings, data augmentation alone can significantly boost accuracy and robustness in adversarial training. We find that the hardness and the diversity of data augmentation are important factors in combating robust overfitting. In general, diversity can improve both accuracy and robustness, while hardness can boost robustness at the cost of accuracy within a certain limit and degrade them both over that limit. To mitigate robust overfitting, we first propose a new crop transformation Cropshift with improved  diversity compared to the conventional one (Padcrop). We then propose a new data augmentation scheme, based on Cropshift, with much improved diversity and well-balanced hardness. Empirically, our augmentation method achieves the state-of-the-art accuracy and robustness for data augmentations in adversarial training. Furthermore, it matches, or even exceeds when combined with weight averaging, the performance of the best contemporary regularization methods for alleviating robust overfitting.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1344. <h><a href=https://openreview.net/pdf?id=Oc2vlWU0jFY>Reversible Column Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuxuan Cai, Yizhuang Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Li, Xiangyu Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Oc2vlWU0jFY>6.0 - [6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Oc2vlWU0jFY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a new neural network design paradigm Reversible Column Network (RevCol). The main body of RevCol is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes RevCol very different behavior from conventional networks: during forward propagation, features in RevCol are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that CNN-style RevCol models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after ImageNet-22K pre-training, RevCol-XL obtains 88.2% ImageNet-1K accuracy. Given more pre-training data, our largest model RevCol-H reaches 90.0% on ImageNet-1K, 63.8% AP$_{box}$ on COCO detection minival set, 61.0% mIoU on ADE20k segmentation. To our knowledge, it is the best COCO detection and ADE20k segmentation result among pure (static) CNN models. Moreover, as a general macro architecture fashion, RevCol can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and NLP tasks. 
We release code and models at https://github.com/megvii-research/RevCol
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1345. <h><a href=https://openreview.net/pdf?id=XWkWK2UagFR>General Neural Gauge Fields</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fangneng Zhan, Lingjie Liu, Adam Kortylewski, Christian Theobalt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XWkWK2UagFR>5.8 - [6, 6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper explores how to learn the gauge transformations along with radiance fields.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XWkWK2UagFR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The recent advance of neural fields, such as neural radiance fields, has significantly pushed the boundary of scene representation learning. Aiming to boost the computation efﬁciency and rendering quality of 3D scenes, a popular line of research maps the 3D coordinate system to another measuring system, e.g., 2D manifolds and hash tables, for modeling neural fields. The conversion of coordinate systems can be typically dubbed as \emph{gauge transformation}, which is usually a pre-defined mapping function, e.g., orthogonal projection or spatial hash function. This begs a question: can we directly learn a desired gauge transformation along with the neural field in an end-to-end manner? In this work, we extend this problem to a general paradigm with a taxonomy of discrete and continuous cases, and develop an end-to-end learning framework to jointly optimize the gauge transformation and neural fields. To counter the problem that the learning of gauge transformations can collapse easily, we derive a general regularization mechanism from the principle of information conservation during the gauge transformation. To circumvent the high computation cost in gauge learning with regularization, we directly derive an information-invariant gauge transformation which allows to preserve scene information inherently and yield superior performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1346. <h><a href=https://openreview.net/pdf?id=gUZWOE42l6Q>Out-of-distribution Representation Learning for Time Series Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, Xing Xie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gUZWOE42l6Q>5.8 - [6, 5, 5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel perspective on time series classification and present algorithms and theory to solve it, with solid experiments.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gUZWOE42l6Q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Time series classification is an important problem in the real world. Due to its non-stationary property that the distribution changes over time, it remains challenging to build models for generalization to unseen distributions. In this paper, we propose to view time series classification from the distribution perspective. We argue that the temporal complexity of a time series dataset could attribute to unknown latent distributions that need characterize. To this end, we propose DIVERSIFY for out-of-distribution (OOD) representation learning on dynamic distributions of times series. DIVERSIFY takes an iterative process: it first obtains the ‘worst-case’ latent distribution scenario via adversarial training, then reduces the gap between these latent distributions. We then show that such an algorithm is theoretically supported. Extensive experiments are conducted on seven datasets with different OOD settings across gesture recognition, speech commands recognition, wearable stress and affect detection, and sensor-based human activity recognition. Qualitative and quantitative results demonstrate that DIVERSIFY significantly outperforms other baselines and effectively characterizes the latent distributions. Code is available at https://github.com/microsoft/robustlearn.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1347. <h><a href=https://openreview.net/pdf?id=_4n3k3d1ob>Continuous-time identification of dynamic state-space models by deep subspace encoding</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gerben I. Beintema, Maarten Schoukens, Roland Tóth</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_4n3k3d1ob>5.8 - [5, 6, 5, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work proposes a method for the estimation of continuous-time nonlinear state-pace models parameterized by ANN&#39;s which is robust and well theoretically motivated.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_4n3k3d1ob.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1348. <h><a href=https://openreview.net/pdf?id=iP77_axu0h3> BEEF: Bi-Compatible Class-Incremental Learning via Energy-Based Expansion and Fusion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fu-Yun Wang, Da-Wei Zhou, Liu Liu, Han-Jia Ye, Yatao Bian, De-Chuan Zhan, Peilin Zhao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=iP77_axu0h3>5.8 - [6, 6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A unifying energy-based theory and framework called 3EF to analyze and achieve the goal of class-incremental learning. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/iP77_axu0h3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks suffer from catastrophic forgetting when sequentially learning tasks phase-by-phase, making them inapplicable in dynamically updated systems. Class-incremental learning (CIL) aims to enable neural networks to learn different categories at multi-stages. Recently, dynamic-structure-based CIL methods achieve remarkable performance. However, these methods train all modules in a coupled manner and do not consider possible conflicts among modules, resulting in spoilage of eventual predictions. In this work, we propose a unifying energy-based theory and framework called Bi-Compatible Energy-Based Expansion and Fusion (BEEF) to analyze and achieve the goal of CIL. We demonstrate the possibility of training independent modules in a decoupled manner while achieving bi-directional compatibility among modules through two additionally allocated prototypes, and then integrating them into a unifying classifier with minimal cost. Furthermore, BEEF extends the exemplar-set to a more challenging setting, where exemplars are randomly selected and imbalanced, and maintains its performance when prior methods fail dramatically.
Extensive experiments on three widely used benchmarks: CIFAR-100, ImageNet-100, and ImageNet-1000 demonstrate that BEEF achieves state-of-the-art performance in both the ordinary and challenging  CIL settings. The Code is available at https://github.com/G-U-N/ICLR23-BEEF.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1349. <h><a href=https://openreview.net/pdf?id=1UbNwQC89a>RGI: robust GAN-inversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shancong Mou, Xiaoyi Gu, Meng Cao, Haoping Bai, Ping Huang, Jiulong Shan, Jianjun Shi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=1UbNwQC89a>5.8 - [6, 6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/1UbNwQC89a.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generative adversarial networks (GANs), trained on a large-scale image dataset, can be a good approximator of the natural image manifold. GAN-inversion, using a pre-trained generator as a deep generative prior, is a promising tool for image restoration under corruptions. However, the performance of GAN-inversion can be limited by a lack of robustness to unknown gross corruptions, i.e., the restored image might easily deviate from the ground truth. In this paper, we propose a Robust GAN-inversion (RGI) method with a provable robustness guarantee to achieve image restoration under unknown \textit{gross} corruptions, where a small fraction of pixels are completely corrupted. Under mild assumptions, we show that the restored image and the identified corrupted region mask converge asymptotically to the ground truth. Moreover, we extend RGI to Relaxed-RGI (R-RGI) for generator fine-tuning to mitigate the gap between the GAN learned manifold and the true image manifold while avoiding trivial overfitting to the corrupted input image, which further improves the image restoration and corrupted region mask identification performance. The proposed RGI/R-RGI method unifies two important applications with state-of-the-art (SOTA) performance: (i) mask-free semantic inpainting, where the corruptions are unknown missing regions, the restored background can be used to restore the missing content. (ii) unsupervised pixel-wise anomaly detection, where the corruptions are unknown anomalous regions, the retrieved mask can be used as the anomalous region’s segmentation mask.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1350. <h><a href=https://openreview.net/pdf?id=A9WQaxYsfx>Panning for Gold in Federated Learning: Targeted Text Extraction under Arbitrarily Large-Scale Aggregation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hong-Min Chu, Jonas Geiping, Liam H Fowl, Micah Goldblum, Tom Goldstein</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=A9WQaxYsfx>5.8 - [6, 6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a method that extracts target sequences by keywords under extremely large-scale aggregation in federated learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/A9WQaxYsfx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    As federated learning (FL) matures, privacy attacks against FL systems in turn become more numerous and complex. Attacks on language models have progressed from recovering single sentences in simple classification tasks to recovering larger parts of user data. Current attacks against federated language models are sequence-agnostic and aim to extract as much data as possible from an FL update - often at the expense of fidelity for any particular sequence. Because of this, current attacks fail to extract any meaningful data under large-scale aggregation. In realistic settings, an attacker cares most about a small portion of user data that contains sensitive personal information, for example sequences containing the phrase &#34;my credit card number is ...&#34;. In this work, we propose the first attack on FL that achieves targeted extraction of sequences that contain privacy-critical phrases, whereby we employ maliciously modified parameters to allow the transformer itself to filter relevant sequences from aggregated user data and encode them in the gradient update. Our attack can effectively extract sequences of interest even against extremely large-scale aggregation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1351. <h><a href=https://openreview.net/pdf?id=3VFQfAG3vwi>Variational Latent Branching Model for Off-Policy Evaluation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qitong Gao, Ge Gao, Min Chi, Miroslav Pajic</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3VFQfAG3vwi>5.75 - [6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3VFQfAG3vwi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Model-based methods have recently shown great potential for off-policy evaluation (OPE); offline trajectories induced by behavioral policies are fitted to transitions of Markov decision processes (MDPs), which are used to rollout simulated trajectories and estimate the performance of policies. Model-based OPE methods face two key challenges. First, as offline trajectories are usually fixed, they tend to cover limited state and action space. Second, the performance of model-based methods can be sensitive to the initialization of their parameters. In this work, we propose the variational latent branching model (VLBM) to learn the transition function of MDPs by formulating the environmental dynamics as a compact latent space, from which the next states and rewards are then sampled. Specifically, VLBM leverages and extends the variational inference framework with the recurrent state alignment (RSA), which is designed to capture as much information underlying the limited training data, by smoothing out the information flow between the variational (encoding) and generative (decoding) part of VLBM. Moreover, we also introduce the branching architecture to improve the model’s robustness against randomly initialized model weights. The effectiveness of the VLBM is evaluated on the deep OPE (DOPE) benchmark, from which the training trajectories are designed to result in varied coverage of the state-action space. We show that the VLBM outperforms existing state-of-the-art OPE methods in general.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1352. <h><a href=https://openreview.net/pdf?id=rdfgqiwz7lZ>A Learning Based Hypothesis Test for Harmful Covariate Shift</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tom Ginsberg, Zhongyuan Liang, Rahul G Krishnan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rdfgqiwz7lZ>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the Detectron, a method based that detects covariate shift using an ensemble of classifiers trained to disagree with each other on unlabeled samples.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rdfgqiwz7lZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to quickly and accurately identify covariate shift at test time is a critical and often overlooked component of safe machine learning systems deployed in high-risk domains. While methods exist for detecting when predictions should not be made on out-of-distribution test examples, identifying distributional level differences between training and test time can help determine when a model should be removed from the deployment setting and retrained. In this work, we define harmful covariate shift (HCS) as a change in distribution that may weaken the generalization of a predictive model. To detect HCS, we use the discordance between an ensemble of classifiers trained to agree on training data and disagree on test data. We derive a loss function for training this ensemble and show that the disagreement rate and entropy represent powerful discriminative statistics for HCS. Empirically, we demonstrate the ability of our method to detect harmful covariate shift with statistical certainty on a variety of high-dimensional datasets. Across numerous domains and modalities, we show state-of-the-art performance compared to existing methods, particularly when the number of observed test samples is small.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1353. <h><a href=https://openreview.net/pdf?id=_wSHsgrVali>Revisiting the Assumption of Latent Separability for Backdoor Defenses</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, Prateek Mittal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_wSHsgrVali>5.75 - [3, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Adaptive Backdoor Attacks against Latent Separation Based Defenses
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_wSHsgrVali.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent studies revealed that deep learning is susceptible to backdoor poisoning attacks. An adversary can embed a hidden backdoor into a model to manipulate its predictions by only modifying a few training data, without controlling the training process. Currently, a tangible signature has been widely observed across a diverse set of backdoor poisoning attacks --- models trained on a poisoned dataset tend to learn separable latent representations for poison and clean samples. This latent separation is so pervasive that a family of backdoor defenses directly take it as a default assumption (dubbed latent separability assumption), based on which to identify poison samples via cluster analysis in the latent space. An intriguing question consequently follows: is the latent separation unavoidable for backdoor poisoning attacks? This question is central to understanding whether the assumption of latent separability provides a reliable foundation for defending against backdoor poisoning attacks. In this paper, we design adaptive backdoor poisoning attacks to present counter-examples against this assumption. Our methods include two key components: (1) a set of trigger-planted samples correctly labeled to their semantic classes (other than the target class) that can regularize backdoor learning; (2) asymmetric trigger planting strategies that help to boost attack success rate (ASR) as well as to diversify latent representations of poison samples. Extensive experiments on benchmark datasets verify the effectiveness of our adaptive attacks in bypassing existing latent separation based backdoor defenses. Moreover, our attacks still maintain a high attack success rate with negligible clean accuracy drop. Our studies call for defense designers to take caution when leveraging latent separation as an assumption in their defenses. Our codes are available at https://github.com/Unispac/Circumventing-Backdoor-Defenses.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1354. <h><a href=https://openreview.net/pdf?id=sAOOeI878Ns>Characterizing intrinsic compositionality in transformers with Tree Projections</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D Manning</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sAOOeI878Ns>5.75 - [8, 6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a method to functionally project a transformer into the space of tree structured models and use it to uncover intrinsic compositionality of transformers trained on language data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sAOOeI878Ns.jpg loading="lazy">
                </div>
                <div class="abstract">
                    When trained on language data, do transformers learn some arbitrary computation that utilizes the full capacity of the architecture or do they learn a simpler, tree-like computation, hypothesized to underlie compositional meaning systems like human languages? There is an apparent tension between compositional accounts of human language understanding, which are based on a restricted bottom-up computational process, and the enormous success of neural models like transformers, which can route information arbitrarily between different parts of their input. One possibility is that these models, while extremely flexible in principle, in practice learn to interpret language hierarchically, ultimately building sentence representations close to those predictable by a bottom-up, tree-structured model. To evaluate this possibility, we describe an unsupervised and parameter-free method to \emph{functionally project} the behavior of any transformer into the space of tree-structured networks. Given an input sentence, we produce a binary tree that approximates the transformer&#39;s representation-building process and a score that captures how ``tree-like&#39;&#39; the transformer&#39;s behavior is on the input. While calculation of this score does not require training any additional models, it provably upper-bounds the fit between a transformer and any tree-structured approximation. Using this method, we show that transformers for three different tasks become more tree-like over the course of training, in some cases unsupervisedly recovering the same trees as supervised parsers. These trees, in turn, are predictive of model behavior, with more tree-like models generalizing better on tests of compositional generalization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1355. <h><a href=https://openreview.net/pdf?id=98p5x51L5af>Prompting GPT-3 To Be Reliable</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, Lijuan Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=98p5x51L5af>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We establish simple and effective prompting methods to make GPT-3 reliable in terms of: robustness, fairness, calibration, factuality. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/98p5x51L5af.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3’s reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM’s factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1356. <h><a href=https://openreview.net/pdf?id=sVU54nyaA9K>Learning Adversarial Linear Mixture Markov Decision Processes with Bandit Feedback and Unknown Transition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Canzhe Zhao, Ruofeng Yang, Baoxiang Wang, Shuai Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sVU54nyaA9K>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We make the first step to establish a provably efficient algorithm in adversarial linear mixture mdps with bandit feedback and unknown transition.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sVU54nyaA9K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study reinforcement learning (RL) with linear function approximation, unknown transition, and adversarial losses in the bandit feedback setting. Specifically, the unknown transition probability function is a linear mixture model \citep{AyoubJSWY20,ZhouGS21,HeZG22} with a given feature mapping, and the learner only observes the losses of the experienced state-action pairs instead of the whole loss function. We propose an efficient algorithm LSUOB-REPS which achieves $\widetilde{O}(dS^2\sqrt{K}+\sqrt{HSAK})$ regret guarantee with high probability, where $d$ is the ambient dimension of the feature mapping, $S$ is the size of the state space, $A$ is the size of the action space, $H$ is the episode length and $K$ is the number of episodes. Furthermore, we also prove a lower bound of order $\Omega(dH\sqrt{K}+\sqrt{HSAK})$ for this setting. To the best of our knowledge, we make the first step to establish a provably efficient algorithm with a sublinear regret guarantee in this challenging setting and solve the open problem of \citet{HeZG22}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1357. <h><a href=https://openreview.net/pdf?id=FUiDMCr_W4o>A Statistical Framework for Personalized Federated Learning and Estimation: Theory, Algorithms, and Privacy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaan Ozkara, Antonious M. Girgis, Deepesh Data, Suhas Diggavi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=FUiDMCr_W4o>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We utilize a statistical framework to enable our design of new personalized Federated Learning/Estimation algorithms with privacy guarantees.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/FUiDMCr_W4o.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A distinguishing characteristic of federated learning is that the (local) client data could have statistical heterogeneity. This heterogeneity has motivated the design of personalized learning, where individual (personalized) models are trained, through collaboration. There have been various personalization methods proposed in literature, with seemingly very different forms and methods ranging from use of a single global model for local regularization and model interpolation, to use of multiple global models for personalized clustering, etc. In this work, we begin with a statistical framework that  unifies several different algorithms as well as suggest new algorithms.  We apply our framework to personalized estimation, and connect it to the classical empirical Bayes&#39; methodology. We develop novel private personalized estimation under this framework. We then use our statistical framework to propose new personalized learning algorithms, including AdaPeD based on information-geometry regularization, which numerically outperforms several known algorithms. We develop privacy for personalized learning methods with guarantees for user-level privacy and composition. We numerically evaluate the performance as well as the privacy for both the estimation and learning problems, demonstrating the advantages of our proposed methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1358. <h><a href=https://openreview.net/pdf?id=-cqvvvb-NkI>Recitation-Augmented Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-cqvvvb-NkI>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel recitation-augmented generation framework to improve language models’ performance in the closed-book question-answering setting.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-cqvvvb-NkI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a new paradigm to help Large Language Models (LLMs) generate more accurate factual knowledge without retrieving from an external corpus, called RECITation-augmented gEneration (RECITE). Different from retrieval-augmented language models that retrieve relevant documents before generating the outputs, given an input, RECITE first recites one or several relevant passages from LLMs’ own memory via sampling, and then produces the final answers. We show that RECITE is a powerful paradigm for knowledge-intensive NLP tasks. Specifically, we show that by utilizing recitation as the intermediate step, a recite-and-answer scheme can achieve new state-of-the-art performance in various closed-book question answering (CBQA) tasks. In experiments, we verify the effectiveness of RECITE on three pre-trained models (In-house LM, UL2, and OPT) and three CBQA tasks (Natural Questions, TriviaQA, and HotpotQA). Our code is available at &#34;https://github.com/Edward-Sun/RECITE&#34;.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1359. <h><a href=https://openreview.net/pdf?id=CL-sVR9pvF>Weighted Ensemble Self-Supervised Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yangjun Ruan, Saurabh Singh, Warren Richard Morningstar, Alexander A Alemi, Sergey Ioffe, Ian Fischer, Joshua V. Dillon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CL-sVR9pvF>5.75 - [6, 8, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We efficiently ensemble SSL methods and train them with new objectives to get SOTA results on ImageNet-1K SSL evaluations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CL-sVR9pvF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Ensembling has proven to be a powerful technique for boosting model performance, uncertainty estimation, and robustness in supervised learning. Advances in self-supervised learning (SSL) enable leveraging large unlabeled corpora for state-of-the-art few-shot and supervised learning performance. In this paper, we explore how ensemble methods can improve recent SSL techniques by developing a framework that permits data-dependent weighted cross-entropy losses.  We refrain from ensembling the representation backbone; this choice yields an efficient ensemble method that incurs a small training cost and requires no architectural changes or computational overhead to downstream evaluation. The effectiveness of our method is demonstrated with two state-of-the-art SSL methods, DINO (Caron et al., 2021) and MSN (Assran et al., 2022). Our method outperforms both in multiple evaluation metrics on ImageNet-1K, particularly in the few-shot setting. We explore several weighting schemes and find that those which increase the diversity of ensemble heads lead to better downstream evaluation results. Thorough experiments yield improved prior art baselines which our method still surpasses; e.g., our overall improvement with MSN ViT-B/16 is 3.9 p.p. for 1-shot learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1360. <h><a href=https://openreview.net/pdf?id=eXkhH12DTD9>Pseudo-label Training and Model Inertia in Neural Machine Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Benjamin Hsu, Anna Currey, Xing Niu, Maria Nadejde, Georgiana Dinu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=eXkhH12DTD9>5.75 - [5, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> pseudo-label training improves model stablity to updates and input perturbations
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/eXkhH12DTD9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Like many other machine learning applications, neural machine translation (NMT) benefits from over-parameterized deep neural models. However, these models have been observed to be brittle: NMT model predictions are sensitive to small input changes and can show significant variation across re-training or incremental model updates. This work studies a frequently used method in NMT, pseudo-label training (PLT), which is common to the related techniques of forward-translation (or self-training) and sequence-level knowledge distillation. While the effect of PLT on quality is well-documented, we highlight a lesser-known effect: PLT can enhance a model&#39;s stability to model updates and input perturbations, a set of properties we call \textit{model inertia}. We study inertia effects under different training settings and we identify distribution simplification as a mechanism behind the observed results. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1361. <h><a href=https://openreview.net/pdf?id=p6jsTidUkPx>Quantile Risk Control: A Flexible Framework for Bounding the Probability of High-Loss Predictions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jake Snell, Thomas P Zollo, Zhun Deng, Toniann Pitassi, Richard Zemel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=p6jsTidUkPx>5.75 - [6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a framework to rigorously and flexible control the quantiles of the loss distribution incurred by a predictor or set of predictors.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/p6jsTidUkPx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Rigorous guarantees about the performance of predictive algorithms are necessary in order to ensure their responsible use. Previous work has largely focused on bounding the expected loss of a predictor, but this is not sufficient in many risk-sensitive applications where the distribution of errors is important. In this work, we propose a flexible framework to produce a family of bounds on quantiles of the loss distribution incurred by a predictor. Our method takes advantage of the order statistics of the observed loss values rather than relying on the sample mean alone. We show that a quantile is an informative way of quantifying predictive performance, and that our framework applies to a variety of quantile-based metrics, each targeting important subsets of the data distribution. We analyze the theoretical properties of our proposed method and demonstrate its ability to rigorously control loss quantiles on several real-world datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1362. <h><a href=https://openreview.net/pdf?id=XIzO8zr-WbM>Data Valuation Without Training of a Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ki Nohyun, Hoyong Choi, Hye Won Chung</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XIzO8zr-WbM>5.75 - [6, 6, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we define a training-free data valuation score, which can be directly computed from data and can effectively quantify the impact of individual instances in optimization and generalization of neural networks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XIzO8zr-WbM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many recent works on understanding deep learning try to quantify how much individual data instances influence the optimization and generalization of a model. Such attempts reveal characteristics and importance of individual instances, which may provide useful information in diagnosing and improving deep learning. However, most of the existing works on data valuation require actual training of a model, which often demands high-computational cost. In this paper, we provide a training-free data valuation score, called complexity-gap score, which is a data-centric score to quantify the influence of individual instances in generalization of two-layer overparameterized neural networks. The proposed score can quantify irregularity of the instances and measure how much each data instance contributes in the total movement of the network parameters during training. We theoretically analyze and empirically demonstrate the effectiveness of the complexity-gap score in finding `irregular or mislabeled&#39; data instances, and also provide applications of the score in analyzing datasets and diagnosing training dynamics. Our code is publicly available at https://github.com/JJchy/CG_score.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1363. <h><a href=https://openreview.net/pdf?id=lMO7TC7cuuh>When Data Geometry Meets Deep Function: Generalizing Offline Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jianxiong Li, Xianyuan Zhan, Haoran Xu, Xiangyu Zhu, Jingjing Liu, Ya-Qin Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lMO7TC7cuuh>5.75 - [3, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lMO7TC7cuuh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In offline reinforcement learning (RL), one detrimental issue to policy learning is the error accumulation of deep \textit{Q} function in out-of-distribution (OOD) areas. Unfortunately, existing offline RL methods are often over-conservative, inevitably hurting generalization performance outside data distribution. In our study, one interesting observation is that deep \textit{Q} functions approximate well inside the convex hull of training data. Inspired by this, we propose a new method, \textit{DOGE (Distance-sensitive Offline RL with better GEneralization)}. DOGE marries dataset geometry with deep function approximators in offline RL, and enables exploitation in generalizable OOD areas rather than strictly constraining policy within data distribution. Specifically, DOGE trains a state-conditioned distance function that can be readily plugged into standard actor-critic methods as a policy constraint. Simple yet elegant, our algorithm enjoys better generalization compared to state-of-the-art methods on D4RL benchmarks. Theoretical analysis demonstrates the superiority of our approach to existing methods that are solely based on data distribution or support constraints. Code is available at https://github.com/Facebear-ljx/DOGE.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1364. <h><a href=https://openreview.net/pdf?id=mfIX4QpsARJ>EAGLE: Large-scale Learning of Turbulent Fluid Dynamics with Mesh Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Steeven JANNY, Aurélien Bénéteau, Madiha Nadri, Julie Digne, Nicolas THOME, Christian Wolf</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mfIX4QpsARJ>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a new large-scale dataset for learning non-steady fluid mechanics and a method based on self-attention on graphs
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mfIX4QpsARJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Estimating fluid dynamics is classically done through the simulation and integration of numerical models solving the Navier-Stokes equations, which is computationally complex and time-consuming even on high-end hardware. This is a notoriously hard problem to solve, which has recently been addressed with machine learning, in particular graph neural networks (GNN) and variants trained and evaluated on datasets of static objects in static scenes with fixed geometry. We attempt to go beyond existing work in complexity and introduce a new model, method and benchmark. We propose EAGLE: a large-scale dataset of ∼1.1 million 2D meshes resulting from simulations of unsteady fluid dynamics caused by a moving flow source interacting with nonlinear scene structure of varying geometries, with 600 different scenes of three different types in total. To perform future forecasting of pressure and velocity on the challenging EAGLE dataset, we introduce a new mesh transformer. It leverages node clustering, graph pooling and global attention to learn long-range dependencies between spatially distant data points without needing a large number of iterations, as existing GNN methods do. We show that our transformer outperforms state-of-the-art performance on, both, existing synthetic and real datasets and on EAGLE. Finally, we highlight that our approach learns to attend to airflow, integrating complex information in a single iteration.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1365. <h><a href=https://openreview.net/pdf?id=j3cUWIMsFBN>Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingjie Li, Yifei Wang, Yisen Wang, Zhouchen Lin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=j3cUWIMsFBN>5.75 - [6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/j3cUWIMsFBN.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph Neural Networks (GNNs) are widely used deep learning models that can extract meaningful representations from graph datasets and achieve great success in many machine learning tasks. Among them, graph neural networks with iterative iterations like unfolded GNNs and implicit GNNs can effectively capture long-range dependencies in graphs and demonstrate superior performance on large graphs since they can mathematically ensure its convergence to some nontrivial solution after lots of aggregations. However, the aggregation time for such models costs a lot as they need to aggregate the full graph in each update. Such weakness limits the scalability of the implicit graph models. To tackle such limitations, we propose two unbiased stochastic proximal solvers inspired by the stochastic proximal gradient descent method and its variance reduction variant called USP and USP-VR solvers. From the point of stochastic optimization, we theoretically prove that our solvers are unbiased, which can converge to the same solution as the original solvers for unfolded GNNs and implicit GNNs. Furthermore, the computation complexities for unfolded GNNs and implicit GNNs with our proposed solvers are significantly less than their vanilla versions. Experiments on various large graph datasets show that our proposed solvers are more efficient and can achieve state-of-the-art performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1366. <h><a href=https://openreview.net/pdf?id=xIWfWvKM7aQ>TextShield: Beyond Successfully Detecting Adversarial Sentences in text classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lingfeng Shen, Ze Zhang, Haiyun Jiang, Ying Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xIWfWvKM7aQ>5.75 - [5, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A defense that extends adversarial detection paradigm in NLP
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xIWfWvKM7aQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Adversarial attack serves as a major challenge for neural network models in NLP, which precludes the model&#39;s deployment in safety-critical applications. A recent line of work, detection-based defense, aims to distinguish adversarial sentences from benign ones. However, {the core limitation of previous detection methods is being incapable of giving correct predictions on adversarial sentences unlike defense methods from other paradigms.} To solve this issue, this paper proposes TextShield: (1) we discover a link between text attack and saliency information, and then we propose a saliency-based detector, which can effectively detect whether an input sentence is adversarial or not. (2) We design a saliency-based corrector, which converts the detected adversary sentences to benign ones. By combining the saliency-based detector and corrector, TextShield extends the detection-only paradigm to a detection-correction paradigm, thus filling the gap in the existing detection-based defense. Comprehensive experiments show that (a) TextShield consistently achieves higher or comparable performance than state-of-the-art defense methods across various attacks on different benchmarks. (b) our saliency-based detector outperforms existing detectors for detecting adversarial sentences.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1367. <h><a href=https://openreview.net/pdf?id=C_PRLz8bEJx>DM-NeRF: 3D Scene Geometry Decomposition and Manipulation from 2D Images</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bing WANG, Lu Chen, Bo Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=C_PRLz8bEJx>5.75 - [8, 6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we study the problem of 3D scene geometry decomposition and manipulation from 2D views.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/C_PRLz8bEJx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we study the problem of 3D scene geometry decomposition and manipulation from 2D views. By leveraging the recent implicit neural representation techniques, particularly the appealing neural radiance fields, we introduce an object field component to learn unique codes for all individual objects in 3D space only from 2D supervision. The key to this component is a series of carefully designed loss functions to enable every 3D point, especially in non-occupied space, to be effectively optimized even without 3D labels. In addition, we introduce an inverse query algorithm to freely manipulate any specified 3D object shape in the learned scene representation. Notably, our manipulation algorithm can explicitly tackle key issues such as object collisions and visual occlusions. Our method, called DM-NeRF, is among the first to simultaneously reconstruct, decompose, manipulate and render complex 3D scenes in a single pipeline. Extensive experiments on three datasets clearly show that our method can accurately decompose all 3D objects from 2D views, allowing any interested object to be freely manipulated in 3D space such as translation, rotation, size adjustment, and deformation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1368. <h><a href=https://openreview.net/pdf?id=JjCAdMUlu9v>Auto-Encoding Goodness of Fit</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aaron Palmer, Zhiyi Chi, Derek Aguiar, Jinbo Bi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JjCAdMUlu9v>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JjCAdMUlu9v.jpg loading="lazy">
                </div>
                <div class="abstract">
                    For generative autoencoders to learn a meaningful latent representation for data generation, a careful balance must be achieved between reconstruction error and how close the distribution in the latent space is to the prior. However, this balance is challenging to achieve due to a lack of criteria that work both at the mini-batch (local) and aggregated posterior (global) level. In this work, we develop the Goodness of Fit Autoencoder (GoFAE), which incorporates hypothesis tests at two levels. At the mini-batch level, it uses GoF test statistics as regularization objectives. At a more global level, it selects a regularization coefficient based on higher criticism, i.e., a test on the uniformity of the local GoF p-values. We justify the use of GoF tests by providing a relaxed $L_2$-Wasserstein bound on the distance between the latent distribution and target prior. We propose to use GoF tests and prove that optimization based on these tests can be done with stochastic gradient (SGD) descent on a compact Riemannian manifold. Empirically, we show that our higher criticism parameter selection procedure balances reconstruction and generation using mutual information and uniformity of p-values respectively. Finally, we show that GoFAE achieves comparable FID scores and mean squared errors with competing deep generative models while retaining statistical indistinguishability from Gaussian in the latent space based on a variety of hypothesis tests.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1369. <h><a href=https://openreview.net/pdf?id=U2WjB9xxZ9q>3D generation on ImageNet</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ivan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian Ren, Hsin-Ying Lee, Peter Wonka, Sergey Tulyakov</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=U2WjB9xxZ9q>5.75 - [6, 8, 3, 6] - Accept: notable-top-5%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> 3D generation on ImageNet
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/U2WjB9xxZ9q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    All existing 3D-from-2D generators are designed for well-curated single-category datasets, where all the objects have (approximately) the same scale, 3D location, and orientation, and the camera always points to the center of the scene. This makes them inapplicable to diverse, in-the-wild datasets of non-alignable scenes rendered from arbitrary camera poses. In this work, we develop a 3D generator with Generic Priors (3DGP): a 3D synthesis framework with more general assumptions about the training data, and show that it scales to very challenging datasets, like ImageNet. Our model is based on three new ideas. First, we incorporate an inaccurate off-the-shelf depth estimator into 3D GAN training via a special depth adaptation module to handle the imprecision. Then, we create a flexible camera model and a regularization strategy for it to learn its distribution parameters during training. Finally, we extend the recent ideas of transferring knowledge from pretrained classifiers into GANs for patch-wise trained models by employing a simple distillation-based technique on top of the discriminator. It achieves more stable training than the existing methods and speeds up the convergence by at least 40%. We explore our model on four datasets: SDIP Dogs $256^2$, SDIP Elephants $256^2$, LSUN Horses $256^2$, and ImageNet $256^2$ and demonstrate that 3DGP outperforms the recent state-of-the-art in terms of both texture and geometry quality. Code and visualizations: https://snap-research.github.io/3dgp.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1370. <h><a href=https://openreview.net/pdf?id=lQVpasnQS62>Human MotionFormer: Transferring Human Motions with Vision Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hongyu Liu, Xintong Han, Chenbin Jin, Lihui Qian, Huawei Wei, Zhe Lin, Faqiang Wang, Haoye Dong, Yibing Song, Jia Xu, Qifeng Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lQVpasnQS62>5.75 - [6, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lQVpasnQS62.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human motion transfer aims to transfer motions from a target dynamic person to a source static one for motion synthesis. An accurate matching between the source person and the target motion in both large and subtle motion changes is vital for improving the transferred motion quality. In this paper, we propose Human MotionFormer, a hierarchical ViT framework that leverages global and local perceptions to capture large and subtle motion matching, respectively. It consists of two ViT encoders to extract input features (i.e., a target motion image and a source human image) and a ViT decoder with several cascaded blocks for feature matching and motion transfer. In each block, we set the target motion feature as Query and the source person as Key and Value, calculating the cross-attention maps to conduct a global feature matching. Further, we introduce a convolutional layer to improve the local perception after the global cross-attention computations. This matching process is implemented in both warping and generation branches to guide the motion transfer. During training, we propose a mutual learning loss to enable the co-supervision between warping and generation branches for better motion representations. Experiments show that our Human MotionFormer sets the new state-of-the-art performance both qualitatively and quantitatively. Project page: https://github.com/KumapowerLIU/Human-MotionFormer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1371. <h><a href=https://openreview.net/pdf?id=6SRDbbvU8s>Learning Multimodal Data Augmentation in Feature Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zichang Liu, Zhiqiang Tang, Xingjian Shi, Aston Zhang, Mu Li, Anshumali Shrivastava, Andrew Gordon Wilson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6SRDbbvU8s>5.75 - [6, 8, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6SRDbbvU8s.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to jointly learn from multiple modalities, such as text, audio, and visual data, is a defining feature of intelligent systems. While there have been promising advances in designing neural networks to harness multimodal data, the enormous success of data augmentation currently remains limited to single-modality tasks like image classification. Indeed, it is particularly difficult to augment each modality while preserving the overall semantic structure of the data; for example, a caption may no longer be a good description of an image after standard augmentations have been applied, such as translation. Moreover, it is challenging to specify reasonable transformations that are not tailored to a particular modality. In this paper, we introduce LeMDA, Learning Multimodal Data Augmentation, an easy-to-use method that automatically learns to jointly augment multimodal data in feature space, with no constraints on the identities of the modalities or the relationship between modalities. We show that LeMDA can (1) profoundly improve the performance of multimodal deep learning architectures, (2) apply to combinations of modalities that have not been previously considered, and (3) achieve state-of-the-art results on a wide range of applications comprised of image, text, and tabular data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1372. <h><a href=https://openreview.net/pdf?id=S3D9NLzjnQ5>Distilling Cognitive Backdoor Patterns within an Image</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=S3D9NLzjnQ5>5.75 - [6, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel method effectively and robustly detect backdoor samples in the dataset. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/S3D9NLzjnQ5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the ``minimal essence&#34; from an input image responsible for the model&#39;s prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. 
One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. 
We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor attacks.
We also show that CD can potentially be applied to help detect potential biases from face datasets.
Code is available at https://github.com/HanxunH/CognitiveDistillation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1373. <h><a href=https://openreview.net/pdf?id=a65YK0cqH8g>Noise Is Not the Main Factor Behind the Gap Between Sgd and Adam on Transformers, But Sign Descent Might Be</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, Mark Schmidt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=a65YK0cqH8g>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A hypothesis for Adam&#39;s success is that it handles heavy-tailed noise better than SGD, but it works even better without noise; with big batch sizes, it performs very similarly to sign descent, which might help explain why it works.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/a65YK0cqH8g.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The success of the Adam optimizer on a wide array of architectures has made it the default in settings where stochastic gradient descent (SGD) performs poorly. However, our theoretical understanding of this discrepancy is lagging, preventing the development of significant improvements on either algorithm. Recent work advances the hypothesis that Adam and other heuristics like gradient clipping outperform SGD on language tasks because the distribution of the error induced by sampling has heavy tails. This suggests that Adam outperform SGD because it uses a more robust gradient estimate. We evaluate this hypothesis by varying the batch size, up to the entire dataset, to control for stochasticity. We present evidence that stochasticity and heavy-tailed noise  are not major factors in the performance gap between SGD and Adam. Rather, Adam performs better as the batch size increases,  while SGD is less effective at taking advantage of the reduction in noise. This raises the question as to why Adam outperforms SGD in the full-batch setting. Through further investigation of simpler variants of SGD, we find that  the behavior of Adam with large batches is similar to sign descent with momentum.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1374. <h><a href=https://openreview.net/pdf?id=SZynfVLGd5>Boosting Adversarial Transferability using Dynamic Cues</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Muzammal Naseer, Ahmad Mahmood, Salman Khan, Fahad Khan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SZynfVLGd5>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new approach for optimizing temporal prompts through frozen image models to capture motion dynamics for better transferability
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SZynfVLGd5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The transferability of adversarial perturbations between image models has been extensively studied. In this case, an attack is generated from a known surrogate \eg, the ImageNet trained model, and transferred to change the decision of an unknown (black-box) model trained on an image dataset. However, attacks generated from image models do not capture the dynamic nature of a moving object or a changing scene due to a lack of temporal cues within image models. This leads to reduced transferability of adversarial attacks from representation-enriched \emph{image} models such as Supervised Vision Transformers (ViTs), Self-supervised ViTs (\eg, DINO), and Vision-language models (\eg, CLIP) to black-box \emph{video} models. In this work, we induce dynamic cues within the image models without sacrificing their original performance on images. To this end, we optimize \emph{temporal prompts} through frozen image models to capture motion dynamics. Our temporal prompts are the result of a learnable transformation that allows optimizing for temporal gradients during an adversarial attack to fool the motion dynamics. Specifically, we introduce spatial (image) and temporal (video) cues within the same source model through task-specific prompts. Attacking such prompts maximizes the adversarial transferability from image-to-video and image-to-image models using the attacks designed for image models. As an example, an iterative attack launched from image model Deit-B with temporal prompts reduces generalization (top1 \% accuracy) of a video model by 35\% on Kinetics-400. Our approach also improves adversarial transferability to image models by 9\% on ImageNet w.r.t the current state-of-the-art approach. Our attack results indicate that the attacker does not need specialized architectures, \eg, divided space-time attention, 3D convolutions, or multi-view convolution networks for different data modalities. Image models are effective surrogates to optimize an adversarial attack to fool black-box models in a changing environment over time. Code is available at \url{https://bit.ly/3Xd9gRQ}
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1375. <h><a href=https://openreview.net/pdf?id=OysfLgrk8mk>Graph Domain Adaptation via Theory-Grounded Spectral Regularization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuning You, Tianlong Chen, Zhangyang Wang, Yang Shen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OysfLgrk8mk>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OysfLgrk8mk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transfer learning on graphs drawn from varied distributions (domains) is in great demand across many applications. Emerging methods attempt to learn domain-invariant representations using graph neural networks (GNNs), yet the empirical performances vary and the theoretical foundation is limited. This paper aims at designing theory-grounded algorithms for graph domain adaptation (GDA). (i) As the first attempt, we derive a model-based GDA bound closely related to two GNN spectral properties: spectral smoothness (SS) and maximum frequency response (MFR). This is achieved by cross-pollinating between the OT-based (optimal transport) DA and graph filter theories. (ii) Inspired by the theoretical results, we propose algorithms regularizing spectral properties of SS and MFR to improve GNN transferability. We further extend the GDA theory into the more challenging scenario of conditional shift, where spectral regularization still applies. (iii) More importantly, our analyses of the theory reveal which regularization would improve performance of what transfer learning scenario, (iv) with numerical agreement with extensive real-world experiments: SS and MFR regularizations bring more benefits to the scenarios of node transfer and link transfer, respectively. In a nutshell, our study paves the way toward explicitly constructing and training GNNs that can capture more transferable representations across graph domains. Codes are released at https://github.com/Shen-Lab/GDA-SpecReg.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1376. <h><a href=https://openreview.net/pdf?id=ZrEbzL9eQ3W>Scaling Laws for a Multi-Agent Reinforcement Learning Model</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Oren Neumann, Claudius Gros</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZrEbzL9eQ3W>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We examine scaling laws for AlphaZero.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZrEbzL9eQ3W.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The recent observation of neural power-law scaling relations has made a significant impact in the field of deep learning. A substantial amount of attention has been dedicated as a consequence to the description of scaling laws, although mostly for supervised learning and only to a reduced extent for reinforcement learning frameworks. In this paper we present an extensive study of performance scaling for a cornerstone reinforcement learning algorithm, AlphaZero. On the basis of a relationship between Elo rating, playing strength and power-law scaling, we train AlphaZero agents on the games Connect Four and Pentago and analyze their performance. We find that player strength scales as a power law in neural network parameter count when not bottlenecked by available compute, and as a power of compute when training optimally sized agents. We observe nearly identical scaling exponents for both games. Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models. We find that the predicted scaling of optimal neural network size fits our data for both games. This scaling law implies that previously published state-of-the-art game-playing models are significantly smaller than their optimal size, given the respective compute budgets. We also show that large AlphaZero models are more sample efficient, performing better than smaller models with the same amount of training data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1377. <h><a href=https://openreview.net/pdf?id=rnRiiHw8Vy>FastFill: Efficient Compatible Model Update</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Florian Jaeckle, Fartash Faghri, Ali Farhadi, Oncel Tuzel, Hadi Pouransari</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rnRiiHw8Vy>5.75 - [8, 6, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new uncertainty based updating scheme for online model upgrades of image retrieval systems for compatible representation learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rnRiiHw8Vy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In many retrieval systems the original high dimensional data (e.g., images) is mapped to a lower dimensional feature through a learned embedding model. The task of retrieving the most similar data from a gallery set to a given query data is performed through similarity comparison on features. When the embedding model is updated, it might produce features that are not comparable/compatible with features already in the gallery computed with the old model. Subsequently, all features in the gallery need to be re-computed using the new embedding model -- a computationally expensive process called backfilling. Recently, compatible representation learning methods have been proposed to avoid back-filling. Despite their relative success, there is an inherent trade-off between new model performance and its compatibility with the old model. In this work, we introduce FastFill: a compatible model update process using feature alignment and policy based partial backfilling to promptly elevate retrieval performance. We show that previous backfilling strategies suffer from decreased performance and demonstrate the importance of both the training objective and the ordering in online partial backfilling. We propose a new training method for feature alignment between old and new embedding models using uncertainty estimation. Compared to previous works, we obtain significantly improved backfilling results on a variety of datasets: mAP on ImageNet (+4.4%), Places-365 (+2.7%), and VGG-Face2 (+1.3%). Further, we demonstrate that when updating a biased model with FastFill, the minority subgroup accuracy gap promptly vanishes with a small fraction of partial backfilling.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1378. <h><a href=https://openreview.net/pdf?id=A7v2DqLjZdq>Bridge the Inference Gaps of Neural Processes via Expectation Maximization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qi Wang, Marco Federici, Herke van Hoof</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=A7v2DqLjZdq>5.75 - [8, 6, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/A7v2DqLjZdq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The neural process (NP) is a family of computationally efficient models for learning distributions over functions. However, it suffers from under-fitting and shows suboptimal performance in practice. Researchers have primarily focused on incorporating diverse structural inductive biases, e.g. attention or convolution, in modeling. The topic of inference suboptimality and an analysis of the NP from the optimization objective perspective has hardly been studied in earlier work. To fix this issue, we propose a surrogate objective of the target log-likelihood of the meta dataset within the expectation maximization framework. The resulting model, referred to as the Self-normalized Importance weighted Neural Process (SI-NP), can learn a more accurate functional prior and has an improvement guarantee concerning the target log-likelihood. Experimental results show the competitive performance of SI-NP over other NPs objectives and illustrate that structural inductive biases, such as attention modules, can also augment our method to achieve SOTA performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1379. <h><a href=https://openreview.net/pdf?id=dMMPUvNSYJr>Sequential Learning of Neural Networks for Prequential MDL</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jorg Bornschein, Yazhe Li, Marcus Hutter</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dMMPUvNSYJr>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Techniques for obtaining short MDL descriptions lengths for image datasets with modern NN architectures and continual learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dMMPUvNSYJr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Minimum Description Length (MDL) provides a framework and an objective for principled model evaluation. It formalizes Occam&#39;s Razor and can be applied to data from non-stationary sources. In the prequential formulation of MDL, the objective is to minimize the cumulative next-step log-loss when sequentially going through the data and using previous observations for parameter estimation. It thus closely resembles a continual- or online-learning problem. In this study, we evaluate approaches for computing prequential description lengths for image classification datasets with neural networks. Considering the computational cost, we find that online-learning with rehearsal has favorable performance compared to the previously widely used block-wise estimation. We propose forward-calibration to better align the models predictions with the empirical observations and introduce replay-streams, a minibatch incremental training technique to efficiently implement approximate random replay while avoiding large in-memory replay buffers. As a result, we present description lengths for a suite of image classification datasets that improve upon previously reported results by large margins.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1380. <h><a href=https://openreview.net/pdf?id=-bVsNeR56KS>Modeling Sequential Sentence Relation to Improve Cross-lingual Dense Retrieval</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shunyu Zhang, Yaobo Liang, MING GONG, Daxin Jiang, Nan Duan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-bVsNeR56KS>5.75 - [3, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-bVsNeR56KS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently multi-lingual pre-trained language models (PLM) such as mBERT and XLM-R have achieved impressive strides in cross-lingual dense retrieval. Despite its successes, they are general-purpose PLM while the multilingual PLM tailored for cross-lingual retrieval is still unexplored. Motivated by an observation that the sentences in parallel documents are approximately in the same order, which is universal across languages, we propose to model this sequential sentence relation to facilitate cross-lingual representation learning. Specifically, we propose a multilingual PLM called masked sentence model (MSM), which consists of a sentence encoder to generate the sentence representations, and a document encoder applied to a sequence of sentence vectors from a document. The document encoder is shared for all languages to model the universal sequential sentence relation across languages. To train the model, we propose a masked sentence prediction task, which masks and predicts the sentence vector via a hierarchical contrastive loss with sampled negatives. Comprehensive experiments on four cross-lingual retrieval tasks show MSM significantly outperforms existing advanced pre-training models, demonstrating the effectiveness and stronger cross-lingual retrieval capabilities of our approach. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1381. <h><a href=https://openreview.net/pdf?id=81VJDmOE2ol>Markup-to-Image Diffusion Models with Scheduled Sampling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuntian Deng, Noriyuki Kojima, Alexander M Rush</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=81VJDmOE2ol>5.75 - [3, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/81VJDmOE2ol.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Building on recent advances in image generation, we present a fully data-driven approach to rendering markup into images. The approach is based on diffusion models, which parameterize the distribution of data using a sequence of denoising operations on top of a Gaussian noise distribution. We view the diffusion denoising process a sequential decision making process, and show that it exhibits compounding errors similar to exposure bias issues in imitation learning problems. To mitigate these issues, we adapt the scheduled sampling algorithm to diffusion training. We conduct experiments on four markup datasets: formulas (LaTeX), table layouts (HTML), sheet music (LilyPond), and molecular images (SMILES). These experiments each verify the effectiveness of diffusion and the use of scheduled sampling to fix generation issues. These results also show that the markup-to-image task presents a useful controlled compositional setting for diagnosing and analyzing generative image models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1382. <h><a href=https://openreview.net/pdf?id=jpR98ZdIm2q>Efficient Edge Inference by Selective Query</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Anil Kag, Igor Fedorov, Aditya Gangrade, Paul Whatmough, Venkatesh Saligrama</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jpR98ZdIm2q>5.75 - [3, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Low-complexity model(edge) performs poorly on large-scale tasks; For efficient inference, it must learn to identify examples that benefit by querying; it has to identify both hard-to-classify examples and those that the cloud model would misclassify.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jpR98ZdIm2q.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Edge devices provide inference on predictive tasks to many end-users. However, deploying deep neural networks that achieve state-of-the-art accuracy on these devices is infeasible due to edge resource constraints. Nevertheless, cloud-only processing, the de-facto standard, is also problematic, since uploading large amounts of data imposes severe communication bottlenecks. We propose a novel end-to-end hybrid learning framework that allows the edge to selectively query only those hard examples that the cloud can classify correctly. Our framework optimizes over neural architectures and trains edge predictors and routing models so that the overall accuracy remains high while minimizing the overall latency. Training a hybrid learner is difficult since we lack annotations of hard edge-examples. We introduce a novel proxy supervision in this context and show that our method adapts seamlessly and near optimally across different latency regimes. On the ImageNet dataset, our proposed method deployed on a micro-controller unit exhibits $25\%$ reduction in latency compared to cloud-only processing while suffering no excess loss.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1383. <h><a href=https://openreview.net/pdf?id=gmSZ-GPNY6>Noise Injection Node Regularization for Robust Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Noam Itzhak Levi, Itay Mimouni Bloch, Marat Freytsis, Tomer Volansky</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gmSZ-GPNY6>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide analytical and empirical evidence indicating that training using a large amount of adaptive noise injection results in an emergent regularization scheme, improving robustness against a number of tests.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gmSZ-GPNY6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce Noise Injection Node Regularization (NINR), a method of injecting structured noise into Deep Neural Networks (DNN) during the training stage, resulting in an emergent regularizing effect. We present theoretical and empirical evidence for substantial improvement in robustness against various test data perturbations for feed-forward DNNs when trained under NINR. The novelty in our approach comes from the interplay of adaptive noise injection and initialization conditions such that noise is the dominant driver of dynamics at the start of training. As it simply requires the addition of external nodes without altering the existing network structure or optimization algorithms, this method can be easily incorporated into many standard problem specifications. We find improved stability against a number of data perturbations, including domain shifts, with the most dramatic improvement obtained for unstructured noise, where our technique outperforms other existing methods such as dropout or $L_2$ regularization, in some cases. We further show that desirable generalization properties on clean data are generally maintained.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1384. <h><a href=https://openreview.net/pdf?id=boik01yhssB>Average Sensitivity of Decision Tree Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Satoshi Hara, Yuichi Yoshida</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=boik01yhssB>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design decision tree learning algorithms that are stable against perturbations in the training data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/boik01yhssB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A decision tree is a fundamental model used in data mining and machine learning. In practice, the training data used to construct a decision tree may change over time or contain noise, and a drastic change in the learned tree structure owing to such data perturbation is unfavorable. For example, in data mining, a change in the tree implies a change in the extracted knowledge, which raises the question of whether the extracted knowledge is truly reliable or is only a noisy artifact. To alleviate this issue, we design decision tree learning algorithms that are stable against insignificant perturbations in the training data. Specifically, we adopt the notion of average sensitivity as a stability measure, and design an algorithm with low average sensitivity that outputs a decision tree whose accuracy is close to the optimal decision tree. The experimental results on real-world datasets demonstrate that the proposed algorithm enables users to select suitable decision trees considering the trade-off between average sensitivity and accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1385. <h><a href=https://openreview.net/pdf?id=8pvnfTAbu1f>Denoising Diffusion Samplers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Francisco Vargas, Will Sussman Grathwohl, Arnaud Doucet</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8pvnfTAbu1f>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> How to use denoising diffusion models ideas to sample unnormalized target densities and estimate their normalizing constants
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8pvnfTAbu1f.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Denoising diffusion models are a popular class of generative models providing state-of-the-art results in many domains. One adds gradually noise to data using a diffusion to transform the data distribution into a Gaussian distribution. Samples from the generative model are then obtained by simulating an approximation of the time-reversal of this diffusion initialized by Gaussian samples. Practically, the intractable score terms appearing in the time-reversed process are approximated using score matching techniques. We explore here a similar idea to sample approximately from unnormalized probability density functions and estimate their normalizing constants. We consider a process where the target density diffuses towards a Gaussian. Denoising Diffusion Samplers (DDS) are obtained by approximating the corresponding time-reversal.  While score matching is not applicable in this context, we can leverage many of the ideas introduced in generative modeling for Monte Carlo sampling. Existing theoretical results from denoising diffusion models also provide theoretical guarantees for DDS. We discuss the connections between DDS, optimal control and Schr\&#34;odinger bridges and finally demonstrate DDS experimentally on a variety of challenging sampling tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1386. <h><a href=https://openreview.net/pdf?id=sCrnllCtjoE>Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mohammad Amin Shabani, Amir H. Abdi, Lili Meng, Tristan Sylvain</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sCrnllCtjoE>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Propose a new framework to improve recent state-of-the-arts on time-series forecasting using transformers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sCrnllCtjoE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The performance of time series forecasting has recently been greatly improved by the introduction of transformers. In this paper, we propose a general multi-scale framework that can be applied to state-of-the-art transformer-based time series forecasting models
(FEDformer, Autoformer, etc.). Using iteratively refining a forecasted time series at multiple scales with shared weights, architecture adaptations and a specially-designed normalization scheme, we are able to achieve significant performance improvements with minimal additional computational overhead. Via detailed ablation studies, we demonstrate the effectiveness of our proposed architectural and methodological innovations. Furthermore, our experiments on various public datasets demonstrate that the proposed method outperforms the corresponding baselines. Depending on the choice of transformer architecture, our mutli-scale framework results in mean squared error reductions ranging from 5.5% to 38.5%. Our code is publicly available in https://github.com/BorealisAI/scaleformer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1387. <h><a href=https://openreview.net/pdf?id=yEsj8pGNl1>Projective Proximal Gradient Descent for Nonconvex Nonsmooth Optimization: Fast Convergence Without Kurdyka-Lojasiewicz (KL) Property</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yingzhen Yang, Ping Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yEsj8pGNl1>5.75 - [3, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose Projected Proximal Gradient Descent (PPGD) which solves a class of non-convex and non-smooth optimization problems with the Nesterov&#39;s optimal convergence rate.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yEsj8pGNl1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Nonconvex and nonsmooth optimization problems are important and challenging for statistics and machine learning. In this paper, we propose Projected Proximal Gradient Descent (PPGD) which solves a class of nonconvex and nonsmooth optimization problems, where the nonconvexity and nonsmoothness come from a nonsmooth regularization term which is nonconvex but piecewise convex. In contrast with existing convergence analysis of accelerated PGD methods for nonconvex and nonsmooth problems based on the Kurdyka-\L{}ojasiewicz (K\L{}) property, we provide a new theoretical analysis showing local fast convergence of PPGD. It is proved that PPGD achieves a fast convergence rate of $O(1/k^2)$ when the iteration number $k \ge k_0$ for a finite $k_0$ on a class of nonconvex and nonsmooth problems under mild assumptions, which is locally the Nesterov&#39;s optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Experimental results demonstrate the effectiveness of PPGD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1388. <h><a href=https://openreview.net/pdf?id=7wrq3vHcMM>First Steps Toward Understanding the Extrapolation of Nonlinear Models to Unseen Domains</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kefan Dong, Tengyu Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7wrq3vHcMM>5.75 - [6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove the first result for understanding the extrapolation of nonlinear model class with structured domain shifts.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7wrq3vHcMM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Real-world machine learning applications often involve deploying neural networks to domains that are not seen in the training time. Hence, we need to understand the extrapolation of \textit{nonlinear} models---under what conditions on the distributions and function class, models can be guaranteed to extrapolate to new test distributions. The question is very challenging because even two-layer neural networks cannot be guaranteed to extrapolate outside the support of the training distribution without further assumptions on the domain shift. This paper makes some initial steps towards analyzing the extrapolation of nonlinear models for structured domain shift. We primarily consider settings where the \textit{marginal} distribution of each coordinate of the data (or subset of coordinates) do not shift significantly across the training and test distributions, but the joint distribution may have a much bigger shift. We prove that the family of nonlinear models of the form $f(x)=\sum f_i(x_i)$, where $f_i$ is an \emph{arbitrary} function on the subset of features $x_i$, can extrapolate to unseen distributions, if the covariance of the features is well-conditioned. To the best of our knowledge, this is the first result that goes beyond linear models and the bounded density ratio assumption, even though the assumptions on the distribution shift and function class are stylized.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1389. <h><a href=https://openreview.net/pdf?id=4D4TSJE6-K>Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ansong Ni, Jeevana Priya Inala, Chenglong Wang, Alex Polozov, Christopher Meek, Dragomir Radev, Jianfeng Gao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4D4TSJE6-K>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose to let pretrained language models sample additional solutions for each problem and learn from the self-sampled solutions that are correct or partially-correct.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4D4TSJE6-K.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pretrained language models have shown superior performance on many natural language processing tasks, yet they still struggle at multi-step formal reasoning tasks like grade school math problems. One key challenge of finetuning them to solve such math reasoning problems is that many existing datasets only contain one reference solution for each problem, despite the fact that there are often alternative solutions resembling different reasoning paths to the final answer. This way, the finetuned models are biased towards the limited reference solutions, which limits their generalization to unseen examples. To mitigate this issue, we propose to let the model perform sampling during training and learn from both self-sampled fully-correct solutions, which yield the correct answer upon execution, and partially-correct solutions, whose intermediate state matches an intermediate state of a known correct solution. We show that our use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space. Additionally, we explore various training objectives to support learning from multiple solutions per example and find they greatly affect the performance. Experiments on two math reasoning datasets show the effectiveness of our method compared to learning from a single reference solution with MLE, where we improve PASS@100 from 35.5% to 44.5% for GSM8K, and 27.6% to 36.2% PASS@80 for MathQA. Such improvements are also consistent across different model sizes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1390. <h><a href=https://openreview.net/pdf?id=JQc2VowqCzz>Interaction-Based Disentanglement of Entities for Object-Centric World Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Akihiro Nakano, Masahiro Suzuki, Yutaka Matsuo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JQc2VowqCzz>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a structured, action-conditioned probabilistic model that learns to disentangle object representations based on interactions and demonstrate its ability to solve downstream tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JQc2VowqCzz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Perceiving the world compositionally in terms of space and time is essential to understanding object dynamics and solving downstream tasks. Object-centric learning using generative models has improved in its ability to learn distinct representations of individual objects and predict their interactions, and how to utilize the learned representations to solve untrained, downstream tasks is a focal question. However, as models struggle to predict object interactions and track the objects accurately, especially for unseen configurations, using object-centric representations in downstream tasks is still a challenge. This paper proposes STEDIE, a new model that disentangles object representations, based on interactions, into interaction-relevant relational features and interaction-irrelevant global features without supervision. Empirical evaluation shows that the proposed model factorizes global features, unaffected by interactions from relational features that are necessary to predict outcome of interactions. We also show that STEDIE achieves better performance in planning tasks and understanding causal relationships. In both tasks, our model not only achieves better performance in terms of reconstruction ability but also utilizes the disentangled representations to solve the tasks in a structured manner.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1391. <h><a href=https://openreview.net/pdf?id=VWm4o4l3V9e>Block and Subword-Scaling Floating-Point (BSFP) : An Efficient Non-Uniform Quantization For Low Precision Inference</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yun-Chen Lo, Tse-Kuang Lee, Ren-Shuo Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VWm4o4l3V9e>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VWm4o4l3V9e.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we propose Block and Subword-Scaling Floating-Point (BSFP), a non-uniform quantization scheme for the skewed and non-uniform distribution of weight vectors in neural networks. By quantizing each weight vector as the superposition of multiple subword vectors (in two&#39;s complement) with scaling factors (in Low-bit Floating-Point, LBFP), BSFP can effectively fit the distribution of weight vectors while maintaining high computation efficiency. Furthermore, we present a grid search-based MSE-optimal quantization flow and a scaled serial processing engine to complete the quantization pipeline and the infrastructure. 

The experimental results on the ImageNet classification task show that our proposed method outperforms state-of-the-art Microsoft Floating Point (MSFP) by up to 20.56% top-1 accuracy at the same weight precision and reduces up to 10.3% model size. Furthermore, BSFP outperforms MSFP by up to 2.0$\times$ computing throughput and up to 5.3$\times$ energy efficiency under the same silicon area budget.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1392. <h><a href=https://openreview.net/pdf?id=rOFKmzNTbC>Tensor-Based Sketching Method for the Low-Rank Approximation of Data Streams.</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Cuiyu Liu, Xiao Chuanfu, Mingshuo Ding, Chao Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rOFKmzNTbC>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rOFKmzNTbC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Low-rank approximation in data streams is a fundamental and significant task in computing science, machine learning and statistics. Multiple streaming algorithms have emerged over years and most of them are inspired by randomized algorithms, more specifically, sketching methods. However, many algorithms are not able to leverage information of data streams and consequently suffer from low accuracy. Existing data-driven methods improve accuracy but the training cost is expensive in practice. In this paper, from a subspace perspective, we propose a tensor-based sketching method for low-rank approximation of data streams. The proposed algorithm fully exploits the structure of data streams and obtains quasi-optimal sketching matrices by performing tensor decomposition on training data. A series of experiments are carried out and show that the proposed tensor-based method can be more accurate and much faster than the previous work.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1393. <h><a href=https://openreview.net/pdf?id=EMvG1Jdhw_8>Disentangling Learning Representations with Density Estimation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Eric Yeats, Frank Y Liu, Hai Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EMvG1Jdhw_8>5.75 - [6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present GCAE, a scalable disentanglement method that uses the dual total correlation criterion
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EMvG1Jdhw_8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Disentangled learning representations have promising utility in many applications, but they currently suffer from serious reliability issues. We present Gaussian Channel Autoencoder (GCAE), a method which achieves reliable disentanglement via scalable non-parametric density estimation of the latent space. GCAE avoids the curse of dimensionality of density estimation by disentangling subsets of its latent space with the Dual Total Correlation (DTC) metric, thereby representing its high-dimensional latent joint distribution as a collection of many low-dimensional conditional distributions. In our experiments, GCAE achieves highly competitive and reliable disentanglement scores compared with state-of-the-art baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1394. <h><a href=https://openreview.net/pdf?id=SZdfz5k7cd1>Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyu Tang, Yatong Chen, Yang Liu, Kun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SZdfz5k7cd1>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We formulate and investigate a long-term fairness notion that captures decision-distribution interplay via a detailed modeling over both observed and latent causal factors.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SZdfz5k7cd1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The pursuit of long-term fairness involves the interplay between decision-making and the underlying data generating process. In this paper, through causal modeling with a directed acyclic graph (DAG) on the decision-distribution interplay, we investigate the possibility of achieving long-term fairness from a dynamic perspective. We propose Tier Balancing, a technically more challenging but more natural notion to achieve in the context of long-term, dynamic fairness analysis. Different from previous fairness notions that are defined purely on observed variables, our notion goes one step further, capturing behind-the-scenes situation changes on the unobserved latent causal factors that directly carry out the influence from the current decision to the future data distribution. Under the specified dynamics, we prove that in general one cannot achieve the long-term fairness goal only through one-step interventions. Furthermore, in the effort of approaching long-term fairness, we consider the mission of &#34;getting closer to&#34; the long-term fairness goal and present possibility and impossibility results accordingly.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1395. <h><a href=https://openreview.net/pdf?id=GULFHQfgw0g>Neural Agents Struggle to Take Turns in Bidirectional Emergent Communication</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Valentin Taillandier, Dieuwke Hupkes, Benoît Sagot, Emmanuel Dupoux, Paul Michel</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GULFHQfgw0g>5.75 - [6, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Neural agents struggle to develop a turn-taking protocol when playing cooperative game for which they have to communicate.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GULFHQfgw0g.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The spontaneous exchange of turns is a central aspect of human communication. Although turn-taking conventions come to us naturally, artificial dialogue agents struggle to coordinate, and must rely on hard-coded rules to engage in interactive conversations with human interlocutors. In this paper, we investigate the conditions under which artificial agents may naturally develop turn-taking conventions in a simple language game. We describe a cooperative task where success is contingent on the exchange of information along a shared communication channel where talking over each other hinders communication. Despite these environmental constraints, neural-network based agents trained to solve this task with reinforcement learning do not systematically adopt turn-taking conventions. However, we find that agents that do agree on turn-taking protocols end up performing better. 
Moreover, agents that are forced to perform turn-taking can learn to solve the task more quickly. 
This suggests that turn-taking may help to generate conversations that are easier for speakers to interpret.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1396. <h><a href=https://openreview.net/pdf?id=shzu8d6_YAR>FaiREE: fair classification with finite-sample and distribution-free guarantee</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Puheng Li, James Zou, Linjun Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=shzu8d6_YAR>5.75 - [6, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a fair classification algorithm which can satisfy the group fairness constraints with finite-sample and distribution-free guarantees.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/shzu8d6_YAR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depend on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm which can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfying various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1397. <h><a href=https://openreview.net/pdf?id=g1GnnCI1OrC>E-CRF: Embedded Conditional Random Field for Boundary-caused Class Weights Confusion in Semantic Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jie Zhu, Huabin Huang, Banghuai Li, Leye Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=g1GnnCI1OrC>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/g1GnnCI1OrC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Modern semantic segmentation methods devote much effect to adjusting image feature representations to improve the segmentation performance in various ways, such as architecture design, attention mechnism, etc. However, almost all those methods neglect the particularity of class weights (in the classification layer) in segmentation models. In this paper, we notice that the class weights of categories that tend to share many adjacent boundary pixels lack discrimination, thereby limiting the performance. We call this issue Boundary-caused Class Weights Confusion (BCWC). We try to focus on this problem and propose a novel method named Embedded Conditional Random Field (E-CRF) to alleviate it. E-CRF innovatively fuses the CRF into the CNN network as an organic whole for more effective end-to-end optimization. The reasons are two folds. It utilizes CRF to guide the message passing between pixels in high-level features to purify the feature representation of boundary pixels, with the help of inner pixels belonging to the same object. More importantly, it enables optimizing class weights from both scale and direction during backpropagation. We make detailed theoretical analysis to prove it. Besides, superpixel is integrated into E-CRF and served as an auxiliary to exploit the local object prior for more reliable message passing. Finally, our proposed method yields impressive results on ADE20K, Cityscapes, and Pascal Context datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1398. <h><a href=https://openreview.net/pdf?id=k9CF4h3muD>Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Edo Cohen-Karlik, Itamar Menuhin-Gruman, Raja Giryes, Nadav Cohen, Amir Globerson</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k9CF4h3muD>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Linear RNNs optimized with gradient descent have implicit bias leading to solutions with low dimensional state spaces leading to non-trivial extrapolation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k9CF4h3muD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Overparameterization in deep learning refers to settings where a trained Neural Network (NN) has representational capacity to fit the training data in many ways, some of which generalize well, while others do not. In the case of Recurrent Neural Networks (RNNs) there exists an additional layer of overparameterization, in the sense that a model may exhibit many solutions that generalize well for sequence lengths seen in training, some of which \emph{extrapolate} to longer sequences, while others do not. Numerous works studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with solutions that generalize well. On the other hand, its tendency to fit overparameterized RNNs with solutions that extrapolate has been discovered only lately, and is far less understood. In this paper, we analyze the extrapolation properties of GD when applied to overparameterized linear RNNs. In contrast to recent arguments suggesting an implicit bias towards short-term memory, we provide theoretical evidence for learning low dimensional state spaces, which can also model long-term memory. Our result relies on a dynamical characterization showing that GD (with small step size and near zero initialization) strives to maintain a certain form of balancedness, as well as tools developed in the context of the \emph{moment problem} from statistics (recovery of discrete probability distribution from its moments). Experiments corroborate our theory, demonstrating extrapolation via learning low dimensional state spaces with both linear and non-linear RNNs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1399. <h><a href=https://openreview.net/pdf?id=3oWo92cQyxL>Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ivona Najdenkoska, Xiantong Zhen, Marcel Worring</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3oWo92cQyxL>5.75 - [6, 8, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce a novel multimodal few-shot meta-learner, by learning how to bridge large-scale frozen vision and language models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3oWo92cQyxL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1400. <h><a href=https://openreview.net/pdf?id=CtS2Rs_aYk>Stay Moral and Explore: Learn to Behave Morally in Text-based Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zijing Shi, Meng Fang, Yunqiu Xu, Ling Chen, Yali Du</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CtS2Rs_aYk>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CtS2Rs_aYk.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reinforcement learning (RL) in text-based games has developed rapidly and achieved promising results. However, little effort has been expended to design agents that pursue objectives while behaving morally, which is a critical issue in the field of autonomous agents. In this paper, we propose a general framework named Moral Awareness Adaptive Learning (MorAL) that enhances the morality capacity of an agent using a plugin moral-aware learning model. The framework allows the agent to execute task learning and morality learning adaptively. The agent selects trajectories from past experiences during task learning. Meanwhile, the trajectories are used to conduct self-imitation learning with a moral-enhanced objective. In order to achieve the trade-off between morality and task progress, the agent uses the combination of task policy and moral policy for action selection. We evaluate on the Jiminy Cricket benchmark, a set of text-based games with various scenes and dense morality annotations. Our experiments demonstrate that, compared with strong contemporary value alignment approaches,  the proposed framework improves task performance while reducing immoral behaviours in various games.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1401. <h><a href=https://openreview.net/pdf?id=dhYUMMy0_Eg>Equal Improvability: A New Fairness Notion Considering the Long-term Impact</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ozgur Guldogan, Yuchen Zeng, Jy-yong Sohn, Ramtin Pedarsani, Kangwook Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=dhYUMMy0_Eg>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new group fairness notion called Equal Improvability that equalizes the improvement of the rejected individuals across different groups.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/dhYUMMy0_Eg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Devising a fair classifier that does not discriminate against different groups is an important problem in machine learning. Although researchers have proposed various ways of defining group fairness, most of them only focused on the immediate fairness, ignoring the long-term impact of a fair classifier under the dynamic scenario where each individual can improve its feature over time. Such dynamic scenarios happen in real world, e.g., college admission and credit loaning, where each rejected sample makes effort to change its features to get accepted afterwards. In this dynamic setting, the long-term fairness should equalize the samples’ feature distribution across different groups after the rejected samples make some effort to improve. In order to promote long-term fairness, we propose a new fairness notion called Equal Improvability (EI), which equalizes the potential acceptance rate of the rejected samples across different groups assuming a bounded level of effort will be spent by each rejected sample. We analyze the properties of EI and its connections with existing fairness notions. To find a classifier that satisfies the EI requirement, we propose and study three different approaches that solve EI regularized optimization problems. Through experiments on both synthetic and real datasets, we demonstrate that the proposed EI-regularized algorithms encourage us to find a fair classifier in terms of EI. Finally, we provide experimental results on dynamic scenarios which highlight the advantages of our EI metric in achieving the long-term fairness. Codes are available in anonymous GitHub repository.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1402. <h><a href=https://openreview.net/pdf?id=0Q9H_Pgx132>Deep Learning meets Nonparametric Regression: Are Weight-Decayed DNNs Locally Adaptive?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaiqi Zhang, Yu-Xiang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0Q9H_Pgx132>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Parallel NN with only weight decay achieves an estimation error close to the minimax rates for both the Besov and BV classes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0Q9H_Pgx132.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the theory of neural network (NN) from the lens of classical nonparametric regression problems with a focus on NN’s ability to adaptively estimate functions with heterogeneous smoothness — a property of functions in Besov or Bounded Variation (BV) classes. Existing work on this problem requires tuning the NN architecture based on the function spaces and sample sizes. We consider a “Parallel NN” variant of deep ReLU networks and show that the standard weight decay is equivalent to promoting the ℓp -sparsity (0 &lt; p &lt; 1) of the coefficient vector of an end-to-end learned function bases, i.e., a dictionary. Using this equivalence, we further establish that by tuning only the weight decay, such Parallel NN achieves an estimation error arbitrarily close to the minimax rates for both the Besov and BV classes. Notably, it gets exponentially closer to minimax optimal as the NN gets deeper. Our research sheds new lights on why depth matters and how NNs are more powerful than kernel methods
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1403. <h><a href=https://openreview.net/pdf?id=J_Cja7cpgW>Consolidator: Mergable Adapter with Group Connections for Visual Adaptation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianxiang Hao, Hui Chen, Yuchen Guo, Guiguang Ding</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=J_Cja7cpgW>5.75 - [5, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a module named consolidator to achieve both parameter- and inference-efficient transfer learning for vision transformers
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/J_Cja7cpgW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting a well-trained transformer to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every downstream task and thus easily falls into an overfitting situation, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of all parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose consolidator to achieve efficient transfer learning for large vision models. Our consolidator modifies the pre-trained model with the addition of a small set of tunable parameters to temporarily store the task-specific knowledge while freezing the backbone model during adaptation. Motivated by the success of group-wise convolution, we adopt grouped connections across the features extracted by fully connected layers to construct tunable parts in a consolidator. To further enhance the model&#39;s capacity to transfer knowledge under a constrained storage budget and keep inference efficient, we consolidate the parameters in two stages: 1. between adaptation and storage, and 2. between loading and inference. On a series of downstream visual tasks, our consolidator can reach up to 7.56 better accuracy than full fine-tuning with merely 0.35% parameters, and outperform state-of-the-art parameter-efficient tuning methods by a clear margin. Code is available at github.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1404. <h><a href=https://openreview.net/pdf?id=nAgdXgfmqj>Hyperparameter Optimization through Neural Network Partitioning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bruno Kacper Mlodozeniec, Matthias Reisser, Christos Louizos</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nAgdXgfmqj>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We introduce partitioned networks and an out-of-training sample loss for scalable optimization of hyperparameters
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nAgdXgfmqj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance --- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the &#34;out-of-training-sample&#34; loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly computationally cheaper than alternative methods aiming to optimize the marginal likelihood for neural networks. Lastly, we also focus on optimizing hyperparameters in federated learning, where retraining and cross-validation are particularly challenging.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1405. <h><a href=https://openreview.net/pdf?id=ooxDOe7ZtBe>Summarization Programs: Interpretable Abstractive Summarization with Neural Modular Trees</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Swarnadeep Saha, Shiyue Zhang, Peter Hase, Mohit Bansal</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ooxDOe7ZtBe>5.75 - [6, 8, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An interpretable framework for abstractive summarization with neural modular trees
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ooxDOe7ZtBe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Current abstractive summarization models either suffer from a lack of clear interpretability or provide incomplete rationales by only highlighting parts of the source document. To this end, we propose the Summarization Program (SP), an interpretable modular framework consisting of an (ordered) list of binary trees, each encoding the step-by-step generative process of an abstractive summary sentence from the source document. A Summarization Program contains one root node per summary sentence, and a distinct tree connects each summary sentence (root node) to the document sentences (leaf nodes) from which it is derived, with the connecting nodes containing intermediate generated sentences. Edges represent different modular operations involved in summarization such as sentence fusion, compression, and paraphrasing. We first propose an efficient best-first search method over neural modules, SP-Search that identifies SPs for human summaries by directly optimizing for ROUGE scores. Next, using these programs as automatic supervision, we propose seq2seq models that generate Summarization Programs, which are then executed to obtain final summaries. We demonstrate that SP-Search effectively represents the generative process behind human summaries using modules that are typically faithful to their intended behavior. We also conduct a simulation study to show that Summarization Programs improve the interpretability of summarization models by allowing humans to better simulate model reasoning. Summarization Programs constitute a promising step toward interpretable and modular abstractive summarization, a complex task previously addressed primarily through blackbox end-to-end neural systems.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1406. <h><a href=https://openreview.net/pdf?id=Kn6i2BZW69w>DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Joya Chen, Kai Xu, Yuhui Wang, Yifei Cheng, Angela Yao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Kn6i2BZW69w>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> DropIT can save memory &amp; improve accuracy, providing a new perspective of dropping in activation compressed training than quantization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Kn6i2BZW69w.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A standard hardware bottleneck when training deep neural networks is GPU memory. The bulk of memory is occupied by caching intermediate tensors for gradient computation in the backward pass. We propose a novel method to reduce this footprint - Dropping Intermediate Tensors (DropIT).  DropIT drops min-k elements of the intermediate tensors and approximates gradients from the sparsified tensors in the backward pass. Theoretically, DropIT reduces noise on estimated gradients and therefore has a higher rate of convergence than vanilla-SGD. Experiments show that we can drop up to 90\% of the intermediate tensor elements in fully-connected and convolutional layers while achieving higher testing accuracy for Visual Transformers and Convolutional Neural Networks on various tasks (e.g., classification, object detection, instance segmentation). Our code and models are available at https://github.com/chenjoya/dropit.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1407. <h><a href=https://openreview.net/pdf?id=JmC_Tld3v-f>Individual Privacy Accounting with Gaussian Differential Privacy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Antti Koskela, Marlon Tobaben, Antti Honkela</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JmC_Tld3v-f>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Accurate privacy analysis of fully adaptive compositions using Gaussian differential privacy
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JmC_Tld3v-f.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Individual privacy accounting enables bounding differential privacy (DP) loss individually for each participant involved in the analysis. This can be informative as often the individual privacy losses are considerably smaller than those indicated by the DP bounds that are based on considering worst-case bounds at each data access. In order to account for the individual losses in a principled manner, we need a privacy accountant for adaptive compositions of mechanisms, where the loss incurred at a given data access is allowed to be smaller than the worst-case loss. This kind of analysis has been carried out for the Rényi differential privacy by Feldman and Zrnic (2021), however not yet for the so-called optimal privacy accountants. We make first steps in this direction by providing a careful analysis using the Gaussian differential privacy which gives optimal bounds for the Gaussian mechanism, one of the most versatile DP mechanisms. This approach is based on determining a certain supermartingale for the hockey-stick divergence and on extending the Rényi divergence-based fully adaptive composition results by Feldman and Zrnic (2021). We also consider measuring the individual  $(\varepsilon,\delta)$-privacy losses using the so-called privacy loss distributions. Using the Blackwell theorem, we can then use the results of Feldman and Zrnic (2021) to construct an approximative individual $(\varepsilon,\delta)$-accountant. We also show how to speed up the FFT-based individual DP accounting using the Plancherel theorem.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1408. <h><a href=https://openreview.net/pdf?id=-jP_rDkyfpI>Approximate Nearest Neighbor Search through Modern Error-Correcting Codes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Noam Touitou, Nissim Halabi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-jP_rDkyfpI>5.75 - [3, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Using modern error-correcting codes, we present an improved method of using locality-sensitive hash functions for approximate nearest-neighbor search..
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-jP_rDkyfpI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A locality-sensitive hash (or LSH) is a function that can efficiently map dataset points into a latent space while preserving pairwise distances. Such LSH functions have been used in approximate nearest-neighbor search (ANNS) in the following classic way, which we call classic hash clustering (CHC): first, the dataset points are hashed into a low-dimensional binary space using the LSH function; then, the points are clustered by these hash values. Upon receiving a query, its nearest neighbors are sought within its hash-cluster and nearby hash-clusters (i.e., multi-probe). However, CHC mandates a low-dimensional latent space for the LSH function, which distorts distances from the (high-dimensional) original real space; this results in inferior recall. This is often mitigated through using multiple hash tables at additional storage and memory costs.

In this paper, we introduce a better way of using LSH functions for ANNS. Our method, called the Polar Code Nearest-Neighbor (PCNN) algorithm, uses modern error-correcting codes (specifically polar codes) to maintain a manageable number of clusters inside a high-dimensional latent space. Allowing the LSH function to embed into this high-dimensional latent space results in higher recall, as the embedding faithfully captures distances in the original space. The crux of PCNN is using polar codes for probing: we present a multi-probe scheme for PCNN which uses efficient list-decoding methods for polar codes, with time complexity independent of the dataset size. Fixing the choice of LSH, experiment results demonstrate significant performance gains of PCNN over CHC; in particular, PCNN with a single table outperforms CHC with multiple tables, obviating the need for large memory and storage.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1409. <h><a href=https://openreview.net/pdf?id=zQWqV2tzDv>CircNet: Meshing 3D Point Clouds with Circumcenter Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huan Lei, Ruitao Leng, Liang Zheng, Hongdong Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zQWqV2tzDv>5.75 - [6, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a deep neural architecture that detects circumcenters of triangles in the dual space to reconstruct 3D point clouds into triangular meshes efficiently
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zQWqV2tzDv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Reconstructing 3D point clouds into triangle meshes is a key problem in computational geometry and surface reconstruction. Point cloud triangulation solves this problem by providing edge information to the input points. Since no vertex interpolation is involved, it is beneficial to preserve sharp details on the surface. Taking advantage of learning-based techniques in triangulation, existing methods enumerate the complete combinations of candidate triangles, which is both complex and inefficient. In this paper, we leverage the duality between a triangle and its circumcenter, and introduce a deep neural network that detects the circumcenters to achieve point cloud triangulation. Specifically, we introduce multiple anchor priors to divide the neighborhood space of each point. The neural network then learns to predict the presences and locations of circumcenters under the guidance of those anchors. We extract the triangles dual to the detected circumcenters to form a primitive mesh, from which an edge-manifold mesh is produced via simple post-processing. Unlike existing learning-based triangulation methods, the proposed method bypasses an exhaustive enumeration of triangle combinations and local surface parameterization. We validate the efficiency, generalization, and robustness of our method on prominent datasets of both watertight and open surfaces. The code and trained models are provided at \url{https://github.com/Ruitao-L/CircNet}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1410. <h><a href=https://openreview.net/pdf?id=QIRtAqoXwj>Heterogeneous Neuronal and Synaptic Dynamics for Spike-Efficient Unsupervised Learning: Theory and Design Principles</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Biswadeep Chakraborty, Saibal Mukhopadhyay</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QIRtAqoXwj>5.75 - [6, 3, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove that heterogeneity in neuronal dynamics improves the memory capacity while heterogeneity in the STDP synaptic dynamics improves the spike efficiency
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QIRtAqoXwj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper shows that the heterogeneity in neuronal and synaptic dynamics reduces the spiking activity of a Recurrent Spiking Neural Network (RSNN) while improving prediction performance, enabling spike-efficient (unsupervised) learning.
We analytically show that the diversity in neurons&#39; integration/relaxation dynamics improves an RSNN&#39;s ability to learn more distinct input patterns (higher memory capacity), leading to improved classification and prediction performance. We further prove that heterogeneous Spike-Timing-Dependent-Plasticity (STDP) dynamics of synapses reduce spiking activity but preserve memory capacity. The analytical results motivate Heterogeneous RSNN design using Bayesian optimization to determine heterogeneity in neurons and synapses to improve $\mathcal{E}$, defined as the ratio of spiking activity and memory capacity. The empirical results on time series classification and prediction tasks show that optimized HRSNN increases performance and reduces spiking activity compared to a homogeneous RSNN.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1411. <h><a href=https://openreview.net/pdf?id=0uRm1YmFTu>Predictive Inference with Feature Conformal Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiaye Teng, Chuan Wen, Dinghuai Zhang, Yoshua Bengio, Yang Gao, Yang Yuan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0uRm1YmFTu>5.75 - [6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Conformal inference in feature space. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0uRm1YmFTu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Conformal prediction is a distribution-free technique for establishing valid prediction intervals. Although conventionally people conduct conformal prediction in the output space, this is not the only possibility. In this paper, we propose feature conformal prediction, which extends the scope of conformal prediction to semantic feature spaces by leveraging the inductive bias of deep representation learning. From a theoretical perspective, we demonstrate that feature conformal prediction provably outperforms regular conformal prediction under mild assumptions. Our approach could be combined with not only vanilla conformal prediction, but also other adaptive conformal prediction methods. Apart from experiments on existing predictive inference benchmarks, we also demonstrate the state-of-the-art performance of the proposed methods on \textit{large-scale} tasks such as ImageNet classification and Cityscapes image segmentation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1412. <h><a href=https://openreview.net/pdf?id=j1zQGmQQOX1>Differentially Private Adaptive Optimization with Delayed Preconditioners</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tian Li, Manzil Zaheer, Ken Liu, Sashank J. Reddi, Hugh Brendan McMahan, Virginia Smith</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=j1zQGmQQOX1>5.75 - [6, 6, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a private adaptive optimization framework that constructs delayed but less noisy preconditioners, yielding improved privacy/utility trade-offs without the need to access auxiliary data.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/j1zQGmQQOX1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Privacy costs may negate the benefits of using adaptive optimizers in differentially private model training. Prior works typically address this issue by using auxiliary information (e.g., public data) to boost the effectiveness of adaptive optimization. In this work, we explore techniques to estimate and efficiently adapt to gradient geometry in private adaptive optimization without auxiliary data. Motivated by the observation that adaptive methods can tolerate stale preconditioners, we propose differentially private adaptive training with delayed preconditioners (DP^2), a simple method that constructs delayed but less noisy preconditioners to better realize the benefits of adaptivity. Theoretically, we provide convergence guarantees for our method for both convex and non-convex problems, and analyze trade-offs between delay and privacy noise reduction. Empirically, we explore DP^2 across several real-world datasets, demonstrating that it can improve convergence speed by as much as 4× relative to non-adaptive baselines and match the performance of state-of-the-art optimization methods that require auxiliary data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1413. <h><a href=https://openreview.net/pdf?id=nXOhmfFu5n>Amortised Invariance Learning for Contrastive Self-Supervision</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruchika Chavhan, Jan Stuehmer, Calum Heggan, Mehrdad Yaghoobi, Timothy Hospedales</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=nXOhmfFu5n>5.75 - [8, 3, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/nXOhmfFu5n.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contrastive self-supervised learning methods famously produce high quality transferable representations by learning invariances to different data augmentations. Invariances established during pre-training can be interpreted as strong inductive biases. However these may or may not be helpful, depending on if they match the invariance requirements of  downstream tasks or not. This has led to several attempts to learn task-specific invariances during pre-training, however, these methods are highly compute intensive and tedious to train. We introduce the notion of amortized invariance learning for contrastive self supervision. In the pre-training stage, we parameterize the feature extractor by differentiable invariance hyper-parameters that control the invariances encoded by the representation. Then, for any downstream task, both linear readout and task-specific invariance requirements can be efficiently and effectively learned by gradient-descent. We evaluate the notion of amortized invariances for contrastive learning over two different modalities: vision and audio, on two widely-used contrastive learning methods in vision: SimCLR and MoCo-v2 with popular architectures like ResNets and Vision Transformers, and SimCLR with ResNet-18 for audio. We show that our amortized features provide a reliable way to learn diverse downstream tasks with different invariance requirements, while using a single feature and avoiding task-specific pre-training. This provides an exciting perspective that opens up new horizons in the field of general purpose representation learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1414. <h><a href=https://openreview.net/pdf?id=QrnDe_9ZFd8>Task Ambiguity in Humans and Language Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alex Tamkin, Kunal Handa, Avash Shrestha, Noah Goodman</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QrnDe_9ZFd8>5.75 - [6, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We motivate the direction of studying task ambiguity in humans and language models, evaluating them on a new benchmark of ambiguously-specified tasks and develop methods for improving performance
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QrnDe_9ZFd8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Language models have recently achieved strong performance across a wide range of NLP benchmarks. However, real world tasks are often poorly specified, and agents must deduce the intended behavior from a combination of context, instructions, and examples. We investigate how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. We evaluate humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different numbers of labeled examples. We find that the combination of model scaling (to 175B parameters) and reinforcement learning from human feedback (RLHF) enables models to approach or exceed the accuracy of human participants across tasks, but that either one of these alone is not sufficient. In addition, we show how to dramatically improve the accuracy of language models trained without RLHF by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1415. <h><a href=https://openreview.net/pdf?id=6zrOr_Rdhjs>Parallel Deep Neural Networks Have Zero Duality Gap</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yifei Wang, Tolga Ergen, Mert Pilanci</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6zrOr_Rdhjs>5.75 - [3, 6, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6zrOr_Rdhjs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Training deep neural networks is a challenging non-convex optimization problem. Recent work has proven that the strong duality holds (which means zero duality gap) for regularized finite-width two-layer ReLU networks and consequently provided an equivalent convex training problem. However, extending this result to deeper networks remains to be an open problem. In this paper, we prove that the duality gap for deeper linear networks with vector outputs is non-zero. In contrast, we show that the zero duality gap can be obtained by stacking standard deep networks in parallel, which we call a parallel architecture, and modifying the regularization. Therefore, we prove the strong duality and existence of equivalent convex problems that enable globally optimal training of deep networks. As a by-product of our analysis, we demonstrate that the weight decay regularization on the network parameters explicitly encourages low-rank solutions via closed-form expressions. In addition, we show that strong duality holds for three-layer standard ReLU networks given rank-1 data matrices.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1416. <h><a href=https://openreview.net/pdf?id=sAJDi9lD06L>Holistic Adversarially Robust Pruning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qi Zhao, Christian Wressnegger</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sAJDi9lD06L>5.75 - [6, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose HARP that realizes the adversarially robust pruning in a holistic way and yields an outstanding capability at aggressive compression.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sAJDi9lD06L.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural networks can be drastically shrunk in size by removing redundant parameters. While crucial for the deployment on resource-constraint hardware, oftentimes, compression comes with a severe drop in accuracy and lack of adversarial robustness. Despite recent advances, counteracting both aspects has only succeeded for moderate compression rates so far. We propose a novel method, HARP, that copes with aggressive pruning significantly better than prior work. For this, we consider the network holistically. We learn a global compression strategy that optimizes how many parameters (compression rate) and which parameters (scoring connections) to prune specific to each layer individually. Our method fine-tunes an existing model with dynamic regularization, that follows a step-wise incremental function balancing the different objectives. It starts by favoring robustness before shifting focus on reaching the target compression rate and only then handles the objectives equally. The learned compression strategies allow us to maintain the pre-trained model’s natural accuracy and its adversarial robustness for a reduction by 99% of the network’s original size. Moreover, we observe a crucial influence of non-uniform compression across layers. The implementation of HARP is publicly available at https://intellisec.de/research/harp.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1417. <h><a href=https://openreview.net/pdf?id=XGagtiJ8XC>Multi-level Protein Structure Pre-training via Prompt Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zeyuan Wang, Qiang Zhang, Shuang-Wei HU, Haoran Yu, Xurui Jin, Zhichen Gong, Huajun Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XGagtiJ8XC>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XGagtiJ8XC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A protein can focus on different structure levels to implement its functions. Each structure has its own merit and driving forces in describing some specific characteristics, and they cannot replace each other. Most existing function prediction methods take the tertiary structure as input, unintentionally ignoring the other levels of protein structures. Considering protein sequences can determine multi-level structures, in this paper, we aim to realize the comprehensive potential of protein sequences for function prediction. Specifically, we propose a new prompt-guided multi-task pre-training and fine-tuning framework, and the resulting protein model is called PromptProtein. Through the prompt-guided multi-task pre-training, we learn multiple prompt signals to steer the model to focus on different structure levels. We also design a prompt fine-tuning module to provide downstream tasks the on-demand flexibility of utilizing respective levels of structure information. Extensive experiments on function prediction and protein engineering show that PromptProtein outperforms state-of-the-art methods by large margins.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1418. <h><a href=https://openreview.net/pdf?id=sO1QiAftQFv>E3Bind: An End-to-End Equivariant Network for Protein-Ligand Docking</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yangtian Zhang, Huiyu Cai, Chence Shi, Jian Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sO1QiAftQFv>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An end-to-end equivariant framework for protein-ligand docking through iterative coordinate refinement with careful consideration of the geometric constraints in docking and the local context of the binding site.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sO1QiAftQFv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In silico prediction of the ligand binding pose to a given protein target is a crucial but challenging task in drug discovery.
This work focuses on blind flexible self-docking, where we aim to predict the positions, orientations and conformations of docked molecules. Traditional physics-based methods usually suffer from inaccurate scoring functions and high inference cost. Recently, data-driven methods based on deep learning techniques are attracting growing interest thanks to their efficiency during inference and promising performance. These methods usually either adopt a two-stage approach by first predicting the distances between proteins and ligands and then generating the final coordinates based on the predicted distances, or directly predicting the global roto-translation of ligands. In this paper, we take a different route. Inspired by the resounding success of AlphaFold2 for protein structure prediction, we propose E3Bind, an end-to-end equivariant network that iteratively updates the ligand pose. E3Bind models the protein-ligand interaction through careful consideration of the geometric constraints in docking and the local context of the binding site. Experiments on standard benchmark datasets demonstrate the superior performance of our end-to-end trainable model compared to traditional and recently-proposed deep learning methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1419. <h><a href=https://openreview.net/pdf?id=gNI4_85Cyve>QAID: Question Answering Inspired Few-shot Intent Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Asaf Yehudai, Matan Vetzler, Yosi Mass, Koren Lazar, Doron Cohen, Boaz Carmeli</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gNI4_85Cyve>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our method achieve SOTA results on few-shot intent detection by combining Question-Answering architecture, Contrastive Learning techniques and use of the intent name as answer. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gNI4_85Cyve.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Intent detection with semantically similar fine-grained intents is a challenging task. To address it, we reformulate intent detection as a question-answering retrieval task by treating utterances and intent names as questions and answers. To that end, we utilize a question-answering retrieval architecture and adopt a two stages training schema with batch contrastive loss. In the pre-training stage, we improve query representations through self-supervised training. Then, in the fine-tuning stage, we increase contextualized token-level similarity scores between queries and answers from the same intent. Our results on three few-shot intent detection benchmarks achieve state-of-the-art performance.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1420. <h><a href=https://openreview.net/pdf?id=qV_M_rhYajc>Spacetime Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marc T. Law, James Lucas</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qV_M_rhYajc>5.75 - [6, 3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Representation of directed graphs by exploiting the causal structure of spacetimes via Lorentzian pre-length spaces
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qV_M_rhYajc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Much of the data we encounter in the real world can be represented as directed graphs. In this work, we introduce a general family of representations for directed graphs through connected time-oriented Lorentz manifolds, called &#34;spacetimes&#34; in general relativity. Spacetimes intrinsically contain a causal structure that indicates whether or not there exists a causal or even chronological order between points of the manifold, called events. This chronological order allows us to naturally represent directed edges via imposing the correct ordering when the nodes are embedded as events in the spacetime. Previous work in machine learning only considers embeddings lying on the simplest Lorentz manifold or does not exploit the connection between Lorentzian pre-length spaces and directed graphs. We introduce a well-defined approach to map data onto a general family of spacetimes.  We empirically evaluate our framework in the tasks of hierarchy extraction of undirected graphs, directed link prediction and representation of directed graphs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1421. <h><a href=https://openreview.net/pdf?id=VbCMhg7MRmj>Protein Representation Learning via Knowledge Enhanced Primary Structure Reasoning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hong-Yu Zhou, Yunxiang Fu, Zhicheng Zhang, Bian Cheng, Yizhou Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VbCMhg7MRmj>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We perform protein knowledge encoding by learning to exploit knowledge graphs for protein primary structure reasoning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VbCMhg7MRmj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Protein representation learning has primarily benefited from the remarkable development of language models (LMs). Accordingly, pre-trained protein models also suffer from a problem in LMs: a lack of factual knowledge. The recent solution models the relationships between protein and associated knowledge terms as the knowledge encoding objective. However, it fails to explore the relationships at a more granular level, i.e., the token level. To mitigate this, we propose Knowledge-exploited Auto-encoder for Protein (KeAP), which performs token-level knowledge graph exploration for protein representation learning. In practice, non-masked amino acids iteratively query the associated knowledge tokens to extract and integrate helpful information for restoring masked amino acids via attention. We show that KeAP can consistently outperform the previous counterpart on 9 representative downstream applications, sometimes surpassing it by large margins. These results suggest that KeAP provides an alternative yet effective way to perform knowledge enhanced protein representation learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1422. <h><a href=https://openreview.net/pdf?id=Q-WfHzmiG9m>Re-weighting Based Group Fairness Regularization via Classwise Robust Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sangwon Jung, Taeeon Park, Sanghyuk Chun, Taesup Moon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Q-WfHzmiG9m>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Q-WfHzmiG9m.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many existing group fairness-aware training methods aim to achieve the group fairness by either re-weighting underrepresented groups based on certain rules or using weakly approximated surrogates for the fairness metrics in the objective as regularization terms. Although each of the learning schemes has its own strength in terms of applicability or performance, respectively, it is difficult for any method in the either category to be considered as a gold standard since their successful performances are typically limited to specific cases. To that end, we propose a principled method, dubbed as FairDRO, which unifies the two learning schemes by incorporating a well-justified group fairness metric into the training objective using a classwise distributionally robust optimization (DRO) framework. We then develop an iterative optimization algorithm that minimizes the resulting objective by automatically producing the correct re-weights for each group. Our experiments show that FairDRO is scalable and easily adaptable to diverse applications, and consistently achieves the state-of-the-art performance on several benchmark datasets in terms of the accuracy-fairness trade-off, compared to recent strong baselines. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1423. <h><a href=https://openreview.net/pdf?id=uagC-X9XMi8>Are More Layers Beneficial to Graph Transformers?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haiteng Zhao, Shuming Ma, Dongdong Zhang, Zhi-Hong Deng, Furu Wei</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=uagC-X9XMi8>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We analyze and solve the depth bottleneck of graph transformers from the perspective of attention capacity.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/uagC-X9XMi8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite that going deep has proven successful in many neural architectures, the existing graph transformers are relatively shallow. In this work, we explore whether more layers are beneficial to graph transformers, and find that current graph transformers suffer from the bottleneck of improving performance by increasing depth. Our further analysis reveals the reason is that deep graph transformers are limited by the vanishing capacity of global attention, restricting the graph transformer from focusing on the critical substructure and obtaining expressive features. To this end, we propose a novel graph transformer model named DeepGraph that explicitly employs substructure tokens in the encoded representation, and applies local attention on related nodes to obtain substructure based attention encoding. Our model enhances the ability of the global attention to focus on substructures and promotes the expressiveness of the representations, addressing the limitation of self-attention as the graph transformer deepens. Experiments show that our method unblocks the depth limitation of graph transformers and results in state-of-the-art performance across various graph benchmarks with deeper models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1424. <h><a href=https://openreview.net/pdf?id=5MkYIYCbva>Long Range Language Modeling via Gated State Spaces</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Harsh Mehta, Ankit Gupta, Ashok Cutkosky, Behnam Neyshabur</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5MkYIYCbva>5.75 - [6, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Explore and improve state space model family on long range language modeling tasks
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5MkYIYCbva.jpg loading="lazy">
                </div>
                <div class="abstract">
                    State space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named \textit{Gated State Space} (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1425. <h><a href=https://openreview.net/pdf?id=lUpjsrKItz4>Unsupervised Manifold Alignment with Joint Multidimensional Scaling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dexiong Chen, Bowen Fan, Carlos Oliver, Karsten Borgwardt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lUpjsrKItz4>5.75 - [6, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel approach for unsupervised manifold alignment that only requires intra-domain pairwise dissimilarities as input.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lUpjsrKItz4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce Joint Multidimensional Scaling, a novel approach for unsupervised manifold alignment, which maps datasets from two different domains, without any known correspondences between data instances across the datasets, to a common low-dimensional Euclidean space. Our approach integrates Multidimensional Scaling (MDS) and Wasserstein Procrustes analysis into a joint optimization problem to simultaneously generate isometric embeddings of data and learn correspondences between instances from two different datasets, while only requiring intra-dataset pairwise dissimilarities as input. This unique characteristic makes our approach applicable to datasets without access to the input features, such as solving the inexact graph matching problem. We propose an alternating optimization scheme to solve the problem that can fully benefit from the optimization techniques for MDS and Wasserstein Procrustes. We demonstrate the effectiveness of our approach in several applications, including joint visualization of two datasets, unsupervised heterogeneous domain adaptation, graph matching, and protein structure alignment. The implementation of our work is available at https://github.com/BorgwardtLab/JointMDS.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1426. <h><a href=https://openreview.net/pdf?id=DrtSx1z40Ib>Composing Task Knowledge With Modular Successor Feature Approximators</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wilka Torrico Carvalho, Angelos Filos, Richard Lewis, Honglak Lee, Satinder Singh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=DrtSx1z40Ib>5.75 - [3, 8, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A modular neural network for discovering, composing, and transferring predictive knowledge and behavior via Successor Features &amp; Generalized Policy Improvement.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/DrtSx1z40Ib.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, the Successor Features and Generalized Policy Improvement (SF&amp;GPI) framework has been proposed as a method for learning, composing and transferring predictive knowledge and behavior. SF&amp;GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, “Modular Successor Feature Approximators” (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and a modular network that discovers factored state representations.


                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1427. <h><a href=https://openreview.net/pdf?id=LE5LxBgjB4V>Disentangling the Mechanisms Behind Implicit Regularization in SGD</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zachary Novack, Simran Kaur, Tanya Marwah, Saurabh Garg, Zachary Chase Lipton</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=LE5LxBgjB4V>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/LE5LxBgjB4V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A number of competing hypotheses have been proposed to explain why small-batch Stochastic Gradient Descent (SGD) leads to improved generalization over the full-batch regime, with recent work crediting the implicit regularization of various quantities throughout training. However, to date, empirical evidence assessing the explanatory power of these hypotheses is lacking. In this paper, we conduct an extensive empirical evaluation, focusing on the ability of various theorized mechanisms to close the small-to-large batch generalization gap. Additionally, we characterize how the quantities that SGD has been claimed to (implicitly) regularize change over the course of training. By using micro-batches, i.e. disjoint smaller subsets of each mini-batch, we empirically show that explicitly penalizing the gradient norm or the Fisher Information Matrix trace, averaged over micro-batches, in the large-batch regime recovers small-batch SGD generalization, whereas Jacobian-based regularizations fail to do so. This generalization performance is shown to often be correlated with how well the regularized model’s gradient norms resemble those of small-batch SGD. We additionally show that this behavior breaks down as the micro-batch size approaches the batch size. Finally, we note that in this line of inquiry, positive experimental findings on CIFAR10 are often reversed on other datasets like CIFAR100, highlighting the need to test hypotheses on a wider collection of datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1428. <h><a href=https://openreview.net/pdf?id=azCKuYyS74>What Do Self-Supervised Vision Transformers Learn?</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, Sangdoo Yun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=azCKuYyS74>5.75 - [6, 8, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that (1) CL primarily captures global patterns compared with MIM, (2) CL is more shape-oriented whereas MIM is more texture-oriented, and (3) CL plays a key role in the later layers while MIM focuses on the early layers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/azCKuYyS74.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role in the later layers, while MIM mainly focuses on the early layers. Upon these analyses, we find that CL and MIM can complement each other and observe that even the simplest harmonization can help leverage the advantages of both methods. The code is available at https://github.com/naver-ai/cl-vs-mim.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1429. <h><a href=https://openreview.net/pdf?id=flap0Bo6TK_>Graph Neural Network-Inspired Kernels for Gaussian Processes in Semi-Supervised Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zehao Niu, Mihai Anitescu, Jie Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=flap0Bo6TK_>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Graph-based Gaussian process kernels are developed based on graph neural networks, showing competitive semi-supervised learning performance and timing advantage.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/flap0Bo6TK_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Gaussian processes (GPs) are an attractive class of machine learning models because of their simplicity and flexibility as building blocks of more complex Bayesian models. Meanwhile, graph neural networks (GNNs) emerged recently as a promising class of models for graph-structured data in semi-supervised learning and beyond. Their competitive performance is often attributed to a proper capturing of the graph inductive bias. In this work, we introduce this inductive bias into GPs to improve their predictive performance for graph-structured data. We show that a prominent example of GNNs, the graph convolutional network, is equivalent to some GP when its layers are infinitely wide; and we analyze the kernel universality and the limiting behavior in depth. We further present a programmable procedure to compose covariance kernels inspired by this equivalence and derive example kernels corresponding to several interesting members of the GNN family. We also propose a computationally efficient approximation of the covariance matrix for scalable posterior inference with large-scale data. We demonstrate that these graph-based kernels lead to competitive classification and regression performance, as well as advantages in computation time, compared with the respective GNNs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1430. <h><a href=https://openreview.net/pdf?id=zaq4LV55xHl>On Pre-training Language Model for Antibody</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Danqing Wang, Fei YE, Hao Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zaq4LV55xHl>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zaq4LV55xHl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Antibodies are vital proteins offering robust protection for the human body from pathogens. The development of general protein and antibody-specific pre-trained language models both facilitate antibody prediction tasks. However, there have been limited studies that comprehensively explore the representation capability of distinct pre-trained language models on different antibody tasks. To investigate the problem, we aim to answer several key questions in this paper, such as how pre-trained language models perform in antibody tasks with different specificity and how introducing specific biological mechanisms to the pre-training process can benefit the model. Additionally, we evaluate if the learned antibody pre-trained representations can be applied to real-world antibody problems, like drug discovery and immune process understanding. Previously, no benchmark available largely hindered the study to answer these questions. To aid in our investigation, we provide an AnTibody Understanding Evaluation (ATUE) benchmark. We comprehensively evaluate the performance of protein pre-trained language models by empirical study along with conclusions and new insights. Our ATUE and code are released at https://github.com/dqwang122/EATLM.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1431. <h><a href=https://openreview.net/pdf?id=QZfdDpTX1uM>Meta Temporal Point Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wonho Bae, Mohamed Osama Ahmed, Frederick Tung, Gabriel L. Oliveira</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QZfdDpTX1uM>5.75 - [8, 6, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel approach to train temporal point processes in a meta learning framework.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QZfdDpTX1uM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A temporal point process (TPP) is a stochastic process where its realization is a sequence of discrete events in time. Recent work in TPPs model the process using a neural network in a supervised learning framework, where a training set is a collection of all the sequences. In this work, we propose to train TPPs in a meta learning framework, where each sequence is treated as a different task, via a novel framing of TPPs as neural processes (NPs). We introduce context sets to model TPPs as an instantiation of NPs. Motivated by attentive NP, we also introduce local history matching to help learn more informative features. We demonstrate the potential of the proposed method on popular public benchmark datasets and tasks, and compare with state-of-the-art TPP methods.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1432. <h><a href=https://openreview.net/pdf?id=SNwH0dDGl7_>Near-Optimal Deployment Efficiency in Reward-Free Reinforcement Learning with Linear Function Approximation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dan Qiao, Yu-Xiang Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SNwH0dDGl7_>5.75 - [6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design algorithms for reward free RL under linear MDP with near-optimal deployment complexity and sample complexity.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SNwH0dDGl7_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the problem of deployment efficient reinforcement learning (RL) with linear function approximation under the \emph{reward-free} exploration setting. This is a well-motivated problem because deploying new policies is costly in real-life RL applications. Under the linear MDP setting with feature dimension $d$ and planning horizon $H$, we propose a new algorithm that collects at most $\widetilde{O}(\frac{d^2H^5}{\epsilon^2})$ trajectories within $H$ deployments to identify $\epsilon$-optimal policy for any (possibly data-dependent) choice of reward functions. To the best of our knowledge, our approach is the first to achieve optimal deployment complexity and optimal $d$ dependence in sample complexity at the same time, even if the reward is known ahead of time. Our novel techniques include an exploration-preserving policy discretization and a generalized G-optimal experiment design, which could be of independent interest. Lastly, we analyze the related problem of regret minimization in low-adaptive RL and provide information-theoretic lower bounds for switching cost and batch complexity.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1433. <h><a href=https://openreview.net/pdf?id=MxvHVNukama>On The Inadequacy of Optimizing Alignment and Uniformity in Contrastive Learning of Sentence Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhijie Nie, Richong Zhang, Yongyi Mao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MxvHVNukama>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MxvHVNukama.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contrastive learning is widely used in areas such as visual representation learning (VRL) and sentence representation learning (SRL). Considering the differences between VRL and SRL in terms of negative sample size and evaluation focus, we believe that the solid findings obtained in VRL may not be entirely carried over to SRL. In this work, we consider the suitability of the decoupled form of contrastive loss, i.e., alignment and uniformity, in SRL. We find a performance gap between sentence representations obtained by jointly optimizing alignment and uniformity on the STS task and those obtained using contrastive loss. Further, we find that the joint optimization of alignment and uniformity during training is prone to overfitting, which does not occur on the contrastive loss. Analyzing them based on the variation of the gradient norms, we find that there is a property of ``gradient dissipation&#39;&#39; in contrastive loss and believe that it is the key to preventing overfitting. We simulate similar &#34;gradient dissipation&#34; of contrastive loss on four optimization objectives of two forms, and achieve the same or even better performance than contrastive loss on the STS tasks, confirming our hypothesis.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1434. <h><a href=https://openreview.net/pdf?id=OPGy07PojsZ>Rethinking Symbolic Regression: Morphology and Adaptability in the Context of Evolutionary Algorithms</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kei Sen Fong, Shelvia Wongso, Mehul Motani</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=OPGy07PojsZ>5.75 - [3, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/OPGy07PojsZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Symbolic Regression (SR) is the well-studied problem of finding closed-form analytical expressions that describe the relationship between variables in a measurement dataset. In this paper, we rethink SR from two perspectives: morphology and adaptability. Morphology: Current SR algorithms typically use several man-made heuristics to influence the morphology (or structure) of the expressions in the search space. These man-made heuristics may introduce unintentional bias and data leakage, especially with the relatively few equation-recovery benchmark problems available for evaluating SR approaches. To address this, we formulate a novel minimalistic approach, based on constructing a depth-aware mathematical language model trained on terminal walks of expression trees, as a replacement to these heuristics. Adaptability: Current SR algorithms tend to select expressions based on only a single fitness function (e.g., MSE on the training set). We promote the use of an adaptability framework in evolutionary SR which uses fitness functions that alternate across generations. This leads to robust expressions that perform well on the training set and are close to the true functional form. We demonstrate this by alternating fitness functions that quantify faithfulness to values (via MSE) and empirical derivatives (via a novel theoretically justified fitness metric coined MSEDI). Proof-of-concept: We combine these ideas into a minimalistic evolutionary SR algorithm that outperforms all benchmark and state of-the-art SR algorithms in problems with unknown constants added, which we claim are more reflective of SR performance for real-world applications. Our claim is then strengthened by reproducing the superior performance on real-world regression datasets from SRBench. For researchers interested in equation-recovery problems, we also propose a set of conventions that can be used to promote fairness in comparison across SR methods and to reduce unintentional bias.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1435. <h><a href=https://openreview.net/pdf?id=9X-hgLDLYkQ>Learning Hierarchical Protein Representations via Complete 3D Graph Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Limei Wang, Haoran Liu, Yi Liu, Jerry Kurtin, Shuiwang Ji</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9X-hgLDLYkQ>5.75 - [3, 6, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9X-hgLDLYkQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider representation learning for proteins with 3D structures. We build 3D graphs based on protein structures and develop graph networks to learn their representations. Depending on the levels of details that we wish to capture, protein representations can be computed at different levels, \emph{e.g.}, the amino acid, backbone, or all-atom levels. Importantly, there exist hierarchical relations among different levels. In this work, we propose to develop a novel hierarchical graph network, known as ProNet, to capture the relations. Our ProNet is very flexible and can be used to compute protein representations at different levels of granularity. By treating each amino acid as a node in graph modeling as well as harnessing the inherent hierarchies, our ProNet is more effective and efficient than existing methods. We also show that, given a base 3D graph network that is complete, our ProNet representations are also complete at all levels. Experimental results show that ProNet outperforms recent methods on most datasets. In addition, results indicate that different downstream tasks may require representations at different levels. Our code is publicly available as part of the DIG library (\url{https://github.com/divelab/DIG}).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1436. <h><a href=https://openreview.net/pdf?id=xI1ZTtVOtlz>Evidential Uncertainty and Diversity Guided Active Learning for Scene Graph Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuzhou Sun, Shuaifeng Zhi, Janne Heikkilä, Li Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=xI1ZTtVOtlz>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We proposed an Active Learning framework for the Scene Graph Generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/xI1ZTtVOtlz.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Scene Graph Generation (SGG) has already shown its great potential in various downstream tasks, but it comes at the price of a prohibitively expensive annotation process. To reduce the annotation cost, we propose using Active Learning (AL) for sampling the most informative data. However, directly porting current AL methods to the SGG task poses the following challenges: 1) unreliable uncertainty estimates, and 2) data bias problems. To deal with these challenges, we propose EDAL (\textbf{E}vidential Uncertainty and \textbf{D}iversity Guided Deep \textbf{A}ctive \textbf{L}earning), a novel AL framework tailored for the SGG task. For challenge 1), we start with Evidential Deep Learning (EDL) coupled with a global relationship mining approach to estimate uncertainty, which can effectively overcome the perturbations of open-set relationships and background-relationships to obtain reliable uncertainty estimates. To address challenge 2), we seek the diversity-based method and design the Context Blocking Module (CBM) and Image Blocking Module (IBM) to alleviate context-level bias and image-level bias, respectively. Experiments show that our AL framework can approach the performance of a fully supervised SGG model with only about $10\%$ annotation cost. Furthermore, our ablation studies indicate that introducing AL into the SGG will face many challenges not observed in other vision tasks that are successfully overcome by our new modules. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1437. <h><a href=https://openreview.net/pdf?id=XSEBx0iSjFQ>Re-Imagen: Retrieval-Augmented Text-to-Image Generator</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenhu Chen, Hexiang Hu, Chitwan Saharia, William W. Cohen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=XSEBx0iSjFQ>5.75 - [6, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A text-to-image generation model that can retrieve from external knowledge base to generate more faithful images.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/XSEBx0iSjFQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. Though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as `Chortai (dog)&#39; or `Picarones (food)&#39;. To tackle this issue, we present the Retrieval-Augmented Text-to-Image Generator (Re-Imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. Given a text prompt, Re-Imagen accesses an external multi-modal knowledge base to retrieve relevant (image, text) pairs, and uses them as references to generate the image. With this retrieval step, Re-Imagen is augmented with the knowledge of high-level semantics and low-level visual details of the mentioned entities, and thus improves its accuracy in generating the entities&#39; visual appearances. We train Re-Imagen on a constructed dataset containing (image,text,retrieval) triples to teach the model to ground on both text prompt and retrieval. Furthermore, we develop a new sampling strategy to interleave the classifier-free guidance for text and retrieval condition to balance the text and retrieval alignment. Re-Imagen achieves new SoTA FID results on two image generation benchmarks, such as COCO (\ie, FID = 5.25) and WikiImage (\ie, FID = 5.82) without fine-tuning. To further evaluate the capabilities of the model, we introduce EntityDrawBench, a new benchmark that evaluates image generation for diverse entities, from frequent to rare, across multiple visual domains. Human evaluation on EntityDrawBench shows that Re-Imagen performs on par with the best prior models in photo-realism, but with significantly better real-world faithfulness, especially on less frequent entities. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1438. <h><a href=https://openreview.net/pdf?id=yKbprarjc5B>Leveraging Large Language Models for Multiple Choice Question Answering</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Joshua Robinson, David Wingate</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yKbprarjc5B>5.75 - [5, 5, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Large language models that can effectively associate multiple choice answer options with symbols can be prompted in a way that yields dramatically improved performance on multiple choice question answering tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yKbprarjc5B.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., “A”) associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1439. <h><a href=https://openreview.net/pdf?id=JZMR727O29>Backpropagation through Combinatorial Algorithms: Identity with Projection Works</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Subham Sekhar Sahoo, Anselm Paulus, Marin Vlastelica, Vít Musil, Volodymyr Kuleshov, Georg Martius</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JZMR727O29>5.75 - [8, 6, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a simple alternative for differentiating through combinatorial solvers with linear objectives, that is on par with SoTA, has no hyperparameters, and is more robust to perturbations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JZMR727O29.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Embedding discrete solvers as differentiable layers has given modern deep learning architectures combinatorial expressivity and discrete reasoning capabilities. The derivative of these solvers is zero or undefined, therefore a meaningful replacement is crucial for effective gradient-based learning. Prior works rely on smoothing the solver with input perturbations, relaxing the solver to continuous problems, or interpolating the loss landscape with techniques that typically require additional solver calls, introduce extra hyper-parameters, or compromise performance. We propose a principled approach to exploit the geometry of the discrete solution space to treat the solver as a negative identity on the backward pass and further provide a theoretical justification. Our experiments demonstrate that such a straightforward hyper-parameter-free approach is able to compete with previous more complex methods on numerous experiments such as backpropagation through discrete samplers, deep graph matching, and image retrieval. Furthermore, we substitute the previously proposed problem-specific and label-dependent margin with a generic regularization procedure that prevents cost collapse and increases robustness.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1440. <h><a href=https://openreview.net/pdf?id=bAMTaeqluh4>Part-Based Models Improve Adversarial Robustness</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chawin Sitawarin, Kornrapat Pongmala, Yizheng Chen, Nicholas Carlini, David Wagner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bAMTaeqluh4>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Using an auxiliary task and richer annotation in the form of part segmentation can improve robustness of neural networks by a large margin.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bAMTaeqluh4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We show that combining human prior knowledge with end-to-end learning can improve the robustness of deep neural networks by introducing a part-based model for object classification. We believe that the richer form of annotation helps guide neural networks to learn more robust features without requiring more samples or larger models. Our model combines a part segmentation model with a tiny classifier and is trained end-to-end to simultaneously segment objects into parts and then classify the segmented object. Empirically, our part-based models achieve both higher accuracy and higher adversarial robustness than a ResNet-50 baseline on all three datasets. For instance, the clean accuracy of our part models is up to 15 percentage points higher than the baseline&#39;s, given the same level of robustness. Our experiments indicate that these models also reduce texture bias and yield better robustness against common corruptions and spurious correlations. The code is publicly available at https://github.com/chawins/adv-part-model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1441. <h><a href=https://openreview.net/pdf?id=39z0zPZ0AvB>Don’t forget the nullspace! Nullspace occupancy as a mechanism for out of distribution failure</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daksh Idnani, Vivek Madan, Naman Goyal, David J. Schwab, Shanmukha Ramakrishna Vedantam</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=39z0zPZ0AvB>5.75 - [5, 8, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/39z0zPZ0AvB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Out of distribution (OoD) generalization has received considerable interest in recent years. In this work, we identify a particular failure mode of OoD generalization for discriminative classifiers that is based on test data (from a new domain) lying in the nullspace of features learnt from source data. We demonstrate the existence of this failure mode across multiple networks trained across RotatedMNIST, PACS, TerraIncognita, DomainNet and ImageNet-R datasets. We then study different choices for characterizing the feature space and show that projecting intermediate representations onto the span of directions that obtain maximum training accuracy provides consistent improvements in OoD performance. Finally, we show that such nullspace behavior also provides an insight into neural networks trained on poisoned data. We hope our work galvanizes interest in the relationship between the nullspace occupancy failure mode and generalization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1442. <h><a href=https://openreview.net/pdf?id=mQpmZVzXK1h>Latent Variable Representation for Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tongzheng Ren, Chenjun Xiao, Tianjun Zhang, Na Li, Zhaoran Wang, sujay sanghavi, Dale Schuurmans, Bo Dai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mQpmZVzXK1h>5.75 - [6, 8, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show how the latent variable model can be used for representation learning in reinforcement learning, that can have superior empirical performance as well as complete sample complexity analysis.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mQpmZVzXK1h.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep latent variable models have achieved significant empirical successes in model-based reinforcement learning (RL) due to their expressiveness in modeling complex transition dynamics. On the other hand, it remains unclear theoretically and empirically how latent variable models may facilitate learning, planning, and exploration to improve the sample efficiency of RL. In this paper, we provide a representation view of the latent variable models for state-action value functions, which allows both tractable variational learning algorithm and effective implementation of the optimism/pessimism principle in the face of uncertainty for exploration. In particular, we propose a computationally efficient planning algorithm with UCB exploration by incorporating kernel embeddings of latent variable models. Theoretically, we establish the sample complexity of the proposed approach in the online and offline settings. Empirically, we demonstrate superior performance over current state-of-the-art algorithms across various benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1443. <h><a href=https://openreview.net/pdf?id=bcYZwYo-0t>Decoupled Training for Long-Tailed  Classification With Stochastic Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Giung Nam, Sunguk Jang, Juho Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bcYZwYo-0t>5.75 - [8, 5, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel classifier re-training algorithm for long-tailed classification.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bcYZwYo-0t.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Decoupling representation learning and classifier learning has been shown to be effective in classification with long-tailed data. There are two main ingredients in constructing a decoupled learning scheme; 1) how to train the feature extractor for representation learning so that it provides generalizable representations and 2) how to re-train the classifier that constructs proper decision boundaries by handling class imbalances in long-tailed data. In this work, we first apply Stochastic Weight Averaging (SWA), an optimization technique for improving the generalization of deep neural networks, to obtain better generalizing feature extractors for long-tailed classification. We then propose a novel classifier re-training algorithm based on stochastic representation obtained from the SWA-Gaussian, a Gaussian perturbed SWA, and a self-distillation strategy that can harness the diverse stochastic representations based on uncertainty estimates to build more robust classifiers. Extensive experiments on CIFAR10/100-LT, ImageNet-LT, and iNaturalist-2018 benchmarks show that our proposed method improves upon previous methods both in terms of prediction accuracy and uncertainty estimation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1444. <h><a href=https://openreview.net/pdf?id=9vcXCMp9VEp>Fair Attribute Completion on Graph with Missing Attributes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dongliang Guo, Zhixuan Chu, Sheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9vcXCMp9VEp>5.75 - [5, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9vcXCMp9VEp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Tackling unfairness in graph learning models is a challenging task, as the unfairness issues on graphs involve both attributes and topological structures. Existing work on fair graph learning simply assumes that attributes of all nodes are available for model training and then makes fair predictions. In practice, however, the attributes of some nodes might not be accessible due to missing data or privacy concerns, which makes fair graph learning even more challenging. In this paper, we propose FairAC, a fair attribute completion method, to complement missing information and learn fair node embeddings for graphs with missing attributes. FairAC adopts an attention mechanism to deal with the attribute missing problem and meanwhile, it mitigates two types of unfairness, i.e., feature unfairness from attributes and topological unfairness due to attribute completion. FairAC can work on various types of homogeneous graphs and generate fair embeddings for them and thus can be applied to most downstream tasks to improve their fairness performance. To our best knowledge, FairAC is the first method that jointly addresses the graph attribution completion and graph unfairness problems. Experimental results on benchmark datasets show that our method achieves better fairness performance with less sacrifice in accuracy, compared with the state-of-the-art methods of fair graph learning. Code is available at: https://github.com/donglgcn/FairAC.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1445. <h><a href=https://openreview.net/pdf?id=VzwfoFyYDga>Machine Unlearning of Federated Clusters</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chao Pan, Jin Sima, Saurav Prakash, Vishal Rana, Olgica Milenkovic</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VzwfoFyYDga>5.75 - [8, 3, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the first known unlearning mechanism for federated clustering with privacy criteria that support simple, provable, and efficient data removal at the client and server level.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VzwfoFyYDga.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated clustering (FC) is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems. With the adoption of recent laws ensuring the &#34;right to be forgotten&#34;, the problem of machine unlearning for FC methods has become of significant importance. We introduce, for the first time, the problem of machine unlearning for FC, and propose an efficient unlearning mechanism for a customized secure FC framework. Our FC framework utilizes special initialization procedures that we show are well-suited for unlearning. To protect client data privacy, we develop the secure compressed multiset aggregation (SCMA) framework that addresses sparse secure federated learning (FL) problems encountered during clustering as well as more general problems. To simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into our SCMA pipeline, and prove that the client communication cost is logarithmic in the vector dimension. Additionally, to demonstrate the benefits of our unlearning mechanism over complete retraining, we provide a theoretical analysis for the unlearning performance of our approach. Simulation results show that the new FC framework exhibits superior clustering performance compared to previously reported FC baselines when the cluster sizes are highly imbalanced. Compared to completely retraining K-means++ locally and globally for each removal request, our unlearning procedure offers an average speed-up of roughly 84x across seven datasets. Our implementation for the proposed method is available at https://github.com/thupchnsky/mufc.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1446. <h><a href=https://openreview.net/pdf?id=-EHqoysUYLx>Generalization Bounds for Federated Learning: Fast Rates, Unparticipating Clients and Unbounded Losses</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiaolin Hu, Shaojie Li, Yong Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-EHqoysUYLx>5.71 - [6, 5, 8, 6, 6, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-EHqoysUYLx.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In {federated learning}, the underlying data distributions may be different across clients. This paper provides a theoretical analysis of generalization error of {federated learning}, which captures both heterogeneity and relatedness of the distributions. In particular, we assume that the heterogeneous distributions are sampled from a meta-distribution. In this two-level distribution framework, we characterize the generalization error not only for clients participating in the training but also for unparticipating clients. We first show that the generalization error for unparticipating clients can be bounded by participating generalization error and participating gap caused by clients&#39; sampling. We further establish fast learning bounds of order $\mathcal{O}(\frac{1}{mn} + \frac{1}{m})$ for unparticipating clients, where $m$ is the number of clients and $n$ is the sample size at each client. To our knowledge, the obtained fast bounds are state-of-the-art in the two-level distribution framework. Moreover, previous theoretical results mostly require the loss function to be bounded. We derive convergence bounds of order $\mathcal{O}(\frac{1}{\sqrt{mn}} + \frac{1}{\sqrt{m}})$ under unbounded assumptions, including sub-exponential and sub-Weibull losses. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1447. <h><a href=https://openreview.net/pdf?id=cRxYWKiTan>Better Generative Replay for Continual Federated Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daiqing Qi, Handong Zhao, Sheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cRxYWKiTan>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> introduce a new continual federated leanring setting with generative replay and solve an important technical problem to make it work
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cRxYWKiTan.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated Learning (FL) aims to develop a centralized server that learns from distributed clients via communications without accessing the clients’ local data. However, existing works mainly focus on federated learning in a single task sce- nario with static data. In this paper, we introduce the continual federated learning (CFL) problem, where clients incrementally learn new tasks and history data can- not be stored due to certain reasons, such as limited storage and data retention policy 1. Generative replay (GR) based methods are effective for continual learning without storing history data. However, we fail when trying to intuitively adapt GR models for this setting. By analyzing the behaviors of clients during training, we find the unstable training process caused by distributed training on non-IID data leads to a notable performance degradation. To address this problem, we propose our FedCIL model with two simple but effective solutions: 1. model consolidation and 2. consistency enforcement. Experimental results on multiple benchmark datasets demonstrate that our method significantly outperforms baselines. Code is available at: https://github.com/daiqing98/FedCIL.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1448. <h><a href=https://openreview.net/pdf?id=Mg5CLXZgvLJ>SpeedyZero: Mastering Atari with Limited Data and Time</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yixuan Mei, Jiaxuan Gao, Weirui Ye, Shaohuai Liu, Yang Gao, Yi Wu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Mg5CLXZgvLJ>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> SpeedyZero is a distributed model-based RL training system based on EfficientZero, featuring fast training speed and high sample efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Mg5CLXZgvLJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Many recent breakthroughs of deep reinforcement learning (RL) are mainly built upon large-scale distributed training of model-free methods using millions to billions of samples. On the other hand, state-of-the-art model-based RL methods can achieve human-level sample efficiency but often take a much longer over all training time than model-free methods. However, high sample efficiency and fast training time are both important to many real-world applications. We develop SpeedyZero, a distributed RL system built upon a state-of-the-art model-based RL method, EfficientZero, with a dedicated system design for fast distributed computation. We also develop two novel algorithmic techniques, Priority Refresh and Clipped LARS, to stabilize training with massive parallelization and large batch size. SpeedyZero maintains on-par sample efficiency compared with EfficientZero while achieving a 14.5X speedup in wall-clock time, leading to human-level performances on the Atari benchmark within 35 minutes using only 300k samples. In addition, we also present an in-depth analysis on the fundamental challenges in further scaling our system to bring insights to the community.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1449. <h><a href=https://openreview.net/pdf?id=pWVASryOyFw>Learning Uncertainty for Unknown Domains with Zero-Target-Assumption</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yu Yu, Hassan Sajjad, Jia Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pWVASryOyFw>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pWVASryOyFw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce our Maximum-Entropy Rewarded Reinforcement Learning (MERRL) framework that selects training data for more accurate Natural Language Processing (NLP). Because conventional data selection methods select training samples based on the test domain knowledge and not on real life data,  they frequently fail in unknown domains like patent and Twitter. 
Our approach selects training samples that maximize information uncertainty measured by entropy, including observation entropy like empirical Shannon entropy, Min-entropy, R\&#39;enyi entropy, and prediction entropy using mutual information, to cover more possible queries that may appear in unknown worlds. Our MERRL using regularized A2C and SAC achieves up to -99.7 perplexity decrease (-43.4\% relatively) in language modeling, +25.0 accuracy increase (+40.0\% relatively) in sentiment analysis, and +5.0 F1 score increase (+30.8\% relatively) in named entity recognition over various domains, demonstrating strong generalization power on unknown test sets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1450. <h><a href=https://openreview.net/pdf?id=PvDY71zKsvP>Min-Max Multi-objective Bilevel Optimization with Applications in Robust Machine Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Alex Gu, Songtao Lu, Parikshit Ram, Tsui-Wei Weng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PvDY71zKsvP>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study a generic min-max bilevel multi-objective optimization framework with novel theoretical analysis and applications in representation learning and hyperparameter optimization
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PvDY71zKsvP.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider a generic min-max multi-objective bilevel optimization problem with applications in robust machine learning such as representation learning and hyperparameter optimization. We design MORBiT, a novel single-loop gradient descent-ascent bilevel optimization algorithm, to solve the generic problem and present a novel analysis showing that MORBiT converges to the first-order stationary point at a rate of $\widetilde{\mathcal{O}}(n^{1/2} K^{-2/5})$ for a class of weakly convex problems with $n$ objectives upon $K$ iterations of the algorithm. Our analysis utilizes novel results to handle the non-smooth min-max multi-objective setup and to obtain a sublinear dependence in the number of objectives $n$. Experimental results on robust representation learning and robust hyperparameter optimization showcase (i) the advantages of considering the min-max multi-objective setup, and (ii) convergence properties of the proposed \morbit.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1451. <h><a href=https://openreview.net/pdf?id=Kn-HA8DFik>Cross-Level Distillation and Feature Denoising for Cross-Domain Few-Shot Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hao ZHENG, Runqi Wang, Jianzhuang Liu, Asako Kanezaki</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Kn-HA8DFik>5.67 - [3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design a cross-level distillation and a feature denoising operation for handling cross-domain few-shot classification. Our approach can surpass the SOTA method by 5.44% on 1-shot and 1.37% on 5-shot classification tasks in the BSCD-FSL benchmark.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Kn-HA8DFik.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The conventional few-shot classification aims at learning a model on a large labeled base dataset and rapidly adapting to a target dataset that is from the same distribution as the base dataset. However, in practice, the base and the target datasets of few-shot classification are usually from different domains, which is the problem of cross-domain few-shot classification. We tackle this problem by making a small proportion of unlabeled images in the target domain accessible in the training stage. In this setup, even though the base data are sufficient and labeled, the large domain shift still makes transferring the knowledge from the base dataset difficult. We meticulously design a cross-level knowledge distillation method, which can strengthen the ability of the model to extract more discriminative features in the target dataset by guiding the network&#39;s shallow layers to learn higher-level information. Furthermore, in order to alleviate the overfitting in the evaluation stage, we propose a feature denoising operation which can reduce the feature redundancy and mitigate overfitting. Our approach can surpass the previous state-of-the-art method, Dynamic-Distillation, by 5.44% on 1-shot and 1.37% on 5-shot classification tasks on average in the BSCD-FSL benchmark. The implementation code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/CLDFD.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1452. <h><a href=https://openreview.net/pdf?id=MLJ5TF5FtXH>A Multi-Grained Self-Interpretable Symbolic-Neural Model For Single/Multi-Labeled Text Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xiang Hu, XinYu KONG, Kewei Tu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MLJ5TF5FtXH>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> An inherently interpretable model architecture with explicit unsupervised label to constituent alignments.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MLJ5TF5FtXH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks based on layer-stacking architectures have historically suffered from poor inherent interpretability. Meanwhile, symbolic probabilistic models function with clear interpretability, but how to combine them with neural networks to enhance their performance remains to be explored. In this paper, we try to marry these two systems for text classification via a structured language model. We propose a Symbolic-Neural model that can learn to explicitly predict class labels of text spans from a constituency tree without requiring any access to span-level gold labels. As the structured language model learns to predict constituency trees in a self-supervised manner, only raw texts and sentence-level labels are required as training data, which makes it essentially a general constituent-level self-interpretable classification model. Our experiments demonstrate that our approach could achieve good prediction accuracy in downstream tasks. Meanwhile, the predicted span labels are consistent with human rationales to a certain degree.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1453. <h><a href=https://openreview.net/pdf?id=4DU_HCijfJp>Short-Term Memory Convolutions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Grzegorz Stefański, Krzysztof Arendt, Paweł Daniluk, Bartłomiej Jasik, Artur Szumaczuk</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4DU_HCijfJp>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4DU_HCijfJp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The real-time processing of time series signals is a critical issue for many real-life applications. The idea of real-time processing is especially important in audio domain as the human perception of sound is sensitive to any kind of disturbance in perceived signals, especially the lag between auditory and visual modalities. The rise of deep learning (DL) models complicated the landscape of signal processing. Although they often have superior quality compared to standard DSP methods, this advantage is diminished by higher latency. In this work we propose novel method for minimization of inference time latency and memory consumption, called Short-Term Memory Convolution (STMC) and its transposed counterpart. The main advantage of STMC is the low latency comparable to long short-term memory (LSTM) networks. Furthermore, the training of STMC-based models is faster and more stable as the method is based solely on convolutional neural networks (CNNs). In this study we demonstrate an application of this solution to a U-Net model for a speech separation task and GhostNet model in acoustic scene classification (ASC) task. In case of speech separation we achieved a 5-fold reduction in inference time and a 2-fold reduction in latency without affecting the output quality. The inference time for ASC task was up to 4 times faster while preserving the original accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1454. <h><a href=https://openreview.net/pdf?id=CGBCTp2M6lA>Leveraging Future Relationship Reasoning for Vehicle Trajectory Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daehee Park, Hobin Ryu, Yunseo Yang, Jegyeong Cho, Jiwon Kim, Kuk-Jin Yoon</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CGBCTp2M6lA>5.67 - [8, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We defined and modeled Future Relationship to better modeling interaction between vehicles.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CGBCTp2M6lA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Understanding the interaction between multiple agents is crucial for realistic vehicle trajectory prediction. 
Existing methods have attempted to infer the interaction from the observed past trajectories of agents using pooling, attention, or graph-based methods, which rely on a deterministic approach. 
However, these methods can fail under complex road structures, as they cannot predict various interactions that may occur in the future. 
In this paper, we propose a novel approach that uses lane information to predict a stochastic future relationship among agents. 
To obtain a coarse future motion of agents, our method first predicts the probability of lane-level waypoint occupancy of vehicles. 
We then utilize the temporal probability of passing adjacent lanes for each agent pair, assuming that agents passing adjacent lanes will highly interact. 
We also model the interaction using a probabilistic distribution, which allows for multiple possible future interactions. 
The distribution is learned from the posterior distribution of interaction obtained from ground truth future trajectories. 
We validate our method on popular trajectory prediction datasets: nuScenes and Argoverse. 
The results show that the proposed method brings remarkable performance gain in prediction accuracy, and achieves state-of-the-art performance in long-term prediction benchmark dataset.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1455. <h><a href=https://openreview.net/pdf?id=CjTHVo1dvR>Molecular Geometry Pretraining with SE(3)-Invariant Denoising Distance Matching</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shengchao Liu, Hongyu Guo, Jian Tang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CjTHVo1dvR>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose GeoSSL, a self-supervised learning method using the denoising distance matching for molecular goemetry pretraining.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CjTHVo1dvR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Molecular representation pretraining is critical in various applications for drug and material discovery due to the limited number of labeled molecules, and most existing work focuses on pretraining on 2D molecular graphs. However, the power of pretraining on 3D geometric structures has been less explored. This is owing to the difficulty of finding a sufficient proxy task that can empower the pretraining to effectively extract essential features from the geometric structures. Motivated by the dynamic nature of 3D molecules, where the continuous motion of a molecule in the 3D Euclidean space forms a smooth potential energy surface, we propose GeoSSL, a 3D coordinate denoising pretraining framework to model such an energy landscape. Further by leveraging an SE(3)-invariant score matching method, we propose GeoSSL-DDM in which the coordinate denoising proxy task is effectively boiled down to denoising the pairwise atomic distances in a molecule. Our comprehensive experiments confirm the effectiveness and robustness of our proposed method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1456. <h><a href=https://openreview.net/pdf?id=UiaUEICawgw>Learned Index with Dynamic $\epsilon$</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Daoyuan Chen, Wuchao Li, Yaliang Li, Bolin Ding, Kai Zeng, Defu Lian, Jingren Zhou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=UiaUEICawgw>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Based on the theoretically derived prediction error bounds, we propose a mathematically-grounded learned index framework with dynamic $\epsilon$, which is efficient and pluggable to several state-of-the-art learned index methods.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/UiaUEICawgw.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Index structure is a fundamental component in database and facilitates broad data retrieval applications. Recent learned index methods show superior performance by learning hidden yet useful data distribution with the help of machine learning, and provide a guarantee that the prediction error is no more than a pre-defined $\epsilon$. However, existing learned index methods adopt a fixed $\epsilon$ for all the learned segments, neglecting the diverse characteristics of different data localities. In this paper, we propose a mathematically-grounded learned index framework with dynamic $\epsilon$, which is efficient and pluggable to existing learned index methods. We theoretically analyze prediction error bounds that link $\epsilon$ with data characteristics for an illustrative learned index method. Under the guidance of the derived bounds, we learn how to vary $\epsilon$ and improve the index performance with a better space-time trade-off. Experiments with real-world datasets and several state-of-the-art methods demonstrate the efficiency, effectiveness and usability of the proposed framework.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1457. <h><a href=https://openreview.net/pdf?id=CJd-BtnwtXq>A Non-Asymptotic Analysis of Oversmoothing in Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xinyi Wu, Zhengdao Chen, William Wei Wang, Ali Jadbabaie</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CJd-BtnwtXq>5.67 - [3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We precisely characterize the mechanism of overmoothing via a non-asymptotic analysis and answer why oversmoothing happens in shallow GNNs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CJd-BtnwtXq.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Oversmoothing is a central challenge of building more powerful Graph Neural Networks (GNNs). While previous works have only demonstrated that oversmoothing is inevitable when the number of graph convolutions tends to infinity, in this paper, we precisely characterize the mechanism behind the phenomenon via a non-asymptotic analysis. Specifically, we distinguish between two different effects when applying graph convolutions—an undesirable mixing effect that homogenizes node representations in different classes, and a desirable denoising effect that homogenizes node representations in the same class. By quantifying these two effects on random graphs sampled from the Contextual Stochastic Block Model (CSBM), we show that oversmoothing happens once the mixing effect starts to dominate the denoising effect, and the number of layers required for this transition is $O(\log N/\log (\log N))$ for sufficiently dense graphs with $N$ nodes. We also extend our analysis to study the effects of Personalized PageRank (PPR), or equivalently, the effects of initial residual connections on oversmoothing. Our results suggest that while PPR mitigates oversmoothing at deeper layers, PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs. Finally, we support our theoretical results with numerical experiments, which further suggest that the oversmoothing phenomenon observed in practice can be magnified by the difficulty of optimizing deep GNN models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1458. <h><a href=https://openreview.net/pdf?id=vINj_Hv9szL>Benchmarking Constraint Inference in Inverse Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Guiliang Liu, Yudong Luo, Ashish Gaurav, Kasra Rezaee, Pascal Poupart</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=vINj_Hv9szL>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design a benchmark with important applications for Inverse Constrained Reinforcement Learning and propose a variational Bayesian approach for modeling the distribution of constraints.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/vINj_Hv9szL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    When deploying Reinforcement Learning (RL) agents into a physical system, we must ensure that these agents are well aware of the underlying constraints. In many real-world problems, however, the constraints are often hard to specify mathematically and unknown to the RL agents. To tackle these issues, Inverse Constrained Reinforcement Learning (ICRL) empirically estimates constraints from expert demonstrations. As an emerging research topic, ICRL does not have common benchmarks, and previous works tested algorithms under hand-crafted environments with manually-generated expert demonstrations. In this paper, we construct an ICRL benchmark in the context of RL application domains, including robot control, and autonomous driving. For each environment, we design relevant constraints and train expert agents to generate demonstration data. Besides, unlike existing baselines that learn a deterministic constraint, we propose a variational ICRL method to model a posterior distribution of candidate constraints. We conduct extensive experiments on these algorithms under our benchmark and show how they can facilitate studying important research challenges for ICRL. The benchmark, including the instructions for reproducing ICRL algorithms, is available at https://github.com/Guiliang/ICRL-benchmarks-public.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1459. <h><a href=https://openreview.net/pdf?id=aBWnqqsuot7>D4FT: A Deep Learning Approach to Kohn-Sham Density Functional Theory</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianbo Li, Min Lin, Zheyuan Hu, Kunhao Zheng, Giovanni Vignale, Kenji Kawaguchi, A.H. Castro Neto, Kostya S. Novoselov, Shuicheng YAN</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=aBWnqqsuot7>5.67 - [6, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper propose a deep learning approch to solving Kohn-Sham Density Functional Theory.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/aBWnqqsuot7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Kohn-Sham Density Functional Theory (KS-DFT) has been traditionally solved by the Self-Consistent Field (SCF) method. Behind the SCF loop is the physics intuition of solving a system of non-interactive single-electron wave functions under an effective potential. In this work, we propose a deep learning approach to KS-DFT. First, in contrast to the conventional SCF loop, we propose to directly minimize the total energy by reparameterizing the orthogonal constraint as a feed-forward computation. We prove that such an approach has the same expressivity as the SCF method, yet reduces the computational complexity from O(N^4) to O(N^3). Second, the numerical integration which involves a summation over the quadrature grids can be amortized to the optimization steps. At each step, stochastic gradient descent (SGD) is performed with a sampled minibatch of the grids. Extensive experiments are carried out to demonstrate the advantage of our approach in terms of efficiency and stability. In addition, we show that our approach enables us to explore more complex neural-based wave functions. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1460. <h><a href=https://openreview.net/pdf?id=tcbBPnfwxS>OPTQ: Accurate Quantization for Generative Pre-trained Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=tcbBPnfwxS>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show that Generative Pre-trained Transformer (GPT) models can be quantized down to 3-4 bits without significant loss of accuracy, which leads to significant computational and usability improvements. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/tcbBPnfwxS.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Generative Pre-trained Transformer models, known as GPT or OPT, set themselves apart through breakthrough performance across complex language modelling tasks, but also by their extremely high computational and storage costs. Specifically, due to their massive size, even inference for large, highly-accurate GPT models may require multiple performant GPUs, which limits the usability of such models. While there is emerging work on relieving this pressure via model compression, the applicability and performance of existing compression techniques is limited by the scale and complexity of GPT models. In this paper, we address this challenge, and propose OPTQ, a new one-shot weight quantization method based on approximate second-order information, that is both highly-accurate and highly-efficient. Specifically, OPTQ can quantize GPT models with 175 billion parameters in approximately four GPU hours, reducing the bitwidth down to 3 or 4 bits per weight, with negligible accuracy degradation relative to the uncompressed baseline. Our method more than doubles the compression gains relative to previously-proposed one-shot quantization methods, preserving accuracy, allowing us for the first time to execute an 175 billion-parameter model inside a single GPU for generative inference. Moreover, we also show that our method can still provide reasonable accuracy in the extreme quantization regime, in which weights are quantized to 2-bit or even ternary quantization levels.
We show experimentally that these improvements can be leveraged for end-to-end inference speedups over FP16, of around 3.25x when using high-end GPUs (NVIDIA A100) and 4.5x when using more cost-effective ones (NVIDIA A6000). The implementation is available at https://github.com/IST-DASLab/gptq.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1461. <h><a href=https://openreview.net/pdf?id=PQOlkgsBsik>Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, Ge Yu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=PQOlkgsBsik>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper presents Vision-Language Universal Search (VL-UnivSearch), which builds a unified model for multi-modal retrieval, leans universal representations for images and texts, and achieves the state-of-the-art. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/PQOlkgsBsik.jpg loading="lazy">
                </div>
                <div class="abstract">
                    This paper presents Universal Vision-Language Dense Retrieval (UniVL-DR), which builds a unified model for multi-modal retrieval. UniVL-DR encodes queries and multi-modality resources in an embedding space for searching candidates from different modalities. To learn a unified embedding space for multi-modal retrieval, UniVL-DR proposes two techniques: 1) Universal embedding optimization strategy, which contrastively optimizes the embedding space using the modality-balanced hard negatives; 2) Image verbalization method, which bridges the modality gap between images and texts in the raw data space. UniVL-DR achieves the state-of-the-art on the multi-modal open-domain question answering benchmark, WebQA, and outperforms all retrieval models on the two subtasks, text-text retrieval and text-image retrieval. It demonstrates that universal multi-modal search is feasible to replace the divide-and-conquer pipeline with a united model and also benefits single/cross modality tasks. All source codes of this work are available at https://github.com/OpenMatch/UniVL-DR.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1462. <h><a href=https://openreview.net/pdf?id=lEkl0jdSb7B>Any-scale Balanced Samplers for Discrete Space</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haoran Sun, Bo Dai, Charles Sutton, Dale Schuurmans, Hanjun Dai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=lEkl0jdSb7B>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We identify two key issues of existing gradient based locally balanced samplers, and provide improved proposals with adjusted weight function and 2nd order approximation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/lEkl0jdSb7B.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The locally balanced informed proposal has proved to be highly effective for sampling from discrete spaces. However, its success relies on the &#34;local&#39;&#39; factor, which ensures that whenever the proposal distribution is restricted to be near the current state, the locally balanced weight functions are asymptotically optimal and the gradient approximations are accurate.  In seeking a more efficient sampling algorithm, many recent works have considered increasing the scale of the proposal distributions, but this causes the &#34;local&#39;&#39; factor to no longer hold. Instead, we propose any-scale balanced samplers to repair the gap in non-local proposals. In particular, we substitute the locally balanced function with an any-scale balanced function that can self-adjust to achieve better efficiency for proposal distributions at any scale. We also use quadratic approximations to capture curvature of the target distribution and reduce the error in the gradient approximation, while employing a Gaussian integral trick with a special estimated diagonal to efficiently sample from the quadratic proposal distribution. On various synthetic and real distributions, the proposed sampler substantially outperforms existing approaches.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1463. <h><a href=https://openreview.net/pdf?id=9kBCMNb5mc>Optimistic Exploration with Learned Features Provably Solves Markov Decision Processes with Neural Dynamics</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sirui Zheng, Lingxiao Wang, Shuang Qiu, Zuyue Fu, Zhuoran Yang, Csaba Szepesvari, Zhaoran Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9kBCMNb5mc>5.67 - [8, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We identify a class of Markov decision processes with neural network parameterization and propose an oracle-efficient algorithm whose sample complexity does not depend on the Eluder dimension of the NN class.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9kBCMNb5mc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Incorporated with the recent advances in deep learning, deep reinforcement learning (DRL) has achieved tremendous success in empirical study. However, analyzing DRL is still challenging due to the complexity of the neural network class. In this paper, we address such a challenge by analyzing the Markov decision process (MDP) with neural dynamics, which covers several existing models as special cases, including the kernelized nonlinear regulator (KNR) model and the linear MDP. We propose a novel algorithm that designs exploration incentives via learnable representations of the dynamics model by embedding the neural dynamics into a kernel space induced by the system noise. We further establish an upper bound on the sample complexity of the algorithm, which demonstrates the sample efficiency of the algorithm. We highlight that, unlike previous analyses of RL algorithms with function approximation, our bound on the sample complexity does not depend on the Eluder dimension of the neural network class, which is known to be exponentially large (Dong et al., 2021).
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1464. <h><a href=https://openreview.net/pdf?id=-CoNloheTs>An Exact Poly-Time Membership-Queries Algorithm for Extracting a Three-Layer ReLU Network</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Amit Daniely, Elad Granot</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-CoNloheTs>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A first polynomial-time algorithm to extract the parameters and architecture of two- and three-layer neural networks using membership-queries
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-CoNloheTs.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the natural problem of learning a ReLU network from queries, which was recently remotivated by model extraction attacks. In this work, we present a polynomial-time algorithm that can learn a depth-two ReLU network from queries under mild general position assumptions. We also present a polynomial-time algorithm that, under mild general position assumptions, can learn a rich class of depth-three ReLU networks from queries. For instance, it can learn most networks where the number of first layer neurons is smaller than the dimension and the number of second layer neurons.

These two results substantially improve state-of-the-art: Until our work, polynomial-time algorithms were only shown to learn from queries depth-two networks under the assumption that either the underlying distribution is Gaussian (Chen et al. (2021)) or that the weights matrix rows are linearly independent (Milli et al. (2019)). For depth three or more, there were no known poly-time results.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1465. <h><a href=https://openreview.net/pdf?id=bhfp5GlDtGe>Adversarial Imitation Learning with Preferences</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Aleksandar Taranovic, Andras Gabor Kupcsik, Niklas Freymuth, Gerhard Neumann</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bhfp5GlDtGe>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We extend Adversarial Imitation Learning to simultaneously utilize both demonstrations and preferences.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bhfp5GlDtGe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Designing an accurate and explainable reward function for many Reinforcement Learning tasks is a cumbersome and tedious process. 
Instead, learning policies directly from the feedback of human teachers naturally integrates human domain knowledge into the policy optimization process. 
However, different feedback modalities, such as demonstrations and preferences, provide distinct benefits and disadvantages. For example, demonstrations convey a lot of information about the task but are often hard or costly to obtain from real experts while preferences typically contain less information but are in most cases cheap to generate. 
However, existing methods centered around human feedback mostly focus on a single teaching modality, causing them to miss out on important training data while making them less intuitive to use.
In this paper we propose a novel method for policy learning that incorporates two different feedback types, namely \emph{demonstrations} and \emph{preferences}. 
To this end, we make use of the connection between discriminator training and density ratio estimation to incorporate preferences into the popular Adversarial Imitation Learning paradigm. 
This insight allows us to express loss functions over both demonstrations and preferences in a unified framework.
Besides expert demonstrations, we are also able to learn from imperfect ones and combine them with preferences to achieve improved task performance.
We experimentally validate the effectiveness of combining both preferences and demonstrations on common benchmarks and also show that our method can efficiently learn challenging robot manipulation tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1466. <h><a href=https://openreview.net/pdf?id=yIxtevizEA>Latent Bottlenecked Attentive Neural Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Leo Feng, Hossein Hajimirsadeghi, Yoshua Bengio, Mohamed Osama Ahmed</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=yIxtevizEA>5.67 - [8, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this work, we propose Latent Bottlenecked Attentive Neural Processes (LBANPs), a new computationally efficient sub-quadratic NP variant, that has a querying computational complexity independent of the number of context datapoints.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/yIxtevizEA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural Processes (NPs) are popular methods in meta-learning that can estimate predictive uncertainty on target datapoints by conditioning on a context dataset. Previous state-of-the-art method Transformer Neural Processes (TNPs) achieve strong performance but require quadratic computation with respect to the number of context datapoints, significantly limiting its scalability. Conversely, existing sub-quadratic NP variants perform significantly worse than that of TNPs. Tackling this issue, we propose Latent Bottlenecked Attentive Neural Processes (LBANPs), a new computationally efficient sub-quadratic NP variant, that has a querying computational complexity independent of the number of context datapoints. The model encodes the context dataset into a constant number of latent vectors on which self-attention is performed. When making predictions, the model retrieves higher-order information from the context dataset via multiple cross-attention mechanisms on the latent vectors. We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion, and contextual multi-armed bandits. We demonstrate that LBANPs can trade-off the computational cost and performance according to the number of latent vectors. Finally, we show LBANPs can scale beyond existing attention-based NP variants to larger dataset settings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1467. <h><a href=https://openreview.net/pdf?id=sVzBN-DlJRi>Budgeted Training for Vision Transformer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>zhuofan xia, Xuran Pan, Xuan Jin, Yuan He, Hui Xue&#39;, Shiji Song, Gao Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=sVzBN-DlJRi>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/sVzBN-DlJRi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The superior performances of Vision Transformers often come with higher training costs. Compared to their CNN counterpart, Transformer models are hungry for large-scale data and their training schedules are usually prolonged. This sets great restrictions on training Transformers with limited resources, where a proper trade-off between training cost and model performance is longed. In this paper, we address the problem by proposing a framework that enables the training process under \textit{any training budget} from the perspective of model structure, while achieving competitive model performances. Specifically, based on the observation that Transformer exhibits different levels of model redundancies at different training stages, we propose to dynamically control the activation rate of the model structure along the training process and meet the demand on the training budget by adjusting the duration on each level of model complexity. Extensive experiments demonstrate that our framework is applicable to various Vision Transformers, and achieves competitive performances on a wide range of training budgets. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1468. <h><a href=https://openreview.net/pdf?id=5OygDd-4Eeh>An Additive Instance-Wise Approach to Multi-class Model Interpretation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Vy Vo, Van Nguyen, Trung Le, Quan Hung Tran, Reza Haf, Seyit Camtepe, Dinh Phung</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5OygDd-4Eeh>5.67 - [3, 6, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5OygDd-4Eeh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Interpretable machine learning offers insights into what factors drive a certain prediction of a black-box system. A large number of interpreting methods focus on identifying explanatory input features, which generally fall into two main categories: attribution and selection. A popular attribution-based approach is to exploit local neighborhoods for learning instance-specific explainers in an additive manner. The process is thus inefficient and susceptible to poorly-conditioned samples. Meanwhile, many selection-based methods directly optimize local feature distributions in an instance-wise training framework, thereby being capable of leveraging global information from other inputs. However, they can only interpret single-class predictions and many suffer from inconsistency across different settings, due to a strict reliance on a pre-defined number of features selected. This work exploits the strengths of both methods and proposes a framework for learning local explanations simultaneously for multiple target classes. Our model explainer significantly outperforms additive and instance-wise counterparts on faithfulness with more compact and comprehensible explanations. We also demonstrate the capacity to select stable and important features through extensive experiments on various data sets and black-box model architectures.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1469. <h><a href=https://openreview.net/pdf?id=a70lGJ-rwy>Few-shot Backdoor Attacks via Neural Tangent Kernels</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jonathan Hayase, Sewoong Oh</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=a70lGJ-rwy>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide a new algorithm based on intuitions from kernel regression that uses neural tangent kernels to design stronger backdoor attacks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/a70lGJ-rwy.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In a backdoor attack, an attacker injects corrupted examples into the training set. The goal of the attacker is to cause the final trained model to predict the attacker&#39;s desired target label when a predefined trigger is added to test inputs. Central to these attacks is the trade-off between the success rate of the attack and the number of corrupted training examples injected. We pose this attack as a novel bilevel optimization problem: construct strong poison examples that maximize the attack success rate of the trained model. We use neural tangent kernels to approximate the training dynamics of the model being attacked and automatically learn strong poison examples. We experiment on subclasses of CIFAR-10 and ImageNet with WideResNet-34 and ConvNeXt architectures on periodic and patch trigger attacks and show that NTBA-designed poisoned examples achieve, for example, an attack success rate of  90% with ten times smaller number of poison examples injected compared to the baseline. We provided an interpretation of the NTBA-designed attacks using the analysis of kernel linear regression. We further demonstrate a vulnerability in overparametrized deep neural networks, which is revealed by the shape of the neural tangent kernel.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1470. <h><a href=https://openreview.net/pdf?id=mbxz9Cjehr>A CMDP-within-online framework for Meta-Safe Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Vanshaj Khattar, Yuhao Ding, Bilgehan Sel, Javad Lavaei, Ming Jin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mbxz9Cjehr>5.67 - [8, 6, 3] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the problem of meta-reinforcement learning (meta-RL) for constrained Markov decision processes (CMDPs) through the inexact CMDP-within-online framework.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mbxz9Cjehr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Meta-reinforcement learning has widely been used as a learning-to-learn framework to solve unseen tasks with limited experience. However, the aspect of constraint violations has not been adequately addressed in the existing works, making their application restricted in real-world settings. In this paper, we study the problem of meta-safe reinforcement learning (meta-SRL) through the CMDP-within-online framework. We obtain task-averaged regret guarantees for the reward maximization (optimality gap) and constraint violations using gradient-based meta-learning and show that the task-averaged optimality gap and constraint satisfaction improve with task-similarity in the static environment, or task-relatedness in the changing environment. Several technical challenges arise when making this framework practical while still having strong theoretical guarantees. To address these challenges, we propose a meta-algorithm that performs inexact online learning on the upper bounds of intra-task optimality gap and constraint violations estimated by off-policy stationary distribution corrections. Furthermore, we enable the learning rates to be adapted for every task and extend our approach to settings with the dynamically changing task environments. Finally, experiments are conducted to demonstrate the effectiveness of our approach. The proposed theoretical framework is the first to handle the nonconvexity and stochastic nature of within-task CMDPs, while exploiting inter-task dependency for multi-task safe learning.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1471. <h><a href=https://openreview.net/pdf?id=z92lBy1ehjI>Winning Both the Accuracy of Floating Point Activation and the Simplicity of Integer Arithmetic</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yulhwa Kim, Jaeyong Jang, Jehun Lee, Jihoon Park, Jeonghoon Kim, Byeongwook Kim, Baeseong park, Se Jung Kwon, Dongsoo Lee, jae-joon kim</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=z92lBy1ehjI>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/z92lBy1ehjI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Even though floating point (FP) numbers have been adopted as a de facto standard data format for deep learning computing, the complexity of FP arithmetic impedes a broader deployment of Deep Neural Networks (DNNs). Recent works such as quantization have attempted to replace the FP matrix multiplication (MatMul) of DNNs with simple integer MatMul by transforming the datatypes of both weights and activations into integers. Unfortunately, unlike weight values that are static, it is challenging to represent dynamic activations with integers. In this paper, to simultaneously achieve the accuracy of FP activation and the simplicity of integer arithmetic, we present a method for replacing FP arithmetic with integer one without changing FP activations in the storage format while weights are quantized. The proposed method pre-aligns the significands of FP activations just ahead of the MatMul on-the-fly so that the aligned significands (integers) can be used for the computation. Inspired by an observation that conventional FP arithmetic does not produce precise results due to rounding, we demonstrate that our proposed integer arithmetic-based scheme can produce the same level of errors as that of the FP arithmetic in case DNNs use FP activations and quantized weights. Experimental results show that the hardware based on the proposed scheme shows significant improvement over FP arithmetic-based designs in terms of energy efficiency and throughput-per-area while maintaining a similar level of accuracy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1472. <h><a href=https://openreview.net/pdf?id=_qVhsWyWB9>Deep Learning From Crowdsourced Labels: Coupled Cross-Entropy Minimization, Identifiability, and Regularization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shahana Ibrahim, Tri Nguyen, Xiao Fu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_qVhsWyWB9>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_qVhsWyWB9.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Using noisy crowdsourced labels from multiple annotators, a deep learning-based end-to-end (E2E) system aims to learn the label correction mechanism and the neural classifier simultaneously. To this end, many E2E systems concatenate the neural classifier with multiple annotator-specific label confusion layers and co-train the two parts in a parameter-coupled manner. The formulated coupled cross-entropy minimization (CCEM)-type criteria are intuitive and work well in practice. Nonetheless, theoretical understanding of the CCEM criterion has been limited. The contribution of this work is twofold: First, performance guarantees of the CCEM criterion are presented. Our analysis reveals for the first time that the CCEM can indeed correctly identify the annotators&#39; confusion characteristics and the desired ``ground-truth&#39;&#39; neural classifier under realistic conditions, e.g., when only incomplete annotator labeling and finite samples are available. Second, based on the insights learned from our analysis, two regularized variants of the CCEM are proposed.  The regularization terms provably enhance the identifiability of the target model parameters in various more challenging cases. A series of synthetic and real data experiments are presented to showcase the effectiveness of our approach.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1473. <h><a href=https://openreview.net/pdf?id=9HiGqC9C-KA>simpleKT: A Simple But Tough-to-Beat Baseline for Knowledge Tracing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zitao Liu, Qiongqiong Liu, Jiahao Chen, Shuyan Huang, Weiqi Luo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9HiGqC9C-KA>5.67 - [6, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose \textsc{simpleKT}, a simple but tough-to-beat KT baseline that is simple to implement, computationally friendly and robust to a wide range of KT datasets across different domains
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9HiGqC9C-KA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Knowledge tracing (KT) is the problem of predicting students&#39; future performance based on their historical interactions with intelligent tutoring systems. Recently, many works present lots of special methods for applying deep neural networks to KT from different perspectives like model architecture, adversarial augmentation and etc., which make the overall algorithm and system become more and more complex. Furthermore, due to the lack of standardized evaluation protocol \citep{liu2022pykt}, there is no widely agreed KT baselines and published experimental comparisons become inconsistent and self-contradictory, i.e., the reported AUC scores of DKT on ASSISTments2009 range from 0.721 to 0.821 \citep{minn2018deep,yeung2018addressing}. Therefore, in this paper, we provide a strong but simple baseline method to deal with the KT task named \textsc{simpleKT}. Inspired by the Rasch model in psychometrics, we explicitly model question-specific variations to capture the individual differences among questions covering the same set of knowledge components that are a generalization of terms of concepts or skills needed for learners to accomplish steps in a task or a problem. Furthermore, instead of using sophisticated representations to capture student forgetting behaviors, we use the ordinary dot-product attention function to extract the time-aware information embedded in the student learning interactions. Extensive experiments show that such a simple baseline is able to always rank top 3 in terms of AUC scores and achieve 57 wins, 3 ties and 16 loss against 12 DLKT baseline methods on 7 public datasets of different domains. We believe this work serves as a strong baseline for future KT research. Code is available at \url{https://github.com/pykt-team/pykt-toolkit}\footnote{We merged our model to the \textsc{pyKT} benchmark at \url{https://pykt.org/}.}.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1474. <h><a href=https://openreview.net/pdf?id=BR_ZhvcYbGJ>Explaining Temporal Graph Models through an Explorer-Navigator Framework</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenwen Xia, Mincai Lai, Caihua Shan, Yao Zhang, Xinnan Dai, Xiang Li, Dongsheng Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=BR_ZhvcYbGJ>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A MCTS-based explainer for temporal graph models.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/BR_ZhvcYbGJ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While GNN explanation has recently received significant attention, existing works are consistently designed for static graphs. Due to the prevalence of temporal graphs, many temporal graph models have been proposed, but explaining their predictions remains to be explored. To bridge the gap, in this paper, we propose T-GNNExplainer for temporal graph model explanation. Specifically, we regard a temporal graph constituted by a sequence of temporal events. Given a target event, our task is to find a subset of previously occurred events that lead to the model&#39;s prediction for it. To handle this combinatorial optimization problem, T-GNNExplainer includes an explorer to find the event subsets with Monte Carlo Tree Search (MCTS)  and a navigator that learns the correlations between events and helps reduce the search space. In particular, the navigator is trained in advance and then integrated with the explorer to speed up searching and achieve better results. To the best of our knowledge, T-GNNExplainer is the first explainer tailored for temporal graph models. We conduct extensive experiments to evaluate the performance of T-GNNExplainer. Experimental results on both real-world and synthetic datasets demonstrate that T-GNNExplainer can achieve superior performance with up to about 50% improvement in Area under Fidelity-Sparsity Curve. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1475. <h><a href=https://openreview.net/pdf?id=YtntjusJV6>Interpretations of Domain Adaptations via Layer Variational Analysis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Huan-Hsin Tseng, Hsin-Yi Lin, Kuo-Hsuan Hung, Yu Tsao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YtntjusJV6>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Interpretations of Domain Adaptations via Layer Variational Analysis
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YtntjusJV6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transfer learning is known to perform efficiently in many applications empirically, yet limited literature reports the mechanism behind the scene. This study establishes both formal derivations and heuristic analysis to formulate the theory of transfer learning in deep learning. Our framework utilizing layer variational analysis proves that the success of transfer learning can be guaranteed with corresponding data conditions. Moreover, our theoretical calculation yields intuitive interpretations towards the knowledge transfer process. Subsequently, an alternative method for network-based transfer learning is derived. The method shows an increase in efficiency and accuracy for domain adaptation. It is particularly advantageous when new domain data is sufficiently sparse during adaptation. Numerical experiments over diverse tasks validated our theory and verified that our analytic expression achieved better performance in domain adaptation than the gradient descent method.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1476. <h><a href=https://openreview.net/pdf?id=5mqFra2ZSuf>SP2 : A Second Order Stochastic Polyak Method</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shuang Li, William Joseph Swartworth, Martin Takáč, Deanna Needell, Robert M. Gower</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5mqFra2ZSuf>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5mqFra2ZSuf.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently the SP (Stochastic Polyak step size) method has emerged as a competitive adaptive method for setting the step sizes of SGD.  SP can be interpreted as a method specialized to interpolated models, since it solves the interpolation equations. SP solves these equation by using local linearizations of the model.  We take a step further and develop a method for solving the interpolation equations that uses the local second-order approximation of the model. Our resulting method SP2 uses Hessian-vector products to speed-up the convergence of SP. Furthermore, and rather uniquely among second-order methods, the design of SP2 in no way relies on positive definite Hessian matrices or convexity of the objective function. We show SP2 is competitive both in experiments and in theory. 
We show SP2 is very competitive on matrix completion, non-convex test problems and logistic regression. We also provide a convergence theory on sums-of-quadratics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1477. <h><a href=https://openreview.net/pdf?id=n0Pb9T5kmb>ArCL: Enhancing Contrastive Learning with Augmentation-Robust Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xuyang Zhao, Tianqi Du, Yisen Wang, Jun Yao, Weiran Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=n0Pb9T5kmb>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper studies the out-of-distribution generalization of contrastive self-supervised learning, and propose an augmentation-robust contrastive learning algorithm to improve the OOD performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/n0Pb9T5kmb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Self-Supervised Learning (SSL) is a paradigm that leverages unlabeled data for model training. Empirical studies show that SSL can achieve promising performance in distribution shift scenarios, where the downstream and training distributions differ. However, the theoretical understanding of its transferability remains limited. In this paper, we develop a theoretical framework to analyze the transferability of self-supervised contrastive learning, by investigating the impact of data augmentation on it. Our results reveal that the downstream performance of contrastive learning depends largely on the choice of data augmentation.  Moreover, we show that contrastive learning fails to learn domain-invariant features, which limits its transferability. Based on these theoretical insights, we propose a novel method called Augmentation-robust Contrastive Learning (ArCL), which guarantees to learn domain-invariant features and can be easily integrated with existing contrastive learning algorithms. We conduct experiments on several datasets and show that ArCL significantly improves the transferability of contrastive learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1478. <h><a href=https://openreview.net/pdf?id=bZjxxYURKT>FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yan Sun, Li Shen, Tiansheng Huang, Liang Ding, Dacheng Tao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bZjxxYURKT>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel and practical federated learning method with theoretical analysis guarantees achieves higher performance in the common federated settings.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bZjxxYURKT.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning (FL) is an emerging distributed machine learning framework which jointly trains a global model via a large number of local devices with data privacy protections. Its performance suffers from the non-vanishing biases introduced by the local inconsistent optimal and the rugged client-drifts by the local over-fitting. In this paper, we propose a novel and practical method, FedSpeed, to alleviate the negative impacts posed by these problems.  Concretely, FedSpeed applies the prox-correction term on the current local updates to efficiently reduce the biases introduced by the prox-term, a necessary regularizer to maintain the strong local consistency. Furthermore, FedSpeed merges the vanilla stochastic gradient with a perturbation computed from an extra gradient ascent step in the neighborhood, thereby alleviating the issue of local over-fitting. Our theoretical analysis indicates that the convergence rate is related to both the communication rounds $T$ and local intervals $K$ with a tighter upper bound $\mathcal{O}(\frac{1}{T})$ if $K=\mathcal{O}(T)$.  Moreover, we conduct extensive experiments on the real-world dataset to demonstrate the efficiency of our proposed FedSpeed, which converges significantly faster and achieves the state-of-the-art (SOTA) performance on the general FL experimental settings than several baselines including FedAvg, FedProx, FedCM, FedAdam, SCAFFOLD, FedDyn, FedADMM, etc.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1479. <h><a href=https://openreview.net/pdf?id=CDlHZ78-Xzi>MACTA: A Multi-agent Reinforcement Learning Approach for Cache Timing Attacks and Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiaxun Cui, Xiaomeng Yang, Mulong Luo, Geunbae Lee, Peter Stone, Hsien-Hsin S. Lee, Benjamin Lee, G. Edward Suh, Wenjie Xiong, Yuandong Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CDlHZ78-Xzi>5.67 - [5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CDlHZ78-Xzi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Security vulnerabilities in computer systems raise serious concerns as computers process an unprecedented amount of private and sensitive data today. Cache timing attacks (CTA) pose an important practical threat as they can effectively breach many protection mechanisms in today’s systems. However, the current detection techniques for cache timing attacks heavily rely on heuristics and expert knowledge, which can lead to brittleness and the inability to adapt to new attacks. To mitigate the CTA threat, we propose MACTA, a multi-agent reinforcement learning (MARL) approach that leverages population-based training to train both attackers and detectors. Following best practices, we develop a realistic simulated MARL environment, MA-AUTOCAT, which enables training and evaluation of cache-timing attackers and detectors. Our empirical results suggest that MACTA is an effective solution without any manual input from security experts. MACTA detectors can generalize to a heuristic attack not exposed in training with a 97.8% detection rate and reduce the attack bandwidth of adaptive attackers by 20% on average. In the meantime, MACTA attackers are qualitatively more effective than other attacks studied, and the average evasion rate of MACTA attackers against an unseen state-of-the-art detector can reach up to 99%. Furthermore, we found that agents equipped with a Transformer encoder can learn effective policies in situations when agents with multi-layer perceptron encoders do not in this environment, suggesting the potential of Transformer structures in CTA problems.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1480. <h><a href=https://openreview.net/pdf?id=fzberKYWKsI>An efficient encoder-decoder architecture with top-down attention for speech separation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kai Li, Runxuan Yang, Xiaolin Hu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=fzberKYWKsI>5.67 - [6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an encoder-decoder speech separation structure with top-down attention, which can improve separation efficiency while ensuring the separation performance.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/fzberKYWKsI.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep neural networks have shown excellent prospects in speech separation tasks. However, obtaining good results while keeping a low model complexity remains challenging in real-world applications. In this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain’s top-down attention, called TDANet, with decreased model complexity without sacrificing performance. The top-down attention in TDANet is extracted by the global attention (GA) module and the cascaded local attention (LA) layers. The GA module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. The LA layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. On three benchmark datasets, TDANet consistently achieved competitive separation performance to previous state-of-the-art (SOTA) methods with higher efficiency. Specifically, TDANet’s multiply-accumulate operations (MACs) are only 5% of Sepformer, one of the previous SOTA models, and CPU inference time is only 10% of Sepformer. In addition, a large-size version of TDANet obtained SOTA results on three datasets, with MACs still only 10% of Sepformer and the CPU inference time only 24% of Sepformer. Our study suggests that top-down attention can be a more efficient strategy for speech separation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1481. <h><a href=https://openreview.net/pdf?id=-CefY2EOupj>Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-CefY2EOupj>5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-CefY2EOupj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    1-bit gradient compression and local steps are two representative techniques that enable drastic communication reduction in distributed SGD. Their benefits, however, remain an open question on Adam-based large model pre-training (e.g. BERT and GPT). In this paper, we demonstrate the non-linearity in Adam causes slow convergence even when 1-bit compression or local steps are individually applied. To alleviate this limitation, we propose \textbf{0/1 Adam} that linearizes each Adam step via approximating its optimizer states using their stale estimates and linear correlation. \textbf{0/1 Adam} performs an Adam-like step to preserve the adaptivity, while its linearity allows utilizing 1-bit compression and local steps simultaneously for wall-clock time speed up. We provide convergence guarantee for \textbf{0/1 Adam} on smooth non-convex objectives. On various large-scale benchmarks such as BERT-Base, BERT-Large, GPT-2 pre-training and ImageNet, we demonstrate on up to 128 GPUs that \textbf{0/1 Adam} is able to reduce up to 87\% of data volume, 54\% of communication rounds, and achieve up to 2$\times$ higher training throughput and end-to-end training time reduction compared to the state-of-the-art baseline 1-bit Adam; while enjoying the same statistical convergence speed and end task model accuracy on GLUE dataset and ImageNet validation set. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1482. <h><a href=https://openreview.net/pdf?id=6Wl7-M2BC->An Adaptive Policy to Employ Sharpness-Aware Minimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Weisen Jiang, Hansi Yang, Yu Zhang, James Kwok</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6Wl7-M2BC->5.67 - [6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We design an adaptive policy to employ SAM and propose two efficient algorithms to reduce the fraction of SAM updates.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6Wl7-M2BC-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Sharpness-aware minimization (SAM), which searches for flat minima by min-max optimization, has been shown to be useful in improving model generalization. However, since each SAM update requires computing two gradients, its computational cost and training time are both doubled compared to standard empirical risk minimization (ERM). Recent state-of-the-arts reduce the fraction of SAM updates and thus accelerate SAM by switching between SAM and ERM updates randomly or periodically. In this paper, we design an adaptive policy to employ SAM based on the loss landscape geometry. Two efficient algorithms, AE-SAM and AE-LookSAM, are proposed. We theoretically show that AE-SAM has the same convergence rate as SAM. Experimental results on various datasets and architectures demonstrate the efficiency and effectiveness of the adaptive policy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1483. <h><a href=https://openreview.net/pdf?id=hxEIgUXLFF>PerFedMask: Personalized Federated Learning with Optimized Masking Vectors</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mehdi Setayesh, Xiaoxiao Li, Vincent W.S. Wong</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hxEIgUXLFF>5.67 - [8, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b>  We propose PerFedMask to address both the data and device heterogeneity issues in federated learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hxEIgUXLFF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, various personalized federated learning (FL) algorithms have been proposed to tackle data heterogeneity. To mitigate device heterogeneity, a common approach is to use masking.  In this paper, we first show that using random masking can lead to a bias in the obtained solution of the learning model. To this end, we propose a personalized FL algorithm with optimized masking vectors called PerFedMask. In particular, PerFedMask facilitates each device to obtain its optimized masking vector based on its computational capability before training.  Fine-tuning is performed after training. PerFedMask is a generalization of a recently proposed personalized FL algorithm, FedBABU (Oh et al., 2022). PerFedMask can be combined with other FL algorithms including HeteroFL (Diao et al., 2021) and Split-Mix FL (Hong et al., 2022). Results based on CIFAR-10 and CIFAR-100 datasets show that the proposed PerFedMask algorithm provides a higher test accuracy after fine-tuning and lower average number of trainable parameters when compared with six existing state-of-the-art FL algorithms in the literature. The codes are available at https://github.com/MehdiSet/PerFedMask.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1484. <h><a href=https://openreview.net/pdf?id=qihMOPw4Sf_>Valid P-Value for Deep Learning-driven Salient Region</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Miwa Daiki, Vo Nguyen Le Duy, Ichiro Takeuchi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=qihMOPw4Sf_>5.6 - [6, 6, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel method to quantify the reliability of neural network-driven saliency region in statistical hypothesis testing framework.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/qihMOPw4Sf_.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Various saliency map methods have been proposed to interpret and explain predictions of deep learning models. Saliency maps allow us to interpret which parts of the input signals have a strong influence on the prediction results. However, since a saliency map is obtained by complex computations in deep learning models, it is often difficult to know how reliable the saliency map itself is. In this study, we propose a method to quantify the reliability of a saliency region in the form of p-values. Our idea is to consider a saliency map as a selected hypothesis by the trained deep learning model and employ the selective inference framework. The proposed method provably provides a valid p-value for the detected salient region, i.e., we can provably control the false positive rate of the detected salient region. We demonstrate the validity of the proposed method through numerical examples in synthetic and real datasets. Furthermore, we develop a Keras-based framework for conducting the proposed selective inference for a wide class of CNNs without additional implementation cost.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1485. <h><a href=https://openreview.net/pdf?id=X9yCkmT5Qrl>GNNDelete: A General Strategy for Unlearning in Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiali Cheng, George Dasoulas, Huan He, Chirag Agarwal, Marinka Zitnik</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=X9yCkmT5Qrl>5.6 - [5, 8, 6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/X9yCkmT5Qrl.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph unlearning, which involves deleting graph elements such as nodes, node labels, and relationships from a trained graph neural network (GNN) model, is crucial for real-world applications where data elements may become irrelevant, inaccurate, or privacy-sensitive. However, existing methods for graph unlearning either deteriorate model weights shared across all nodes or fail to effectively delete edges due to their strong dependence on local graph neighborhoods. To address these limitations, we introduce GNNDelete, a novel model-agnostic layer-wise operator that optimizes two critical properties, namely, Deleted Edge Consistency and Neighborhood Influence, for graph unlearning. Deleted Edge Consistency ensures that the influence of deleted elements is removed from both model weights and neighboring representations, while Neighborhood Influence guarantees that the remaining model knowledge is preserved after deletion. GNNDelete updates representations to delete nodes and edges from the model while retaining the rest of the learned knowledge. We conduct experiments on seven real-world graphs, showing that GNNDelete outperforms existing approaches by up to 38.8% (AUC) on edge, node, and node feature deletion tasks, and 32.2% on distinguishing deleted edges from non-deleted ones. Additionally, GNNDelete is efficient, taking 12.3x less time and 9.3x less space than retraining GNN from scratch on WordNet18.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1486. <h><a href=https://openreview.net/pdf?id=hDDV1lsRV8>Federated Learning from Small Datasets</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Kamp, Jonas Fischer, Jilles Vreeken</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hDDV1lsRV8>5.6 - [5, 6, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose federated daisy chaining to allow multiple parties to successfully train a joint model collaboratively from small local datasets, retaining the privacy benefits of federated learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hDDV1lsRV8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning allows multiple parties to collaboratively train a joint model without having to share any local data. It enables applications of machine learning in settings where data is inherently distributed and undisclosable, such as in the medical domain. Joint training is usually achieved by aggregating local models. When local datasets are small, locally trained models can vary greatly from a globally good model. Bad local models can arbitrarily deteriorate the aggregate model quality, causing federating learning to fail in these settings. We propose a novel approach that avoids this problem by interleaving model aggregation and permutation steps. During a permutation step we redistribute local models across clients through the server, while preserving data privacy, to allow each local model to train on a daisy chain of local datasets. This enables successful training in data-sparse domains. Combined with model aggregation, this approach enables effective learning even if the local datasets are extremely small, while retaining the privacy benefits of federated learning.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1487. <h><a href=https://openreview.net/pdf?id=pgU3k7QXuz0>Spiking Convolutional Neural Networks for Text Classification</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Changze Lv, Jianhan Xu, Xiaoqing Zheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pgU3k7QXuz0>5.5 - [5, 3, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Spiking Convolutional Neural Networks for Text Classification
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pgU3k7QXuz0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Spiking neural networks (SNNs) offer a promising pathway to implement deep neural networks (DNNs) in a more energy-efficient manner since their neurons are sparsely activated and inferences are event-driven. However, there have been very few works that have demonstrated the efficacy of SNNs in language tasks partially because it is non-trivial to represent words in the forms of spikes and to deal with variable-length texts by SNNs. This work presents a &#34;conversion + fine-tuning&#39;&#39; two-step method for training SNN for text classification and proposes a simple but effective way to encode pre-trained word embeddings as spike trains. We show empirically that after further fine-tuning with surrogate gradients, the converted SNNs achieve comparable results to their DNN counterparts across multiple datasets for Both English and Chinese. We also demonstrate that such SNNs are more robust against adversarial attacks than DNNs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1488. <h><a href=https://openreview.net/pdf?id=MhuFzFsrfvH>Optimal Transport for Offline Imitation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yicheng Luo, zhengyao jiang, Samuel Cohen, Edward Grefenstette, Marc Peter Deisenroth</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=MhuFzFsrfvH>5.5 - [5, 6, 5, 6] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present an offline imitation learning based on optimal transport that demonstrates strong performance and sample efficiency
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/MhuFzFsrfvH.jpg loading="lazy">
                </div>
                <div class="abstract">
                    With the advent of large datasets, offline reinforcement learning is a promising framework for learning good decision-making policies without the need to interact with the real environment.
However, offline RL requires the dataset to be reward-annotated, which presents practical challenges when reward engineering is difficult or when obtaining reward annotations is labor-intensive.
In this paper, we introduce Optimal Transport Relabeling (OTR), an imitation learning algorithm that can automatically relabel offline data of mixed and unknown quality with rewards from a few good demonstrations. OTR&#39;s key idea is to use optimal transport to compute an optimal alignment between an unlabeled trajectory in the dataset and an expert demonstration to obtain a similarity measure that can be interpreted as a reward, which can then be used by an offline RL algorithm to learn the policy. OTR is easy to implement and computationally efficient. On D4RL benchmarks, we demonstrate that OTR with a single demonstration can consistently match the performance of offline RL with ground-truth rewards.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1489. <h><a href=https://openreview.net/pdf?id=5Egggz1q575>Explaining RL Decisions with Trajectories</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shripad Vilasrao Deshmukh, Arpan Dasgupta, Balaji Krishnamurthy, Nan Jiang, Chirag Agarwal, Georgios Theocharous, Jayakumar Subramanian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5Egggz1q575>5.5 - [5, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This work focuses on idea of explaining actions of offline RL agent by attributing the actions to trajectories encountered during the training.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5Egggz1q575.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Explanation is a key component for the adoption of reinforcement learning (RL) in many real-world decision-making problems. In the literature,  the explanation is often provided by saliency attribution to the features of the RL agent&#39;s state. In this work, we propose a complementary approach to these explanations, particularly for offline RL, where we attribute the policy decisions of a trained RL agent to the trajectories encountered by it during training. To do so, we encode trajectories in offline training data individually as well as collectively (encoding a set of trajectories). We then attribute policy decisions to a set of trajectories in this encoded space by estimating the sensitivity of the decision with respect to that set.  Further, we demonstrate the effectiveness of the proposed approach in terms of quality of attributions as well as practical scalability in diverse environments that involve both discrete and continuous state and action spaces such as grid-worlds, video games (Atari) and continuous control (MuJoCo). We also conduct a human study on a simple navigation task to observe how their understanding of the task compares with data attributed for a trained RL policy.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1490. <h><a href=https://openreview.net/pdf?id=3nM5uhPlfv6>Stochastic Differentially Private and Fair Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Andrew Lowy, Devansh Gupta, Meisam Razaviyayn</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3nM5uhPlfv6>5.5 - [5, 8, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> The first efficient differentially private fair learning algorithm that is guaranteed to converge, even when stochastic minibatches of data are used in each iteration of training. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3nM5uhPlfv6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning models are increasingly used in high-stakes decision-making systems. In such applications, a major concern is that these models sometimes discriminate against certain demographic groups such as individuals with certain race, gender, or age. Another major concern in these applications is the violation of the privacy of users. While fair learning algorithms have been developed to mitigate discrimination issues, these algorithms can still leak sensitive information, such as individuals’ health or financial records. Utilizing the notion of differential privacy (DP), prior works aimed at developing learning algorithms that are both private and fair. However, existing algorithms for DP fair learning are either not guaranteed to converge or require full batch of data in each iteration of the algorithm to converge. In this paper, we provide the first stochastic differentially private algorithm for fair learning that is guaranteed to converge. Here, the term “stochastic&#34; refers to the fact that our proposed algorithm converges even when minibatches of data are used at each iteration (i.e. stochastic optimization). Our framework is flexible enough to permit different fairness notions, including demographic parity and equalized odds. In addition, our algorithm can be applied to non-binary classification tasks with multiple (non-binary) sensitive attributes. As a byproduct of our convergence analysis, we provide the first utility guarantee for a DP algorithm for solving nonconvex-strongly concave min-max problems. Our numerical experiments show that the proposed algorithm consistently offers significant performance gains over the state-of-the-art baselines, and can be applied to larger scale problems with non-binary target/sensitive attributes.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1491. <h><a href=https://openreview.net/pdf?id=zH9GcZ3ZGXu>Feature Reconstruction From Outputs Can Mitigate Simplicity Bias in Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Sravanti Addepalli, Anshul Nasery, Venkatesh Babu Radhakrishnan, Praneeth Netrapalli, Prateek Jain</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zH9GcZ3ZGXu>5.5 - [6, 6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a regularizer that enforces the reconstruction of features from the output logits of neural networks, in order to overcome Simplicity Bias and boost their OOD generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zH9GcZ3ZGXu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep Neural Networks are known to be brittle to even minor distribution shifts compared to the training distribution. While one line of work has demonstrated that \emph{Simplicity Bias} (SB) of DNNs -- bias towards learning only the simplest features -- is a key reason for this brittleness, another recent line of work has surprisingly found that diverse/ complex features are indeed learned by the backbone, and their brittleness is due to the linear classification head relying primarily on the simplest features. To bridge the gap between these two lines of work, we first hypothesize and verify that while SB may not altogether preclude learning complex features, it amplifies simpler features over complex ones. Namely, simple features are replicated several times in the learned representations while complex features might not be replicated. This phenomenon, we term \emph{Feature  Replication  Hypothesis}, coupled with the \emph{Implicit Bias} of SGD to converge to maximum margin solutions in the feature space, leads the models to rely mostly on the simple features for classification. To mitigate this bias, we propose \emph{Feature Reconstruction Regularizer (FRR)} to ensure that the learned features can be reconstructed back from the logits. The use of \emph{FRR} in linear layer training (\emph{FRR-L}) encourages the use of more diverse features for classification. We further propose to finetune the full network by freezing the weights of the linear layer trained using \emph{FRR-L}, to refine the learned features, making them more suitable for classification. Using this simple solution, we demonstrate up to 15\% gains in OOD accuracy on the recently introduced semi-synthetic datasets with extreme distribution shifts. Moreover, we demonstrate noteworthy gains over existing SOTA methods on the standard OOD benchmark DomainBed as well.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1492. <h><a href=https://openreview.net/pdf?id=0pdSt3oyJa1>Specformer: Spectral Graph Neural Networks Meet Transformers</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Deyu Bo, Chuan Shi, Lele Wang, Renjie Liao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0pdSt3oyJa1>5.5 - [5, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel set-to-set spectral graph filter by using a spectral domain Transformer.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0pdSt3oyJa1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1493. <h><a href=https://openreview.net/pdf?id=7sn6Vxp92xV>Exploring The Role of Mean Teachers in Self-supervised Masked Auto-Encoders</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Youngwan Lee, Jeffrey Ryan Willette, Jonghee Kim, Juho Lee, Sung Ju Hwang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7sn6Vxp92xV>5.5 - [6, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We conduct analysis of the dynamics of the self-distillation scheme in masked auto-encoder.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7sn6Vxp92xV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Masked image modeling (MIM) has become a popular strategy for self-supervised learning (SSL) of visual representations with Vision Transformers. A representative MIM model, the masked auto-encoder (MAE), randomly masks a subset of image patches and reconstructs the masked patches given the unmasked patches. Concurrently, many recent works in self-supervised learning utilize the student/teacher paradigm which provides the student with an additional target based on the output of a teacher composed of an exponential moving average (EMA) of previous students. Although common, relatively little is known about the dynamics of the interaction between the student and teacher. 
Through analysis on a simple linear model, we find that the teacher conditionally removes previous gradient directions based on feature similarities which effectively acts as a conditional momentum regularizer. From this analysis, we present a simple SSL method, the Reconstruction-Consistent Masked Auto-Encoder (RC-MAE) by adding an EMA teacher to MAE. We find that RC-MAE converges faster and requires less memory usage than state-of-the-art self-distillation methods during pre-training, which may provide a way to enhance the practicality of prohibitively expensive self-supervised learning of Vision Transformer models. Additionally, we show that RC-MAE achieves more robustness and better performance compared to MAE on downstream tasks such as ImageNet-1K classification, object detection, and instance segmentation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1494. <h><a href=https://openreview.net/pdf?id=QwKvL6wC8Yi>Coverage-centric Coreset Selection for High Pruning Rates</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Haizhong Zheng, Rui Liu, Fan Lai, Atul Prakash</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QwKvL6wC8Yi>5.5 - [5, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study the importance of data coverage in coreset selection and propose a coverage-centric method for coreset selection, which we show achieves significantly better accuracy than SOTA methods with high pruning rates.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QwKvL6wC8Yi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    One-shot coreset selection aims to select a representative subset of the training data, given a pruning rate, that can later be used to train future models while retaining high accuracy. State-of-the-art coreset selection methods pick the highest importance examples based on an importance metric and are found to perform well at low pruning rates.  However, at high pruning rates, they suffer from a catastrophic accuracy drop, performing worse than even random sampling. This paper explores the reasons behind this accuracy drop both theoretically and empirically. We first propose a novel metric to measure the coverage of a dataset on a specific distribution by extending the classical geometric set cover problem to a distribution cover problem. This metric helps explain why coresets selected by SOTA methods at high pruning rates perform poorly compared to random sampling because of worse data coverage. We then propose a novel one-shot coreset selection method, Coverage-centric Coreset Selection (CCS), that jointly considers overall data coverage upon a distribution as well as the importance of each example. We evaluate CCS on five datasets and show that, at high pruning rates (e.g., 90%), it achieves significantly better accuracy than previous SOTA methods (e.g., at least 19.56% higher on CIFAR10) as well as random selection (e.g., 7.04% higher on CIFAR10) and comparable accuracy at low pruning rates.  We make our code publicly available at https://github.com/haizhongzheng/Coverage-centric-coreset-selection.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1495. <h><a href=https://openreview.net/pdf?id=wKPmPBHSnT6>Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yunchong Song, Chenghu Zhou, Xinbing Wang, Zhouhan Lin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wKPmPBHSnT6>5.5 - [3, 8, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> In this paper, we propose a novel GNN model to tackle heterophily and over-smoothing simultaneously by aligning the rooted-tree hierarchy with node embedding structure.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wKPmPBHSnT6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Most graph neural networks follow the message passing mechanism. However, it faces the over-smoothing problem when multiple times of message passing is applied to a graph, causing indistinguishable node representations and prevents the model to effectively learn dependencies between farther-away nodes. On the other hand, features of neighboring nodes with different labels are likely to be falsely mixed, resulting in the heterophily problem. In this work, we propose to order the messages passing into the node representation, with specific blocks of neurons targeted for message passing within specific hops. This is achieved by aligning the hierarchy of the rooted-tree of a central node with the ordered neurons in its node representation. Experimental results on an extensive set of datasets show that our model can simultaneously achieve the state-of-the-art in both homophily and heterophily settings, without any targeted design. Moreover, its performance maintains pretty well while the model becomes really deep, effectively preventing the over-smoothing problem. Finally, visualizing the gating vectors shows that our model learns to behave differently between homophily and heterophily settings, providing an explainable graph neural model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1496. <h><a href=https://openreview.net/pdf?id=9Jaz4APHtWD>Link Prediction with Non-Contrastive Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>William Shiao, Zhichun Guo, Tong Zhao, Evangelos E. Papalexakis, Yozen Liu, Neil Shah</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9Jaz4APHtWD>5.5 - [5, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We evaluate the performance of non-contrastive methods on link prediction and propose a new method to improve its performance in the inductive setting.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9Jaz4APHtWD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural networks (GNNs) are prominent in the graph machine learning domain, owing to their strong performance across various tasks. A recent focal area is the space of graph self-supervised learning (SSL), which aims to derive useful node representations without labeled data. Notably, many state-of-the-art graph SSL methods are contrastive methods, which use a combination of positive and negative samples to learn node representations. Owing to challenges in negative sampling (slowness and model sensitivity), recent literature introduced non-contrastive methods, which instead only use positive samples. Though such methods have shown promising performance in node-level tasks, their suitability for link prediction tasks, which are concerned with predicting link existence between pairs of nodes (and have broad applicability to recommendation systems contexts) is yet unexplored. In this work, we extensively evaluate the performance of existing non-contrastive methods for link prediction in both transductive and inductive settings. While most existing non-contrastive methods perform poorly overall, we find that, surprisingly, BGRL generally performs well in transductive settings. However, it performs poorly in the more realistic inductive settings where the model has to generalize to links to/from unseen nodes. We find that non-contrastive models tend to overfit to the training graph and use this analysis to propose T-BGRL, a novel non-contrastive framework that incorporates cheap corruptions to improve the generalization ability of the model. This simple modification strongly improves inductive performance in 5/6 of our datasets, with up to a 120% improvement in Hits@50 - all with comparable speed to other non-contrastive baselines, and up to $14\times$ faster than the best-performing contrastive baseline. Our work imparts interesting findings about non-contrastive learning for link prediction and paves the way for future researchers to further expand upon this area.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1497. <h><a href=https://openreview.net/pdf?id=2XLRBjY46O6>ViewCo: Discovering Text-Supervised Segmentation Masks via Multi-View Semantic Consistency</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pengzhen Ren, Changlin Li, Hang Xu, Yi Zhu, Guangrun Wang, Jianzhuang Liu, Xiaojun Chang, Xiaodan Liang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2XLRBjY46O6>5.5 - [3, 3, 8, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Discovering text-supervised segmentation masks via multi-view semantic consistency
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2XLRBjY46O6.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recently, great success has been made in learning visual representations from text supervision, facilitating the emergence of text-supervised semantic segmentation. However, existing works focus on pixel grouping and cross-modal semantic alignment, while ignoring the correspondence among multiple augmented views of the same image. To overcome such limitation, we propose multi-View Consistent learning (ViewCo) for text-supervised semantic segmentation. Specifically, we first propose text-to-views consistency modeling to learn correspondence for multiple views of the same input image. Additionally, we propose cross-view segmentation consistency modeling to address the ambiguity issue of text supervision by contrasting the segment features of Siamese visual encoders.  The text-to-views consistency benefits dense assignment of the visual features by encouraging different crops to align with the same text, while the cross-view segmentation consistency modeling provides additional self-supervision, overcoming the limitation of ambiguous text supervision for segmentation masks. Trained with large-scale image-text data, our model can directly segment objects of arbitrary categories in a zero-shot manner. Extensive experiments show that ViewCo outperforms state-of-the-art methods on average by up to 2.9%, 1.6%, and 2.4% mIoU on PASCAL VOC2012, PASCAL Context, and COCO, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1498. <h><a href=https://openreview.net/pdf?id=2EpjkjzdCAa>Effectively Modeling Time Series with Simple Discrete State Spaces</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Zhang, Khaled Kamal Saab, Michael Poli, Tri Dao, Karan Goel, Christopher Re</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2EpjkjzdCAa>5.5 - [8, 5, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose SpaceTime, a deep state space time series model that achieves state-of-the-art results on forecasting and classification benchmarks, by improving expressiveness, forecasting flexibility, and training efficiency over prior approaches. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2EpjkjzdCAa.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Time series modeling is a well-established problem, which often requires that methods (1) expressively represent complicated dependencies, (2) forecast long horizons, and (3) efficiently train over long sequences. State-space models (SSMs) are classical models for time series, and prior works combine SSMs with deep learning layers for efficient sequence modeling. However, we find fundamental limitations with these prior approaches, proving their SSM representations cannot express  autoregressive time series processes. We thus introduce SpaceTime, a new state-space time series architecture that improves all three criteria. For expressivity, we propose a new SSM parameterization based on the companion matrix---a canonical representation for discrete-time processes---which enables SpaceTime&#39;s SSM layers to learn desirable autoregressive processes. For long horizon forecasting, we introduce a &#34;closed-loop&#34; variation of the companion SSM, which enables SpaceTime to predict many future time-steps by generating its own layer-wise inputs. For efficient training and inference, we introduce an algorithm that reduces the memory and compute of a forward pass with the companion matrix. With sequence length $\ell$ and state-space size $d$, we go from $\tilde{O}(d \ell)$ naïvely to $\tilde{O}(d + \ell)$. In experiments, our contributions lead to state-of-the-art results on extensive and diverse benchmarks, with best or second-best AUROC on 6 / 7 ECG and speech time series classification, and best MSE on 14 / 16 Informer forecasting tasks. Furthermore, we find SpaceTime (1) fits AR($p$) processes that prior deep SSMs fail on, (2) forecasts notably more accurately on longer horizons than prior state-of-the-art, and (3) speeds up training on real-world ETTh1 data by 73% and 80% relative wall-clock time over Transformers and LSTMs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1499. <h><a href=https://openreview.net/pdf?id=6QkjC_cs03X>A VAE for Transformers with Nonparametric Variational Information Bottleneck</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>James Henderson, Fabio James Fehr</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6QkjC_cs03X>5.5 - [5, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a Variational AutoEncoder using Bayesian nonparametrics to regularise a Transformer encoder-decoder with latent mixture distributions.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6QkjC_cs03X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a Variational AutoEncoder (VAE) for Transformers by developing a Variational Information Bottleneck (VIB) regulariser for Transformer embeddings.  We formalise such attention-based representations as mixture distributions, and use Bayesian nonparametrics to develop a Nonparametric VIB (NVIB) for them.  The variable number of mixture components supported by nonparametrics captures the variable number of vectors supported by attention, and exchangeable distributions from nonparametrics capture the permutation invariance of attention.  Our Transformer VAE (NVAE) uses NVIB to regularise the information passing from the Transformer encoder to the Transformer decoder.  Evaluations of a NVAE, trained on natural language text, demonstrate that NVIB can regularise the number of mixture components in the induced embedding whilst maintaining generation quality and reconstruction capacity.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1500. <h><a href=https://openreview.net/pdf?id=NUl0ylt7SM>Simple Emergent Action Representations from Multi-Task Policy Training</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Pu Hua, Yubei Chen, Huazhe Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NUl0ylt7SM>5.5 - [6, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We discover emergent action representations from multi-task training and further use them to perform task generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NUl0ylt7SM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The low-level sensory and motor signals in deep reinforcement learning, which exist in high-dimensional spaces such as image observations or motor torques, are inherently challenging to understand or utilize directly for downstream tasks. While sensory representations have been extensively studied, the representations of motor actions are still an area of active exploration. Our work reveals that a space containing meaningful action representations emerges when a multi-task policy network takes as inputs both states and task embeddings. Moderate constraints are added to improve its representation ability. Therefore, interpolated or composed embeddings can function as a high-level interface within this space, providing instructions to the agent for executing meaningful action sequences. Empirical results demonstrate that the proposed action representations are effective for intra-action interpolation and inter-action composition with limited or no additional learning. Furthermore, our approach exhibits superior task adaptation ability compared to strong baselines in Mujoco locomotion tasks. Our work sheds light on the promising direction of learning action representations for efficient, adaptable, and composable RL, forming the basis of abstract action planning and the understanding of motor signal space. Project page: https://sites.google.com/view/emergent-action-representation/
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1501. <h><a href=https://openreview.net/pdf?id=VuuDXDgujAc>HiT-MDP: Learning the SMDP option framework on MDPs with Hidden Temporal Embeddings</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chang Li, Dongjin Song, Dacheng Tao</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=VuuDXDgujAc>5.5 - [5, 3, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/VuuDXDgujAc.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The standard option framework is developed on the Semi-Markov Decision Process (SMDP) which is unstable to optimize and sample inefficient. To this end, we propose the Hidden Temporal MDP (HiT-MDP) and prove that the option-induced HiT-MDP is homomorphic equivalent to the option-induced SMDP. A novel transformer-based framework is introduced to learn options&#39; embedding vectors (rather than conventional option tuples) on HiT-MDPs. We then derive a stable and sample efficient option discovering method under the maximum-entropy policy gradient framework. Extensive experiments on challenging Mujoco environments demonstrate HiT-MDP&#39;s efficiency and effectiveness: under widely used configurations, HiT-MDP achieves competitive, if not better, performance compared to the state-of-the-art baselines on all finite horizon and transfer learning environments. Moreover, HiT-MDP significantly outperforms all baselines on infinite horizon environments while exhibiting smaller variance, faster convergence, and better interpretability. Our work potentially sheds light on the theoretical ground of extending the option framework into a large-scale foundation model.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1502. <h><a href=https://openreview.net/pdf?id=li7qeBbCR1t>Building Normalizing Flows with Stochastic Interpolants</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Michael Samuel Albergo, Eric Vanden-Eijnden</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=li7qeBbCR1t>5.5 - [3, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A method is proposed to construct normalizing flows based on stochastic interpolants, yielding an efficient training algorithm compared to equivalent ODE methods, and providing a theoretical framework to map score based diffusions to ODEs.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/li7qeBbCR1t.jpg loading="lazy">
                </div>
                <div class="abstract">
                    A generative model based on a continuous-time normalizing flow between any pair of base and target probability densities is proposed. The velocity field of this flow is inferred from the probability current of a time-dependent density that interpolates between the base and the target in finite time. Unlike conventional normalizing flow inference methods based the maximum likelihood principle, which require costly backpropagation through ODE solvers, our interpolant approach leads to a simple quadratic loss for the velocity itself which is expressed in terms of expectations that are readily amenable to empirical estimation. The flow can be used to generate samples from either the base or target, and to estimate the likelihood at any time along the interpolant. In addition, the flow can be optimized to minimize the path length of the interpolant density, thereby paving the way for building optimal transport maps. In situations where the base is a Gaussian density, we also show that the velocity of our normalizing flow can also be used to construct a diffusion model to sample the target as well as estimate its score. However, our approach shows that we can bypass this diffusion completely and work at the level of the probability flow with greater simplicity, opening an avenue for methods based solely on ordinary differential equations as an alternative to those based on stochastic differential equations. Benchmarking on density estimation tasks illustrates that the learned flow can match and surpass conventional continuous flows at a fraction of the cost, and compares well with diffusions on image generation on CIFAR-10 and ImageNet $32 \times 32$. The method scales ab-initio ODE flows to previously unreachable image resolutions, demonstrated up to $128\times128$.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1503. <h><a href=https://openreview.net/pdf?id=EBS4C77p_5S>SLTUNET: A Simple Unified Model for Sign Language Translation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Biao Zhang, Mathias Müller, Rico Sennrich</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=EBS4C77p_5S>5.5 - [6, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A simple unified model for sign language translation that achieves (near) state-of-the-art performance
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/EBS4C77p_5S.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite recent successes with neural models for sign language translation (SLT), translation quality still lags behind spoken languages because of the data scarcity and modality gap between sign video and text. To address both problems, we investigate strategies for cross-modality representation sharing for SLT. We propose SLTUNET, a simple unified neural model designed to support multiple SLTrelated tasks jointly, such as sign-to-gloss, gloss-to-text and sign-to-text translation. Jointly modeling different tasks endows SLTUNET with the capability to explore the cross-task relatedness that could help narrow the modality gap. In addition, this allows us to leverage the knowledge from external resources, such as abundant parallel data used for spoken-language machine translation (MT). We show in experiments that SLTUNET achieves competitive and even state-of-the-art performance on PHOENIX-2014T and CSL-Daily when augmented with MT data and equipped with a set of optimization techniques. We further use the DGS Corpus for end-to-end SLT for the first time. It covers broader domains with a significantly larger vocabulary, which is more challenging and which we consider to allow for a more realistic assessment of the current state of SLT than the former two. Still, SLTUNET obtains improved results on the DGS Corpus. Code is available at https://github.com/bzhangGo/sltunet.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1504. <h><a href=https://openreview.net/pdf?id=QUaDoIdgo0>CO3: Cooperative Unsupervised 3D Representation Learning for Autonomous Driving</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Runjian Chen, Yao Mu, Runsen Xu, Wenqi Shao, Chenhan Jiang, Hang Xu, Yu Qiao, Zhenguo Li, Ping Luo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QUaDoIdgo0>5.5 - [5, 6, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose CO3, namely {Co}operative {Co}ntrastive Learning and {Co}ntextual Shape Prediction, to learn 3D representation for outdoor-scene point clouds in an unsupervised manner.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QUaDoIdgo0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised contrastive learning for indoor-scene point clouds has achieved great successes. However, unsupervised representation learning on outdoor-scene point clouds remains challenging because previous methods need to reconstruct the whole scene and capture partial views for the contrastive objective. This is infeasible in outdoor scenes with moving objects, obstacles, and sensors. In this paper, we propose CO3, namely {Co}operative {Co}ntrastive Learning and {Co}ntextual Shape Prediction, to learn 3D representation for outdoor-scene point clouds in an unsupervised manner. CO3 has several merits compared to existing methods. (1) It utilizes LiDAR point clouds from vehicle-side and infrastructure-side to build views that differ enough but meanwhile maintain common semantic information for contrastive learning, which are more appropriate than views built by previous methods. (2) Alongside the contrastive objective, we propose contextual shape prediction to bring more task-relevant information for unsupervised 3D point cloud representation learning and we also provide a theoretical analysis for this pre-training goal. (3) As compared to previous methods, representation learned by CO3 is able to be transferred to different outdoor scene dataset collected by different type of LiDAR sensors. (4) CO3 improves current state-of-the-art methods on Once, KITTI and NuScenes datasets by up to 2.58 mAP in 3D object detection task and 3.54 mIoU in LiDAR semantic segmentation task. Codes and models will be released.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1505. <h><a href=https://openreview.net/pdf?id=2SV2dlfBuE3>Predictor-corrector algorithms for stochastic optimization under gradual distribution shift</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Subha Maity, Debarghya Mukherjee, Moulinath Banerjee, Yuekai Sun</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2SV2dlfBuE3>5.5 - [6, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2SV2dlfBuE3.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Time-varying stochastic optimization problems frequently arise in machine learning practice (e.g., gradual domain shift, object tracking, strategic classification). Often, the underlying process that drives the distribution shift is continuous in nature. We exploit this underlying continuity by developing predictor-corrector algorithms for time-varying stochastic optimization that anticipates changes in the underlying data generating process through a predictor-corrector term in the update rule.  The key challenge is the estimation of the predictor-corrector term; a naive approach based on sample-average approximation may lead to non-convergence. We develop a general moving-average based method to estimate the predictor-corrector term and provide error bounds for the iterates, both in presence of pure and noisy access to the queries from the relevant derivatives of the loss function. Furthermore, we show (theoretically and empirically in several examples) that our method outperforms non-predictor corrector methods that do not anticipate changes in the data generating process.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1506. <h><a href=https://openreview.net/pdf?id=rdjeCNUS6TG>Multivariate Time-series Imputation with Disentangled Temporal Representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>SHUAI LIU, Xiucheng Li, Gao Cong, Yile Chen, YUE JIANG</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rdjeCNUS6TG>5.5 - [5, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a multivariate time-series imputation model based on matrix factorization, which composes meaningful disentangled temporal representations that account for multiple explanatory factors (trend, seasonality, local bias).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rdjeCNUS6TG.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Multivariate time series often faces the problem of missing value. Many time series imputation methods have been developed in the literature. However, these methods all rely on an entangled representation to model dynamics of time series, which may fail to fully exploit the multiple factors (e.g., periodic patterns) contained in the time series. Moreover, the entangled representation usually has no semantic meaning, and thus they often lack interpretability. In addition, many recent models are proposed to deal with the whole time series to capture cross-channel correlations and identify temporal dynamics, but they are not scalable to large-scale datasets. Different from existing approaches, we propose TIDER, a novel matrix factorization-based method with disentangled temporal representations that account for multiple factors, namely trend, seasonality, and local bias, to model complex dynamics. The learned disentanglement makes the imputation process more reliable and offers explainability for imputation results. Moreover, TIDER is scalable to large datasets. Empirical results show that our method not only outperforms existing approaches by notable margins on three real-world datasets, but also scales well to large datasets on which existing deep learning based methods struggle. Disentanglement validation experiments further demonstrate the robustness of our model in obtaining accurate and explainable disentangled components.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1507. <h><a href=https://openreview.net/pdf?id=kIAx30hYi_p>Self-Supervised Set Representation Learning for Unsupervised Meta-Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Dong Bok Lee, Seanie Lee, Kenji Kawaguchi, Yunji Kim, Jihwan Bang, Jung-Woo Ha, Sung Ju Hwang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kIAx30hYi_p>5.5 - [5, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose self-supervised set representation learning for unsupervised meta-learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kIAx30hYi_p.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Unsupervised meta-learning (UML) essentially shares the spirit of self-supervised learning (SSL) in that their goal aims at learning models without any human supervision so that the models can be adapted to downstream tasks. Further, the learning objective of self-supervised learning, which pulls positive pairs closer and repels negative pairs, also resembles metric-based meta-learning. Metric-based meta-learning is one of the most successful meta-learning methods, which learns to minimize the distance between representations from the same class. 
One notable aspect of metric-based meta-learning, however, is that it is widely interpreted as a set-level problem since the inference of discriminative class prototypes (or set representations) from few examples is crucial for the performance of downstream tasks. Motivated by this, we propose Set-SimCLR, a novel self-supervised set representation learning framework for targeting UML problem. Specifically, our Set-SimCLR learns a set encoder on top of instance representations to maximize the agreement between two sets of augmented samples, which are generated by applying stochastic augmentations to a given image. We theoretically analyze how our proposed set representation learning can potentially improve the generalization performance at the meta-test. We also empirically validate its effectiveness on various benchmark datasets, showing that Set-SimCLR largely outperforms both UML and instance-level self-supervised learning baselines.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1508. <h><a href=https://openreview.net/pdf?id=CMPIBjmhpo>Neural Implicit Shape Editing using Boundary Sensitivity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Arturs Berzins, Moritz Ibing, Leif Kobbelt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CMPIBjmhpo>5.5 - [6, 5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CMPIBjmhpo.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Neural fields are receiving increased attention as a geometric representation due to their ability to compactly store detailed and smooth shapes and easily undergo topological changes. Compared to classic geometry representations, however, neural representations do not allow the user to exert intuitive control over the shape. Motivated by this, we leverage boundary sensitivity to express how perturbations in parameters move the shape boundary. This allows us to interpret the effect of each learnable parameter and study achievable deformations. With this, we perform geometric editing: finding a parameter update that best approximates a globally prescribed deformation. Prescribing the deformation only locally allows the rest of the shape to change according to some prior, such as semantics or deformation rigidity. Our method is agnostic to the model and its training and updates the NN in-place. Furthermore, we show how boundary sensitivity helps to optimize and constrain objectives (such as surface area and volume), which are difficult to compute without first converting to another representation, such as a mesh.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1509. <h><a href=https://openreview.net/pdf?id=cYijsVZhb5>Is a Caption Worth a Thousand Images? A Study on Representation Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Shibani Santurkar, Yann Dubois, Rohan Taori, Percy Liang, Tatsunori Hashimoto</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cYijsVZhb5>5.5 - [3, 6, 5, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Our work performs a systematic investigation into whether additional language supervision (in CLIP) helps models learn more transferrable representations.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cYijsVZhb5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The development of CLIP [Radford et al., 2021] has sparked a debate on whether adding language supervision can yield vision models with more transferable representations than traditional image-only methods. Our work studies this question through a carefully controlled comparison of two approaches, in terms of their ability to learn representations that generalize to downstream classification tasks. We find that when the pre-training data meets certain criteria---it is sufficiently large and contains descriptive captions with low variability----image-only methods do not match CLIP&#39;s  performance even when they are trained with more image data. However, contrary to what one might expect, there are practical settings in which these criteria are not met, wherein added supervision through captions is actually detrimental.
Motivated by our findings, we devise simple data and algorithmic interventions to improve the transfer performance of CLIP-style models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1510. <h><a href=https://openreview.net/pdf?id=NpsVSN6o4ul>Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, Jacob Steinhardt</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NpsVSN6o4ul>5.5 - [8, 8, 3, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We find a large circuit for a natural language task in GPT-2 small and provide quantitative evaluation of our human-understandable explanation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NpsVSN6o4ul.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Research in mechanistic interpretability seeks to explain behaviors of ML models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task that requires logical reasoning: indirect object identification (IOI). Our explanation encompasses 28 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches including causal interventions and projections.
To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior &#34;in the wild&#34; in a language model.  We evaluate the reliability of our explanation using three quantitative criteria - faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. 
Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1511. <h><a href=https://openreview.net/pdf?id=SEcSahl0Ql>Iterative Circuit Repair Against Formal Specifications</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Matthias Cosler, Frederik Schmitt, Christopher Hahn, Bernd Finkbeiner</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=SEcSahl0Ql>5.5 - [5, 5, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a deep learning approach for repairing sequential circuits against formal specifications given in linear-time temporal logic (LTL).
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/SEcSahl0Ql.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We present a deep learning approach for repairing sequential circuits against formal specifications given in linear-time temporal logic (LTL). Given a defective circuit and its formal specification, we train Transformer models to output circuits that satisfy the corresponding specification. We propose a separated hierarchical Transformer for multimodal representation learning of the formal specification and the circuit. We introduce a data generation algorithm that enables generalization to more complex specifications and out-of-distribution datasets. In addition, our proposed repair mechanism significantly improves the automated synthesis of circuits from LTL specifications with Transformers. It improves the state-of-the-art by $6.8$ percentage points on held-out instances and $11.8$ percentage points on an out-of-distribution dataset from the annual reactive synthesis competition.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1512. <h><a href=https://openreview.net/pdf?id=jHc8dCx6DDr>Memory Gym: Partially Observable Challenges to Memory-Based Agents</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Marco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jHc8dCx6DDr>5.5 - [3, 6, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Memory Gym is a novel challenge especially to memory-based agents.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jHc8dCx6DDr.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Memory Gym is a novel benchmark for challenging Deep Reinforcement Learning agents to memorize events across long sequences, be robust to noise, and generalize. It consists of the partially observable 2D and discrete control environments Mortar Mayhem, Mystery Path, and Searing Spotlights. These environments are believed to be unsolvable by memory-less agents because they feature strong dependencies on memory and frequent agent-memory interactions. Empirical results based on Proximal Policy Optimization (PPO) and Gated Recurrent Unit (GRU) underline the strong memory dependency of the contributed environments. The hardness of these environments can be smoothly scaled, while different levels of difficulty (some of them unsolved yet) emerge for Mortar Mayhem and Mystery Path. Surprisingly, Searing Spotlights poses a tremendous challenge to GRU-PPO, which remains an open puzzle. Even though the
randomly moving spotlights reveal parts of the environment’s ground truth, environmental ablations hint that these pose a severe perturbation to agents that leverage recurrent model architectures as their memory. 
Source Code: https://github.com/MarcoMeter/drl-memory-gym/
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1513. <h><a href=https://openreview.net/pdf?id=2b2s9vd7wYv>LogicDP: Creating Labels for Graph Data via Inductive Logic Programming</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuan Yang, Faramarz Fekri, James Clayton Kerce, Ali Payani</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=2b2s9vd7wYv>5.5 - [8, 3, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A data programming framework for generating training labels for graph data
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/2b2s9vd7wYv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph data, such as scene graphs and knowledge graphs, see wide use in AI systems. In real-world and large applications graph data are usually incomplete, motivating graph reasoning models for missing-fact or missing-relationship inference. While these models can achieve state-of-the-art performance, they require a large amount of training data.

Recent years have witnessed the rising interest in label creation with data programming (DP) methods, which aim to generate training labels from heuristic labeling functions. However, existing methods typically focus on unstructured data and are not optimized for graphs. In this work, we propose LogicDP, a data programming framework for graph data. Unlike existing DP methods, (1) LogicDP utilizes the inductive logic programming (ILP) technique and automatically discovers the labeling functions from the graph data; (2) LogicDP employs a budget-aware framework to iteratively refine the functions by querying an oracle, which significantly reduces the human efforts in function creations. Experiments show that LogicDP achieves better data efficiency in both scene graph and knowledge graph reasoning tasks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1514. <h><a href=https://openreview.net/pdf?id=7IG0wsTND7w>Domain Generalisation via Domain Adaptation: An Adversarial Fourier Amplitude Approach</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Minyoung Kim, Da Li, Timothy Hospedales</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7IG0wsTND7w>5.5 - [6, 6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We tackle the domain generalisation problem by posing it as a domain adaptation task where we adversarially synthesise the worst-case target domain via Fourier amplitude generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7IG0wsTND7w.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We tackle the domain generalisation (DG) problem by posing it as a domain adaptation (DA) task where we adversarially synthesise the worst-case `target&#39; domain and adapt a model to that worst-case domain, thereby improving the model’s robustness. To synthesise data that is challenging yet semantics-preserving, we generate Fourier amplitude images and combine them with source domain phase images, exploiting the widely believed conjecture from signal processing that amplitude spectra mainly determines image style, while phase data mainly captures image semantics. To synthesise a worst-case domain for adaptation, we train the classifier and the amplitude generator adversarially. Specifically, we exploit the maximum classifier discrepancy (MCD) principle from DA that relates the target domain performance to the discrepancy of classifiers in the model hypothesis space. By Bayesian hypothesis modeling, we express the model hypothesis space effectively as a posterior distribution over classifiers given the source domains, making adversarial MCD minimisation feasible. On the DomainBed benchmark including the large-scale DomainNet dataset, the proposed approach yields significantly improved domain generalisation performance over the state-of-the-art.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1515. <h><a href=https://openreview.net/pdf?id=gOZ_pKANaPW>Unsupervised Model Selection for Time Series Anomaly Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mononito Goswami, Cristian Ignacio Challu, Laurent Callot, Lenon Minorics, Andrey Kan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=gOZ_pKANaPW>5.5 - [8, 6, 3, 5] - Accept: notable-top-25%</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper answers the question-- Given an unlabeled dataset and a set of candidate time series anomaly detectors, how can we select the most accurate model?
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/gOZ_pKANaPW.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question \textit{i.e.} Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, \textit{prediction error}, \textit{model centrality}, and \textit{performance on injected synthetic anomalies}, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multiple imperfect surrogate metrics as a robust rank aggregation problem. We then provide theoretical justification behind the proposed approach. Large-scale experiments on multiple real-world datasets demonstrate that our proposed unsupervised approach is as effective as selecting the most accurate model based on partially labeled data.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1516. <h><a href=https://openreview.net/pdf?id=HnSceSzlfrY>RPM: Generalizable Multi-Agent Policies for Multi-Agent Reinforcement Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wei Qiu, Xiao Ma, Bo An, Svetlana Obraztsova, Shuicheng YAN, Zhongwen Xu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HnSceSzlfrY>5.5 - [5, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HnSceSzlfrY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Despite the recent advancement in multi-agent reinforcement learning (MARL), the MARL agents easily overfit the training environment and perform poorly in evaluation scenarios where other agents behave differently. Obtaining generalizable policies for MARL agents is thus necessary but challenging mainly due to complex multi-agent interactions. In this work, we model the MARL problem with Markov Games and propose a simple yet effective method, called ranked policy memory (RPM), i.e., to maintain a look-up memory of policies to achieve good generalizability. The main idea of RPM is to train MARL policies via gathering massive multi-agent interaction data. In particular, we first rank each agent’s policies by its training episode return, i.e., the episode return of each agent in the training environment; we then save the ranked policies in the memory; when an episode starts, each agent can randomly select a policy from the RPM as the behavior policy. Each agent uses the behavior policy to gather multi-agent interaction data for MARL training. This innovative self-play framework guarantees the diversity of multi-agent interaction in the training data. Experimental results on Melting Pot demonstrate that RPM enables MARL agents to interact with unseen agents in multi-agent generalization evaluation scenarios and complete given tasks. It significantly boosts the performance up to 818% on average.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1517. <h><a href=https://openreview.net/pdf?id=hfUJ4ShyDEU>Achieve the Minimum Width of Neural Networks for Universal Approximation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yongqiang Cai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hfUJ4ShyDEU>5.5 - [8, 5, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We prove that the minimum width of FNN for UAP is $w^*_{\min} = \max(d_x,d_y)$ which is achievable.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hfUJ4ShyDEU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The universal approximation property (UAP) of neural networks is fundamental for deep learning, and it is well known that wide neural networks are universal approximators of continuous functions within both the $L^p$ norm and the continuous/uniform norm. However, the exact minimum width, $w_{\min}$, for the UAP has not been studied thoroughly. Recently, using a decoder-memorizer-encoder scheme, \citet{Park2021Minimum} found that $w_{\min} = \max(d_x+1,d_y)$ for both the $L^p$-UAP of ReLU networks and the $C$-UAP of ReLU+STEP networks, where $d_x,d_y$ are the input and output dimensions, respectively. In this paper, we consider neural networks with an arbitrary set of activation functions. We prove that both $C$-UAP and $L^p$-UAP for functions on compact domains share a universal lower bound of the minimal width; that is, $w^*_{\min} = \max(d_x,d_y)$. In particular, the critical width, $w^*_{\min}$, for $L^p$-UAP can be achieved by leaky-ReLU networks, provided that the input or output dimension is larger than one. Our construction is based on the approximation power of neural ordinary differential equations and the ability to approximate flow maps by neural networks. The nonmonotone or discontinuous activation functions case and the one-dimensional case are also discussed.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1518. <h><a href=https://openreview.net/pdf?id=j3GK3_xZydY>Revisiting Intrinsic Reward for Exploration in Procedurally Generated Environments</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kaixin Wang, Kuangqi Zhou, Bingyi Kang, Jiashi Feng, Shuicheng YAN</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=j3GK3_xZydY>5.4 - [8, 5, 3, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/j3GK3_xZydY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Exploration under sparse rewards remains a key challenge in deep reinforcement learning. Recently, studying exploration in procedurally-generated environments has drawn increasing attention. Existing works generally combine lifelong intrinsic rewards and episodic intrinsic rewards to encourage exploration. Though various lifelong and episodic intrinsic rewards have been proposed, the individual contributions of the two kinds of intrinsic rewards to improving exploration are barely investigated. To bridge this gap, we disentangle these two parts and conduct ablative experiments. We consider lifelong and episodic intrinsic rewards used in prior works, and compare the performance of all lifelong-episodic combinations on the commonly used MiniGrid benchmark. Experimental results show that only using episodic intrinsic rewards can match or surpass prior state-of-the-art methods. On the other hand, only using lifelong intrinsic rewards hardly makes progress in exploration. This demonstrates that episodic intrinsic reward is more crucial than lifelong one in boosting exploration. Moreover, we find through experimental analysis that the lifelong intrinsic reward does not accurately reflect the novelty of states, which explains why it does not help much in improving exploration.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1519. <h><a href=https://openreview.net/pdf?id=CBfYffLqWqb>Evolving Populations of Diverse RL Agents with MAP-Elites</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Thomas PIERROT, Arthur Flajolet</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=CBfYffLqWqb>5.33 - [5, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/CBfYffLqWqb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such as hyperparameter sensitivity, high stochasticity as well as training instability, including when the population size increases as some components are shared across the population in recent approaches. Furthermore, existing approaches mixing ME with RL tend to be tied to a specific RL algorithm, which effectively prevents their use on problems where the corresponding RL algorithm fails. To address these shortcomings, we introduce a flexible framework that allows the use of any RL algorithm and alleviates the aforementioned limitations by evolving populations of agents (whose definition include hyperparameters and all learnable parameters) instead of just policies. We demonstrate the benefits brought about by our framework through extensive numerical experiments on a number of robotics control problems, some of which with deceptive rewards, taken from the QD-RL literature. We open source an efficient JAX-based implementation of our algorithm in the QDax library. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1520. <h><a href=https://openreview.net/pdf?id=kJqXEPXMsE0>3D Equivariant Diffusion for Target-Aware Molecule Generation and Affinity Prediction</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, Jianzhu Ma</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kJqXEPXMsE0>5.33 - [6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kJqXEPXMsE0.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Rich data and powerful machine learning models allow us to design drugs for a specific protein target &lt;em&gt;in silico&lt;/em&gt;. Recently, the inclusion of 3D structures during targeted drug design shows superior performance to other target-free models as the atomic interaction in the 3D space is explicitly modeled. However, current 3D target-aware models either rely on the voxelized atom densities or the autoregressive sampling process, which are not equivariant to rotation or easily violate geometric constraints resulting in unrealistic structures. In this work, we develop a 3D equivariant diffusion model to solve the above challenges. To achieve target-aware molecule design, our method learns a joint generative process of both continuous atom coordinates and categorical atom types with a SE(3)-equivariant network. Moreover, we show that our model can serve as an unsupervised feature extractor to estimate the binding affinity under proper parameterization, which provides an effective way for drug screening. To evaluate our model, we propose a comprehensive framework to evaluate the quality of sampled molecules from different dimensions. Empirical studies show our model could generate molecules with more realistic 3D structures and better affinities towards the protein targets, and improve binding affinity ranking and prediction without retraining.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1521. <h><a href=https://openreview.net/pdf?id=5fvXH49wk2>D4AM: A General Denoising Framework for Downstream Acoustic Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chi-Chang Lee, Yu Tsao, Hsin-Min Wang, Chu-Song Chen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5fvXH49wk2>5.33 - [5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a general denoising framework for various downstream acoustic models (D4AM) by adopting an effective joint training scheme with the regression (denoising) objective and the classification (ASR) objective.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5fvXH49wk2.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The performance of acoustic models degrades notably in noisy environments. Speech enhancement (SE) can be used as a front-end strategy to aid automatic speech recognition (ASR) systems. However, existing training objectives of SE methods are not fully effective at integrating speech-text and noise-clean paired data for training toward unseen ASR systems. In this study, we propose a general denoising framework, D4AM, for various downstream acoustic models. Our framework fine-tunes the SE model with the backward gradient according to a specific acoustic model and the corresponding classification objective. In addition, our method aims to consider the regression objective as an auxiliary loss to make the SE model generalize to other unseen acoustic models. To jointly train an SE unit with regression and classification objectives, D4AM uses an adjustment scheme to directly estimate suitable weighting coefficients rather than undergoing a grid search process with additional training costs. The adjustment scheme consists of two parts: gradient calibration and regression objective weighting. The experimental results show that D4AM can consistently and effectively provide improvements to various unseen acoustic models and outperforms other combination setups. Specifically, when evaluated on the Google ASR API with real noisy data completely unseen during SE training, D4AM achieves a relative WER reduction of 24.65% compared with the direct feeding of noisy input. To our knowledge, this is the first work that deploys an effective combination scheme of regression (denoising) and classification (ASR) objectives to derive a general pre-processor applicable to various unseen ASR systems. Our code is available at https://github.com/ChangLee0903/D4AM.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1522. <h><a href=https://openreview.net/pdf?id=6t0Kwf8-jrj>Editing models with task arithmetic</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6t0Kwf8-jrj>5.33 - [5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We study a new paradigm for editing pre-trained models, where weight vectors obtained via fine-tuning can be combined to efficiently and effectively steer model behavior.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6t0Kwf8-jrj.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Changing how pre-trained models behave---e.g., improving their performance on a downstream task or mitigating biases learned during pre-training---is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around task vectors. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Moreover, task vectors can be added together to improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D&#34;, combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1523. <h><a href=https://openreview.net/pdf?id=YDJRFWBMNby>HotProtein: A Novel Framework for Protein Thermostability Prediction and Editing</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tianlong Chen, Chengyue Gong, Daniel Jesus Diaz, Xuxi Chen, Jordan Tyler Wells, qiang liu, Zhangyang Wang, Andrew Ellington, Alex Dimakis, Adam Klivans</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=YDJRFWBMNby>5.33 - [8, 3, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A new dataset and a novel learning framework for protein thermostability prediction and editing.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/YDJRFWBMNby.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The molecular basis of protein thermal stability is only partially understood and has major significance for drug and vaccine discovery.  The lack of datasets and standardized benchmarks considerably limits learning-based discovery methods. We present \texttt{HotProtein}, a large-scale protein dataset with \textit{growth temperature} annotations of thermostability, containing $182$K amino acid sequences and $3$K folded structures from $230$ different species with a wide temperature range $-20^{\circ}\texttt{C}\sim 120^{\circ}\texttt{C}$. Due to functional domain differences and data scarcity within each species, existing methods fail to generalize well on our dataset. We address this problem through a novel learning framework, consisting of ($1$) Protein structure-aware pre-training (SAP) which leverages 3D information to enhance sequence-based pre-training; ($2$) Factorized sparse tuning (FST) that utilizes low-rank and sparse priors as an implicit regularization, together with feature augmentations. Extensive empirical studies demonstrate that our framework improves thermostability prediction compared to other deep learning models. Finally, we introduce a novel editing algorithm to efficiently generate positive amino acid mutations that improve thermostability. Codes are available in https://github.com/VITA-Group/HotProtein.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1524. <h><a href=https://openreview.net/pdf?id=HtAfbHa7LAL>MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Neo Wei Ming, Zhehui Wang, Cheng Liu, Rick Siow Mong Goh, Tao Luo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HtAfbHa7LAL>5.33 - [6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> MA-BERT completely eliminates the complex non-linear functions in BERT and achieves matrix arithmetic-only operation with trivial ReLU, which could benefit inference on both general computing units and accelerator designs for edge applications
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HtAfbHa7LAL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Due to their superior results, Transformer-based models such as BERT have become de facto standards in many Natural Language Processing (NLP) applications. However, the intensive use of complex non-linear functions within the Transformer architecture impairs its computing efficiency and complicates corresponding accelerator designs, because non-linear functions are generally computation-intensive and require special hardware support. In light of this, we propose MA-BERT, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss. Specifically, we propose four correlated techniques that include approximating softmax with a two-layer neural network, replacing GELU with ReLU, fusing normalization layers with adjacent linear layers, and leveraging knowledge transfer from baseline models. Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy. With mainly regular matrix arithmetic operations, MA-BERT enables hardware-friendly processing on various computing engines, including CPUs and GPUs. Our experimental results show that MA-BERT achieves up to 27% and 41% reduction in inference time on CPU and GPU, respectively, with comparable accuracy on many downstream tasks compared to the baseline BERT models. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1525. <h><a href=https://openreview.net/pdf?id=Rvee9CAX4fi>Verifying the Union of Manifolds Hypothesis for Image Data</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Bradley CA Brown, Anthony L. Caterini, Brendan Leigh Ross, Jesse C Cresswell, Gabriel Loaiza-Ganem</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=Rvee9CAX4fi>5.33 - [3, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show data of interest has varying intrinsic dimension, thus conforming to a union of manifolds hypothesis rather than the manifold hypothesis; and we study some implications in deep learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/Rvee9CAX4fi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep learning has had tremendous success at learning low-dimensional representations of high-dimensional data. This success would be impossible if there was no hidden low-dimensional structure in data of interest; this existence is posited by the manifold hypothesis, which states that the data lies on an unknown manifold of low intrinsic dimension. In this paper, we argue that this hypothesis does not properly capture the low-dimensional structure typically present in image data. Assuming that data lies on a single manifold implies intrinsic dimension is identical across the entire data space, and does not allow for subregions of this space to have a different number of factors of variation. To address this deficiency, we consider the union of manifolds hypothesis, which states that data lies on a disjoint union of manifolds of varying intrinsic dimensions. We empirically verify this hypothesis on commonly-used image datasets, finding that indeed, observed data lies on a disconnected set and that intrinsic dimension is not constant. We also provide insights into the implications of the union of manifolds hypothesis in deep learning, both supervised and unsupervised, showing that designing models with an inductive bias for this structure improves performance across classification and generative modelling tasks. Our code is available at https://github.com/layer6ai-labs/UoMH.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1526. <h><a href=https://openreview.net/pdf?id=9yE2xEj0BH7>Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Gang Li, Yang Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=9yE2xEj0BH7>5.33 - [5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an enhanced vision-language model for UI tasks that achieves SoTA on representative UI tasks and supports few-shot and multitask learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/9yE2xEj0BH7.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Mobile UI understanding is important for enabling various interaction tasks such as UI automation and accessibility. Previous mobile UI modeling often depends on the view hierarchy information of a screen, which directly provides the structural data of the UI, with the hope to bypass challenging tasks of visual modeling from screen pixels. However, view hierarchies are not always available, and are often corrupted with missing object descriptions or misaligned structure information. As a result, despite the use of view hierarchies could offer short-term gains, it may ultimately hinder the applicability and performance of the model. In this paper, we propose Spotlight, a vision-only approach for mobile UI understanding. Specifically, we enhance a vision-language model that only takes the screenshot of the UI and a region of interest on the screen---the focus---as the input. This general architecture of Spotlight is easily scalable and capable of performing a range of UI modeling tasks. Our experiments show that our model establishes SoTA results on several representative UI tasks and outperforms previous methods that use both screenshots and view hierarchies as inputs. Furthermore, we explore multi-task learning and few-shot prompting capacities of the proposed models, demonstrating promising results in the multi-task learning direction.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1527. <h><a href=https://openreview.net/pdf?id=HRwN7IQLUKA>Accelerated Single-Call Methods for Constrained Min-Max Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yang Cai, Weiqiang Zheng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HRwN7IQLUKA>5.33 - [5, 8, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose the first single-call single-projection algorithms with optimal convergence rate for constrained min-max optimization problems in the nonconvex-nonconcave setting.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HRwN7IQLUKA.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study first-order methods for constrained min-max optimization. Existing methods either require two gradient calls or two projections in each iteration, which may be costly in some applications. In this paper, we first show that a variant of the \emph{Optimistic Gradient (OG)} method, a \emph{single-call single-projection} algorithm,  has $O(\frac{1}{\sqrt{T}})$ best-iterate convergence rate for inclusion problems with operators that satisfy the weak Minty variation inequality (MVI). Our second result is the first single-call single-projection algorithm -- the \emph{Accelerated Reflected Gradient (ARG)} method that achieves the \emph{optimal $O(\frac{1}{T})$} last-iterate convergence rate for inclusion problems that satisfy negative comonotonicity. Both the weak MVI and negative comonotonicity are well-studied assumptions and capture a rich set of non-convex non-concave min-max optimization problems. Finally, we show that the \emph{Reflected Gradient (RG)} method, another \emph{single-call single-projection} algorithm,  has $O(\frac{1}{\sqrt{T}})$ last-iterate convergence rate for constrained convex-concave min-max optimization, answering an open problem of [Hsieh et al., 2019]. Our convergence rates hold for standard measures such as the tangent residual and the natural residual. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1528. <h><a href=https://openreview.net/pdf?id=ZW5aK4yCRqU>Modelling Long Range Dependencies in $N$D: From Task-Specific to a General Purpose CNN</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>David M Knigge, David W. Romero, Albert Gu, Efstratios Gavves, Erik J Bekkers, Jakub Mikolaj Tomczak, Mark Hoogendoorn, Jan-jakob Sonke</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ZW5aK4yCRqU>5.25 - [6, 6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We formulate a CNN architecture that can be used across input resolutions, lengths and dimensionalities (1D, 2D, 3D) showing its viability across several 1D, 2D and 3D tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ZW5aK4yCRqU.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Performant Convolutional Neural Network (CNN) architectures must be tailored to specific tasks in order to consider the length, resolution, and dimensionality of the input data. In this work, we tackle the need for problem-specific CNN architectures. We present the Continuous Convolutional Neural Network (CCNN): a single CNN able to process data of arbitrary resolution, dimensionality and length without any structural changes.  Its key component are its continuous convolutional kernels which model long-range dependencies at every layer, and thus remove the need of current CNN architectures for task-dependent downsampling and depths. We showcase the generality of our method by using the same architecture for tasks on sequential ($1{\rm D}$), visual ($2{\rm D}$) and point-cloud ($3{\rm D}$) data. Our CCNN matches and often outperforms the current state-of-the-art across all tasks considered.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1529. <h><a href=https://openreview.net/pdf?id=_-FN9mJsgg>Improving Object-centric Learning with Query Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Baoxiong Jia, Yu Liu, Siyuan Huang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_-FN9mJsgg>5.25 - [6, 3, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> With simple adjustments to slot attention, we propose a model that significantly outperforms previous baselines (~10%) on both synthetic and real images, and shows potential in concept binding and generalization.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_-FN9mJsgg.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The ability to decompose complex natural scenes into meaningful object-centric abstractions lies at the core of human perception and reasoning. In the recent culmination of unsupervised object-centric learning, the Slot-Attention module has played an important role with its simple yet effective design and fostered many powerful variants. These methods, however, have been exceedingly difficult to train
without supervision and are ambiguous in the notion of object, especially for complex natural scenes. In this paper, we propose to address these issues by investigating the potential of learnable queries as initializations for Slot-Attention learning, uniting it with efforts from existing attempts on improving Slot-Attention learning with bi-level optimization. With simple code adjustments on Slot-Attention, our model, Bi-level Optimized Query Slot Attention, achieves state-of-the-art results on 3 challenging synthetic and 7 complex real-world datasets in unsupervised image segmentation and reconstruction, outperforming previous baselines by a large margin. We provide thorough ablative studies to validate the necessity and effectiveness of our design. Additionally, our model exhibits great potential for concept binding and zero-shot learning. Our work is made publicly available at https://bo-qsa.github.io.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1530. <h><a href=https://openreview.net/pdf?id=m6ahb1mpwwX>InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhuoran Yu, Yin Li, Yong Jae Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=m6ahb1mpwwX>5.25 - [6, 6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A novel pseudo-labeling approach that views pseudo-labeling as an evolving &#34;in-distribution vs out-of-distribution&#34; classification problem for imbalanced semi-supervised learning
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/m6ahb1mpwwX.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Recent state-of-the-art methods in imbalanced semi-supervised learning (SSL) rely on confidence-based pseudo-labeling with consistency regularization. To obtain high-quality pseudo-labels, a high confidence threshold is typically adopted.  However, it has been shown that softmax-based confidence scores in deep networks can be arbitrarily high for samples far from the training data, and thus, the pseudo-labels for even high-confidence unlabeled samples may still be unreliable.  In this work, we present a new perspective of pseudo-labeling for imbalanced SSL. Without relying on model confidence, we propose to measure whether an unlabeled sample is likely to be &#34;in-distribution&#39;&#39;; i.e., close to the current training data. To decide whether an unlabeled sample is &#34;in-distribution&#39;&#39; or &#34;out-of-distribution&#39;&#39;, we adopt the energy score from out-of-distribution detection literature. As training progresses and more unlabeled samples become in-distribution and contribute to training, the combined labeled and pseudo-labeled data can better approximate the true class distribution to improve the model. Experiments demonstrate that our energy-based pseudo-labeling method, InPL, albeit conceptually simple, significantly outperforms confidence-based methods on imbalanced SSL benchmarks. For example, it produces a 4-6% absolute accuracy improvement on CIFAR10-LT when the imbalance ratio is higher than 50. When combined with state-of-the-art long-tailed SSL methods, further improvements are attained. In particular, in one of the most challenging scenarios, InPL achieves a 6.9% accuracy improvement over the best competitor.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1531. <h><a href=https://openreview.net/pdf?id=zOHQGKO3WGY>Semi-supervised learning with a principled likelihood from a generative model of data curation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Stoil Krasimirov Ganev, Laurence Aitchison</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zOHQGKO3WGY>5.25 - [5, 3, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We develop Bayesian semi-supervised learning, by showing that standard SSL objectives can be understood as lower bounds on a principled log-likelihood
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zOHQGKO3WGY.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We currently do not have an understanding of semi-supervised learning (SSL) objectives such as pseudo-labelling and entropy minimization as log-likelihoods, which precludes the development of e.g. Bayesian SSL. Here, we note that benchmark image datasets such as CIFAR-10 are carefully curated, and we formulate SSL objectives as a log-likelihood in a generative model of data curation. We show that SSL objectives, from entropy minimization and pseudo-labelling, to state-of-the-art techniques similar to FixMatch can be understood as lower-bounds on our principled log-likelihood. We are thus able to introduce a Bayesian extension of SSL, which gives considerable improvements over standard SSL in the setting of 40 labelled points on CIFAR-10, with performance of $92.2\pm 0.3\%$ vs $88.6\%$ in the original FixMatch paper. Finally, our theory suggests that SSL is effective in part due to the statistical patterns induced by data curation. This provides an explanation of past results which show SSL performs better on clean datasets without any ``out of distribution&#39;&#39; examples. Confirming these results we find that SSL gave much larger performance improvements on curated than on uncurated data, using matched curated and uncurated datasets based on Galaxy Zoo 2.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1532. <h><a href=https://openreview.net/pdf?id=boNyg20-JDm>Harnessing Out-Of-Distribution Examples via Augmenting Content and Style</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhuo Huang, Xiaobo Xia, Li Shen, Bo Han, Mingming Gong, Chen Gong, Tongliang Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=boNyg20-JDm>5.25 - [6, 5, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Harnessing OOD examples through data augmentation that changes the content and style. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/boNyg20-JDm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Machine learning models are vulnerable to Out-Of-Distribution (OOD) examples, such a problem has drawn much attention. However, current methods lack a full understanding of different types of OOD data: there are benign OOD data that can be properly adapted to enhance the learning performance, while other malign OOD data would severely degenerate the classification result. To Harness OOD data, this paper proposes HOOD method that can leverage the content and style from each image instance to identify benign and malign OOD data. Particularly, we design a variational inference framework to causally disentangle content and style features by constructing a structural causal model. Subsequently, we augment the content and style through an intervention process to produce malign and benign OOD data, respectively. The benign OOD data contain novel styles but hold our interested contents, and they can be leveraged to help train a style-invariant model. In contrast, the malign OOD data inherit unknown contents but carry familiar styles, by detecting them can improve model robustness against deceiving anomalies. Thanks to the proposed novel disentanglement and data augmentation techniques, HOOD can effectively deal with OOD examples in unknown and open environments, whose effectiveness is empirically validated in three typical OOD applications including OOD detection, open-set semi-supervised learning, and open-set domain adaptation.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1533. <h><a href=https://openreview.net/pdf?id=QzbKH8nNq_V>MaskFusion: Feature Augmentation for Click-Through Rate Prediction via Input-adaptive Mask Fusion</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chao Liao, Jianchao Tan, Jiyuan Jia, Yi Guo, Chengru Song</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=QzbKH8nNq_V>5.25 - [5, 3, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Feature Augmentation via Adaptive Mask Fusion
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/QzbKH8nNq_V.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Click-through rate (CTR) prediction plays important role in the advertisement, recommendation, and retrieval applications. Given the feature set, how to fully utilize the information from the feature set is an active topic in deep CTR model designs. There are several existing deep CTR works focusing on feature interactions, feature attentions, and so on. They attempt to capture high-order feature interactions to enhance the generalization ability of deep CTR models. However, these works either suffer from poor high-order feature interaction modeling using DNN or ignore the balance between generalization and memorization during the recommendation. To mitigate these problems, we propose an adaptive feature fusion framework called MaskFusion, to additionally capture the explicit interactions between the input feature and the existing deep part structure of deep CTR models dynamically, besides the common feature interactions proposed in existing works.  MaskFusion is an instance-aware feature augmentation method, which makes deep CTR models more personalized by assigning each feature with an instance-adaptive mask and fusing each feature with each hidden state vector in the deep part structure. MaskFusion can also be integrated into any existing deep CTR models flexibly. MaskFusion achieves state-of-the-art (SOTA) performance on all seven benchmarks deep CTR models with three public datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1534. <h><a href=https://openreview.net/pdf?id=mkJm5Uy4HrQ>Incompatibility Clustering as a Defense Against Backdoor Poisoning Attacks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Charles Jin, Melinda Sun, Martin Rinard</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mkJm5Uy4HrQ>5.25 - [6, 3, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We observe that training with poisoned data does not improve clean accuracy (and vice-versa), and develop a defense that exploits this property.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mkJm5Uy4HrQ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a novel clustering mechanism based on an incompatibility property between subsets of data that emerges during model training. This mechanism partitions the dataset into subsets that generalize only to themselves, i.e., training on one subset does not improve performance on the other subsets. Leveraging the interaction between the dataset and the training process, our clustering mechanism partitions datasets into clusters that are defined by—and therefore meaningful to—the objective of the training process.

We apply our clustering mechanism to defend against data poisoning attacks, in which the attacker injects malicious poisoned data into the training dataset to affect the trained model&#39;s output. Our evaluation focuses on backdoor attacks against deep neural networks trained to perform image classification using the GTSRB and CIFAR-10 datasets. Our results show that (1) these attacks produce poisoned datasets in which the poisoned and clean data are incompatible and (2) our technique successfully identifies (and removes) the poisoned data. In an end-to-end evaluation, our defense reduces the attack success rate to below 1% on 134 out of 165 scenarios, with only a 2% drop in clean accuracy on CIFAR-10 and a negligible drop in clean accuracy on GTSRB.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1535. <h><a href=https://openreview.net/pdf?id=4gc3MGZra1d>On Representing Mixed-Integer Linear Programs by Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ziang Chen, Jialin Liu, Xinshang Wang, Wotao Yin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4gc3MGZra1d>5.25 - [6, 1, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4gc3MGZra1d.jpg loading="lazy">
                </div>
                <div class="abstract">
                    While Mixed-integer linear programming (MILP) is NP-hard in general, practical MILP has received roughly 100--fold speedup in the past twenty years. Still, many classes of MILPs quickly become unsolvable as their sizes increase, motivating researchers to seek new acceleration techniques for MILPs. With deep learning, they have obtained strong empirical results, and many results were obtained by applying graph neural networks (GNNs) to making decisions in various stages of MILP solution processes. This work discovers a fundamental limitation: there exist feasible and infeasible MILPs that all GNNs will, however, treat equally, indicating GNN&#39;s lacking power to express general MILPs. Then, we show that, by restricting the MILPs to unfoldable ones or by adding random features, there exist GNNs that can reliably predict MILP feasibility, optimal objective values, and optimal solutions up to prescribed precision.  We conducted small-scale numerical experiments to validate our theoretical findings.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1536. <h><a href=https://openreview.net/pdf?id=a2-aoqmeYM4>Approximate Bayesian Inference with Stein Functional Variational Gradient Descent</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tobias Pielok, Bernd Bischl, David Rügamer</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=a2-aoqmeYM4>5.25 - [5, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/a2-aoqmeYM4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We propose a general-purpose variational algorithm that forms a natural analogue of Stein variational gradient descent (SVGD) in function space. While SVGD successively updates a set of particles to match a target density, the method introduced here of Stein functional variational gradient descent (SFVGD) updates a set of particle functions to match a target stochastic process (SP). The update step is found by minimizing the functional derivative of the Kullback-Leibler divergence between SPs. SFVGD can either be used to train Bayesian neural networks (BNNs) or for ensemble gradient boosting. We show the efficacy of training BNNs with SFVGD on various real-world datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1537. <h><a href=https://openreview.net/pdf?id=5cAI0qXxyv>$\mathscr{N}$-WL: A New Hierarchy of Expressivity for Graph Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qing Wang, Dillon Ze Chen, Asiri Wijesinghe, Shouheng Li, Muhammad Farhan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=5cAI0qXxyv>5.25 - [5, 5, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/5cAI0qXxyv.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The expressive power of Graph Neural Networks (GNNs) is fundamental for understanding their capabilities and  limitations, i.e., what graph properties can or cannot be learnt by a GNN. Since standard GNNs have been characterised to be upper-bounded by the Weisfeiler-Lehman (1-WL) algorithm, recent attempts concentrated on developing more expressive GNNs in terms of the $k$-WL hierarchy, a well-established framework for graph isormorphism tests. In this work we show that, contrary to the widely accepted view, the $k$-WL hierarchy is not well-suited for measuring expressive GNNs. This is due to limitations that are inherent to high-dimensional WL algorithms such as the lack of a natural interpretation and high computational costs, which makes it difficult to draw any firm conclusions about the expressive power of GNNs beyond 1-WL. Thus, we propose a novel hierarchy of graph isomorphism tests, namely Neighbourhood WL ($\mathscr{N}$-WL), and also establish a new theorem on the equivalence of expressivity between induced connected subgraphs and induced subgraphs within this hierarchy. Further, we design a GNN model upon $\mathscr{N}$-WL, Graph Neighbourhood Neural Network (G3N), and empirically verify its expressive power on synthetic and real-world benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1538. <h><a href=https://openreview.net/pdf?id=8XqDnrmZQNF>Causality Compensated Attention for Contextual Biased Visual Recognition</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ruyang Liu, Jingjia Huang, Thomas H. Li, Ge Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8XqDnrmZQNF>5.25 - [3, 6, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8XqDnrmZQNF.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Visual attention does not always capture the essential object representation desired for robust predictions. Attention modules tend to underline not only the target object but also the common co-occurring context that the module thinks helpful in the training. The problem is rooted in the confounding effect of the context leading to incorrect causalities between objects and predictions, which is further exacerbated by visual attention. In this paper, to learn causal object features robust for contextual bias, we propose a novel attention module named Interventional Dual Attention (IDA) for visual recognition. Specifically, IDA adopts two attention layers with multiple sampling intervention, which compensates the attention against the confounder context. Note that our method is model-agnostic and thus can be implemented on various backbones. Extensive experiments show our model obtains significant improvements in classification and detection with lower computation. In particular, we achieve the state-of-the-art results in multi-label classification on MS-COCO and PASCAL-VOC. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1539. <h><a href=https://openreview.net/pdf?id=6qZC7pfenQm>Improving Deep Policy Gradients with Value Function Search</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Enrico Marchesini, Christopher Amato</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=6qZC7pfenQm>5.25 - [5, 6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a Value Function Search that employs a gradient-free population of perturbed value networks to improve Deep Policy Gradient primitives, leading to higher returns and better sample efficiency.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/6qZC7pfenQm.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Deep Policy Gradient (PG) algorithms employ value networks to drive the learning of parameterized policies and reduce the variance of the gradient estimates. However, value function approximation gets stuck in local optima and struggles to fit the actual return, limiting the variance reduction efficacy and leading policies to sub-optimal performance. This paper focuses on improving value approximation and analyzing the effects on Deep PG primitives such as value prediction, variance reduction, and correlation of gradient estimates with the true gradient. To this end, we introduce a Value Function Search that employs a population of perturbed value networks to search for a better approximation. Our framework does not require additional environment interactions, gradient computations, or ensembles, providing a computationally inexpensive approach to enhance the supervised learning task on which value networks train. Crucially, we show that improving Deep PG primitives results in improved sample efficiency and policies with higher returns using common continuous control benchmark domains.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1540. <h><a href=https://openreview.net/pdf?id=zZhX4eYNeeh>Achieving Sub-linear Regret in Infinite Horizon Average Reward Constrained MDP with Linear Function Approximation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Arnob Ghosh, Xingyu Zhou, Ness Shroff</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=zZhX4eYNeeh>5.25 - [6, 3, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We provide the first sub-linear regret and sub-linear constraint violation for constrained MDP for linear function approximation using model-free RL algorithm
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/zZhX4eYNeeh.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We study the infinite horizon average reward constrained Markov Decision Process (CMDP). In contrast to existing works on model-based, finite state space, we consider the model-free linear CMDP setup.  We first propose a computationally inefficient algorithm and show that $\tilde{\mathcal{O}}(\sqrt{d^3T})$ regret and constraint violation can be achieved, in which $T$ is the number of interactions, and $d$ is the dimension of the feature mapping. We also propose an efficient variant based on the primal-dual adaptation of the LSVI-UCB algorithm and show that $\tilde{\mathcal{O}}((dT)^{3/4})$ regret and constraint violation can be achieved. 
This improves the known regret bound of $\tilde{\mathcal{O}}(T^{5/6})$ for the finite state-space model-free constrained RL which was obtained under a stronger assumption compared to ours.  We also develop an efficient policy-based algorithm via novel adaptation of the MDP-EXP2 algorithm to our primal-dual set up with $\tilde{\mathcal{O}}(\sqrt{T})$ regret and even zero constraint violation bound under a stronger set of assumptions.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1541. <h><a href=https://openreview.net/pdf?id=0vqjc50HfcC>DDM$^2$: Self-Supervised Diffusion MRI Denoising with Generative Diffusion Models</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Tiange Xiang, Mahmut Yurt, Ali B Syed, Kawin Setsompop, Akshay Chaudhari</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=0vqjc50HfcC>5.25 - [8, 6, 6, 1] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/0vqjc50HfcC.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Magnetic resonance imaging (MRI) is a common and life-saving medical imaging technique. However, acquiring high signal-to-noise ratio MRI scans requires long scan times, resulting in increased costs and patient discomfort, and decreased throughput. Thus, there is great interest in denoising MRI scans, especially for the subtype of diffusion MRI scans that are severely SNR-limited. While most prior MRI denoising methods are supervised in nature, acquiring supervised training datasets for the multitude of anatomies, MRI scanners, and scan parameters proves impractical. Here, we propose Denoising Diffusion Models for Denoising Diffusion MRI (DDM^2), a self-supervised denoising method for MRI denoising using diffusion denoising generative models. Our three-stage framework integrates statistic-based denoising theory into diffusion models and performs denoising through conditional generation. During inference, we represent input noisy measurements as a sample from an intermediate posterior distribution within the diffusion Markov chain. We conduct experiments on 4 real-world in-vivo diffusion MRI datasets and show that our DDM^2 demonstrates superior denoising performances ascertained with clinically-relevant visual qualitative and quantitative metrics.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1542. <h><a href=https://openreview.net/pdf?id=GPJVuyX4p_h>SIMPLE: A Gradient Estimator for k-Subset Sampling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Kareem Ahmed, Zhe Zeng, Mathias Niepert, Guy Van den Broeck</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=GPJVuyX4p_h>5.25 - [6, 6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/GPJVuyX4p_h.jpg loading="lazy">
                </div>
                <div class="abstract">
                    $k$-subset sampling is ubiquitous in machine learning, enabling regularization and interpretability through sparsity. The challenge lies in rendering $k$-subset sampling amenable to end-to-end learning. This has typically involved relaxing the reparameterized samples to allow for backpropagation, but introduces both bias and variance. In this work, we fall back to discrete $k$-subset sampling on the forward pass. This is coupled with using the gradient with respect to the exact marginals, computed efficiently, as a proxy for the true gradient. We show that our gradient estimator exhibits lower bias and variance compared to state-of-the-art estimators. Empirical results show improved performance on learning to explain and sparse models benchmarks. We provide an algorithm for computing the exact ELBO for the $k$-subset distribution, obtaining significantly lower loss compared to state-of-the-art discrete sparse VAEs. All of our algorithms are exact and efficient.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1543. <h><a href=https://openreview.net/pdf?id=HQ67mj5rJdR>Perfectly Secure Steganography Using Minimum Entropy Coupling</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Christian Schroeder de Witt, Samuel Sokota, J Zico Kolter, Jakob Nicolaus Foerster, Martin Strohmeier</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=HQ67mj5rJdR>5.25 - [6, 1, 8, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A scalable, perfect security approach to information-theoretic steganography based on minimum entropy coupling 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/HQ67mj5rJdR.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning. While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques. In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)&#39;s information theoretic-model of steganography if and only if it is induced by a coupling. Furthermore, we show that, among perfectly secure procedures, a procedure is maximally efficient if and only if it is induced by a minimum entropy coupling. These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees with non-trivial efficiency; additionally, these algorithms are highly scalable. To provide empirical validation, we compare a minimum entropy coupling-based approach to three modern baselines---arithmetic coding, Meteor, and adaptive dynamic grouping---using GPT-2, WaveRNN, and Image Transformer as communication channels. We find that the minimum entropy coupling-based approach achieves superior encoding efficiency, despite its stronger security constraints. In aggregate, these results suggest that it may be natural to view information-theoretic steganography through the lens of minimum entropy coupling.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1544. <h><a href=https://openreview.net/pdf?id=rLguqxYvYHB>A Neural Mean Embedding Approach for Back-door and Front-door Adjustment</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Liyuan Xu, Arthur Gretton</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=rLguqxYvYHB>5.25 - [8, 6, 6, 1] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/rLguqxYvYHB.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We consider the estimation of average and counterfactual treatment effects, under two settings:  back-door adjustment and front-door adjustment. The goal in both cases is to recover the treatment effect without having an access to a hidden confounder. This objective is attained by first estimating the conditional mean of the desired outcome variable given relevant covariates (the ``first stage&#34; regression), and then taking the (conditional) expectation of this function as a ``second stage&#34; procedure.  
We propose to compute these conditional expectations directly using a regression function to the learned input features of the first stage, thus avoiding the need for sampling or density estimation. All functions and features (and in particular, the output features in the second stage) are neural networks learned adaptively from data, with the sole requirement that the final layer of the first stage should be linear. The proposed method is shown to converge to the true causal parameter, and outperforms the recent state-of-the-art methods on challenging causal benchmarks, including settings involving high-dimensional image data. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1545. <h><a href=https://openreview.net/pdf?id=AdPJb9cud_Y>VoGE: A Differentiable Volume Renderer using Gaussian Ellipsoids for Analysis-by-Synthesis</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Angtian Wang, Peng Wang, Jian Sun, Adam Kortylewski, Alan Yuille</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=AdPJb9cud_Y>5.25 - [5, 3, 8, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> VoGE is a differentiable renderer based on ray tracing volume densities, which gives better gradients for occlusion reasoning and yields better pose estimation results.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/AdPJb9cud_Y.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Differentiable rendering allows the application of computer graphics on vision tasks, e.g. object pose and shape fitting, via analysis-by-synthesis, where gradients at occluded regions are important when inverting the rendering process.To obtain those gradients, state-of-the-art (SoTA) differentiable renderers use rasterization to collect a set of nearest components for each pixel and aggregate them based on the viewing distance. In this paper, we propose VoGE, which uses ray tracing to capture nearest components with their volume density distributions on the rays and aggregates via integral of the volume densities based on Gaussian ellipsoids, which brings more efficient and stable gradients. To efficiently render via VoGE, we propose an approximate close-form solution for the volume density aggregation and a coarse-to-fine rendering strategy. Finally, we provide a CUDA implementation of VoGE, which gives a competitive rendering speed in comparison to PyTorch3D. Quantitative and qualitative experiment results show VoGE outperforms SoTA counterparts when applied to various vision tasks, e.g., object pose estimation, shape/texture fitting, and occlusion reasoning. The VoGE code is available at: https://github.com/Angtian/VoGE.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1546. <h><a href=https://openreview.net/pdf?id=bPiHuNUNv_R>The Power of Regularization in Solving Extensive-Form Games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Mingyang Liu, Asuman E. Ozdaglar, Tiancheng Yu, Kaiqing Zhang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=bPiHuNUNv_R>5.25 - [8, 3, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/bPiHuNUNv_R.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In this paper, we investigate the power of {\it regularization},  a common technique in reinforcement learning and optimization, in solving extensive-form games (EFGs). We propose a series of new algorithms based on regularizing the payoff functions of the game, and establish a set of convergence results that strictly improve over the existing ones, with either weaker assumptions or stronger convergence guarantees. In particular, we first show that dilated optimistic mirror descent (DOMD),  an efficient variant of OMD for solving EFGs, with adaptive regularization can achieve a fast $\tilde O(1/T)$  last-iterate convergence in terms of duality gap and distance to the set of Nash equilibrium (NE) without uniqueness assumption of the NE. Second, we show that regularized counterfactual regret minimization  (\texttt{Reg-CFR}), with a variant of optimistic mirror descent algorithm as regret-minimizer, can achieve $O(1/T^{1/4})$ best-iterate,  and $O(1/T^{3/4})$ average-iterate convergence rate for finding NE in EFGs. Finally, we show that \texttt{Reg-CFR} can achieve asymptotic last-iterate convergence, and optimal $O(1/T)$ average-iterate convergence rate, for finding the NE of perturbed EFGs, which is useful for finding approximate  extensive-form perfect equilibria (EFPE). To  the best of our knowledge, they constitute the first last-iterate convergence results  for CFR-type  algorithms, while matching the state-of-the-art average-iterate convergence rate in finding NE for non-perturbed EFGs. We also provide numerical results to corroborate the advantages of our algorithms.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1547. <h><a href=https://openreview.net/pdf?id=ElC6LYO4MfD>Faster federated optimization under second-order similarity</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ahmed Khaled, Chi Jin</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ElC6LYO4MfD>5.2 - [5, 5, 6, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ElC6LYO4MfD.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Federated learning (FL) is a subfield of machine learning where multiple clients try to collaboratively learn a model over a network under communication constraints. We consider finite-sum federated optimization under a second-order function similarity condition and strong convexity, and propose two new algorithms: SVRP and Catalyzed SVRP. This second-order similarity condition has grown popular recently, and is satisfied in many applications including distributed statistical learning and differentially private empirical risk minimization. The first algorithm, SVRP, combines approximate stochastic proximal point evaluations, client sampling, and variance reduction. We show that SVRP is communication efficient and achieves superior performance to many existing algorithms when function similarity is high enough. Our second algorithm, Catalyzed SVRP, is a Catalyst-accelerated variant of SVRP that achieves even better performance and uniformly improves upon existing algorithms for federated optimization under second-order similarity and strong convexity. In the course of analyzing these algorithms, we provide a new analysis of the Stochastic Proximal Point Method (SPPM) that might be of independent interest. Our analysis of SPPM is simple, allows for approximate proximal point evaluations, does not require any smoothness assumptions, and shows a clear benefit in communication complexity over ordinary distributed stochastic gradient descent.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1548. <h><a href=https://openreview.net/pdf?id=ylMq8MBnAp>Topology-aware Robust Optimization for Out-of-Distribution Generalization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fengchun Qiao, Xi Peng</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ylMq8MBnAp>5.2 - [6, 3, 6, 5, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a new principled optimization method that seamlessly integrates topological information to develop strong OOD resilience
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ylMq8MBnAp.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. 
Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach, and empirically show that it significantly outperforms the state of the arts in a wide range of tasks including classification, regression, and semantic segmentation. Moreover, we empirically find the data-driven distributional topology is consistent with domain knowledge, enhancing the explainability of our approach. 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1549. <h><a href=https://openreview.net/pdf?id=udNhDCr2KQe>Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zhun Yang, Adam Ishay, Joohyung Lee</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=udNhDCr2KQe>5.0 - [6, 8, 3, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We show how Recurrent Transformer can be trained to perform logical reasoning on constraint satisfaction problems in an end-to-end manner and how to inject discrete constraints into its training
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/udNhDCr2KQe.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Constraint satisfaction problems (CSPs) are about finding values of variables that satisfy the given constraints. We show that Transformer extended with recurrence is a viable approach to learning to solve CSPs in an end-to-end manner, having clear advantages over state-of-the-art methods such as Graph Neural Networks, SATNet, and some neuro-symbolic models. With the ability of Transformer to handle visual input, the proposed Recurrent Transformer can straightforwardly be applied to visual constraint reasoning problems while successfully addressing the symbol grounding problem. We also show how to leverage deductive knowledge of discrete constraints in the Transformer&#39;s inductive learning to achieve sample-efficient learning and semi-supervised learning for CSPs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1550. <h><a href=https://openreview.net/pdf?id=-Yzz6vlX7V->Compositionality with Variation Reliably Emerges in Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Henry Conklin, Kenny Smith</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=-Yzz6vlX7V->5.0 - [5, 5, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Compositional systems reliably emerge between neural networks- just with natural language like variation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/-Yzz6vlX7V-.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Human languages enable robust generalization, letting us leverage our prior experience to communicate about novel meanings. This is partly due to language being compositional, where the meaning of a whole expression is a function of its parts. Natural languages also exhibit extensive variation, encoding meaning predictably enough to enable generalization without limiting speakers to one and only one way of expressing something. Previous work looking at the languages that emerge between neural networks in a communicative task has shown languages that enable robust communication and generalization reliably emerge. Despite this those languages score poorly on existing measures of compositionality leading to claims that a language&#39;s degree of compositionality has little bearing on how well it can generalise. We argue that the languages that emerge between networks are in fact straightforwardly compositional, but with a degree of natural language-like variation that can obscure their compositionality from existing measures. We introduce 4 measures of linguistic variation and show that early in training measures of variation correlate with generalization performance, but that this effect goes away over time as the languages that emerge become regular enough to generalize robustly. Like natural languages, emergent languages appear able to support a high degree of variation while retaining the generalizability we expect from compositionality. In an effort to decrease the variability of emergent languages we show how reducing a model&#39;s capacity results in greater regularity, in line with claims about factors shaping the emergence of regularity in human language.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1551. <h><a href=https://openreview.net/pdf?id=wq0luyH3m4>Hard-Meta-Dataset++: Towards Understanding Few-Shot Performance on Difficult Tasks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Samyadeep Basu, Megan Stanley, John F Bronskill, Soheil Feizi, Daniela Massiceti</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=wq0luyH3m4>5.0 - [5, 6, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose (i) a general and computationally efficient algorithm to extract difficult few-shot classification tasks from large-scale vision datasets, and (ii) a new test benchmark of these difficult tasks to stress test few-shot classifiers.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/wq0luyH3m4.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Few-shot classification is the ability to adapt to any new classification task from only a few training examples. The performance of current top-performing few-shot classifiers varies widely across different tasks where they often fail on a subset of `difficult&#39; tasks.
This phenomenon has real-world consequences for deployed few-shot systems where safety and reliability are paramount, yet little has been done to understand these failure cases. In this paper, we study these difficult tasks to gain a more nuanced understanding of the limitations of current methods. To this end, we develop a general and computationally efficient algorithm called FastDiffSel to extract difficult tasks from any large-scale vision dataset. Notably, our algorithm can extract tasks at least 20x faster than existing methods enabling its use on large-scale datasets. We use FastDiffSel to extract difficult tasks from Meta-Datasset, a widely-used few-shot classification benchmark, and other challenging large-scale vision datasets including ORBIT, CURE-OR and ObjectNet. These tasks are curated into Hard-MD++, a new few-shot testing benchmark to promote the development of methods that are robust to even the most difficult tasks. We use Hard-MD++ to stress-test an extensive suite of few-shot classification methods and show that state-of-the-art approaches fail catastrophically on difficult tasks. We believe that our extraction algorithm FastDiffSel and Hard-MD++ will aid researchers in further understanding failure modes of few-shot classification models.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1552. <h><a href=https://openreview.net/pdf?id=8efJYMBrNb>Multiple sequence alignment as a sequence-to-sequence learning problem</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Edo Dotan, Yonatan Belinkov, Oren Avram, Elya Wygoda, Noa Ecker, Michael Alburquerque, Omri Keren, Gil Loewenthal, Tal Pupko</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=8efJYMBrNb>5.0 - [6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/8efJYMBrNb.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The sequence alignment problem is one of the most fundamental problems in bioinformatics and a plethora of methods were devised to tackle it. Here we introduce BetaAlign, a methodology for aligning sequences using an NLP approach. BetaAlign accounts for the possible variability of the evolutionary process among different datasets by using an ensemble of transformers, each trained on millions of samples generated from a different evolutionary model. Our approach leads to alignment accuracy that is similar and often better than commonly used methods, such as MAFFT, DIALIGN, ClustalW, T-Coffee, PRANK, and MUSCLE.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1553. <h><a href=https://openreview.net/pdf?id=jPVAFXHlbL>Calibrating Transformers via Sparse Gaussian Processes</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Wenlong Chen, Yingzhen Li</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jPVAFXHlbL>5.0 - [6, 3, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> This paper proposes to improve the uncertainty calibration for transformers by performing Bayesian inference for the outputs of multi-head attention blocks using sparse Gaussian processes.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jPVAFXHlbL.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer’s success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1554. <h><a href=https://openreview.net/pdf?id=3KUfbI9_DQE>Distributionally Robust Post-hoc Classifiers under Prior Shifts</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiaheng Wei, Harikrishna Narasimhan, Ehsan Amid, Wen-Sheng Chu, Yang Liu, Abhishek Kumar</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3KUfbI9_DQE>5.0 - [3, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a method for scaling the model predictions at test-time for improved distribution robustness to prior shifts. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3KUfbI9_DQE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization objective is inspired from a natural notion of robustness to controlled distribution shifts. Our method comes with provable guarantees and empirically makes a strong case for distributional robust post-hoc classifiers. An empirical implementation is available at https://github.com/weijiaheng/Drops.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1555. <h><a href=https://openreview.net/pdf?id=3c13LptpIph>Behavior Proximal Policy Optimization </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zifeng Zhuang, Kun LEI, Jinxin Liu, Donglin Wang, Yilang Guo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=3c13LptpIph>5.0 - [3, 5, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b>  We propose Behavior Proximal Policy Optimization (BPPO), which bases on on-policy method (PPO) and effectively solves offline RL without any extra constraint or regularization introduced. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/3c13LptpIph.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Offline reinforcement learning (RL) is a challenging setting where existing off-policy actor-critic methods perform poorly due to overestimating of out-of-distribution state-action pairs. Thus, various additional augmentations are proposed to keep the learned policy close to the offline dataset (or the behavior policy). In this work, starting from the analysis of offline monotonic policy improvement, we reach a surprising conclusion that online on-policy algorithms are naturally able to solve offline RL. Specifically, the inherent conservatism of these on-policy algorithms is exactly what the offline RL method needs to overcome the overestimation. Based on this, we propose Behavior Proximal Policy Optimization (BPPO), which solves offline RL without any extra constraint or regularization introduced compared to PPO. Extensive experiments on the D4RL benchmark empirically show this extremely succinct method outperforms state-of-the-art offline RL algorithms. Our implementation is available at https://github.com/Dragon-Zhuang/BPPO.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1556. <h><a href=https://openreview.net/pdf?id=k5PEHHY4spM>An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuqiao Wen, Yongchang Hao, Yanshuai Cao, Lili Mou</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=k5PEHHY4spM>5.0 - [6, 6, 5, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an efficient, effective, and theoretically understood EqHard-EM algorithm for diverse dialogue generation.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/k5PEHHY4spM.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Open-domain dialogue systems aim to interact with humans through natural language texts in an open-ended fashion. Despite the recent success of super large dialogue systems such as ChatGPT, using medium-to-small-sized dialogue systems remains the common practice as they are more lightweight and accessible; however, generating diverse dialogue responses is challenging, especially with smaller models. In this work, we propose an Equal-size Hard Expectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model for diverse dialogue generation. Our algorithm assigns a sample to a decoder in a hard manner and additionally imposes an equal-assignment constraint to ensure that all decoders are well-trained. We provide detailed theoretical analysis to justify our approach. Further, experiments on two large-scale open-domain dialogue datasets verify that our EqHard-EM algorithm generates high-quality diverse responses.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1557. <h><a href=https://openreview.net/pdf?id=ReDQ1OUQR0X>Human alignment of neural network representations</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Lukas Muttenthaler, Jonas Dippel, Lorenz Linhardt, Robert A. Vandermeulen, Simon Kornblith</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ReDQ1OUQR0X>5.0 - [6, 1, 8] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We evaluate the alignment of neural network representations with human judgments about object similarities in an odd-one-out triplet task, finding that dataset and objective function, but not model size or architecture, have a significant impact.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ReDQ1OUQR0X.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Today’s computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concepts such as food and animals are well-represented by neural networks whereas others such as royal or sports-related objects are not. Overall, although models trained on larger, more diverse datasets achieve better alignment with humans than models trained on ImageNet alone, our results indicate that scaling alone is unlikely to be sufficient to train neural networks with conceptual representations that match those used by humans.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1558. <h><a href=https://openreview.net/pdf?id=mjHlitXvReu>Learning Object-Language Alignments for Open-Vocabulary Object Detection</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chuang Lin, Peize Sun, Yi Jiang, Ping Luo, Lizhen Qu, Gholamreza Haffari, Zehuan Yuan, Jianfei Cai</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=mjHlitXvReu>5.0 - [3, 6, 6, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/mjHlitXvReu.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Existing object detection methods are bounded in a fixed-set vocabulary by costly labeled data. When dealing with novel categories, the model has to be retrained with more bounding box annotations. Natural language supervision is an attractive alternative for its annotation-free attributes and broader object concepts. However, learning open-vocabulary object detection from language is challenging since image-text pairs do not contain fine-grained object-language alignments.  Previous solutions rely on either expensive grounding annotations or distilling classification-oriented vision models. In this paper, we propose a novel open-vocabulary object detection framework directly learning from image-text pair data. We formulate object-language alignment as a set matching problem between a set of image region features and a set of word embeddings. It enables us to train an open-vocabulary object detector on image-text pairs in a much simple and effective way. Extensive experiments on two benchmark datasets, COCO and LVIS, demonstrate our superior performance over the competing approaches on novel categories, e.g. achieving  32.0% mAP on COCO and 21.7% mask mAP on LVIS. Code will be released.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1559. <h><a href=https://openreview.net/pdf?id=4BPFwvKOvo5>Towards convergence to Nash equilibria in two-team zero-sum games</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Fivos Kalogiannis, Ioannis Panageas, Emmanouil-Vasileios Vlatakis-Gkaragkounis</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=4BPFwvKOvo5>5.0 - [6, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Common no-regret algorithms fail to converge to a Nash equilibrium in two-team zero-sum games but a novel approach does converge locally.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/4BPFwvKOvo5.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contemporary applications of machine learning raise important and overlooked theoretical questions regarding optimization in two-team games. Formally, two-team zero-sum games are defined as multi-player games where players are split into two competing sets of agents, each experiencing a utility identical to that of their teammates and opposite to that of the opposing team. We focus on the solution concept of Nash equilibria and prove $\textrm{CLS}$-hardness of computing them in this class of games. To further examine the capabilities of online learning algorithms in games with full-information feedback, we propose a benchmark of a simple ---yet nontrivial--- family of such games. These games do not enjoy the properties used to prove convergence for relevant algorithms. In particular, we use a dynamical systems perspective to demonstrate that gradient descent-ascent, its optimistic variant, optimistic multiplicative weights update, and extra gradient fail to converge (even locally) to a Nash equilibrium. On a brighter note, we propose a first-order method that leverages control theory techniques and under some conditions enjoys last-iterate local convergence to a Nash equilibrium. We also believe our proposed method is of independent interest for general min-max optimization.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1560. <h><a href=https://openreview.net/pdf?id=G_HSyfLk0m>Graph Signal Sampling for Inductive One-Bit Matrix Completion: a Closed-form Solution</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Chao Chen, Haoyu Geng, Gang Zeng, Zhaobing Han, Hua Chai, Xiaokang Yang, Junchi Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=G_HSyfLk0m>5.0 - [6, 3, 3, 8] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/G_HSyfLk0m.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Inductive one-bit matrix completion is motivated by modern applications such as recommender systems, where new users would appear at test stage with the ratings consisting of only ones and no zeros. We propose a unified graph signal sampling framework which enjoys the benefits of graph signal analysis and processing. The key idea is to transform each user&#39;s ratings on the items to a function (signal) on the vertices of an item-item graph, then learn structural graph properties to recover the function from its values on certain vertices --- the problem of graph signal sampling. We propose a class of regularization functionals that takes into account discrete random label noise in the graph vertex domain, then develop the GS-IMC approach which biases the reconstruction towards functions that vary little between adjacent vertices for noise reduction. Theoretical result shows that accurate reconstructions can be achieved under mild conditions. For the online setting, we develop a Bayesian extension, i.e., BGS-IMC which considers continuous random Gaussian noise in the graph Fourier domain and builds upon a prediction-correction update algorithm to obtain the unbiased and minimum-variance reconstruction. Both GS-IMC and BGS-IMC have closed-form solutions and thus are highly scalable in large data. Experiments show that our methods achieve state-of-the-art performance on public benchmarks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1561. <h><a href=https://openreview.net/pdf?id=JVlyfHEEm0k>Understanding Train-Validation Split in Meta-Learning with Neural Networks</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Xinzhe Zuo, Zixiang Chen, Huaxiu Yao, Yuan Cao, Quanquan Gu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=JVlyfHEEm0k>5.0 - [6, 5, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/JVlyfHEEm0k.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The goal of meta-learning is to learn a good prior model from a collection of tasks such that the learned prior is able to adapt quickly to new tasks without accessing many data from the new tasks. A common practice in meta-learning is to perform a train-validation split on each task, where the training set is used for adapting the model parameter to that specific task and the validation set is used for learning a prior model that is shared across all tasks. Despite its success and popularity in multitask learning and few-shot learning, the understanding of the train-validation split is still limited, especially when the neural network models are used. In this paper, we study the benefit of train-validation split for classification problems with neural network models trained by gradient descent. We prove that the train-validation split is necessary to learn a good prior model when the noise in the training sample is large, while the train-train method fails. We validate our theory by conducting experiment on both synthetic and real datasets. To the best of our knowledge, this is the first work towards the theoretical understanding of train-validation split in meta-learning with neural networks.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1562. <h><a href=https://openreview.net/pdf?id=kXwdL1cWOAi>UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Hyung Won Chung, Xavier Garcia, Adam Roberts, Yi Tay, Orhan Firat, Sharan Narang, Noah Constant</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=kXwdL1cWOAi>5.0 - [6, 5, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel language sampling method that  is close to being uniform across languages without introducing harmful repetition and that outperforms the temperature-based sampling.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/kXwdL1cWOAi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language&#39;s corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with UniMax sampling.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1563. <h><a href=https://openreview.net/pdf?id=cCFqcrq0d8>A Unified Framework for Soft Threshold Pruning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yanqi Chen, Zhengyu Ma, Wei Fang, Xiawu Zheng, Zhaofei Yu, Yonghong Tian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=cCFqcrq0d8>5.0 - [3, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/cCFqcrq0d8.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Soft threshold pruning is among the cutting-edge pruning methods with state-of-the-art performance. However, previous methods either perform aimless searching on the threshold scheduler or simply set the threshold trainable, lacking theoretical explanation from a unified perspective. In this work, we reformulate soft threshold pruning as an implicit optimization problem solved using the Iterative Shrinkage-Thresholding Algorithm (ISTA), a classic method from the fields of sparse recovery and compressed sensing. Under this theoretical framework, all threshold tuning strategies proposed in previous studies of soft threshold pruning are concluded as different styles of tuning $L_1$-regularization term. We further derive an optimal threshold scheduler through an in-depth study of threshold scheduling based on our framework. This scheduler keeps $L_1$-regularization coefficient stable, implying a time-invariant objective function from the perspective of optimization. In principle, the derived pruning algorithm could sparsify any mathematical model trained via SGD. We conduct extensive experiments and verify its state-of-the-art performance on both Artificial Neural Networks (ResNet-50 and MobileNet-V1) and Spiking Neural Networks (SEW ResNet-18) on ImageNet datasets. On the basis of this framework, we derive a family of pruning methods, including sparsify-during-training, early pruning, and pruning at initialization. The code is available at https://github.com/Yanqi-Chen/LATS.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1564. <h><a href=https://openreview.net/pdf?id=_i0-12XqVJZ>Asynchronous Distributed Bilevel Optimization</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yang Jiao, Kai Yang, Tiancheng Wu, Dongjin Song, Chengtao Jian</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=_i0-12XqVJZ>5.0 - [5, 5, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/_i0-12XqVJZ.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Bilevel optimization plays an essential role in many machine learning tasks, ranging from hyperparameter optimization to meta-learning. Existing studies on bilevel optimization, however, focus on either centralized or synchronous distributed setting. The centralized bilevel optimization approaches require collecting massive amount of data to a single server, which inevitably incur significant communication expenses and may give rise to data privacy risks. Synchronous distributed bilevel optimization algorithms, on the other hand, often face the straggler problem and will immediately stop working if a few workers fail to respond. As a remedy, we propose Asynchronous Distributed Bilevel Optimization (ADBO) algorithm. The proposed ADBO can tackle bilevel optimization problems with both nonconvex upper-level and lower-level objective functions, and its convergence is theoretically guaranteed. Furthermore, it is revealed through theoretic analysis that the iteration complexity of ADBO to obtain the $\epsilon$-stationary point is upper bounded by $\mathcal{O}(\frac{1}{{{\epsilon ^2}}})$. Thorough empirical studies on public datasets have been conducted to elucidate the effectiveness and efficiency of the proposed ADBO.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1565. <h><a href=https://openreview.net/pdf?id=P44WPn1_aJV>LMSeg: Language-guided Multi-dataset Segmentation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qiang Zhou, Yuang Liu, Chaohui Yu, Jingliang Li, Zhibin Wang, Fan Wang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=P44WPn1_aJV>5.0 - [6, 5, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/P44WPn1_aJV.jpg loading="lazy">
                </div>
                <div class="abstract">
                    It’s a meaningful and attractive topic to build a general and inclusive segmentation model that can recognize more categories in various scenarios. A straightforward way is to combine the existing fragmented segmentation datasets and train a multi-dataset network. However, there are two major issues with multi-dataset segmentation: (i) the inconsistent taxonomy demands manual reconciliation to construct a unified taxonomy; (ii) the inflexible one-hot common taxonomy causes time-consuming model retraining and defective supervision of unlabeled categories. In this paper, we investigate the multi-dataset segmentation and propose a scalable Language-guided Multi-dataset Segmentation framework, dubbed LMSeg, which supports both semantic and panoptic segmentation. Specifically, we introduce a pretrained text encoder to map the category names to a text embedding space as a unified taxonomy, instead of using inflexible one-hot label. The model dynamically aligns the segment queries with the category embeddings. Instead of relabeling each dataset with the unified taxonomy, a category-guided decoding module is designed to dynamically guide predictions to each dataset’s taxonomy. Furthermore, we adopt a dataset-aware augmentation strategy that assigns each dataset a specific image augmentation pipeline, which can suit the proper-
ties of images from different datasets. Extensive experiments demonstrate that our method achieves significant improvements on four segmentation datasets and three panoptic datasets, while the ablation study evaluates the effectiveness of each component.

                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1566. <h><a href=https://openreview.net/pdf?id=pHMpgT5xWaE>A GNN-Guided Predict-and-Search Framework for Mixed-Integer Linear Programming</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Qingyu Han, Linxin Yang, Qian Chen, Xiang Zhou, Dong Zhang, Akang Wang, Ruoyu Sun, Xiaodong Luo</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=pHMpgT5xWaE>4.75 - [5, 5, 6, 3] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/pHMpgT5xWaE.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Mixed-integer linear programming (MILP) is widely employed for modeling combinatorial optimization problems. In practice, similar MILP instances with only coefficient variations are routinely solved, and machine learning (ML) algorithms are capable of capturing common patterns across these MILP instances. In this work, we combine ML with optimization and propose a novel predict-and-search framework for efficiently identifying high-quality feasible solutions. Specifically, we first utilize graph neural networks to predict the marginal probability of each variable, and then search for the best feasible solution within a properly defined ball around the predicted solution. We conduct extensive experiments on public datasets, and computational results demonstrate that our proposed framework achieves 51.1% and 9.9% performance improvements to MILP solvers SCIP and Gurobi on primal gaps, respectively.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1567. <h><a href=https://openreview.net/pdf?id=jpsw-KuOi7r>Improved Sample Complexity for Reward-free Reinforcement Learning under Low-rank MDPs</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Yuan Cheng, Ruiquan Huang, Yingbin Liang, Jing Yang</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=jpsw-KuOi7r>4.75 - [5, 5, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose a novel reward free reinforcement learning algorithm under low-rank MDPs, which improves the sample complexity of previous work. We also provide a lower bound. Finally we study representation learning via reward free reinforement learning.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/jpsw-KuOi7r.jpg loading="lazy">
                </div>
                <div class="abstract">
                    In reward-free reinforcement learning (RL), an agent explores the environment first without any reward information, in order to achieve certain learning goals afterwards for any given reward. In this paper we focus on reward-free RL under low-rank MDP models, in which both the representation and linear weight vectors are unknown. Although various algorithms have been proposed for reward-free low-rank MDPs, the corresponding sample complexity is still far from being satisfactory. In this work, we first provide the first known sample complexity lower bound that holds for any algorithm under low-rank MDPs. This lower bound implies it is strictly harder to find a near-optimal policy under low-rank MDPs than under linear MDPs. We then propose a novel model-based algorithm, coined RAFFLE, and show it can both find an $\epsilon$-optimal policy and achieve an $\epsilon$-accurate system identification via reward-free exploration, with a sample complexity significantly improving the previous results. Such a sample complexity matches our lower bound in the dependence on $\epsilon$, as well as on $K$ {in the large $d$ regime}, where $d$ and $K$ respectively denote the representation dimension and action space cardinality. Finally, we provide a planning algorithm (without further interaction with true environment) for RAFFLE to learn a near-accurate representation, which is the first known representation learning guarantee under the same setting.
 
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1568. <h><a href=https://openreview.net/pdf?id=ObtGcyKmwna>Critic Sequential Monte Carlo</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Vasileios Lioutas, Jonathan Wilder Lavington, Justice Sefas, Matthew Niedoba, Yunpeng Liu, Berend Zwartsenberg, Setareh Dabiri, Frank Wood, Adam Scibior</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=ObtGcyKmwna>4.75 - [6, 1, 6, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We present a novel method called CriticSMC capable of being deployed in model-predictive planning and model-free online control cases within environments with hard constraints taking advantage of informative prior policies.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/ObtGcyKmwna.jpg loading="lazy">
                </div>
                <div class="abstract">
                    We introduce CriticSMC, a new algorithm for planning as inference built from a composition of sequential Monte Carlo with learned Soft-Q function heuristic factors. These heuristic factors, obtained from parametric approximations of the marginal likelihood ahead, more effectively guide SMC towards the desired target distribution, which is particularly helpful for planning in environments with hard constraints placed sparsely in time. Compared with previous work, we modify the placement of such heuristic factors, which allows us to cheaply propose and evaluate large numbers of putative action particles, greatly increasing inference and planning efficiency. CriticSMC is compatible with informative priors, whose density function need not be known, and can be used as a model-free control algorithm. Our experiments on collision avoidance in a high-dimensional simulated driving task show that CriticSMC significantly reduces collision rates at a low computational cost while maintaining realism and diversity of driving behaviors across vehicles and environment scenarios.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1569. <h><a href=https://openreview.net/pdf?id=hZftxQGJ4Re>Deep Ensembles for Graphs with Higher-order Dependencies</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Steven Krieg, William Burgis, Patrick Soga, Nitesh Chawla</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=hZftxQGJ4Re>4.75 - [5, 5, 3, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> We propose an ensemble of GNNs that exploits variance in the neighborhood subspaces of nodes in graphs with higher-order dependencies and consistently outperforms baselines on semisupervised and supervised learning tasks.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/hZftxQGJ4Re.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Graph neural networks (GNNs) continue to achieve state-of-the-art performance on many graph learning tasks, but rely on the assumption that a given graph is a sufficient approximation of the true neighborhood structure. In the presence of higher-order sequential dependencies, we show that the tendency of traditional graph representations to underfit each node&#39;s neighborhood causes existing GNNs to generalize poorly. To address this, we propose a novel Deep Graph Ensemble (DGE), which captures neighborhood variance by training an ensemble of GNNs on different neighborhood subspaces of the same node within a higher-order network structure. We show that DGE consistently outperforms existing GNNs on semisupervised and supervised tasks on six real-world data sets with known higher-order dependencies, even under a similar parameter budget. We demonstrate that learning diverse and accurate base classifiers is central to DGE&#39;s success, and discuss the implications of these findings for future work on GNNs.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1570. <h><a href=https://openreview.net/pdf?id=x0BPR9iXc1>Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning </a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Zaid Khan, Yun Fu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=x0BPR9iXc1>4.67 - [6, 5, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Changing a small number (&lt;7%) of parameters in already trained language and image models can match the performance of full model training for creating CLIP-like models. 
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/x0BPR9iXc1.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Contrastive vision-language models (e.g. CLIP) are typically created by updating all the parameters of a vision model and language model through contrastive training. Can such models be created by a small number of parameter updates to an already-trained language model and vision model? The literature describes techniques that can create vision-language models by updating a small number of parameters in a language model, but these require already aligned visual representations and are non-contrastive, hence unusable for latency-sensitive applications such as neural search. We explore the feasibility and benefits of parameter-efficient contrastive vision-language alignment through transfer learning: creating a model such as CLIP by minimally updating an already-trained vision and language model. We find that a minimal set of parameter updates ($&lt;$7\%) can achieve the same performance as full-model training, and updating specific components ($&lt;$1\% of parameters) can match 75\% of full-model training. We describe a series of experiments: we show that existing knowledge is conserved more strongly in parameter-efficient training and that parameter-efficient scaling scales with model and dataset size. Where paired-image text data is scarce but strong multilingual language models exist (e.g. low resource languages), parameter-efficient training is even preferable to full-model training. Given a fixed compute budget, parameter-efficient training allows training larger models on the same hardware, achieving equivalent performance in less time. Parameter-efficient training hence constitutes an energy-efficient and effective training strategy for contrastive vision-language models that may be preferable to the full-model training paradigm for common use cases.
Code and weights at https://github.com/codezakh/LilT.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1571. <h><a href=https://openreview.net/pdf?id=NnOZT_CR26Z>GoBigger: A Scalable Platform for Cooperative-Competitive Multi-Agent Interactive Simulation</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Ming Zhang, Shenghan Zhang, Zhenjie Yang, Lekai Chen, Jinliang Zheng, Chao Yang, Chuming Li, Hang Zhou, Yazhe Niu, Yu Liu</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=NnOZT_CR26Z>4.67 - [6, 3, 5] - Accept: poster</a>
                    </div>
                
                
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/NnOZT_CR26Z.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The emergence of various multi-agent environments has motivated powerful algorithms to explore agents&#39; cooperation or competition. Even though this has greatly promoted the development of multi-agent reinforcement learning  (MARL), it is still not enough to support further exploration on the behavior of swarm intelligence between multiple teams, and cooperation between multiple agents due to their limited scalability. To alleviate this, we introduce GoBigger, a scalable platform for cooperative-competition multi-agent interactive simulation. GoBigger is an enhanced environment for the Agar-like game, enabling the simulation of multiple scales of agent intra-team cooperation and inter-team competition. Compared with existing multi-agent simulation environments, our platform supports multi-team games with more than two teams simultaneously, which dramatically expands the diversity of agent cooperation and competition, and can more effectively simulate the swarm intelligent agent behavior. Besides, in GoBigger, the cooperation between the agents in a team can lead to much higher performance. We offer a diverse set of challenging scenarios, built-in bots, and visualization tools for best practices in benchmarking. We evaluate several state-of-the-art algorithms on GoBigger and demonstrate the potential of the environment. We believe this platform can inspire various emerging research directions in MARL, swarm intelligence, and large-scale agent interactive learning. Both GoBigger and its related benchmark are open-sourced. More information could be found at https://github.com/opendilab/GoBigger.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1572. <h><a href=https://openreview.net/pdf?id=7oPAgqxNb20>DynaMS: Dyanmic Margin Selection for Efficient Deep Learning</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Jiaxing Wang, Yong Li, Jingwei Zhuo, Xupeng Shi, WEIZHONG ZHANG, Lixing Gong, Tong Tao, Pengzhang Liu, Yongjun Bao, Weipeng Yan</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=7oPAgqxNb20>4.25 - [5, 6, 3, 3] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> A general dynamic data selection framework for efficient deep neural network training that enjoy both theoretical and practical advantages.
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/7oPAgqxNb20.jpg loading="lazy">
                </div>
                <div class="abstract">
                    The great success of deep learning is largely driven by training over-parameterized models on massive datasets. To avoid excessive computation, extracting and training only on the most informative subset is drawing increasing attention. Nevertheless, it is still an open question how to select such a subset on which the model trained generalizes on par with the full data. In this paper, we propose dynamic margin selection (DynaMS). DynaMS leverages the distance from candidate samples to the classification boundary to construct the subset, and the subset is dynamically updated during model training. We show that DynaMS converges with large probability, and for the first time show both in theory and practice that dynamically updating the subset can result in better generalization over previous works. To reduce the additional computation incurred by the selection, a light parameter sharing proxy (PSP) is designed. PSP is able to faithfully evaluate instances with respect to the current model, which is necessary for dynamic selection. Extensive analysis and experiments demonstrate the superiority of the proposed approach in data selection against many state-of-the-art counterparts on benchmark datasets.
                </div>
            </div>
        </div>
    
        <div class="paperrow paper">
            <div class="title">
                1573. <h><a href=https://openreview.net/pdf?id=z37tDDHHgi>Red PANDA: Disambiguating Image Anomaly Detection by Removing Nuisance Factors</a></h>
            </div>
            <div class="papermeta">
                <div class="authors">
                    <i>Niv Cohen, Jonathan Kahana, Yedid Hoshen</i>
                </div>
                
                    <div class="rating">
                        <b>rating :</b> <a href=https://openreview.net/forum?id=z37tDDHHgi>4.0 - [3, 6, 1, 6] - Accept: poster</a>
                    </div>
                
                
                
                    <div class="tldr">
                        <b>tl;dr:</b> Proposing a new anomaly detection setting when the operator specifies a nuisance attribute to be ignored
                    </div>
                
                <div class="thumbimg">
                    <img src=/static/iclr2023/thumbs/z37tDDHHgi.jpg loading="lazy">
                </div>
                <div class="abstract">
                    Anomaly detection methods strive to discover patterns that differ from the norm in a meaningful way. This goal is ambiguous as different human operators may find different attributes meaningful. An image differing from the norm by an attribute such as pose may be considered anomalous by some operators while others may consider the attribute irrelevant. Breaking from previous research, we present a new anomaly detection method that allows operators to exclude an attribute when detecting anomalies. Our approach aims to learn representations which do not contain information regarding such nuisance attributes. Anomaly scoring is performed using a density-based approach. Importantly, our approach does not require specifying the attributes where anomalies could appear, which is typically impossible in anomaly detection, but only attributes to ignore. An empirical investigation is presented verifying the effectiveness of our approach.
                </div>
            </div>
        </div>
    
</div>


    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-4V1KQNF1VW"></script>
    <script>
    var doNotTrack = false;
    if (!doNotTrack) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-D1XT2DX08Y', { 'anonymize_ip': false });
    }
    </script>

</body>
</html>